{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vnWhNJC8M5d5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "from skimage import io, transform\n",
    "\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kC9JUlQMM5d9"
   },
   "source": [
    "### Image Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g6ClqmTUM5d9"
   },
   "outputs": [],
   "source": [
    "class Rescale(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, image):\n",
    "        h, w = image.shape[:2]\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "        img = transform.resize(image, (new_h, new_w))\n",
    "        return img\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, image):\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        return image\n",
    "\n",
    "\n",
    "IMAGE_RESIZE = (256, 256)\n",
    "# Sequentially compose the transforms\n",
    "img_transform = transforms.Compose([Rescale(IMAGE_RESIZE), ToTensor()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K5CM4cBhM5eA"
   },
   "source": [
    "### Captions Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rbEQS053M5eA"
   },
   "outputs": [],
   "source": [
    "class CaptionsPreprocessing:\n",
    "    \"\"\"Preprocess the captions, generate vocabulary and convert words to tensor tokens\n",
    "\n",
    "    Args:\n",
    "        captions_file_path (string): captions tsv file path\n",
    "    \"\"\"\n",
    "    def __init__(self, captions_file_path):\n",
    "        self.captions_file_path = captions_file_path\n",
    "\n",
    "        # Read raw captions\n",
    "        self.raw_captions_dict = self.read_raw_captions()\n",
    "\n",
    "        # Preprocess captions\n",
    "        self.captions_dict = self.process_captions()\n",
    "\n",
    "        # Create vocabulary\n",
    "        self.vocab = self.generate_vocabulary()\n",
    "\n",
    "    def read_raw_captions(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            Dictionary with raw captions list keyed by image ids (integers)\n",
    "        \"\"\"\n",
    "\n",
    "        captions_dict = {}\n",
    "        with open(self.captions_file_path, 'r', encoding='utf-8') as f:\n",
    "            for img_caption_line in f.readlines():\n",
    "                img_captions = img_caption_line.strip().split('\\t')\n",
    "                captions_dict[int(img_captions[0])] = img_captions[1:]\n",
    "\n",
    "        return captions_dict\n",
    "\n",
    "    def process_captions(self):\n",
    "        \"\"\"\n",
    "        Use this function to generate dictionary and other preprocessing on captions\n",
    "        \"\"\"\n",
    "\n",
    "        raw_captions_dict = self.raw_captions_dict\n",
    "\n",
    "        # Do the preprocessing here\n",
    "        captions_dict = raw_captions_dict\n",
    "\n",
    "        return captions_dict\n",
    "\n",
    "    def generate_vocabulary(self):\n",
    "        \"\"\"\n",
    "        Use this function to generate dictionary and other preprocessing on captions\n",
    "        \"\"\"\n",
    "\n",
    "        captions_dict = self.captions_dict\n",
    "\n",
    "        # Generate the vocabulary\n",
    "\n",
    "        return None\n",
    "\n",
    "    def captions_transform(self, img_caption_list):\n",
    "        \"\"\"\n",
    "        Use this function to generate tensor tokens for the text captions\n",
    "        Args:\n",
    "            img_caption_list: List of captions for a particular image\n",
    "        \"\"\"\n",
    "        vocab = self.vocab\n",
    "\n",
    "        # Generate tensors\n",
    "\n",
    "        return torch.zeros(len(img_caption_list), 10)\n",
    "\n",
    "# Set the captions tsv file path\n",
    "CAPTIONS_FILE_PATH = ''\n",
    "captions_preprocessing_obj = CaptionsPreprocessing(CAPTIONS_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lzmf-nlhM5eC"
   },
   "source": [
    "### Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bUuWkFPpM5eD"
   },
   "outputs": [],
   "source": [
    "class ImageCaptionsDataset(Dataset):\n",
    "\n",
    "    def __init__(self, img_dir, captions_dict, img_transform=None, captions_transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_dir (string): Directory with all the images.\n",
    "            captions_dict: Dictionary with captions list keyed by image ids (integers)\n",
    "            img_transform (callable, optional): Optional transform to be applied\n",
    "                on the image sample.\n",
    "\n",
    "            captions_transform: (callable, optional): Optional transform to be applied\n",
    "                on the caption sample (list).\n",
    "        \"\"\"\n",
    "        self.img_dir = img_dir\n",
    "        self.captions_dict = captions_dict\n",
    "        self.img_transform = img_transform\n",
    "        self.captions_transform = captions_transform\n",
    "\n",
    "        self.image_ids = list(captions_dict.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.img_dir, 'image_{}.jpg'.format(self.image_ids[idx]))\n",
    "        image = io.imread(img_name)\n",
    "        captions = self.captions_dict[self.image_ids[idx]]\n",
    "\n",
    "        if self.img_transform:\n",
    "            image = self.img_transform(image)\n",
    "\n",
    "        if self.captions_transform:\n",
    "            captions = self.captions_transform(captions)\n",
    "\n",
    "        sample = {'image': image, 'captions': captions}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W0pi-EQYM5eF"
   },
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6eaz6MgvM5eG"
   },
   "outputs": [],
   "source": [
    "class ImageCaptionsNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageCaptionsNet, self).__init__()\n",
    "\n",
    "        # Define your architecture here\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = image_batch, captions_batch\n",
    "\n",
    "        # Forward Propogation\n",
    "\n",
    "        return captions_batch\n",
    "\n",
    "net = ImageCaptionsNet()\n",
    "\n",
    "# If GPU training is required\n",
    "# net = net.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rUQtOQ5JM5eI"
   },
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZmqfKZIPM5eI"
   },
   "outputs": [],
   "source": [
    "IMAGE_DIR = ''\n",
    "\n",
    "# Creating the Dataset\n",
    "train_dataset = ImageCaptionsDataset(\n",
    "    IMAGE_DIR, captions_preprocessing_obj.captions_dict, img_transform=img_transform,\n",
    "    captions_transform=captions_preprocessing_obj.captions_transform\n",
    ")\n",
    "\n",
    "# Define your hyperparameters\n",
    "NUMBER_OF_EPOCHS = 3\n",
    "LEARNING_RATE = 1e-1\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 0 # Parallel threads for dataloading\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Creating the DataLoader for batching purposes\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "import os\n",
    "for epoch in range(NUMBER_OF_EPOCHS):\n",
    "    for batch_idx, sample in enumerate(train_loader):\n",
    "        net.zero_grad()\n",
    "\n",
    "        image_batch, captions_batch = sample['image'], sample['captions']\n",
    "\n",
    "        # If GPU training required\n",
    "        # image_batch, captions_batch = image_batch.cuda(), captions_batch.cuda()\n",
    "\n",
    "        output_captions = net((image_batch, captions_batch))\n",
    "        loss = loss_function(output_captions, captions_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Iteration: \" + str(epoch + 1))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "pytorchtut.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
