{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Data from the CSV file\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.chdir(\"/home/ayushi/coursework/col774/Machine-Learning-Assignments/Assignment1/data/q2\")\n",
    "print(\"Path has been changed to:\\n\", os.getcwd())\n",
    "\n",
    "#Importing Data X from the CSV file\n",
    "temp = np.genfromtxt('q2test.csv', delimiter=',')\n",
    "#print(temp[1:])\n",
    "\n",
    "X1_test = temp[1:, 0].reshape(-1,1)\n",
    "X2_test = temp[1:, 1].reshape(-1,1)\n",
    "Y_test = temp[1:, 2].reshape(-1,1)\n",
    "\n",
    "\n",
    "os.chdir(\"/home/ayushi/coursework/col774/Machine-Learning-Assignments/Assignment1/ques2\")\n",
    "print(\"Path changed back to python file directory:\\n\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 1000000\n",
    "x0 = np.ones((m,1))\n",
    "x1 = np.random.normal(3, 2, m).reshape(-1,1)\n",
    "x2 = np.random.normal(-1, 2, m).reshape(-1,1)\n",
    "#x1 = np.linspace(-1, 1, m).reshape(-1,1)\n",
    "#x2 = np.linspace(-2, 2, m).reshape(-1,1)\n",
    "X = np.append(x0,x1,axis=1)\n",
    "X = np.append(X, x2, axis=1)\n",
    "print(m)\n",
    "# print(x1.shape)\n",
    "# print(x2.shape)\n",
    "# print(x0.shape)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = np.random.normal(0, np.sqrt(2), m).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.array([[3], [1], [2]])\n",
    "print(theta[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_x = np.dot(X,theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = h_x + epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calc_y(x1, x2, theta, epsilon, points)\n",
    "#     h_x = theta[0] +  points.x1*theta[1] + points.x2*theta[2]   \n",
    "#     points.y = h_x + epsilon\n",
    "#     return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X1, X2 = np.meshgrid(x1, x2)\n",
    "# Point = collections.namedtuple('Point', ['x1', 'x2', 'y'])\n",
    "# points = [Point(x1, x2, y)]\n",
    "\n",
    "# fig = plt.figure(figsize=(15,10))\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "# ax.set_title(\"Y function\", fontsize=15)\n",
    "\n",
    "# cost_error = np.array([calc_y(x1, x2, theta, points) \n",
    "#                for x0, x1 in zip(np.ravel(X1), np.ravel(X2))])\n",
    "# cost_plot = cost_error.reshape(theta0.shape)\n",
    "\n",
    "\n",
    "# Y_data = calc_y(X1, X2, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data is (X, Y)\n",
    "#Randomly shuffle the data\n",
    "temp = np.append(X, Y, axis=1)\n",
    "np.random.shuffle(temp)\n",
    "X = temp[:,0:3]\n",
    "Y = temp[:,-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying SGD on X, Y to learn hypothesis with parameters theta\n",
    "\n",
    "#Model Initial Parameters\n",
    "theta = np.zeros((3,1))\n",
    "#Learning Rate\n",
    "alpha = 0.001 \n",
    "\n",
    "#Batch Size r\n",
    "batch_size = np.array([1, 100, 10000, 1000000])\n",
    "\n",
    "print(\"The model initial theta is = {} and the learning rate is = {}\".format(theta.T, alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the hypothesis function\n",
    "def hypothesis(X, theta):\n",
    "    return np.dot(X, theta)\n",
    "\n",
    "#Define the cost function over the entire sample\n",
    "def cost_total(X, theta, Y, m):\n",
    "    h_x= hypothesis(X, theta)\n",
    "    cost = (1/(2*m)) * np.sum((Y-h_x)**2) # Average Cost Function\n",
    "    return cost\n",
    "\n",
    "#function for calculating the cost function gradient\n",
    "def cost_grad(X, theta, Y, m):\n",
    "    temp = np.dot(X, theta)-Y\n",
    "    theta_grad = np.zeros((3,1))\n",
    "    theta_grad += np.dot(X.transpose(), temp)\n",
    "    return theta_grad/m #average Gradient \n",
    "    \n",
    "print(m, theta)\n",
    "cost_start = cost_total(X, theta, Y, m)\n",
    "print(\"Initial Cost value for the hypothesis with zero parameters={}\".format(cost_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will hold the training sample on which we will keep checking the cost as training progresses\n",
    "index=m//2\n",
    "\n",
    "#r is the current batch size (choose between 1, 100, 10000, 1000000 - defined in batch_size array)\n",
    "r = batch_size[0]\n",
    "\n",
    "#number of batches\n",
    "num_b = m//r\n",
    "print(index,r, num_b)\n",
    "mini_batch = [(X[i:i+r,:], Y[i:i+r]) for i in range(0, m, r)]\n",
    "len(mini_batch)\n",
    "#for b in mini_batch: print(b[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Method 3 - With Normalization, and termination based on change in the cost function\n",
    "i=0\n",
    "J_curr=0.0;\n",
    "theta_all = theta\n",
    "print(theta_all)\n",
    "\n",
    "J_avg = np.array([cost_total(X[0], theta, Y[0], 1)])\n",
    "print(J_avg)\n",
    "\n",
    "#while(np.amax(abs(cost_grad(X, theta, Y, m))) >= 0.0000001):\n",
    "while(i != 1):\n",
    "    #theta_all = np.append(theta_all,theta,axis=1)       \n",
    "    #print(\"{} iterations done:\".format(i))\n",
    "    count = 0\n",
    "    for b in mini_batch:\n",
    "        X_b = b[0]\n",
    "        Y_b = b[1]\n",
    "        #print(Y_b.shape)\n",
    "        J_curr += cost_total(X_b, theta, Y_b, 1)\n",
    "        if(count%50 == 0):\n",
    "            J_curr /= 50; #gives the average cost over 1000 iterations. \n",
    "            J_avg = np.append(J_avg, J_curr)\n",
    "            print(\"Current average cost = {}\".format(J_curr))\n",
    "            J_curr=0.0;            \n",
    "        theta -= alpha * cost_grad(X_b, theta, Y_b, r)\n",
    "        theta_all = np.append(theta_all,theta,axis=1)\n",
    "\n",
    "        #if (count%100000==0): print(theta)\n",
    "        count +=1\n",
    "        #print(\"Theta new= {}\".format(theta))\n",
    "        \n",
    "#     for b in range(1,num_b+1):\n",
    "#         J_curr += cost_total(X[b-1], theta, Y[b-1], 1)\n",
    "#         if(b%5000 == 0):\n",
    "#             J_curr /= 5000; #gives the average cost over 1000 iterations. \n",
    "#             J_avg = np.append(J_avg, J_curr)\n",
    "#             print(\"Current average cost = {}\".format(J_curr))\n",
    "#             J_curr=0.0;            \n",
    "#         theta -= alpha * cost_grad(X[(b-1)*r:(b-1)*r+(r),:], theta, Y[(b-1)*r:(b-1)*r+(r),:], r)\n",
    "#         #print(\"Theta new= {}\".format(theta))\n",
    "    cost_final = cost_total(X, theta, Y, m)\n",
    "    print(\"Cost is = {} in iterations = {}\".format(cost_final, i+1))    \n",
    "    #cost_list = np.append(cost_list, cost_final)\n",
    "    i+=1\n",
    "\n",
    "print(\"Cost is = {} in iterations = {}\".format(cost_final, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_final = cost_total(X, theta, Y, m)\n",
    "print(cost_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.amin(J_avg)\n",
    "# np.amax(abs(cost_grad(X, theta, Y, m)))\n",
    "# cost_grad(X, theta, Y, m)\n",
    "# theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhyp = np.dot(X, theta)\n",
    "plt.scatter(x1, Y, s = 2, label='x1-y')\n",
    "plt.scatter(x2, Y, s = 2, label='x2-y')\n",
    "#plt.scatter(x1, yhyp, s = 2, label='x2-y')\n",
    "plt.legend()\n",
    "#plt.title('Hypothesis plot vs. Function (LR =0.01)')\n",
    "#plt.xlabel('Wine Acidity (x)')\n",
    "#plt.ylabel('Wine Density (y)')\n",
    "plt.show()\n",
    "#plt.savefig('plot_hyp_lr_0.01.png', dpi=1000, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x1, Y, s = 2, label='x1-y')\n",
    "plt.scatter(x1, Y, s = 2, label='x1-y')\n",
    "print(X[0:1,:].shape)\n",
    "print(theta.shape)\n",
    "print(Y[0:1,:].shape)\n",
    "print(int(m/2))\n",
    "ind=int(m/2)\n",
    "np.dot(X[ind,:], theta) - Y[ind,:]\n",
    "\n",
    "print(abs(np.array([-1,-2,-3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
