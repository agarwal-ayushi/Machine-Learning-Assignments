{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------Reading the Data-------------------------\n",
      "----------------Data Reading completed-------------------\n",
      "The total number of training samples = 11050\n",
      "The total number of validation samples = 1950\n",
      "The number of features = 784\n"
     ]
    }
   ],
   "source": [
    "print(\"----------------Reading the Data-------------------------\")\n",
    "PATH = os.getcwd()\n",
    "os.chdir('Alphabets/')\n",
    "\n",
    "X_train = pd.read_csv('train.csv', sep=',', header=None, index_col=False)\n",
    "X_test = pd.read_csv('test.csv', sep=',', header=None, index_col=False)\n",
    "np.random.shuffle(X_train.to_numpy())\n",
    "train_class = X_train[X_train.columns[-1]]\n",
    "test_actual_class = X_test[X_test.columns[-1]]\n",
    "\n",
    "X_train = X_train.drop(X_train.columns[-1], axis=1)\n",
    "X_test = X_test.drop(X_test.columns[-1], axis=1)\n",
    "\n",
    "print(\"----------------Data Reading completed-------------------\")\n",
    "\n",
    "os.chdir('../')\n",
    "\n",
    "X_train = X_train/255\n",
    "X_test = X_test/255\n",
    "\n",
    "m = X_train.shape[0] # Number of Training Samples\n",
    "\n",
    "X_valid = X_train.iloc[(int(0.85*m)):]\n",
    "valid_class = train_class[(int(0.85*m)):]\n",
    "X_train = X_train.iloc[0:int(0.85*m)]\n",
    "train_class = train_class[0:int(0.85*m)]\n",
    "\n",
    "\n",
    "m = X_train.shape[0] # Number of Training Samples\n",
    "n = X_train.shape[1] # Number of input features\n",
    "\n",
    "print(\"The total number of training samples = {}\".format(m))\n",
    "print(\"The total number of validation samples = {}\".format(X_valid.shape[0]))\n",
    "\n",
    "\n",
    "print(\"The number of features = {}\".format(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Perform 1-hot encoding of class labels------------\n"
     ]
    }
   ],
   "source": [
    "#To get the one hot encoding of each label\n",
    "print(\"--------Perform 1-hot encoding of class labels------------\")\n",
    "\n",
    "train_class_enc = pd.get_dummies(train_class).to_numpy()\n",
    "valid_class_enc = pd.get_dummies(valid_class).to_numpy()\n",
    "test_actual_class_enc = pd.get_dummies(test_actual_class).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add the intercept term to the data samples both in training and test dataset\n",
    "X_train = np.hstack((np.ones((m,1)),X_train.to_numpy()))\n",
    "X_valid = np.hstack((np.ones((X_valid.shape[0],1)), X_valid.to_numpy()))\n",
    "X_test = np.hstack((np.ones((X_test.shape[0],1)),X_test.to_numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "arch_test = [1,5,10,50,100]\n",
    "arch = [arch_test[3]] #means one hidden layer with 2 perceptrons \n",
    "batch_size = 100 # Mini-Batch Size\n",
    "r = np.max(train_class) + 1 # Default value of the number of classes = 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of mini-batches formed is = 111\n"
     ]
    }
   ],
   "source": [
    "#Mini-Batch formation\n",
    "mini_batch = [(X_train[i:i+batch_size,:], train_class_enc[i:i+batch_size]) for i in range(0, m, batch_size)]\n",
    "print(\"The number of mini-batches formed is = {}\".format(len(mini_batch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Theta Initialization \n",
    "#np.random.seed(1)\n",
    "def theta_init(arch=[50]):\n",
    "    theta = []\n",
    "    for i in range(len(arch)+1):\n",
    "        if i == 0:\n",
    "            dim0=n+1\n",
    "            dim1=arch[i]\n",
    "        elif (i == len(arch)):\n",
    "            dim0=arch[i-1]\n",
    "            dim1 = r\n",
    "        else:\n",
    "            dim0=arch[i-1]\n",
    "            dim1= arch[i]\n",
    "        theta.append(np.random.normal(0,0.01, (dim0,dim1)))\n",
    "        #theta.append(0.01*(2*np.random.random((dim0, dim1))-1))\n",
    "        #theta.append(np.zeros((dim0, dim1)))\n",
    "        #theta.append(0.01*np.random.standard_normal((dim0, dim1)))\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_act(x):\n",
    "    return np.maximum(0.0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deriv_relu(x):\n",
    "    #x[x<=0] = -0.01\n",
    "    x[x<=0] = 0\n",
    "    x[x>0] = 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu_act(x):\n",
    "    np.where(x > 0, x, x * 0.1) \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deriv_leakyRelu(x):\n",
    "    x[x<=0] = 0.1\n",
    "    x[x>0] = 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softplus(x):\n",
    "    return np.log(1+np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deriv_softplus(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(data, theta):\n",
    "    fm = []\n",
    "    fm.append(data)\n",
    "    for l in range(len(theta)):\n",
    "        if (l != len(theta)-1):\n",
    "            #print(\"relu\")\n",
    "            fm.append(relu_act(np.dot(fm[l], theta[l])))\n",
    "        else:\n",
    "            fm.append(activation(np.dot(fm[l], theta[l])))\n",
    "            #print(\"sigmoid output\")\n",
    "    return fm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 26)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cost_total' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-71bab9fa352d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mcost_total\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_class_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_class_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#fm = forward_prop(X_train, theta)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cost_total' is not defined"
     ]
    }
   ],
   "source": [
    "theta = theta_init([100, 100, 100])\n",
    "print(theta[3].shape)\n",
    "fm = forward_prop(X_train, theta)\n",
    "cost_total(X_train, theta, train_class_enc, m)\n",
    "cross_entropy_loss(X_train, theta, train_class_enc, m)\n",
    "#fm = forward_prop(X_train, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_total(X, theta, Y, m):\n",
    "    fm = forward_prop(X, theta)\n",
    "    cost = (1/(2*m))*np.sum((Y-fm[-1])**2)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(X, theta, Y, m):\n",
    "    fm = forward_prop(X, theta)\n",
    "    cost = -(1/m)*(np.sum(((Y*np.log(fm[-1]))+((1-Y)*(np.log(1-fm[-1]))))))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(data, theta, actual_class):\n",
    "    pred_class = forward_prop(data, theta)\n",
    "    test_pred_class = pred_class[-1]\n",
    "    for i in range(len(test_pred_class)):\n",
    "        test_pred_class[i][test_pred_class[i] == np.max(test_pred_class[i])] = 1\n",
    "        test_pred_class[i][test_pred_class[i] != np.max(test_pred_class[i])] = 0\n",
    "\n",
    "\n",
    "    test_acc = 0\n",
    "    for i in range(len(actual_class)):\n",
    "        if (np.array_equal(test_pred_class[i], actual_class[i])):\n",
    "            test_acc+=1\n",
    "    test_acc /= data.shape[0]\n",
    "\n",
    "    #print(\"The Test Accuracy of the model = {}%\".format(test_acc*100))\n",
    "    return (test_acc*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = []\n",
    "train_accuracy = []\n",
    "valid_accuracy =[]\n",
    "test_accuracy = []\n",
    "train_time = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(785, 100) (100, 100) (100, 26)\n"
     ]
    }
   ],
   "source": [
    "arch=[100, 100]\n",
    "lr=0.1\n",
    "lr0=0.5\n",
    "theta = theta_init(arch)\n",
    "print(theta[0].shape, theta[1].shape, theta[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate =  0.5\n",
      "Initial Cost on dataset for this epoch 1 = 3.250300580987951\n",
      "Error on this batch = 3.250320585983516\n",
      "Error on this batch = 0.49998318754910404\n",
      "Cost on val dataset after 2 epochs is = 0.4999722463171738\n",
      "learning rate =  0.35355339059327373\n",
      "Initial Cost on dataset for this epoch 2 = 0.4999722463171738\n",
      "Error on this batch = 0.49999298528631364\n",
      "Error on this batch = 0.49998230577989483\n",
      "Cost on val dataset after 3 epochs is = 0.49997109974837095\n",
      "learning rate =  0.2886751345948129\n",
      "Initial Cost on dataset for this epoch 3 = 0.49997109974837095\n",
      "Error on this batch = 0.49999256443452733\n",
      "Error on this batch = 0.4999815751536351\n",
      "Cost on val dataset after 4 epochs is = 0.4999700871984247\n",
      "learning rate =  0.25\n",
      "Initial Cost on dataset for this epoch 4 = 0.4999700871984247\n",
      "Error on this batch = 0.49999218662780903\n",
      "Error on this batch = 0.49998091335397987\n",
      "Cost on val dataset after 5 epochs is = 0.4999691482355044\n",
      "learning rate =  0.22360679774997896\n",
      "Initial Cost on dataset for this epoch 5 = 0.4999691482355044\n",
      "Error on this batch = 0.4999918312275113\n",
      "Error on this batch = 0.4999802908362981\n",
      "Cost on val dataset after 6 epochs is = 0.49996825454911437\n",
      "learning rate =  0.20412414523193154\n",
      "Initial Cost on dataset for this epoch 6 = 0.49996825454911437\n",
      "Error on this batch = 0.49999148850267866\n",
      "Error on this batch = 0.49997969285394817\n",
      "Cost on val dataset after 7 epochs is = 0.49996739007473645\n",
      "learning rate =  0.1889822365046136\n",
      "Initial Cost on dataset for this epoch 7 = 0.49996739007473645\n",
      "Error on this batch = 0.49999115293376845\n",
      "Error on this batch = 0.49997911064649553\n",
      "Cost on val dataset after 8 epochs is = 0.4999665445861134\n",
      "learning rate =  0.17677669529663687\n",
      "Initial Cost on dataset for this epoch 8 = 0.4999665445861134\n",
      "Error on this batch = 0.49999082097490166\n",
      "Error on this batch = 0.4999785384924559\n",
      "Cost on val dataset after 9 epochs is = 0.49996571107848287\n",
      "learning rate =  0.16666666666666666\n",
      "Initial Cost on dataset for this epoch 9 = 0.49996571107848287\n",
      "Error on this batch = 0.49999049011972274\n",
      "Error on this batch = 0.499977972350937\n",
      "Cost on val dataset after 10 epochs is = 0.49996488446806264\n",
      "learning rate =  0.15811388300841897\n",
      "Initial Cost on dataset for this epoch 10 = 0.49996488446806264\n",
      "Error on this batch = 0.4999901585083105\n",
      "Error on this batch = 0.49997740918810735\n",
      "Cost on val dataset after 11 epochs is = 0.4999640609216238\n",
      "learning rate =  0.15075567228888181\n",
      "Initial Cost on dataset for this epoch 11 = 0.4999640609216238\n",
      "Error on this batch = 0.49998982470990705\n",
      "Error on this batch = 0.49997684664475317\n",
      "Cost on val dataset after 12 epochs is = 0.49996323722600156\n",
      "learning rate =  0.14433756729740646\n",
      "Initial Cost on dataset for this epoch 12 = 0.49996323722600156\n",
      "Error on this batch = 0.49998948748072225\n",
      "Error on this batch = 0.4999762827879949\n",
      "Cost on val dataset after 13 epochs is = 0.4999624108669432\n",
      "learning rate =  0.1386750490563073\n",
      "Initial Cost on dataset for this epoch 13 = 0.4999624108669432\n",
      "Error on this batch = 0.499989145795182\n",
      "Error on this batch = 0.49997571596612944\n",
      "Cost on val dataset after 14 epochs is = 0.4999615795855021\n",
      "learning rate =  0.1336306209562122\n",
      "Initial Cost on dataset for this epoch 14 = 0.4999615795855021\n",
      "Error on this batch = 0.4999887987250029\n",
      "Error on this batch = 0.49997514478554295\n",
      "Cost on val dataset after 15 epochs is = 0.4999607414593892\n",
      "learning rate =  0.12909944487358055\n",
      "Initial Cost on dataset for this epoch 15 = 0.4999607414593892\n",
      "Error on this batch = 0.4999884454398057\n",
      "Error on this batch = 0.4999745680195549\n",
      "Cost on val dataset after 16 epochs is = 0.4999598948630333\n",
      "learning rate =  0.125\n",
      "Initial Cost on dataset for this epoch 16 = 0.4999598948630333\n",
      "Error on this batch = 0.49998808516299365\n",
      "Error on this batch = 0.49997398457454706\n",
      "Cost on val dataset after 17 epochs is = 0.49995903815596\n",
      "learning rate =  0.12126781251816648\n",
      "Initial Cost on dataset for this epoch 17 = 0.49995903815596\n",
      "Error on this batch = 0.4999877171628193\n",
      "Error on this batch = 0.4999733933415783\n",
      "Cost on val dataset after 18 epochs is = 0.4999581698009692\n",
      "learning rate =  0.11785113019775793\n",
      "Initial Cost on dataset for this epoch 18 = 0.4999581698009692\n",
      "Error on this batch = 0.4999873406869479\n",
      "Error on this batch = 0.49997279336425\n",
      "Cost on val dataset after 19 epochs is = 0.4999572883805867\n",
      "learning rate =  0.11470786693528087\n",
      "Initial Cost on dataset for this epoch 19 = 0.4999572883805867\n",
      "Error on this batch = 0.49998695500877177\n",
      "Error on this batch = 0.49997218378576064\n",
      "Cost on val dataset after 20 epochs is = 0.49995639253592666\n",
      "learning rate =  0.11180339887498948\n",
      "Initial Cost on dataset for this epoch 20 = 0.49995639253592666\n",
      "Error on this batch = 0.49998655941831865\n",
      "Error on this batch = 0.4999715635591845\n",
      "Cost on val dataset after 21 epochs is = 0.4999554809073291\n",
      "learning rate =  0.1091089451179962\n",
      "Initial Cost on dataset for this epoch 21 = 0.4999554809073291\n",
      "Error on this batch = 0.4999861531892577\n",
      "Error on this batch = 0.4999709317925373\n",
      "Cost on val dataset after 22 epochs is = 0.4999545522038606\n",
      "learning rate =  0.10660035817780521\n",
      "Initial Cost on dataset for this epoch 22 = 0.4999545522038606\n",
      "Error on this batch = 0.49998573559559495\n",
      "Error on this batch = 0.4999702876014747\n",
      "Cost on val dataset after 23 epochs is = 0.49995360517727955\n",
      "learning rate =  0.10425720702853739\n",
      "Initial Cost on dataset for this epoch 23 = 0.49995360517727955\n",
      "Error on this batch = 0.49998530588564083\n",
      "Error on this batch = 0.499969630071655\n",
      "Cost on val dataset after 24 epochs is = 0.4999526384421906\n",
      "learning rate =  0.10206207261596577\n",
      "Initial Cost on dataset for this epoch 24 = 0.4999526384421906\n",
      "Error on this batch = 0.4999848632537773\n",
      "Error on this batch = 0.499968958286123\n",
      "Cost on val dataset after 25 epochs is = 0.4999516506788664\n",
      "learning rate =  0.1\n",
      "Initial Cost on dataset for this epoch 25 = 0.4999516506788664\n",
      "Error on this batch = 0.49998440688814944\n",
      "Error on this batch = 0.49996827129139365\n",
      "Cost on val dataset after 26 epochs is = 0.4999506404918588\n",
      "learning rate =  0.09805806756909202\n",
      "Initial Cost on dataset for this epoch 26 = 0.4999506404918588\n",
      "Error on this batch = 0.49998393591747964\n",
      "Error on this batch = 0.4999675680955052\n",
      "Cost on val dataset after 27 epochs is = 0.4999496064239294\n",
      "learning rate =  0.09622504486493763\n",
      "Initial Cost on dataset for this epoch 27 = 0.4999496064239294\n",
      "Error on this batch = 0.4999834494281163\n",
      "Error on this batch = 0.4999668477046336\n",
      "Cost on val dataset after 28 epochs is = 0.4999485470246277\n",
      "learning rate =  0.0944911182523068\n",
      "Initial Cost on dataset for this epoch 28 = 0.4999485470246277\n",
      "Error on this batch = 0.4999829465635846\n",
      "Error on this batch = 0.499966109079466\n",
      "Cost on val dataset after 29 epochs is = 0.4999474608044789\n",
      "learning rate =  0.09284766908852593\n",
      "Initial Cost on dataset for this epoch 29 = 0.4999474608044789\n",
      "Error on this batch = 0.4999824262193813\n",
      "Error on this batch = 0.49996535113703316\n",
      "Cost on val dataset after 30 epochs is = 0.49994634618809203\n",
      "learning rate =  0.09128709291752768\n",
      "Initial Cost on dataset for this epoch 30 = 0.49994634618809203\n",
      "Error on this batch = 0.4999818873423156\n",
      "Error on this batch = 0.49996457307604486\n",
      "Cost on val dataset after 31 epochs is = 0.4999452015062928\n",
      "learning rate =  0.08980265101338746\n",
      "Initial Cost on dataset for this epoch 31 = 0.4999452015062928\n",
      "Error on this batch = 0.4999813287834505\n",
      "Error on this batch = 0.499963773472291\n",
      "Cost on val dataset after 32 epochs is = 0.4999440249401683\n",
      "learning rate =  0.08838834764831843\n",
      "Initial Cost on dataset for this epoch 32 = 0.4999440249401683\n",
      "Error on this batch = 0.4999807493051225\n",
      "Error on this batch = 0.4999629514115501\n",
      "Cost on val dataset after 33 epochs is = 0.4999428146617046\n",
      "learning rate =  0.08703882797784893\n",
      "Initial Cost on dataset for this epoch 33 = 0.4999428146617046\n",
      "Error on this batch = 0.49998014765156795\n",
      "Error on this batch = 0.49996210515169004\n",
      "Cost on val dataset after 34 epochs is = 0.49994156875300827\n",
      "learning rate =  0.08574929257125441\n",
      "Initial Cost on dataset for this epoch 34 = 0.49994156875300827\n",
      "Error on this batch = 0.49997952240915383\n",
      "Error on this batch = 0.49996123332947917\n",
      "Cost on val dataset after 35 epochs is = 0.49994028514921257\n",
      "learning rate =  0.08451542547285165\n",
      "Initial Cost on dataset for this epoch 35 = 0.49994028514921257\n",
      "Error on this batch = 0.4999788721193306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on this batch = 0.4999603344321944\n",
      "Cost on val dataset after 36 epochs is = 0.4999389616519373\n",
      "learning rate =  0.08333333333333333\n",
      "Initial Cost on dataset for this epoch 36 = 0.4999389616519373\n",
      "Error on this batch = 0.4999781951833323\n",
      "Error on this batch = 0.4999594071156056\n",
      "Cost on val dataset after 37 epochs is = 0.49993759598466364\n",
      "learning rate =  0.08219949365267865\n",
      "Initial Cost on dataset for this epoch 37 = 0.49993759598466364\n",
      "Error on this batch = 0.4999774898752588\n",
      "Error on this batch = 0.49995844947958434\n",
      "Cost on val dataset after 38 epochs is = 0.49993618572078136\n",
      "learning rate =  0.08111071056538127\n",
      "Initial Cost on dataset for this epoch 38 = 0.49993618572078136\n",
      "Error on this batch = 0.499976754326319\n",
      "Error on this batch = 0.4999574597120154\n",
      "Cost on val dataset after 39 epochs is = 0.49993472805796707\n",
      "learning rate =  0.08006407690254357\n",
      "Initial Cost on dataset for this epoch 39 = 0.49993472805796707\n",
      "Error on this batch = 0.4999759865005519\n",
      "Error on this batch = 0.49995643588120403\n",
      "Cost on val dataset after 40 epochs is = 0.49993322024461195\n",
      "learning rate =  0.07905694150420949\n",
      "Initial Cost on dataset for this epoch 40 = 0.49993322024461195\n",
      "Error on this batch = 0.4999751842841893\n",
      "Error on this batch = 0.49995537605835416\n",
      "Cost on val dataset after 41 epochs is = 0.4999316590846608\n",
      "learning rate =  0.07808688094430304\n",
      "Initial Cost on dataset for this epoch 41 = 0.4999316590846608\n",
      "Error on this batch = 0.499974345265223\n",
      "Error on this batch = 0.4999542778466699\n",
      "Cost on val dataset after 42 epochs is = 0.4999300414269715\n",
      "learning rate =  0.07715167498104596\n",
      "Initial Cost on dataset for this epoch 42 = 0.4999300414269715\n",
      "Error on this batch = 0.4999734669400784\n",
      "Error on this batch = 0.49995313895030163\n",
      "Cost on val dataset after 43 epochs is = 0.49992836367436855\n",
      "learning rate =  0.07624928516630233\n",
      "Initial Cost on dataset for this epoch 43 = 0.49992836367436855\n",
      "Error on this batch = 0.4999725466972045\n",
      "Error on this batch = 0.4999519566829329\n",
      "Cost on val dataset after 44 epochs is = 0.4999266221003752\n",
      "learning rate =  0.07537783614444091\n",
      "Initial Cost on dataset for this epoch 44 = 0.4999266221003752\n",
      "Error on this batch = 0.4999715814876204\n",
      "Error on this batch = 0.499950728378523\n",
      "Cost on val dataset after 45 epochs is = 0.4999248124408839\n",
      "learning rate =  0.07453559924999299\n",
      "Initial Cost on dataset for this epoch 45 = 0.4999248124408839\n",
      "Error on this batch = 0.49997056784690247\n",
      "Error on this batch = 0.49994945098169996\n",
      "Cost on val dataset after 46 epochs is = 0.49992293044246194\n",
      "learning rate =  0.07372097807744857\n",
      "Initial Cost on dataset for this epoch 46 = 0.49992293044246194\n",
      "Error on this batch = 0.4999695022129781\n",
      "Error on this batch = 0.4999481212351613\n",
      "Cost on val dataset after 47 epochs is = 0.4999209710219155\n",
      "learning rate =  0.07293249574894728\n",
      "Initial Cost on dataset for this epoch 47 = 0.4999209710219155\n",
      "Error on this batch = 0.49996838100793645\n",
      "Error on this batch = 0.4999467355459055\n",
      "Cost on val dataset after 48 epochs is = 0.49991892899624657\n",
      "learning rate =  0.07216878364870323\n",
      "Initial Cost on dataset for this epoch 48 = 0.49991892899624657\n",
      "Error on this batch = 0.4999671995890209\n",
      "Error on this batch = 0.49994529007256516\n",
      "Cost on val dataset after 49 epochs is = 0.4999167986633951\n",
      "learning rate =  0.07142857142857142\n",
      "Initial Cost on dataset for this epoch 49 = 0.4999167986633951\n",
      "Error on this batch = 0.49996595314640824\n",
      "Error on this batch = 0.49994378055080774\n",
      "Cost on val dataset after 50 epochs is = 0.4999145737608459\n",
      "learning rate =  0.07071067811865475\n",
      "Initial Cost on dataset for this epoch 50 = 0.4999145737608459\n",
      "Error on this batch = 0.4999646366955055\n",
      "Error on this batch = 0.4999422023399583\n",
      "Cost on val dataset after 51 epochs is = 0.4999122474008244\n",
      "learning rate =  0.07001400420140048\n",
      "Initial Cost on dataset for this epoch 51 = 0.4999122474008244\n",
      "Error on this batch = 0.4999632445478959\n",
      "Error on this batch = 0.4999405503899873\n",
      "Cost on val dataset after 52 epochs is = 0.4999098119015686\n",
      "learning rate =  0.06933752452815364\n",
      "Initial Cost on dataset for this epoch 52 = 0.4999098119015686\n",
      "Error on this batch = 0.49996176979178886\n",
      "Error on this batch = 0.49993881904963217\n",
      "Cost on val dataset after 53 epochs is = 0.49990725898744637\n",
      "learning rate =  0.06868028197434452\n",
      "Initial Cost on dataset for this epoch 53 = 0.49990725898744637\n",
      "Error on this batch = 0.4999602052146458\n",
      "Error on this batch = 0.4999370023014562\n",
      "Cost on val dataset after 54 epochs is = 0.4999045799443685\n",
      "learning rate =  0.06804138174397717\n",
      "Initial Cost on dataset for this epoch 54 = 0.4999045799443685\n",
      "Error on this batch = 0.4999585429344741\n",
      "Error on this batch = 0.4999350935637207\n",
      "Cost on val dataset after 55 epochs is = 0.4999017644741272\n",
      "learning rate =  0.06741998624632421\n",
      "Initial Cost on dataset for this epoch 55 = 0.4999017644741272\n",
      "Error on this batch = 0.49995677396733523\n",
      "Error on this batch = 0.4999330851280533\n",
      "Cost on val dataset after 56 epochs is = 0.4998988014933175\n",
      "learning rate =  0.0668153104781061\n",
      "Initial Cost on dataset for this epoch 56 = 0.4998988014933175\n",
      "Error on this batch = 0.49995488835868446\n",
      "Error on this batch = 0.4999309686607553\n",
      "Cost on val dataset after 57 epochs is = 0.4998956784505963\n",
      "learning rate =  0.06622661785325219\n",
      "Initial Cost on dataset for this epoch 57 = 0.4998956784505963\n",
      "Error on this batch = 0.49995287544450906\n",
      "Error on this batch = 0.49992873498950147\n",
      "Cost on val dataset after 58 epochs is = 0.4998923818503678\n",
      "learning rate =  0.06565321642986127\n",
      "Initial Cost on dataset for this epoch 58 = 0.4998923818503678\n",
      "Error on this batch = 0.4999507223362498\n",
      "Error on this batch = 0.499926373904495\n",
      "Cost on val dataset after 59 epochs is = 0.49988889622288357\n",
      "learning rate =  0.06509445549041194\n",
      "Initial Cost on dataset for this epoch 59 = 0.49988889622288357\n",
      "Error on this batch = 0.49994841447716654\n",
      "Error on this batch = 0.49992387379745157\n",
      "Cost on val dataset after 60 epochs is = 0.49988520431134065\n",
      "learning rate =  0.06454972243679027\n",
      "Initial Cost on dataset for this epoch 60 = 0.49988520431134065\n",
      "Error on this batch = 0.4999459361967885\n",
      "Error on this batch = 0.49992122164117114\n",
      "Cost on val dataset after 61 epochs is = 0.49988128683564176\n",
      "learning rate =  0.06401843996644799\n",
      "Initial Cost on dataset for this epoch 61 = 0.49988128683564176\n",
      "Error on this batch = 0.4999432692667753\n",
      "Error on this batch = 0.49991840293241346\n",
      "Cost on val dataset after 62 epochs is = 0.4998771220226259\n",
      "learning rate =  0.06350006350009525\n",
      "Initial Cost on dataset for this epoch 62 = 0.4998771220226259\n",
      "Error on this batch = 0.49994039215510244\n",
      "Error on this batch = 0.4999154011925374\n",
      "Cost on val dataset after 63 epochs is = 0.499872685315498\n",
      "learning rate =  0.0629940788348712\n",
      "Initial Cost on dataset for this epoch 63 = 0.499872685315498\n",
      "Error on this batch = 0.49993728063667436\n",
      "Error on this batch = 0.4999121975095768\n",
      "Cost on val dataset after 64 epochs is = 0.499867948118723\n",
      "learning rate =  0.0625\n",
      "Initial Cost on dataset for this epoch 64 = 0.499867948118723\n",
      "Error on this batch = 0.4999339067990873\n",
      "Error on this batch = 0.4999087707573081\n",
      "Cost on val dataset after 65 epochs is = 0.49986287880018915\n",
      "learning rate =  0.062017367294604234\n",
      "Initial Cost on dataset for this epoch 65 = 0.49986287880018915\n",
      "Error on this batch = 0.49993023857180996\n",
      "Error on this batch = 0.49990509614886763\n",
      "Cost on val dataset after 66 epochs is = 0.49985743998973364\n",
      "learning rate =  0.06154574548966636\n",
      "Initial Cost on dataset for this epoch 66 = 0.49985743998973364\n",
      "Error on this batch = 0.4999262382773293\n",
      "Error on this batch = 0.49990114583016676\n",
      "Cost on val dataset after 67 epochs is = 0.49985158961573933\n",
      "learning rate =  0.06108472217815261\n",
      "Initial Cost on dataset for this epoch 67 = 0.49985158961573933\n",
      "Error on this batch = 0.49992186204124345\n",
      "Error on this batch = 0.4998968871763373\n",
      "Cost on val dataset after 68 epochs is = 0.4998452787315853\n",
      "learning rate =  0.06063390625908324\n",
      "Initial Cost on dataset for this epoch 68 = 0.4998452787315853\n",
      "Error on this batch = 0.49991705825221716\n",
      "Error on this batch = 0.4998922822952735\n",
      "Cost on val dataset after 69 epochs is = 0.4998384499952985\n",
      "learning rate =  0.0601929265428846\n",
      "Initial Cost on dataset for this epoch 69 = 0.4998384499952985\n",
      "Error on this batch = 0.4999117657028559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on this batch = 0.49988728719639397\n",
      "Cost on val dataset after 70 epochs is = 0.49983103588808364\n",
      "learning rate =  0.05976143046671968\n",
      "Initial Cost on dataset for this epoch 70 = 0.49983103588808364\n",
      "Error on this batch = 0.4999059115318373\n",
      "Error on this batch = 0.49988184943413205\n",
      "Cost on val dataset after 71 epochs is = 0.49982295689231426\n",
      "learning rate =  0.05933908290969266\n",
      "Initial Cost on dataset for this epoch 71 = 0.49982295689231426\n",
      "Error on this batch = 0.49989940851222614\n",
      "Error on this batch = 0.49987590782612445\n",
      "Cost on val dataset after 72 epochs is = 0.49981411946177917\n",
      "learning rate =  0.05892556509887897\n",
      "Initial Cost on dataset for this epoch 72 = 0.49981411946177917\n",
      "Error on this batch = 0.4998921516598267\n",
      "Error on this batch = 0.49986939000309505\n",
      "Cost on val dataset after 73 epochs is = 0.499804410553468\n",
      "learning rate =  0.058520573598065284\n",
      "Initial Cost on dataset for this epoch 73 = 0.499804410553468\n",
      "Error on this batch = 0.49988401175335884\n",
      "Error on this batch = 0.49986220689810384\n",
      "Cost on val dataset after 74 epochs is = 0.4997936935552426\n",
      "learning rate =  0.05812381937190964\n",
      "Initial Cost on dataset for this epoch 74 = 0.4997936935552426\n",
      "Error on this batch = 0.4998748302326586\n",
      "Error on this batch = 0.49985425234573144\n",
      "Cost on val dataset after 75 epochs is = 0.4997818039149386\n",
      "learning rate =  0.05773502691896257\n",
      "Initial Cost on dataset for this epoch 75 = 0.4997818039149386\n",
      "Error on this batch = 0.4998644136820964\n",
      "Error on this batch = 0.4998453949328797\n",
      "Cost on val dataset after 76 epochs is = 0.49976853753403205\n",
      "learning rate =  0.057353933467640436\n",
      "Initial Cost on dataset for this epoch 76 = 0.49976853753403205\n",
      "Error on this batch = 0.49985251230909583\n",
      "Error on this batch = 0.49983547296147407\n",
      "Cost on val dataset after 77 epochs is = 0.49975364084736423\n",
      "learning rate =  0.05698028822981897\n",
      "Initial Cost on dataset for this epoch 77 = 0.49975364084736423\n",
      "Error on this batch = 0.4998388116843773\n",
      "Error on this batch = 0.49982428495253334\n",
      "Cost on val dataset after 78 epochs is = 0.49973679439448493\n",
      "learning rate =  0.05661385170722978\n",
      "Initial Cost on dataset for this epoch 78 = 0.49973679439448493\n",
      "Error on this batch = 0.49982290852472416\n",
      "Error on this batch = 0.49981157658968556\n",
      "Cost on val dataset after 79 epochs is = 0.49971759124273935\n",
      "learning rate =  0.0562543950463012\n",
      "Initial Cost on dataset for this epoch 79 = 0.49971759124273935\n",
      "Error on this batch = 0.49980427359006385\n",
      "Error on this batch = 0.4997970208152651\n",
      "Cost on val dataset after 80 epochs is = 0.49969550253116013\n",
      "learning rate =  0.05590169943749474\n",
      "Initial Cost on dataset for this epoch 80 = 0.49969550253116013\n",
      "Error on this batch = 0.4997822040783258\n",
      "Error on this batch = 0.49978019224085024\n",
      "Cost on val dataset after 81 epochs is = 0.4996698326761563\n",
      "learning rate =  0.05555555555555555\n",
      "Initial Cost on dataset for this epoch 81 = 0.4996698326761563\n",
      "Error on this batch = 0.4997557490960008\n",
      "Error on this batch = 0.49976054391523195\n",
      "Cost on val dataset after 82 epochs is = 0.49963964624291773\n",
      "learning rate =  0.05521576303742327\n",
      "Initial Cost on dataset for this epoch 82 = 0.49963964624291773\n",
      "Error on this batch = 0.4997235911381337\n",
      "Error on this batch = 0.4997373126732431\n",
      "Cost on val dataset after 83 epochs is = 0.49960365647134264\n",
      "learning rate =  0.05488212999484517\n",
      "Initial Cost on dataset for this epoch 83 = 0.49960365647134264\n",
      "Error on this batch = 0.49968386539299986\n",
      "Error on this batch = 0.49970944763750297\n",
      "Cost on val dataset after 84 epochs is = 0.49956004625506395\n",
      "learning rate =  0.0545544725589981\n",
      "Initial Cost on dataset for this epoch 84 = 0.49956004625506395\n",
      "Error on this batch = 0.49963384260071225\n",
      "Error on this batch = 0.4996754749287682\n",
      "Cost on val dataset after 85 epochs is = 0.49950617373414746\n",
      "learning rate =  0.05423261445466404\n",
      "Initial Cost on dataset for this epoch 85 = 0.49950617373414746\n",
      "Error on this batch = 0.4995694135061028\n",
      "Error on this batch = 0.4996332538236039\n",
      "Cost on val dataset after 86 epochs is = 0.4994380688552123\n",
      "learning rate =  0.053916386601719206\n",
      "Initial Cost on dataset for this epoch 86 = 0.4994380688552123\n",
      "Error on this batch = 0.4994841665858654\n",
      "Error on this batch = 0.49957959933943474\n",
      "Cost on val dataset after 87 epochs is = 0.49934952730443166\n",
      "learning rate =  0.05360562674188974\n",
      "Initial Cost on dataset for this epoch 87 = 0.49934952730443166\n",
      "Error on this batch = 0.4993675923809832\n",
      "Error on this batch = 0.499509648389276\n",
      "Cost on val dataset after 88 epochs is = 0.49923041574662647\n",
      "learning rate =  0.053300179088902604\n",
      "Initial Cost on dataset for this epoch 88 = 0.49923041574662647\n",
      "Error on this batch = 0.49920150875869157\n",
      "Error on this batch = 0.49941577022227224\n",
      "Cost on val dataset after 89 epochs is = 0.49906339962680324\n",
      "learning rate =  0.052999894000318\n",
      "Initial Cost on dataset for this epoch 89 = 0.49906339962680324\n",
      "Error on this batch = 0.49895268339956333\n",
      "Error on this batch = 0.4992860184264273\n",
      "Cost on val dataset after 90 epochs is = 0.4988177430112834\n",
      "learning rate =  0.05270462766947299\n",
      "Initial Cost on dataset for this epoch 90 = 0.4988177430112834\n",
      "Error on this batch = 0.4985563939679292\n",
      "Error on this batch = 0.4991046263121274\n",
      "Cost on val dataset after 91 epochs is = 0.49844114328008887\n",
      "learning rate =  0.052414241836095915\n",
      "Initial Cost on dataset for this epoch 91 = 0.49844114328008887\n",
      "Error on this batch = 0.4978840966535115\n",
      "Error on this batch = 0.49887170550418236\n",
      "Cost on val dataset after 92 epochs is = 0.4978801878182228\n",
      "learning rate =  0.05212860351426869\n",
      "Initial Cost on dataset for this epoch 92 = 0.4978801878182228\n",
      "Error on this batch = 0.49672429058664264\n",
      "Error on this batch = 0.4987256203741071\n",
      "Cost on val dataset after 93 epochs is = 0.49728315395382217\n",
      "learning rate =  0.05184758473652126\n",
      "Initial Cost on dataset for this epoch 93 = 0.49728315395382217\n",
      "Error on this batch = 0.49512146207378566\n",
      "Error on this batch = 0.4989847534279953\n",
      "Cost on val dataset after 94 epochs is = 0.49701179414043284\n",
      "learning rate =  0.05157106231293967\n",
      "Initial Cost on dataset for this epoch 94 = 0.49701179414043284\n",
      "Error on this batch = 0.49398695421518407\n",
      "Error on this batch = 0.49923550543062933\n",
      "Cost on val dataset after 95 epochs is = 0.4968748353806602\n",
      "learning rate =  0.051298917604257706\n",
      "Initial Cost on dataset for this epoch 95 = 0.4968748353806602\n",
      "Error on this batch = 0.4934671771282145\n",
      "Error on this batch = 0.4992481038718045\n",
      "Cost on val dataset after 96 epochs is = 0.4967337662176651\n",
      "learning rate =  0.051031036307982884\n",
      "Initial Cost on dataset for this epoch 96 = 0.4967337662176651\n",
      "Error on this batch = 0.4931050087958929\n",
      "Error on this batch = 0.49919535357302675\n",
      "Cost on val dataset after 97 epochs is = 0.49658957705500506\n",
      "learning rate =  0.05076730825668096\n",
      "Initial Cost on dataset for this epoch 97 = 0.49658957705500506\n",
      "Error on this batch = 0.4927651426625017\n",
      "Error on this batch = 0.49913378009975734\n",
      "Cost on val dataset after 98 epochs is = 0.4964466384920509\n",
      "learning rate =  0.050507627227610534\n",
      "Initial Cost on dataset for this epoch 98 = 0.4964466384920509\n",
      "Error on this batch = 0.4924296835083922\n",
      "Error on this batch = 0.4990741819509797\n",
      "Cost on val dataset after 99 epochs is = 0.49630763955850765\n",
      "learning rate =  0.050251890762960605\n",
      "Initial Cost on dataset for this epoch 99 = 0.49630763955850765\n",
      "Error on this batch = 0.49210232361939876\n",
      "Error on this batch = 0.4990202992553539\n",
      "Cost on val dataset after 100 epochs is = 0.4961749223518964\n",
      "learning rate =  0.05\n",
      "Initial Cost on dataset for this epoch 100 = 0.4961749223518964\n",
      "Error on this batch = 0.4917878034903971\n",
      "Error on this batch = 0.4989730393472332\n",
      "Cost on val dataset after 101 epochs is = 0.4960504406564979\n",
      "learning rate =  0.04975185951049946\n",
      "Initial Cost on dataset for this epoch 101 = 0.4960504406564979\n",
      "Error on this batch = 0.4914908494622942\n",
      "Error on this batch = 0.49893255484500226\n",
      "Cost on val dataset after 102 epochs is = 0.4959350193130664\n",
      "learning rate =  0.04950737714883372\n",
      "Initial Cost on dataset for this epoch 102 = 0.4959350193130664\n",
      "Error on this batch = 0.4912150356444464\n",
      "Error on this batch = 0.4988986037587988\n",
      "Cost on val dataset after 103 epochs is = 0.4958292500735556\n",
      "learning rate =  0.04926646390821466\n",
      "Initial Cost on dataset for this epoch 103 = 0.4958292500735556\n",
      "Error on this batch = 0.49096219271121977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on this batch = 0.4988701470025497\n",
      "Cost on val dataset after 104 epochs is = 0.49573302413574916\n",
      "learning rate =  0.04902903378454601\n",
      "Initial Cost on dataset for this epoch 104 = 0.49573302413574916\n",
      "Error on this batch = 0.4907331599448911\n",
      "Error on this batch = 0.49884627987956565\n",
      "Cost on val dataset after 105 epochs is = 0.49564560415150816\n",
      "learning rate =  0.048795003647426664\n",
      "Initial Cost on dataset for this epoch 105 = 0.49564560415150816\n",
      "Error on this batch = 0.49052699704290703\n",
      "Error on this batch = 0.4988257704735284\n",
      "Cost on val dataset after 106 epochs is = 0.4955658482764615\n",
      "learning rate =  0.04856429311786321\n",
      "Initial Cost on dataset for this epoch 106 = 0.4955658482764615\n",
      "Error on this batch = 0.4903409342750114\n",
      "Error on this batch = 0.49880768299685435\n",
      "Cost on val dataset after 107 epochs is = 0.49549283073079736\n",
      "learning rate =  0.04833682445228318\n",
      "Initial Cost on dataset for this epoch 107 = 0.49549283073079736\n",
      "Error on this batch = 0.4901725962792338\n",
      "Error on this batch = 0.49879137039712435\n",
      "Cost on val dataset after 108 epochs is = 0.4954255636599461\n",
      "learning rate =  0.048112522432468816\n",
      "Initial Cost on dataset for this epoch 108 = 0.4954255636599461\n",
      "Error on this batch = 0.49001911676369125\n",
      "Error on this batch = 0.4987764712316293\n",
      "Cost on val dataset after 109 epochs is = 0.4953630835390209\n",
      "learning rate =  0.04789131426105757\n",
      "Initial Cost on dataset for this epoch 109 = 0.4953630835390209\n",
      "Error on this batch = 0.48987748053985225\n",
      "Error on this batch = 0.4987627695678373\n",
      "Cost on val dataset after 110 epochs is = 0.4953046077711343\n",
      "learning rate =  0.04767312946227962\n",
      "Initial Cost on dataset for this epoch 110 = 0.4953046077711343\n",
      "Error on this batch = 0.4897454237912433\n",
      "Error on this batch = 0.49875020723980856\n",
      "Cost on val dataset after 111 epochs is = 0.49524963721952264\n",
      "learning rate =  0.04745789978762495\n",
      "Initial Cost on dataset for this epoch 111 = 0.49524963721952264\n",
      "Error on this batch = 0.48962128645729536\n",
      "Error on this batch = 0.49873916805858404\n",
      "Cost on val dataset after 112 epochs is = 0.49519755136735716\n",
      "learning rate =  0.0472455591261534\n",
      "Initial Cost on dataset for this epoch 112 = 0.49519755136735716\n",
      "Error on this batch = 0.48950424274403775\n",
      "Error on this batch = 0.4987289482042487\n",
      "Cost on val dataset after 113 epochs is = 0.49514791652859785\n",
      "learning rate =  0.047036043419179864\n",
      "Initial Cost on dataset for this epoch 113 = 0.49514791652859785\n",
      "Error on this batch = 0.48939336172326564\n",
      "Error on this batch = 0.49871961087375477\n",
      "Cost on val dataset after 114 epochs is = 0.49510046069628516\n",
      "learning rate =  0.0468292905790847\n",
      "Initial Cost on dataset for this epoch 114 = 0.49510046069628516\n",
      "Error on this batch = 0.4892880830644194\n",
      "Error on this batch = 0.49871106144370275\n",
      "Cost on val dataset after 115 epochs is = 0.49505475426684176\n",
      "learning rate =  0.04662524041201569\n",
      "Initial Cost on dataset for this epoch 115 = 0.49505475426684176\n",
      "Error on this batch = 0.48918775144956583\n",
      "Error on this batch = 0.4987036174691984\n",
      "Cost on val dataset after 116 epochs is = 0.49501057120787245\n",
      "learning rate =  0.046423834544262965\n",
      "Initial Cost on dataset for this epoch 116 = 0.49501057120787245\n",
      "Error on this batch = 0.48909131988721316\n",
      "Error on this batch = 0.4986967880904922\n",
      "Cost on val dataset after 117 epochs is = 0.49496766018084715\n",
      "learning rate =  0.046225016352102424\n",
      "Initial Cost on dataset for this epoch 117 = 0.49496766018084715\n",
      "Error on this batch = 0.4889982787690112\n",
      "Error on this batch = 0.4986901380074285\n",
      "Cost on val dataset after 118 epochs is = 0.49492592725318707\n",
      "learning rate =  0.04602873089491617\n",
      "Initial Cost on dataset for this epoch 118 = 0.49492592725318707\n",
      "Error on this batch = 0.4889082112562885\n",
      "Error on this batch = 0.4986835476632394\n",
      "Cost on val dataset after 119 epochs is = 0.494885176690868\n",
      "learning rate =  0.045834924851410566\n",
      "Initial Cost on dataset for this epoch 119 = 0.494885176690868\n",
      "Error on this batch = 0.4888207176715717\n",
      "Error on this batch = 0.4986768674424165\n",
      "Cost on val dataset after 120 epochs is = 0.4948452593196605\n",
      "learning rate =  0.04564354645876384\n",
      "Initial Cost on dataset for this epoch 120 = 0.4948452593196605\n",
      "Error on this batch = 0.48873593970361373\n",
      "Error on this batch = 0.4986703178179454\n",
      "Cost on val dataset after 121 epochs is = 0.4948061525301023\n",
      "learning rate =  0.045454545454545456\n",
      "Initial Cost on dataset for this epoch 121 = 0.4948061525301023\n",
      "Error on this batch = 0.48865345934376225\n",
      "Error on this batch = 0.4986634780912262\n",
      "Cost on val dataset after 122 epochs is = 0.4947676787876896\n",
      "learning rate =  0.045267873021259265\n",
      "Initial Cost on dataset for this epoch 122 = 0.4947676787876896\n",
      "Error on this batch = 0.4885732705052408\n",
      "Error on this batch = 0.4986566837792378\n",
      "Cost on val dataset after 123 epochs is = 0.4947296964487122\n",
      "learning rate =  0.045083481733371615\n",
      "Initial Cost on dataset for this epoch 123 = 0.4947296964487122\n",
      "Error on this batch = 0.48849478180207373\n",
      "Error on this batch = 0.49864962669397356\n",
      "Cost on val dataset after 124 epochs is = 0.4946921372396258\n",
      "learning rate =  0.04490132550669373\n",
      "Initial Cost on dataset for this epoch 124 = 0.4946921372396258\n",
      "Error on this batch = 0.48841800216359715\n",
      "Error on this batch = 0.49864239137663136\n",
      "Cost on val dataset after 125 epochs is = 0.4946549817597646\n",
      "learning rate =  0.044721359549995794\n",
      "Initial Cost on dataset for this epoch 125 = 0.4946549817597646\n",
      "Error on this batch = 0.48834283961320907\n",
      "Error on this batch = 0.4986348263950279\n",
      "Cost on val dataset after 126 epochs is = 0.4946182847828178\n",
      "learning rate =  0.0445435403187374\n",
      "Initial Cost on dataset for this epoch 126 = 0.4946182847828178\n",
      "Error on this batch = 0.48826945013776224\n",
      "Error on this batch = 0.4986270559221708\n",
      "Cost on val dataset after 127 epochs is = 0.4945820926902379\n",
      "learning rate =  0.04436782547080569\n",
      "Initial Cost on dataset for this epoch 127 = 0.4945820926902379\n",
      "Error on this batch = 0.4881976562835146\n",
      "Error on this batch = 0.4986190118974161\n",
      "Cost on val dataset after 128 epochs is = 0.4945463587315876\n",
      "learning rate =  0.044194173824159216\n",
      "Initial Cost on dataset for this epoch 128 = 0.4945463587315876\n",
      "Error on this batch = 0.4881270597276076\n",
      "Error on this batch = 0.49861055754157585\n",
      "Cost on val dataset after 129 epochs is = 0.494510899051822\n",
      "learning rate =  0.04402254531628119\n",
      "Initial Cost on dataset for this epoch 129 = 0.494510899051822\n",
      "Error on this batch = 0.4880575337641125\n",
      "Error on this batch = 0.4986018791397963\n",
      "Cost on val dataset after 130 epochs is = 0.49447582308852933\n",
      "learning rate =  0.043852900965351466\n",
      "Initial Cost on dataset for this epoch 130 = 0.49447582308852933\n",
      "Error on this batch = 0.48798928407770087\n",
      "Error on this batch = 0.49859294892031286\n",
      "Cost on val dataset after 131 epochs is = 0.4944411285682494\n",
      "learning rate =  0.0436852028330519\n",
      "Initial Cost on dataset for this epoch 131 = 0.4944411285682494\n",
      "Error on this batch = 0.48792190267900104\n",
      "Error on this batch = 0.49858374652586457\n",
      "Cost on val dataset after 132 epochs is = 0.4944067801410868\n",
      "learning rate =  0.04351941398892446\n",
      "Initial Cost on dataset for this epoch 132 = 0.4944067801410868\n",
      "Error on this batch = 0.4878553400214953\n",
      "Error on this batch = 0.49857420396539015\n",
      "Cost on val dataset after 133 epochs is = 0.49437264305639916\n",
      "learning rate =  0.043355498476206\n",
      "Initial Cost on dataset for this epoch 133 = 0.49437264305639916\n",
      "Error on this batch = 0.4877895134334544\n",
      "Error on this batch = 0.4985644186266681\n",
      "Cost on val dataset after 134 epochs is = 0.49433884407030787\n",
      "learning rate =  0.04319342127906801\n",
      "Initial Cost on dataset for this epoch 134 = 0.49433884407030787\n",
      "Error on this batch = 0.4877248263615813\n",
      "Error on this batch = 0.4985545689651349\n",
      "Cost on val dataset after 135 epochs is = 0.49430543184601555\n",
      "learning rate =  0.04303314829119352\n",
      "Initial Cost on dataset for this epoch 135 = 0.49430543184601555\n",
      "Error on this batch = 0.48766072143539646\n",
      "Error on this batch = 0.49854417382520555\n",
      "Cost on val dataset after 136 epochs is = 0.4942722577340747\n",
      "learning rate =  0.042874646285627205\n",
      "Initial Cost on dataset for this epoch 136 = 0.4942722577340747\n",
      "Error on this batch = 0.4875972532136437\n",
      "Error on this batch = 0.4985338391612895\n",
      "Cost on val dataset after 137 epochs is = 0.494239345381641\n",
      "learning rate =  0.04271788288583805\n",
      "Initial Cost on dataset for this epoch 137 = 0.494239345381641\n",
      "Error on this batch = 0.487534291574097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on this batch = 0.49852324098451306\n",
      "Cost on val dataset after 138 epochs is = 0.4942067104648728\n",
      "learning rate =  0.04256282653793743\n",
      "Initial Cost on dataset for this epoch 138 = 0.4942067104648728\n",
      "Error on this batch = 0.48747172956466395\n",
      "Error on this batch = 0.49851229914853257\n",
      "Cost on val dataset after 139 epochs is = 0.4941742285834865\n",
      "learning rate =  0.042409446483998546\n",
      "Initial Cost on dataset for this epoch 139 = 0.4941742285834865\n",
      "Error on this batch = 0.4874092392197434\n",
      "Error on this batch = 0.4985010860251887\n",
      "Cost on val dataset after 140 epochs is = 0.49414179167358446\n",
      "learning rate =  0.042257712736425826\n",
      "Initial Cost on dataset for this epoch 140 = 0.49414179167358446\n",
      "Error on this batch = 0.4873469896295785\n",
      "Error on this batch = 0.49848930919934903\n",
      "Cost on val dataset after 141 epochs is = 0.4941094739410419\n",
      "learning rate =  0.04210759605332595\n",
      "Initial Cost on dataset for this epoch 141 = 0.4941094739410419\n",
      "Error on this batch = 0.48728497082455263\n",
      "Error on this batch = 0.4984773105183523\n",
      "Cost on val dataset after 142 epochs is = 0.4940773036673868\n",
      "learning rate =  0.041959067914834454\n",
      "Initial Cost on dataset for this epoch 142 = 0.4940773036673868\n",
      "Error on this batch = 0.48722303626841296\n",
      "Error on this batch = 0.4984649146417357\n",
      "Cost on val dataset after 143 epochs is = 0.4940449042163459\n",
      "learning rate =  0.04181210050035454\n",
      "Initial Cost on dataset for this epoch 143 = 0.4940449042163459\n",
      "Error on this batch = 0.4871607741178741\n",
      "Error on this batch = 0.49845179947383517\n",
      "Cost on val dataset after 144 epochs is = 0.49401248692928773\n",
      "learning rate =  0.041666666666666664\n",
      "Initial Cost on dataset for this epoch 144 = 0.49401248692928773\n",
      "Error on this batch = 0.48709818585931736\n",
      "Error on this batch = 0.49843824298576056\n",
      "Cost on val dataset after 145 epochs is = 0.49398011785026813\n",
      "learning rate =  0.041522739926869986\n",
      "Initial Cost on dataset for this epoch 145 = 0.49398011785026813\n",
      "Error on this batch = 0.4870354239201937\n",
      "Error on this batch = 0.4984245688062604\n",
      "Cost on val dataset after 146 epochs is = 0.49394774470077213\n",
      "learning rate =  0.0413802944301184\n",
      "Initial Cost on dataset for this epoch 146 = 0.49394774470077213\n",
      "Error on this batch = 0.48697244332485656\n",
      "Error on this batch = 0.49841042607675845\n",
      "Cost on val dataset after 147 epochs is = 0.4939152754368088\n",
      "learning rate =  0.041239304942116126\n",
      "Initial Cost on dataset for this epoch 147 = 0.4939152754368088\n",
      "Error on this batch = 0.48690923059501073\n",
      "Error on this batch = 0.498395776485517\n",
      "Cost on val dataset after 148 epochs is = 0.4938827083163474\n",
      "learning rate =  0.04109974682633932\n",
      "Initial Cost on dataset for this epoch 148 = 0.4938827083163474\n",
      "Error on this batch = 0.4868456315795791\n",
      "Error on this batch = 0.49838066667890146\n",
      "Cost on val dataset after 149 epochs is = 0.4938500028779323\n",
      "learning rate =  0.04096159602595203\n",
      "Initial Cost on dataset for this epoch 149 = 0.4938500028779323\n",
      "Error on this batch = 0.48678181448943864\n",
      "Error on this batch = 0.49836508577690647\n",
      "Cost on val dataset after 150 epochs is = 0.49381718491438553\n",
      "learning rate =  0.040824829046386304\n",
      "Initial Cost on dataset for this epoch 150 = 0.49381718491438553\n",
      "Error on this batch = 0.4867175383711312\n",
      "Error on this batch = 0.4983489269039879\n",
      "Cost on val dataset after 151 epochs is = 0.4937842341997026\n",
      "learning rate =  0.04068942293855797\n",
      "Initial Cost on dataset for this epoch 151 = 0.4937842341997026\n",
      "Error on this batch = 0.486652687420338\n",
      "Error on this batch = 0.49833234518003366\n",
      "Cost on val dataset after 152 epochs is = 0.493751011568852\n",
      "learning rate =  0.040555355282690636\n",
      "Initial Cost on dataset for this epoch 152 = 0.493751011568852\n",
      "Error on this batch = 0.48658755617421096\n",
      "Error on this batch = 0.49831539790350354\n",
      "Cost on val dataset after 153 epochs is = 0.4937175306100731\n",
      "learning rate =  0.040422604172722164\n",
      "Initial Cost on dataset for this epoch 153 = 0.4937175306100731\n",
      "Error on this batch = 0.48652158401823775\n",
      "Error on this batch = 0.4982977485943322\n",
      "Cost on val dataset after 154 epochs is = 0.49368370383923027\n",
      "learning rate =  0.040291148201269014\n",
      "Initial Cost on dataset for this epoch 154 = 0.49368370383923027\n",
      "Error on this batch = 0.48645496391741144\n",
      "Error on this batch = 0.4982794280441066\n",
      "Cost on val dataset after 155 epochs is = 0.49364966790644194\n",
      "learning rate =  0.04016096644512494\n",
      "Initial Cost on dataset for this epoch 155 = 0.49364966790644194\n",
      "Error on this batch = 0.4863875115280344\n",
      "Error on this batch = 0.49826061291536916\n",
      "Cost on val dataset after 156 epochs is = 0.49361521050843415\n",
      "learning rate =  0.04003203845127178\n",
      "Initial Cost on dataset for this epoch 156 = 0.49361521050843415\n",
      "Error on this batch = 0.48631921990567356\n",
      "Error on this batch = 0.49824100684649003\n",
      "Cost on val dataset after 157 epochs is = 0.49358030465477115\n",
      "learning rate =  0.03990434422338111\n",
      "Initial Cost on dataset for this epoch 157 = 0.49358030465477115\n",
      "Error on this batch = 0.48625002542769047\n",
      "Error on this batch = 0.4982206972303631\n",
      "Cost on val dataset after 158 epochs is = 0.49354497475157666\n",
      "learning rate =  0.0397778642087865\n",
      "Initial Cost on dataset for this epoch 158 = 0.49354497475157666\n",
      "Error on this batch = 0.48617978041121596\n",
      "Error on this batch = 0.4981997358230622\n",
      "Cost on val dataset after 159 epochs is = 0.49350916660388655\n",
      "learning rate =  0.03965257928590721\n",
      "Initial Cost on dataset for this epoch 159 = 0.49350916660388655\n",
      "Error on this batch = 0.4861087142320227\n",
      "Error on this batch = 0.49817813205356387\n",
      "Cost on val dataset after 160 epochs is = 0.4934727225628008\n",
      "learning rate =  0.03952847075210474\n",
      "Initial Cost on dataset for this epoch 160 = 0.4934727225628008\n",
      "Error on this batch = 0.48603647876197115\n",
      "Error on this batch = 0.4981557500021312\n",
      "Cost on val dataset after 161 epochs is = 0.4934356421992368\n",
      "learning rate =  0.03940552031195503\n",
      "Initial Cost on dataset for this epoch 161 = 0.4934356421992368\n",
      "Error on this batch = 0.48596298905510843\n",
      "Error on this batch = 0.49813253071604613\n",
      "Cost on val dataset after 162 epochs is = 0.49339790277478285\n",
      "learning rate =  0.039283710065919304\n",
      "Initial Cost on dataset for this epoch 162 = 0.49339790277478285\n",
      "Error on this batch = 0.4858882324602237\n",
      "Error on this batch = 0.49810849519998607\n",
      "Cost on val dataset after 163 epochs is = 0.49335939300677417\n",
      "learning rate =  0.03916302249939787\n",
      "Initial Cost on dataset for this epoch 163 = 0.49335939300677417\n",
      "Error on this batch = 0.48581213244029464\n",
      "Error on this batch = 0.49808358471488784\n",
      "Cost on val dataset after 164 epochs is = 0.49332011298898143\n",
      "learning rate =  0.03904344047215152\n",
      "Initial Cost on dataset for this epoch 164 = 0.49332011298898143\n",
      "Error on this batch = 0.48573461234537835\n",
      "Error on this batch = 0.49805784451809204\n",
      "Cost on val dataset after 165 epochs is = 0.49327985405161084\n",
      "learning rate =  0.03892494720807615\n",
      "Initial Cost on dataset for this epoch 165 = 0.49327985405161084\n",
      "Error on this batch = 0.48565569723787805\n",
      "Error on this batch = 0.49803091786505976\n",
      "Cost on val dataset after 166 epochs is = 0.49323881324315055\n",
      "learning rate =  0.03880752628531664\n",
      "Initial Cost on dataset for this epoch 166 = 0.49323881324315055\n",
      "Error on this batch = 0.4855752827646335\n",
      "Error on this batch = 0.4980030371316419\n",
      "Cost on val dataset after 167 epochs is = 0.4931967152596577\n",
      "learning rate =  0.03869116162670684\n",
      "Initial Cost on dataset for this epoch 167 = 0.4931967152596577\n",
      "Error on this batch = 0.4854929560777228\n",
      "Error on this batch = 0.49797404182007854\n",
      "Cost on val dataset after 168 epochs is = 0.49315353025639347\n",
      "learning rate =  0.03857583749052298\n",
      "Initial Cost on dataset for this epoch 168 = 0.49315353025639347\n",
      "Error on this batch = 0.4854089633936347\n",
      "Error on this batch = 0.49794409325643835\n",
      "Cost on val dataset after 169 epochs is = 0.4931092710193045\n",
      "learning rate =  0.038461538461538464\n",
      "Initial Cost on dataset for this epoch 169 = 0.4931092710193045\n",
      "Error on this batch = 0.4853232492615462\n",
      "Error on this batch = 0.4979130541953818\n",
      "Cost on val dataset after 170 epochs is = 0.49306379530772926\n",
      "learning rate =  0.03834824944236852\n",
      "Initial Cost on dataset for this epoch 170 = 0.49306379530772926\n",
      "Error on this batch = 0.4852359738133785\n",
      "Error on this batch = 0.4978809645334255\n",
      "Cost on val dataset after 171 epochs is = 0.4930171218072624\n",
      "learning rate =  0.038235955645093626\n",
      "Initial Cost on dataset for this epoch 171 = 0.4930171218072624\n",
      "Error on this batch = 0.48514691652139774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on this batch = 0.4978477310042395\n",
      "Cost on val dataset after 172 epochs is = 0.49296919185925553\n",
      "learning rate =  0.038124642583151166\n",
      "Initial Cost on dataset for this epoch 172 = 0.49296919185925553\n",
      "Error on this batch = 0.48505570121913866\n",
      "Error on this batch = 0.49781331719227495\n",
      "Cost on val dataset after 173 epochs is = 0.4929197204838961\n",
      "learning rate =  0.038014296063485276\n",
      "Initial Cost on dataset for this epoch 173 = 0.4929197204838961\n",
      "Error on this batch = 0.48496217508036554\n",
      "Error on this batch = 0.4977776322128878\n",
      "Cost on val dataset after 174 epochs is = 0.4928687787456167\n",
      "learning rate =  0.03790490217894517\n",
      "Initial Cost on dataset for this epoch 174 = 0.4928687787456167\n",
      "Error on this batch = 0.4848661857416218\n",
      "Error on this batch = 0.4977404838775721\n",
      "Cost on val dataset after 175 epochs is = 0.4928162610753039\n",
      "learning rate =  0.03779644730092272\n",
      "Initial Cost on dataset for this epoch 175 = 0.4928162610753039\n",
      "Error on this batch = 0.4847677925907488\n",
      "Error on this batch = 0.49770183668632895\n",
      "Cost on val dataset after 176 epochs is = 0.49276203953590003\n",
      "learning rate =  0.037688918072220454\n",
      "Initial Cost on dataset for this epoch 176 = 0.49276203953590003\n",
      "Error on this batch = 0.4846669570726784\n",
      "Error on this batch = 0.49766196018325565\n",
      "Cost on val dataset after 177 epochs is = 0.49270602286632253\n",
      "learning rate =  0.037582301400141446\n",
      "Initial Cost on dataset for this epoch 177 = 0.49270602286632253\n",
      "Error on this batch = 0.4845633800543027\n",
      "Error on this batch = 0.49762017099715944\n",
      "Cost on val dataset after 178 epochs is = 0.4926481648708541\n",
      "learning rate =  0.03747658444979307\n",
      "Initial Cost on dataset for this epoch 178 = 0.4926481648708541\n",
      "Error on this batch = 0.4844574311966915\n",
      "Error on this batch = 0.497576935021171\n",
      "Cost on val dataset after 179 epochs is = 0.49258840564752354\n",
      "learning rate =  0.037371754637596795\n",
      "Initial Cost on dataset for this epoch 179 = 0.49258840564752354\n",
      "Error on this batch = 0.484348538494825\n",
      "Error on this batch = 0.49753200943626896\n",
      "Cost on val dataset after 180 epochs is = 0.49252675214459496\n",
      "learning rate =  0.037267799624996496\n",
      "Initial Cost on dataset for this epoch 180 = 0.49252675214459496\n",
      "Error on this batch = 0.4842367401584555\n",
      "Error on this batch = 0.49748535014660633\n",
      "Cost on val dataset after 181 epochs is = 0.4924629494437123\n",
      "learning rate =  0.03716470731235832\n",
      "Initial Cost on dataset for this epoch 181 = 0.4924629494437123\n",
      "Error on this batch = 0.4841219734876245\n",
      "Error on this batch = 0.4974370270654562\n",
      "Cost on val dataset after 182 epochs is = 0.49239716812650286\n",
      "learning rate =  0.03706246583305506\n",
      "Initial Cost on dataset for this epoch 182 = 0.49239716812650286\n",
      "Error on this batch = 0.4840044220383984\n",
      "Error on this batch = 0.4973872537038166\n",
      "Cost on val dataset after 183 epochs is = 0.4923291385037995\n",
      "learning rate =  0.03696106354772864\n",
      "Initial Cost on dataset for this epoch 183 = 0.4923291385037995\n",
      "Error on this batch = 0.4838836268805457\n",
      "Error on this batch = 0.4973355539100747\n",
      "Cost on val dataset after 184 epochs is = 0.49225877882985347\n",
      "learning rate =  0.036860489038724284\n",
      "Initial Cost on dataset for this epoch 184 = 0.49225877882985347\n",
      "Error on this batch = 0.48375936349394716\n",
      "Error on this batch = 0.4972816157690654\n",
      "Cost on val dataset after 185 epochs is = 0.49218584470986737\n",
      "learning rate =  0.036760731104690386\n",
      "Initial Cost on dataset for this epoch 185 = 0.49218584470986737\n",
      "Error on this batch = 0.4836311577589345\n",
      "Error on this batch = 0.4972255242190121\n",
      "Cost on val dataset after 186 epochs is = 0.49211012569615276\n",
      "learning rate =  0.036661778755338326\n",
      "Initial Cost on dataset for this epoch 186 = 0.49211012569615276\n",
      "Error on this batch = 0.48349922609662926\n",
      "Error on this batch = 0.49716700294963084\n",
      "Cost on val dataset after 187 epochs is = 0.4920314325603195\n",
      "learning rate =  0.036563621206356534\n",
      "Initial Cost on dataset for this epoch 187 = 0.4920314325603195\n",
      "Error on this batch = 0.483362860539309\n",
      "Error on this batch = 0.4971060668574201\n",
      "Cost on val dataset after 188 epochs is = 0.49194953443305767\n",
      "learning rate =  0.03646624787447364\n",
      "Initial Cost on dataset for this epoch 188 = 0.49194953443305767\n",
      "Error on this batch = 0.4832218301099427\n",
      "Error on this batch = 0.4970424746192505\n",
      "Cost on val dataset after 189 epochs is = 0.491864264813155\n",
      "learning rate =  0.036369648372665396\n",
      "Initial Cost on dataset for this epoch 189 = 0.491864264813155\n",
      "Error on this batch = 0.4830762062188515\n",
      "Error on this batch = 0.4969761253620466\n",
      "Cost on val dataset after 190 epochs is = 0.4917753482443575\n",
      "learning rate =  0.03627381250550058\n",
      "Initial Cost on dataset for this epoch 190 = 0.4917753482443575\n",
      "Error on this batch = 0.4829253602570252\n",
      "Error on this batch = 0.4969067315053191\n",
      "Cost on val dataset after 191 epochs is = 0.4916825973697534\n",
      "learning rate =  0.03617873026462108\n",
      "Initial Cost on dataset for this epoch 191 = 0.4916825973697534\n",
      "Error on this batch = 0.4827690709031922\n",
      "Error on this batch = 0.4968335497625026\n",
      "Cost on val dataset after 192 epochs is = 0.49158599582452933\n",
      "learning rate =  0.036084391824351615\n",
      "Initial Cost on dataset for this epoch 192 = 0.49158599582452933\n",
      "Error on this batch = 0.48260656817411696\n",
      "Error on this batch = 0.4967574381960521\n",
      "Cost on val dataset after 193 epochs is = 0.49148519576818767\n",
      "learning rate =  0.035990787537434725\n",
      "Initial Cost on dataset for this epoch 193 = 0.49148519576818767\n",
      "Error on this batch = 0.48243852051854447\n",
      "Error on this batch = 0.49667733043939294\n",
      "Cost on val dataset after 194 epochs is = 0.491380433441611\n",
      "learning rate =  0.03589790793088691\n",
      "Initial Cost on dataset for this epoch 194 = 0.491380433441611\n",
      "Error on this batch = 0.4822647115826286\n",
      "Error on this batch = 0.4965939897990073\n",
      "Cost on val dataset after 195 epochs is = 0.4912717322540196\n",
      "learning rate =  0.03580574370197165\n",
      "Initial Cost on dataset for this epoch 195 = 0.4912717322540196\n",
      "Error on this batch = 0.48208560157322217\n",
      "Error on this batch = 0.49650709650671476\n",
      "Cost on val dataset after 196 epochs is = 0.4911592222337956\n",
      "learning rate =  0.03571428571428571\n",
      "Initial Cost on dataset for this epoch 196 = 0.4911592222337956\n",
      "Error on this batch = 0.48190170787436404\n",
      "Error on this batch = 0.4964172276187726\n",
      "Cost on val dataset after 197 epochs is = 0.4910432272887863\n",
      "learning rate =  0.035623524993954825\n",
      "Initial Cost on dataset for this epoch 197 = 0.4910432272887863\n",
      "Error on this batch = 0.48171315126330827\n",
      "Error on this batch = 0.49632406430323645\n",
      "Cost on val dataset after 198 epochs is = 0.49092365129698606\n",
      "learning rate =  0.035533452725935076\n",
      "Initial Cost on dataset for this epoch 198 = 0.49092365129698606\n",
      "Error on this batch = 0.4815197878712348\n",
      "Error on this batch = 0.496227562206672\n",
      "Cost on val dataset after 199 epochs is = 0.49080072582919954\n",
      "learning rate =  0.03544406025041679\n",
      "Initial Cost on dataset for this epoch 199 = 0.49080072582919954\n",
      "Error on this batch = 0.481321855048329\n",
      "Error on this batch = 0.49612808981273704\n",
      "Cost on val dataset after 200 epochs is = 0.49067469209762626\n",
      "learning rate =  0.035355339059327376\n",
      "Initial Cost on dataset for this epoch 200 = 0.49067469209762626\n",
      "Error on this batch = 0.4811195309203934\n",
      "Error on this batch = 0.4960256405220539\n",
      "Cost on val dataset after 201 epochs is = 0.4905456109282715\n",
      "learning rate =  0.035267280792929914\n",
      "Initial Cost on dataset for this epoch 201 = 0.4905456109282715\n",
      "Error on this batch = 0.48091318427803914\n",
      "Error on this batch = 0.4959204271080225\n",
      "Cost on val dataset after 202 epochs is = 0.4904137560901112\n",
      "learning rate =  0.03517987723651459\n",
      "Initial Cost on dataset for this epoch 202 = 0.4904137560901112\n",
      "Error on this batch = 0.4807032445548833\n",
      "Error on this batch = 0.4958126768277053\n",
      "Cost on val dataset after 203 epochs is = 0.4902791029410553\n",
      "learning rate =  0.03509312031717982\n",
      "Initial Cost on dataset for this epoch 203 = 0.4902791029410553\n",
      "Error on this batch = 0.48048994659088423\n",
      "Error on this batch = 0.49570146137798804\n",
      "Cost on val dataset after 204 epochs is = 0.49014183990334936\n",
      "learning rate =  0.03500700210070024\n",
      "Initial Cost on dataset for this epoch 204 = 0.49014183990334936\n",
      "Error on this batch = 0.48027340312937283\n",
      "Error on this batch = 0.4955874960597418\n",
      "Cost on val dataset after 205 epochs is = 0.49000188077453427\n",
      "learning rate =  0.03492151478847891\n",
      "Initial Cost on dataset for this epoch 205 = 0.49000188077453427\n",
      "Error on this batch = 0.4800541021661547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on this batch = 0.49547100367018204\n",
      "Cost on val dataset after 206 epochs is = 0.489859531910643\n",
      "learning rate =  0.034836650714580884\n",
      "Initial Cost on dataset for this epoch 206 = 0.489859531910643\n",
      "Error on this batch = 0.4798325316080728\n",
      "Error on this batch = 0.4953522883468962\n",
      "Cost on val dataset after 207 epochs is = 0.4897152541493147\n",
      "learning rate =  0.034752402342845795\n",
      "Initial Cost on dataset for this epoch 207 = 0.4897152541493147\n",
      "Error on this batch = 0.47960854032564676\n",
      "Error on this batch = 0.4952313080057735\n",
      "Cost on val dataset after 208 epochs is = 0.48956951624768913\n",
      "learning rate =  0.03466876226407682\n",
      "Initial Cost on dataset for this epoch 208 = 0.48956951624768913\n",
      "Error on this batch = 0.4793835892981591\n",
      "Error on this batch = 0.4951087940921018\n",
      "Cost on val dataset after 209 epochs is = 0.4894228188885598\n",
      "learning rate =  0.03458572319330373\n",
      "Initial Cost on dataset for this epoch 209 = 0.4894228188885598\n",
      "Error on this batch = 0.47915832129668817\n",
      "Error on this batch = 0.49498531009959773\n",
      "Cost on val dataset after 210 epochs is = 0.48927542538068264\n",
      "learning rate =  0.03450327796711771\n",
      "Initial Cost on dataset for this epoch 210 = 0.48927542538068264\n",
      "Error on this batch = 0.4789330981048537\n",
      "Error on this batch = 0.49486122586489345\n",
      "Cost on val dataset after 211 epochs is = 0.48912797449578843\n",
      "learning rate =  0.034421419541075714\n",
      "Initial Cost on dataset for this epoch 211 = 0.48912797449578843\n",
      "Error on this batch = 0.47870876795826806\n",
      "Error on this batch = 0.4947366718873876\n",
      "Cost on val dataset after 212 epochs is = 0.48898124861898695\n",
      "learning rate =  0.03434014098717226\n",
      "Initial Cost on dataset for this epoch 212 = 0.48898124861898695\n",
      "Error on this batch = 0.47848648767419655\n",
      "Error on this batch = 0.4946122186401328\n",
      "Cost on val dataset after 213 epochs is = 0.4888354817688637\n",
      "learning rate =  0.03425943549137658\n",
      "Initial Cost on dataset for this epoch 213 = 0.4888354817688637\n",
      "Error on this batch = 0.47826675688739223\n",
      "Error on this batch = 0.49448847660084094\n",
      "Cost on val dataset after 214 epochs is = 0.48869120770134045\n",
      "learning rate =  0.034179296351233165\n",
      "Initial Cost on dataset for this epoch 214 = 0.48869120770134045\n",
      "Error on this batch = 0.47804991757631565\n",
      "Error on this batch = 0.4943655795470705\n",
      "Cost on val dataset after 215 epochs is = 0.48854869219133995\n",
      "learning rate =  0.034099716973523674\n",
      "Initial Cost on dataset for this epoch 215 = 0.48854869219133995\n",
      "Error on this batch = 0.47783586492468216\n",
      "Error on this batch = 0.4942437218866973\n",
      "Cost on val dataset after 216 epochs is = 0.4884083047203623\n",
      "learning rate =  0.034020690871988585\n",
      "Initial Cost on dataset for this epoch 216 = 0.4884083047203623\n",
      "Error on this batch = 0.4776254719173015\n",
      "Error on this batch = 0.4941233513392767\n",
      "Cost on val dataset after 217 epochs is = 0.4882702447621125\n",
      "learning rate =  0.03394221166510653\n",
      "Initial Cost on dataset for this epoch 217 = 0.4882702447621125\n",
      "Error on this batch = 0.4774191795130004\n",
      "Error on this batch = 0.49400454480192196\n",
      "Cost on val dataset after 218 epochs is = 0.4881348335532956\n",
      "learning rate =  0.03386427307392982\n",
      "Initial Cost on dataset for this epoch 218 = 0.4881348335532956\n",
      "Error on this batch = 0.47721734179488884\n",
      "Error on this batch = 0.49388787910815884\n",
      "Cost on val dataset after 219 epochs is = 0.4880022974238873\n",
      "learning rate =  0.033786868919974296\n",
      "Initial Cost on dataset for this epoch 219 = 0.4880022974238873\n",
      "Error on this batch = 0.47701971450640507\n",
      "Error on this batch = 0.49377321998236584\n",
      "Cost on val dataset after 220 epochs is = 0.48787278895812985\n",
      "learning rate =  0.033709993123162106\n",
      "Initial Cost on dataset for this epoch 220 = 0.48787278895812985\n",
      "Error on this batch = 0.47682695036775746\n",
      "Error on this batch = 0.49366078775984035\n",
      "Cost on val dataset after 221 epochs is = 0.48774646732204435\n",
      "learning rate =  0.03363363969981562\n",
      "Initial Cost on dataset for this epoch 221 = 0.48774646732204435\n",
      "Error on this batch = 0.47663934344760195\n",
      "Error on this batch = 0.493551174733557\n",
      "Cost on val dataset after 222 epochs is = 0.4876234544839849\n",
      "learning rate =  0.033557802760701215\n",
      "Initial Cost on dataset for this epoch 222 = 0.4876234544839849\n",
      "Error on this batch = 0.47645679464493035\n",
      "Error on this batch = 0.4934440692479817\n",
      "Cost on val dataset after 223 epochs is = 0.48750372780494655\n",
      "learning rate =  0.033482476509121256\n",
      "Initial Cost on dataset for this epoch 223 = 0.48750372780494655\n",
      "Error on this batch = 0.4762791724067185\n",
      "Error on this batch = 0.49333975384878276\n",
      "Cost on val dataset after 224 epochs is = 0.4873875549644613\n",
      "learning rate =  0.03340765523905305\n",
      "Initial Cost on dataset for this epoch 224 = 0.4873875549644613\n",
      "Error on this batch = 0.4761064231660332\n",
      "Error on this batch = 0.4932383858603486\n",
      "Cost on val dataset after 225 epochs is = 0.4872748216171115\n",
      "learning rate =  0.03333333333333333\n",
      "Initial Cost on dataset for this epoch 225 = 0.4872748216171115\n",
      "Error on this batch = 0.4759388523277068\n",
      "Error on this batch = 0.49313989662569085\n",
      "Cost on val dataset after 226 epochs is = 0.4871654995185754\n",
      "learning rate =  0.033259505261886965\n",
      "Initial Cost on dataset for this epoch 226 = 0.4871654995185754\n",
      "Error on this batch = 0.47577623170856653\n",
      "Error on this batch = 0.4930444324644111\n",
      "Cost on val dataset after 227 epochs is = 0.48705976023355463\n",
      "learning rate =  0.0331861655799986\n",
      "Initial Cost on dataset for this epoch 227 = 0.48705976023355463\n",
      "Error on this batch = 0.4756186700028215\n",
      "Error on this batch = 0.4929520114071151\n",
      "Cost on val dataset after 228 epochs is = 0.48695731269494696\n",
      "learning rate =  0.033113308926626096\n",
      "Initial Cost on dataset for this epoch 228 = 0.48695731269494696\n",
      "Error on this batch = 0.475465814411602\n",
      "Error on this batch = 0.49286250362174316\n",
      "Cost on val dataset after 229 epochs is = 0.486858178523059\n",
      "learning rate =  0.03304093002275449\n",
      "Initial Cost on dataset for this epoch 229 = 0.486858178523059\n",
      "Error on this batch = 0.47531771620721314\n",
      "Error on this batch = 0.49277585496243864\n",
      "Cost on val dataset after 230 epochs is = 0.4867622724635183\n",
      "learning rate =  0.03296902366978935\n",
      "Initial Cost on dataset for this epoch 230 = 0.4867622724635183\n",
      "Error on this batch = 0.4751741291203982\n",
      "Error on this batch = 0.4926921192544836\n",
      "Cost on val dataset after 231 epochs is = 0.4866695474855293\n",
      "learning rate =  0.03289758474798845\n",
      "Initial Cost on dataset for this epoch 231 = 0.4866695474855293\n",
      "Error on this batch = 0.47503532095339024\n",
      "Error on this batch = 0.49261132198420726\n",
      "Cost on val dataset after 232 epochs is = 0.4865800180574512\n",
      "learning rate =  0.032826608214930636\n",
      "Initial Cost on dataset for this epoch 232 = 0.4865800180574512\n",
      "Error on this batch = 0.4749011137494811\n",
      "Error on this batch = 0.4925334046402198\n",
      "Cost on val dataset after 233 epochs is = 0.48649350414424214\n",
      "learning rate =  0.03275608910402092\n",
      "Initial Cost on dataset for this epoch 233 = 0.48649350414424214\n",
      "Error on this batch = 0.4747709800429009\n",
      "Error on this batch = 0.49245822374810333\n",
      "Cost on val dataset after 234 epochs is = 0.4864099124816072\n",
      "learning rate =  0.03268602252303067\n",
      "Initial Cost on dataset for this epoch 234 = 0.4864099124816072\n",
      "Error on this batch = 0.4746450000284256\n",
      "Error on this batch = 0.49238578646429076\n",
      "Cost on val dataset after 235 epochs is = 0.48632910610232527\n",
      "learning rate =  0.03261640365267211\n",
      "Initial Cost on dataset for this epoch 235 = 0.48632910610232527\n",
      "Error on this batch = 0.4745227634013521\n",
      "Error on this batch = 0.4923158110048565\n",
      "Cost on val dataset after 236 epochs is = 0.48625112890382877\n",
      "learning rate =  0.03254722774520597\n",
      "Initial Cost on dataset for this epoch 236 = 0.48625112890382877\n",
      "Error on this batch = 0.4744044366269485\n",
      "Error on this batch = 0.4922485497531694\n",
      "Cost on val dataset after 237 epochs is = 0.4861758160860657\n",
      "learning rate =  0.032478490123081544\n",
      "Initial Cost on dataset for this epoch 237 = 0.4861758160860657\n",
      "Error on this batch = 0.47428973361034865\n",
      "Error on this batch = 0.4921837777308991\n",
      "Cost on val dataset after 238 epochs is = 0.486103040262359\n",
      "learning rate =  0.03241018617760822\n",
      "Initial Cost on dataset for this epoch 238 = 0.486103040262359\n",
      "Error on this batch = 0.47417858098352583\n",
      "Error on this batch = 0.49212125762239683\n",
      "Cost on val dataset after 239 epochs is = 0.4860326907078225\n",
      "learning rate =  0.03234231136765754\n",
      "Initial Cost on dataset for this epoch 239 = 0.4860326907078225\n",
      "Error on this batch = 0.4740705805476397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on this batch = 0.49206095537759326\n",
      "Cost on val dataset after 240 epochs is = 0.48596470050377366\n",
      "learning rate =  0.03227486121839514\n",
      "Initial Cost on dataset for this epoch 240 = 0.48596470050377366\n",
      "Error on this batch = 0.4739657219040933\n",
      "Error on this batch = 0.4920028638969535\n",
      "Cost on val dataset after 241 epochs is = 0.48589897713229707\n",
      "learning rate =  0.03220783132004154\n",
      "Initial Cost on dataset for this epoch 241 = 0.48589897713229707\n",
      "Error on this batch = 0.4738639436490294\n",
      "Error on this batch = 0.49194692655953093\n",
      "Cost on val dataset after 242 epochs is = 0.4858353720745522\n",
      "learning rate =  0.03214121732666125\n",
      "Initial Cost on dataset for this epoch 242 = 0.4858353720745522\n",
      "Error on this batch = 0.47376520577556713\n",
      "Error on this batch = 0.4918930153217293\n",
      "Cost on val dataset after 243 epochs is = 0.48577378165895196\n",
      "learning rate =  0.032075014954979206\n",
      "Initial Cost on dataset for this epoch 243 = 0.48577378165895196\n",
      "Error on this batch = 0.47366943540171386\n",
      "Error on this batch = 0.4918410665469186\n",
      "Cost on val dataset after 244 epochs is = 0.4857140195797161\n",
      "learning rate =  0.032009219983223994\n",
      "Initial Cost on dataset for this epoch 244 = 0.4857140195797161\n",
      "Error on this batch = 0.47357627193989393\n",
      "Error on this batch = 0.4917910169121084\n",
      "Cost on val dataset after 245 epochs is = 0.48565615917572413\n",
      "learning rate =  0.031943828249996996\n",
      "Initial Cost on dataset for this epoch 245 = 0.48565615917572413\n",
      "Error on this batch = 0.47348564704678625\n",
      "Error on this batch = 0.4917428838011641\n",
      "Cost on val dataset after 246 epochs is = 0.4856002858922585\n",
      "learning rate =  0.03187883565316691\n",
      "Initial Cost on dataset for this epoch 246 = 0.4856002858922585\n",
      "Error on this batch = 0.47339729935122743\n",
      "Error on this batch = 0.4916964939597388\n",
      "Cost on val dataset after 247 epochs is = 0.4855462355018399\n",
      "learning rate =  0.031814238148788886\n",
      "Initial Cost on dataset for this epoch 247 = 0.4855462355018399\n",
      "Error on this batch = 0.4733112644157839\n",
      "Error on this batch = 0.49165192235504246\n",
      "Cost on val dataset after 248 epochs is = 0.48549383405793495\n",
      "learning rate =  0.031750031750047626\n",
      "Initial Cost on dataset for this epoch 248 = 0.48549383405793495\n",
      "Error on this batch = 0.47322744823168267\n",
      "Error on this batch = 0.49160905015792167\n",
      "Cost on val dataset after 249 epochs is = 0.4854429592682157\n",
      "learning rate =  0.031686212526223896\n",
      "Initial Cost on dataset for this epoch 249 = 0.4854429592682157\n",
      "Error on this batch = 0.4731458466340878\n",
      "Error on this batch = 0.4915676276396569\n",
      "Cost on val dataset after 250 epochs is = 0.4853936843180985\n",
      "learning rate =  0.03162277660168379\n",
      "Initial Cost on dataset for this epoch 250 = 0.4853936843180985\n",
      "Error on this batch = 0.47306645278447534\n",
      "Error on this batch = 0.4915278565468942\n",
      "Cost on val dataset after 251 epochs is = 0.48534586221901876\n",
      "learning rate =  0.031559720154890156\n",
      "Initial Cost on dataset for this epoch 251 = 0.48534586221901876\n",
      "Error on this batch = 0.47298911756354023\n",
      "Error on this batch = 0.49148962780462085\n",
      "Cost on val dataset after 252 epochs is = 0.4852994518752257\n",
      "learning rate =  0.0314970394174356\n",
      "Initial Cost on dataset for this epoch 252 = 0.4852994518752257\n",
      "Error on this batch = 0.4729138257373828\n",
      "Error on this batch = 0.49145271608124436\n",
      "Cost on val dataset after 253 epochs is = 0.48525442425261833\n",
      "learning rate =  0.03143473067309657\n",
      "Initial Cost on dataset for this epoch 253 = 0.48525442425261833\n",
      "Error on this batch = 0.4728404081282902\n",
      "Error on this batch = 0.49141730343324086\n",
      "Cost on val dataset after 254 epochs is = 0.48521062960992006\n",
      "learning rate =  0.03137279025690793\n",
      "Initial Cost on dataset for this epoch 254 = 0.48521062960992006\n",
      "Error on this batch = 0.4727685515025445\n",
      "Error on this batch = 0.49138275047311797\n",
      "Cost on val dataset after 255 epochs is = 0.4851681357087255\n",
      "learning rate =  0.03131121455425748\n",
      "Initial Cost on dataset for this epoch 255 = 0.4851681357087255\n",
      "Error on this batch = 0.47269838475586684\n",
      "Error on this batch = 0.49134952815193983\n",
      "Cost on val dataset after 256 epochs is = 0.48512685274573913\n",
      "learning rate =  0.03125\n",
      "Initial Cost on dataset for this epoch 256 = 0.48512685274573913\n",
      "Error on this batch = 0.4726297795218596\n",
      "Error on this batch = 0.49131742747970863\n",
      "Cost on val dataset after 257 epochs is = 0.48508670471318593\n",
      "learning rate =  0.031189143077590267\n",
      "Initial Cost on dataset for this epoch 257 = 0.48508670471318593\n",
      "Error on this batch = 0.4725628714506674\n",
      "Error on this batch = 0.49128646414955457\n",
      "Cost on val dataset after 258 epochs is = 0.4850476362284054\n",
      "learning rate =  0.031128640318234518\n",
      "Initial Cost on dataset for this epoch 258 = 0.4850476362284054\n",
      "Error on this batch = 0.47249749291224263\n",
      "Error on this batch = 0.4912565647150626\n",
      "Cost on val dataset after 259 epochs is = 0.48500963438259304\n",
      "learning rate =  0.031068488300060003\n",
      "Initial Cost on dataset for this epoch 259 = 0.48500963438259304\n",
      "Error on this batch = 0.4724337797517373\n",
      "Error on this batch = 0.491227671474378\n",
      "Cost on val dataset after 260 epochs is = 0.48497264917457245\n",
      "learning rate =  0.031008683647302117\n",
      "Initial Cost on dataset for this epoch 260 = 0.48497264917457245\n",
      "Error on this batch = 0.4723714485353549\n",
      "Error on this batch = 0.49119983626792346\n",
      "Cost on val dataset after 261 epochs is = 0.48493662789925984\n",
      "learning rate =  0.030949223029508647\n",
      "Initial Cost on dataset for this epoch 261 = 0.48493662789925984\n",
      "Error on this batch = 0.47231053542253326\n",
      "Error on this batch = 0.491172934808542\n",
      "Cost on val dataset after 262 epochs is = 0.484901564819016\n",
      "learning rate =  0.03089010316076077\n",
      "Initial Cost on dataset for this epoch 262 = 0.484901564819016\n",
      "Error on this batch = 0.4722511171455716\n",
      "Error on this batch = 0.49114689505406367\n",
      "Cost on val dataset after 263 epochs is = 0.4848674425896897\n",
      "learning rate =  0.030831320798910367\n",
      "Initial Cost on dataset for this epoch 263 = 0.4848674425896897\n",
      "Error on this batch = 0.4721929103499723\n",
      "Error on this batch = 0.49112197671217545\n",
      "Cost on val dataset after 264 epochs is = 0.4848341752503272\n",
      "learning rate =  0.03077287274483318\n",
      "Initial Cost on dataset for this epoch 264 = 0.4848341752503272\n",
      "Error on this batch = 0.4721360142211535\n",
      "Error on this batch = 0.49109812486447035\n",
      "Cost on val dataset after 265 epochs is = 0.4848017614950434\n",
      "learning rate =  0.03071475584169756\n",
      "Initial Cost on dataset for this epoch 265 = 0.4848017614950434\n",
      "Error on this batch = 0.4720806320219102\n",
      "Error on this batch = 0.49107507492068153\n",
      "Cost on val dataset after 266 epochs is = 0.48477018740239425\n",
      "learning rate =  0.03065696697424829\n",
      "Initial Cost on dataset for this epoch 266 = 0.48477018740239425\n",
      "Error on this batch = 0.4720265042447479\n",
      "Error on this batch = 0.4910529073753583\n",
      "Cost on val dataset after 267 epochs is = 0.4847393865931565\n",
      "learning rate =  0.03059950306810523\n",
      "Initial Cost on dataset for this epoch 267 = 0.4847393865931565\n",
      "Error on this batch = 0.47197359884630546\n",
      "Error on this batch = 0.49103141321872773\n",
      "Cost on val dataset after 268 epochs is = 0.4847093608769438\n",
      "learning rate =  0.030542361089076306\n",
      "Initial Cost on dataset for this epoch 268 = 0.4847093608769438\n",
      "Error on this batch = 0.4719218409076945\n",
      "Error on this batch = 0.49101067432048234\n",
      "Cost on val dataset after 269 epochs is = 0.4846800470730132\n",
      "learning rate =  0.030485538042484616\n",
      "Initial Cost on dataset for this epoch 269 = 0.4846800470730132\n",
      "Error on this batch = 0.4718711656295878\n",
      "Error on this batch = 0.49099047494541376\n",
      "Cost on val dataset after 270 epochs is = 0.484651476179658\n",
      "learning rate =  0.03042903097250923\n",
      "Initial Cost on dataset for this epoch 270 = 0.484651476179658\n",
      "Error on this batch = 0.4718217155809738\n",
      "Error on this batch = 0.4909709974595234\n",
      "Cost on val dataset after 271 epochs is = 0.4846235585877616\n",
      "learning rate =  0.03037283696153935\n",
      "Initial Cost on dataset for this epoch 271 = 0.4846235585877616\n",
      "Error on this batch = 0.471773297118053\n",
      "Error on this batch = 0.49095202825243256\n",
      "Cost on val dataset after 272 epochs is = 0.4845963270663025\n",
      "learning rate =  0.03031695312954162\n",
      "Initial Cost on dataset for this epoch 272 = 0.4845963270663025\n",
      "Error on this batch = 0.47172592591172574\n",
      "Error on this batch = 0.4909336941894914\n",
      "Cost on val dataset after 273 epochs is = 0.48456970623889367\n",
      "learning rate =  0.030261376633440123\n",
      "Initial Cost on dataset for this epoch 273 = 0.48456970623889367\n",
      "Error on this batch = 0.4716795593621809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on this batch = 0.490915853311557\n",
      "Cost on val dataset after 274 epochs is = 0.4845437150961798\n",
      "learning rate =  0.03020610466650885\n",
      "Initial Cost on dataset for this epoch 274 = 0.4845437150961798\n",
      "Error on this batch = 0.47163418008187735\n",
      "Error on this batch = 0.49089856385467073\n",
      "Cost on val dataset after 275 epochs is = 0.4845183123569713\n",
      "learning rate =  0.030151134457776358\n",
      "Initial Cost on dataset for this epoch 275 = 0.4845183123569713\n",
      "Error on this batch = 0.4715897665979689\n",
      "Error on this batch = 0.4908817885961693\n",
      "Cost on val dataset after 276 epochs is = 0.4844934784041603\n",
      "learning rate =  0.0300964632714423\n",
      "Initial Cost on dataset for this epoch 276 = 0.4844934784041603\n",
      "Error on this batch = 0.4715462200886432\n",
      "Error on this batch = 0.49086547768836997\n",
      "Cost on val dataset after 277 epochs is = 0.48446922552918786\n",
      "learning rate =  0.030042088406305484\n",
      "Initial Cost on dataset for this epoch 277 = 0.48446922552918786\n",
      "Error on this batch = 0.4715036285741544\n",
      "Error on this batch = 0.490849645962027\n",
      "Cost on val dataset after 278 epochs is = 0.48444554226072284\n",
      "learning rate =  0.02998800719520336\n",
      "Initial Cost on dataset for this epoch 278 = 0.48444554226072284\n",
      "Error on this batch = 0.47146190965746976\n",
      "Error on this batch = 0.4908343180132902\n",
      "Cost on val dataset after 279 epochs is = 0.48442233210597563\n",
      "learning rate =  0.02993421700446248\n",
      "Initial Cost on dataset for this epoch 279 = 0.48442233210597563\n",
      "Error on this batch = 0.47142098279764943\n",
      "Error on this batch = 0.49081936031473744\n",
      "Cost on val dataset after 280 epochs is = 0.4843996471699996\n",
      "learning rate =  0.02988071523335984\n",
      "Initial Cost on dataset for this epoch 280 = 0.4843996471699996\n",
      "Error on this batch = 0.47138093140500403\n",
      "Error on this batch = 0.49080486090119807\n",
      "Cost on val dataset after 281 epochs is = 0.4843774415701746\n",
      "learning rate =  0.02982749931359468\n",
      "Initial Cost on dataset for this epoch 281 = 0.4843774415701746\n",
      "Error on this batch = 0.47134170228575223\n",
      "Error on this batch = 0.49079076459283416\n",
      "Cost on val dataset after 282 epochs is = 0.4843557044294844\n",
      "learning rate =  0.029774566708770683\n",
      "Initial Cost on dataset for this epoch 282 = 0.4843557044294844\n",
      "Error on this batch = 0.47130322788780266\n",
      "Error on this batch = 0.4907770385305668\n",
      "Cost on val dataset after 283 epochs is = 0.48433439357100694\n",
      "learning rate =  0.0297219149138882\n",
      "Initial Cost on dataset for this epoch 283 = 0.48433439357100694\n",
      "Error on this batch = 0.47126553912960234\n",
      "Error on this batch = 0.4907637136945971\n",
      "Cost on val dataset after 284 epochs is = 0.4843135209802779\n",
      "learning rate =  0.02966954145484633\n",
      "Initial Cost on dataset for this epoch 284 = 0.4843135209802779\n",
      "Error on this batch = 0.47122863384137986\n",
      "Error on this batch = 0.4907507640713773\n",
      "Cost on val dataset after 285 epochs is = 0.48429308736362847\n",
      "learning rate =  0.029617443887954616\n",
      "Initial Cost on dataset for this epoch 285 = 0.48429308736362847\n",
      "Error on this batch = 0.47119247449719104\n",
      "Error on this batch = 0.4907381377254963\n",
      "Cost on val dataset after 286 epochs is = 0.48427305817877137\n",
      "learning rate =  0.02956561979945413\n",
      "Initial Cost on dataset for this epoch 286 = 0.48427305817877137\n",
      "Error on this batch = 0.4711569409070886\n",
      "Error on this batch = 0.4907258358019365\n",
      "Cost on val dataset after 287 epochs is = 0.4842534551858409\n",
      "learning rate =  0.029514066805047763\n",
      "Initial Cost on dataset for this epoch 287 = 0.4842534551858409\n",
      "Error on this batch = 0.47112203078248444\n",
      "Error on this batch = 0.49071387523713994\n",
      "Cost on val dataset after 288 epochs is = 0.4842342290799506\n",
      "learning rate =  0.029462782549439483\n",
      "Initial Cost on dataset for this epoch 288 = 0.4842342290799506\n",
      "Error on this batch = 0.4710878197656018\n",
      "Error on this batch = 0.4907022253409028\n",
      "Cost on val dataset after 289 epochs is = 0.4842153910716322\n",
      "learning rate =  0.029411764705882353\n",
      "Initial Cost on dataset for this epoch 289 = 0.4842153910716322\n",
      "Error on this batch = 0.47105424722220063\n",
      "Error on this batch = 0.4906908645140174\n",
      "Cost on val dataset after 290 epochs is = 0.4841969175792651\n",
      "learning rate =  0.029361010975735173\n",
      "Initial Cost on dataset for this epoch 290 = 0.4841969175792651\n",
      "Error on this batch = 0.4710213433228174\n",
      "Error on this batch = 0.49067980978714715\n",
      "Cost on val dataset after 291 epochs is = 0.48417880397589463\n",
      "learning rate =  0.02931051908802746\n",
      "Initial Cost on dataset for this epoch 291 = 0.48417880397589463\n",
      "Error on this batch = 0.47098909245889076\n",
      "Error on this batch = 0.49066901916908295\n",
      "Cost on val dataset after 292 epochs is = 0.4841610539602577\n",
      "learning rate =  0.029260286799032642\n",
      "Initial Cost on dataset for this epoch 292 = 0.4841610539602577\n",
      "Error on this batch = 0.47095744510793297\n",
      "Error on this batch = 0.49065852662031545\n",
      "Cost on val dataset after 293 epochs is = 0.48414362866746885\n",
      "learning rate =  0.0292103118918493\n",
      "Initial Cost on dataset for this epoch 293 = 0.48414362866746885\n",
      "Error on this batch = 0.47092635849799336\n",
      "Error on this batch = 0.4906482730979794\n",
      "Cost on val dataset after 294 epochs is = 0.4841265318304901\n",
      "learning rate =  0.029160592175990215\n",
      "Initial Cost on dataset for this epoch 294 = 0.4841265318304901\n",
      "Error on this batch = 0.4708958099836802\n",
      "Error on this batch = 0.4906382612819561\n",
      "Cost on val dataset after 295 epochs is = 0.4841097704494725\n",
      "learning rate =  0.0291111254869791\n",
      "Initial Cost on dataset for this epoch 295 = 0.4841097704494725\n",
      "Error on this batch = 0.4708658026382222\n",
      "Error on this batch = 0.4906285097359629\n",
      "Cost on val dataset after 296 epochs is = 0.48409333059645077\n",
      "learning rate =  0.02906190968595482\n",
      "Initial Cost on dataset for this epoch 296 = 0.48409333059645077\n",
      "Error on this batch = 0.47083637189881744\n",
      "Error on this batch = 0.4906190056608864\n",
      "Cost on val dataset after 297 epochs is = 0.48407719628518353\n",
      "learning rate =  0.029012942659282972\n",
      "Initial Cost on dataset for this epoch 297 = 0.48407719628518353\n",
      "Error on this batch = 0.47080746857322675\n",
      "Error on this batch = 0.49060971729595926\n",
      "Cost on val dataset after 298 epochs is = 0.4840613497368429\n",
      "learning rate =  0.028964222318174613\n",
      "Initial Cost on dataset for this epoch 298 = 0.4840613497368429\n",
      "Error on this batch = 0.47077898725138745\n",
      "Error on this batch = 0.4906006589698235\n",
      "Cost on val dataset after 299 epochs is = 0.484045778351862\n",
      "learning rate =  0.02891574659831201\n",
      "Initial Cost on dataset for this epoch 299 = 0.484045778351862\n",
      "Error on this batch = 0.4707509945804466\n",
      "Error on this batch = 0.49059179391819857\n",
      "Cost on val dataset after 300 epochs is = 0.48403049072665133\n",
      "learning rate =  0.028867513459481284\n",
      "Initial Cost on dataset for this epoch 300 = 0.48403049072665133\n",
      "Error on this batch = 0.4707234920130089\n",
      "Error on this batch = 0.49058313905489753\n",
      "Cost on val dataset after 301 epochs is = 0.48401545247280614\n",
      "learning rate =  0.02881952088521175\n",
      "Initial Cost on dataset for this epoch 301 = 0.48401545247280614\n",
      "Error on this batch = 0.4706963816328625\n",
      "Error on this batch = 0.49057466228415986\n",
      "Cost on val dataset after 302 epochs is = 0.4840006889377146\n",
      "learning rate =  0.0287717668824218\n",
      "Initial Cost on dataset for this epoch 302 = 0.4840006889377146\n",
      "Error on this batch = 0.4706698261051366\n",
      "Error on this batch = 0.4905663897603524\n",
      "Cost on val dataset after 303 epochs is = 0.4839861843137919\n",
      "learning rate =  0.0287242494810713\n",
      "Initial Cost on dataset for this epoch 303 = 0.4839861843137919\n",
      "Error on this batch = 0.47064382007107297\n",
      "Error on this batch = 0.4905583179251741\n",
      "Cost on val dataset after 304 epochs is = 0.4839719207886761\n",
      "learning rate =  0.028676966733820218\n",
      "Initial Cost on dataset for this epoch 304 = 0.4839719207886761\n",
      "Error on this batch = 0.4706182506203018\n",
      "Error on this batch = 0.4905504149967741\n",
      "Cost on val dataset after 305 epochs is = 0.48395791571053276\n",
      "learning rate =  0.028629916715693413\n",
      "Initial Cost on dataset for this epoch 305 = 0.48395791571053276\n",
      "Error on this batch = 0.47059313492238125\n",
      "Error on this batch = 0.49054268439924387\n",
      "Cost on val dataset after 306 epochs is = 0.4839441548728752\n",
      "learning rate =  0.028583097523751475\n",
      "Initial Cost on dataset for this epoch 306 = 0.4839441548728752\n",
      "Error on this batch = 0.4705684307655731\n",
      "Error on this batch = 0.4905351204968866\n",
      "Cost on val dataset after 307 epochs is = 0.4839306099906693\n",
      "learning rate =  0.02853650727676748\n",
      "Initial Cost on dataset for this epoch 307 = 0.4839306099906693\n",
      "Error on this batch = 0.47054405575919006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on this batch = 0.49052775128715315\n",
      "Cost on val dataset after 308 epochs is = 0.48391728236945636\n",
      "learning rate =  0.028490144114909487\n",
      "Initial Cost on dataset for this epoch 308 = 0.48391728236945636\n",
      "Error on this batch = 0.47052009127691186\n",
      "Error on this batch = 0.49052051089631604\n",
      "Cost on val dataset after 309 epochs is = 0.4839041873083247\n",
      "learning rate =  0.028444006199428714\n",
      "Initial Cost on dataset for this epoch 309 = 0.4839041873083247\n",
      "Error on this batch = 0.47049648143770184\n",
      "Error on this batch = 0.49051344456271795\n",
      "Cost on val dataset after 310 epochs is = 0.48389130021793386\n",
      "learning rate =  0.02839809171235324\n",
      "Initial Cost on dataset for this epoch 310 = 0.48389130021793386\n",
      "Error on this batch = 0.4704733083613665\n",
      "Error on this batch = 0.4905065292092749\n",
      "Cost on val dataset after 311 epochs is = 0.48387862640214724\n",
      "learning rate =  0.028352398856187136\n",
      "Initial Cost on dataset for this epoch 311 = 0.48387862640214724\n",
      "Error on this batch = 0.4704505263714438\n",
      "Error on this batch = 0.49049974189376744\n",
      "Cost on val dataset after 312 epochs is = 0.4838661634277489\n",
      "learning rate =  0.02830692585361489\n",
      "Initial Cost on dataset for this epoch 312 = 0.4838661634277489\n",
      "Error on this batch = 0.47042806415993227\n",
      "Error on this batch = 0.4904931275925178\n",
      "Cost on val dataset after 313 epochs is = 0.48385387864700263\n",
      "learning rate =  0.028261670947211076\n",
      "Initial Cost on dataset for this epoch 313 = 0.48385387864700263\n",
      "Error on this batch = 0.47040596516089334\n",
      "Error on this batch = 0.49048666009569736\n",
      "Cost on val dataset after 314 epochs is = 0.4838417762039662\n",
      "learning rate =  0.028216632399155017\n",
      "Initial Cost on dataset for this epoch 314 = 0.4838417762039662\n",
      "Error on this batch = 0.4703842537647674\n",
      "Error on this batch = 0.49048028944607863\n",
      "Cost on val dataset after 315 epochs is = 0.48382988854216247\n",
      "learning rate =  0.02817180849095055\n",
      "Initial Cost on dataset for this epoch 315 = 0.48382988854216247\n",
      "Error on this batch = 0.470362903760569\n",
      "Error on this batch = 0.49047406510674324\n",
      "Cost on val dataset after 316 epochs is = 0.4838181698653533\n",
      "learning rate =  0.0281271975231506\n",
      "Initial Cost on dataset for this epoch 316 = 0.4838181698653533\n",
      "Error on this batch = 0.470341857552781\n",
      "Error on this batch = 0.4904679635693826\n",
      "Cost on val dataset after 317 epochs is = 0.48380665079003116\n",
      "learning rate =  0.02808279781508652\n",
      "Initial Cost on dataset for this epoch 317 = 0.48380665079003116\n",
      "Error on this batch = 0.4703211956595754\n",
      "Error on this batch = 0.4904619889907177\n",
      "Cost on val dataset after 318 epochs is = 0.4837953107449567\n",
      "learning rate =  0.028038607704602217\n",
      "Initial Cost on dataset for this epoch 318 = 0.4837953107449567\n",
      "Error on this batch = 0.4703008862965391\n",
      "Error on this batch = 0.49045613624526535\n",
      "Cost on val dataset after 319 epochs is = 0.4837841471501808\n",
      "learning rate =  0.027994625547792716\n",
      "Initial Cost on dataset for this epoch 319 = 0.4837841471501808\n",
      "Error on this batch = 0.47028082343201816\n",
      "Error on this batch = 0.49045041457436855\n",
      "Cost on val dataset after 320 epochs is = 0.4837731370316046\n",
      "learning rate =  0.02795084971874737\n",
      "Initial Cost on dataset for this epoch 320 = 0.4837731370316046\n",
      "Error on this batch = 0.4702611048690658\n",
      "Error on this batch = 0.4904447969372436\n",
      "Cost on val dataset after 321 epochs is = 0.48376231282114973\n",
      "learning rate =  0.02790727860929738\n",
      "Initial Cost on dataset for this epoch 321 = 0.48376231282114973\n",
      "Error on this batch = 0.4702417183348821\n",
      "Error on this batch = 0.49043929025859007\n",
      "Cost on val dataset after 322 epochs is = 0.4837516495180664\n",
      "learning rate =  0.02786391062876764\n",
      "Initial Cost on dataset for this epoch 322 = 0.4837516495180664\n",
      "Error on this batch = 0.47022258972048925\n",
      "Error on this batch = 0.4904338803692614\n",
      "Cost on val dataset after 323 epochs is = 0.483741149846141\n",
      "learning rate =  0.027820744203732862\n",
      "Initial Cost on dataset for this epoch 323 = 0.483741149846141\n",
      "Error on this batch = 0.4702037453045125\n",
      "Error on this batch = 0.4904285770477537\n",
      "Cost on val dataset after 324 epochs is = 0.48373080069400265\n",
      "learning rate =  0.027777777777777776\n",
      "Initial Cost on dataset for this epoch 324 = 0.48373080069400265\n",
      "Error on this batch = 0.47018516831033086\n",
      "Error on this batch = 0.4904233912361295\n",
      "Cost on val dataset after 325 epochs is = 0.4837206095752199\n",
      "learning rate =  0.027735009811261455\n",
      "Initial Cost on dataset for this epoch 325 = 0.4837206095752199\n",
      "Error on this batch = 0.47016687392837525\n",
      "Error on this batch = 0.4904182909987482\n",
      "Cost on val dataset after 326 epochs is = 0.48371056885409325\n",
      "learning rate =  0.027692438781085564\n",
      "Initial Cost on dataset for this epoch 326 = 0.48371056885409325\n",
      "Error on this batch = 0.47014886106297377\n",
      "Error on this batch = 0.49041330264469407\n",
      "Cost on val dataset after 327 epochs is = 0.48370066872794887\n",
      "learning rate =  0.02765006318046655\n",
      "Initial Cost on dataset for this epoch 327 = 0.48370066872794887\n",
      "Error on this batch = 0.47013107417812744\n",
      "Error on this batch = 0.4904083747284957\n",
      "Cost on val dataset after 328 epochs is = 0.48369093568160243\n",
      "learning rate =  0.027607881518711633\n",
      "Initial Cost on dataset for this epoch 328 = 0.48369093568160243\n",
      "Error on this batch = 0.47011351039555027\n",
      "Error on this batch = 0.49040356270557467\n",
      "Cost on val dataset after 329 epochs is = 0.4836813419484592\n",
      "learning rate =  0.027565892320998563\n",
      "Initial Cost on dataset for this epoch 329 = 0.4836813419484592\n",
      "Error on this batch = 0.4700961904097414\n",
      "Error on this batch = 0.49039883746241375\n",
      "Cost on val dataset after 330 epochs is = 0.483671898810534\n",
      "learning rate =  0.027524094128159017\n",
      "Initial Cost on dataset for this epoch 330 = 0.483671898810534\n",
      "Error on this batch = 0.47007910871771474\n",
      "Error on this batch = 0.4903942286103103\n",
      "Cost on val dataset after 331 epochs is = 0.4836625623736992\n",
      "learning rate =  0.027482485496465633\n",
      "Initial Cost on dataset for this epoch 331 = 0.4836625623736992\n",
      "Error on this batch = 0.47006226387334193\n",
      "Error on this batch = 0.4903896745540282\n",
      "Cost on val dataset after 332 epochs is = 0.4836533753027227\n",
      "learning rate =  0.027441064997422587\n",
      "Initial Cost on dataset for this epoch 332 = 0.4836533753027227\n",
      "Error on this batch = 0.4700456975481979\n",
      "Error on this batch = 0.49038521389940615\n",
      "Cost on val dataset after 333 epochs is = 0.48364431753833353\n",
      "learning rate =  0.02739983121755955\n",
      "Initial Cost on dataset for this epoch 333 = 0.48364431753833353\n",
      "Error on this batch = 0.4700293336581036\n",
      "Error on this batch = 0.4903808273934017\n",
      "Cost on val dataset after 334 epochs is = 0.48363538277973\n",
      "learning rate =  0.027358782758229137\n",
      "Initial Cost on dataset for this epoch 334 = 0.48363538277973\n",
      "Error on this batch = 0.4700132129718464\n",
      "Error on this batch = 0.4903765263257713\n",
      "Cost on val dataset after 335 epochs is = 0.48362656214040306\n",
      "learning rate =  0.027317918235407655\n",
      "Initial Cost on dataset for this epoch 335 = 0.48362656214040306\n",
      "Error on this batch = 0.46999729019301456\n",
      "Error on this batch = 0.4903723171342335\n",
      "Cost on val dataset after 336 epochs is = 0.4836178590348784\n",
      "learning rate =  0.02727723627949905\n",
      "Initial Cost on dataset for this epoch 336 = 0.4836178590348784\n",
      "Error on this batch = 0.4699815851930623\n",
      "Error on this batch = 0.490368147234337\n",
      "Cost on val dataset after 337 epochs is = 0.4836092808585639\n",
      "learning rate =  0.027236735535142165\n",
      "Initial Cost on dataset for this epoch 337 = 0.4836092808585639\n",
      "Error on this batch = 0.46996607816123104\n",
      "Error on this batch = 0.4903640691061199\n",
      "Cost on val dataset after 338 epochs is = 0.4836008166420232\n",
      "learning rate =  0.02719641466102106\n",
      "Initial Cost on dataset for this epoch 338 = 0.4836008166420232\n",
      "Error on this batch = 0.46995078207793695\n",
      "Error on this batch = 0.4903600786222652\n",
      "Cost on val dataset after 339 epochs is = 0.48359245487797825\n",
      "learning rate =  0.027156272329678422\n",
      "Initial Cost on dataset for this epoch 339 = 0.48359245487797825\n",
      "Error on this batch = 0.4699356571795997\n",
      "Error on this batch = 0.4903561333870916\n",
      "Cost on val dataset after 340 epochs is = 0.48358421583939876\n",
      "learning rate =  0.02711630722733202\n",
      "Initial Cost on dataset for this epoch 340 = 0.48358421583939876\n",
      "Error on this batch = 0.469920732060053\n",
      "Error on this batch = 0.4903522793854372\n",
      "Cost on val dataset after 341 epochs is = 0.48357608626738663\n",
      "learning rate =  0.027076518053694116\n",
      "Initial Cost on dataset for this epoch 341 = 0.48357608626738663\n",
      "Error on this batch = 0.4699060122283849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on this batch = 0.4903485085979875\n",
      "Cost on val dataset after 342 epochs is = 0.48356804561854955\n",
      "learning rate =  0.02703690352179376\n",
      "Initial Cost on dataset for this epoch 342 = 0.48356804561854955\n",
      "Error on this batch = 0.46989147251502794\n",
      "Error on this batch = 0.4903447729141591\n",
      "Cost on val dataset after 343 epochs is = 0.48356012720750263\n",
      "learning rate =  0.02699746235780194\n",
      "Initial Cost on dataset for this epoch 343 = 0.48356012720750263\n",
      "Error on this batch = 0.469877150653285\n",
      "Error on this batch = 0.4903411304239916\n",
      "Cost on val dataset after 344 epochs is = 0.4835522863219069\n",
      "learning rate =  0.026958193300859603\n",
      "Initial Cost on dataset for this epoch 344 = 0.4835522863219069\n",
      "Error on this batch = 0.4698629953777537\n",
      "Error on this batch = 0.49033752892343585\n",
      "Cost on val dataset after 345 epochs is = 0.48354455926083423\n",
      "learning rate =  0.026919095102908276\n",
      "Initial Cost on dataset for this epoch 345 = 0.48354455926083423\n",
      "Error on this batch = 0.46984910149022197\n",
      "Error on this batch = 0.4903339990213298\n",
      "Cost on val dataset after 346 epochs is = 0.4835369246599477\n",
      "learning rate =  0.026880166528523517\n",
      "Initial Cost on dataset for this epoch 346 = 0.4835369246599477\n",
      "Error on this batch = 0.4698353251676294\n",
      "Error on this batch = 0.49033052991288517\n",
      "Cost on val dataset after 347 epochs is = 0.4835293804796541\n",
      "learning rate =  0.02684140635475095\n",
      "Initial Cost on dataset for this epoch 347 = 0.4835293804796541\n",
      "Error on this batch = 0.4698217360128829\n",
      "Error on this batch = 0.490327104580807\n",
      "Cost on val dataset after 348 epochs is = 0.48352193925272613\n",
      "learning rate =  0.02680281337094487\n",
      "Initial Cost on dataset for this epoch 348 = 0.48352193925272613\n",
      "Error on this batch = 0.4698082971111268\n",
      "Error on this batch = 0.4903237526479414\n",
      "Cost on val dataset after 349 epochs is = 0.4835145785458475\n",
      "learning rate =  0.02676438637860946\n",
      "Initial Cost on dataset for this epoch 349 = 0.4835145785458475\n",
      "Error on this batch = 0.4697950773812391\n",
      "Error on this batch = 0.4903204368864751\n",
      "Cost on val dataset after 350 epochs is = 0.4835073283015772\n",
      "learning rate =  0.026726124191242435\n",
      "Initial Cost on dataset for this epoch 350 = 0.4835073283015772\n",
      "Error on this batch = 0.46978198182411623\n",
      "Error on this batch = 0.4903172115078958\n",
      "Cost on val dataset after 351 epochs is = 0.48350012871555087\n",
      "learning rate =  0.02668802563418119\n",
      "Initial Cost on dataset for this epoch 351 = 0.48350012871555087\n",
      "Error on this batch = 0.46976900889363893\n",
      "Error on this batch = 0.4903140157373389\n",
      "Cost on val dataset after 352 epochs is = 0.48349301466299816\n",
      "learning rate =  0.026650089544451302\n",
      "Initial Cost on dataset for this epoch 352 = 0.48349301466299816\n",
      "Error on this batch = 0.4697562257084638\n",
      "Error on this batch = 0.49031085959013665\n",
      "Cost on val dataset after 353 epochs is = 0.4834860117950656\n",
      "learning rate =  0.026612314770617474\n",
      "Initial Cost on dataset for this epoch 353 = 0.4834860117950656\n",
      "Error on this batch = 0.4697435428071725\n",
      "Error on this batch = 0.49030778991298973\n",
      "Cost on val dataset after 354 epochs is = 0.48347905542212744\n",
      "learning rate =  0.026574700172636696\n",
      "Initial Cost on dataset for this epoch 354 = 0.48347905542212744\n",
      "Error on this batch = 0.4697310555170688\n",
      "Error on this batch = 0.49030475587203554\n",
      "Cost on val dataset after 355 epochs is = 0.4834721873592278\n",
      "learning rate =  0.026537244621713762\n",
      "Initial Cost on dataset for this epoch 355 = 0.4834721873592278\n",
      "Error on this batch = 0.4697187243928895\n",
      "Error on this batch = 0.49030176258396146\n",
      "Cost on val dataset after 356 epochs is = 0.48346542127067627\n",
      "learning rate =  0.026499947000159\n",
      "Initial Cost on dataset for this epoch 356 = 0.48346542127067627\n",
      "Error on this batch = 0.4697065819822444\n",
      "Error on this batch = 0.4902988218121375\n",
      "Cost on val dataset after 357 epochs is = 0.4834587136487422\n",
      "learning rate =  0.026462806201248155\n",
      "Initial Cost on dataset for this epoch 357 = 0.4834587136487422\n",
      "Error on this batch = 0.46969452892657393\n",
      "Error on this batch = 0.4902959363728822\n",
      "Cost on val dataset after 358 epochs is = 0.48345208267732825\n",
      "learning rate =  0.026425821129084495\n",
      "Initial Cost on dataset for this epoch 358 = 0.48345208267732825\n",
      "Error on this batch = 0.46968263087780854\n",
      "Error on this batch = 0.490293086773311\n",
      "Cost on val dataset after 359 epochs is = 0.48344552333516927\n",
      "learning rate =  0.026388990698462976\n",
      "Initial Cost on dataset for this epoch 359 = 0.48344552333516927\n",
      "Error on this batch = 0.46967089440087306\n",
      "Error on this batch = 0.4902902923504796\n",
      "Cost on val dataset after 360 epochs is = 0.48343904029422874\n",
      "learning rate =  0.026352313834736494\n",
      "Initial Cost on dataset for this epoch 360 = 0.48343904029422874\n",
      "Error on this batch = 0.46965918285386155\n",
      "Error on this batch = 0.490287540564892\n",
      "Cost on val dataset after 361 epochs is = 0.48343262602084286\n",
      "learning rate =  0.02631578947368421\n",
      "Initial Cost on dataset for this epoch 361 = 0.48343262602084286\n",
      "Error on this batch = 0.46964765643796585\n",
      "Error on this batch = 0.4902848218232539\n",
      "Cost on val dataset after 362 epochs is = 0.4834262916947109\n",
      "learning rate =  0.026279416561381837\n",
      "Initial Cost on dataset for this epoch 362 = 0.4834262916947109\n",
      "Error on this batch = 0.4696362698988723\n",
      "Error on this batch = 0.49028215814485765\n",
      "Cost on val dataset after 363 epochs is = 0.4834200060194154\n",
      "learning rate =  0.0262431940540739\n",
      "Initial Cost on dataset for this epoch 363 = 0.4834200060194154\n",
      "Error on this batch = 0.4696250068967095\n",
      "Error on this batch = 0.4902795261503237\n",
      "Cost on val dataset after 364 epochs is = 0.4834138120156669\n",
      "learning rate =  0.026207120918047958\n",
      "Initial Cost on dataset for this epoch 364 = 0.4834138120156669\n",
      "Error on this batch = 0.46961391383363216\n",
      "Error on this batch = 0.4902769392136685\n",
      "Cost on val dataset after 365 epochs is = 0.48340768608199003\n",
      "learning rate =  0.026171196129510688\n",
      "Initial Cost on dataset for this epoch 365 = 0.48340768608199003\n",
      "Error on this batch = 0.46960289897280255\n",
      "Error on this batch = 0.4902743916452411\n",
      "Cost on val dataset after 366 epochs is = 0.4834016341973087\n",
      "learning rate =  0.026135418674465834\n",
      "Initial Cost on dataset for this epoch 366 = 0.4834016341973087\n",
      "Error on this batch = 0.4695919794877414\n",
      "Error on this batch = 0.49027188076102357\n",
      "Cost on val dataset after 367 epochs is = 0.48339563183399764\n",
      "learning rate =  0.026099787548594027\n",
      "Initial Cost on dataset for this epoch 367 = 0.48339563183399764\n",
      "Error on this batch = 0.4695812057255374\n",
      "Error on this batch = 0.49026940694684523\n",
      "Cost on val dataset after 368 epochs is = 0.483389710726805\n",
      "learning rate =  0.026064301757134346\n",
      "Initial Cost on dataset for this epoch 368 = 0.483389710726805\n",
      "Error on this batch = 0.469570569727931\n",
      "Error on this batch = 0.49026697872673647\n",
      "Cost on val dataset after 369 epochs is = 0.48338384153450104\n",
      "learning rate =  0.026028960314767677\n",
      "Initial Cost on dataset for this epoch 369 = 0.48338384153450104\n",
      "Error on this batch = 0.4695600334823961\n",
      "Error on this batch = 0.49026457785344163\n",
      "Cost on val dataset after 370 epochs is = 0.48337802969147475\n",
      "learning rate =  0.02599376224550182\n",
      "Initial Cost on dataset for this epoch 370 = 0.48337802969147475\n",
      "Error on this batch = 0.4695495822560881\n",
      "Error on this batch = 0.49026222564236543\n",
      "Cost on val dataset after 371 epochs is = 0.48337227782901476\n",
      "learning rate =  0.02595870658255825\n",
      "Initial Cost on dataset for this epoch 371 = 0.48337227782901476\n",
      "Error on this batch = 0.4695392698233212\n",
      "Error on this batch = 0.4902598993594124\n",
      "Cost on val dataset after 372 epochs is = 0.483366600079251\n",
      "learning rate =  0.02592379236826063\n",
      "Initial Cost on dataset for this epoch 372 = 0.483366600079251\n",
      "Error on this batch = 0.46952907821412043\n",
      "Error on this batch = 0.4902576127971953\n",
      "Cost on val dataset after 373 epochs is = 0.4833609749228801\n",
      "learning rate =  0.025889018653924886\n",
      "Initial Cost on dataset for this epoch 373 = 0.4833609749228801\n",
      "Error on this batch = 0.4695189054192436\n",
      "Error on this batch = 0.4902553520671972\n",
      "Cost on val dataset after 374 epochs is = 0.48335540737569854\n",
      "learning rate =  0.025854384499750957\n",
      "Initial Cost on dataset for this epoch 374 = 0.48335540737569854\n",
      "Error on this batch = 0.46950890280768903\n",
      "Error on this batch = 0.4902531374322935\n",
      "Cost on val dataset after 375 epochs is = 0.48334989509469956\n",
      "learning rate =  0.025819888974716113\n",
      "Initial Cost on dataset for this epoch 375 = 0.48334989509469956\n",
      "Error on this batch = 0.46949900554493523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on this batch = 0.4902509469494685\n",
      "Cost on val dataset after 376 epochs is = 0.4833444434307385\n",
      "learning rate =  0.025785531156469834\n",
      "Initial Cost on dataset for this epoch 376 = 0.4833444434307385\n",
      "Error on this batch = 0.46948916823306563\n",
      "Error on this batch = 0.49024879806953536\n",
      "Cost on val dataset after 377 epochs is = 0.4833390432123322\n",
      "learning rate =  0.025751310131230238\n",
      "Initial Cost on dataset for this epoch 377 = 0.4833390432123322\n",
      "Error on this batch = 0.46947946041178745\n",
      "Error on this batch = 0.49024668190495085\n",
      "Cost on val dataset after 378 epochs is = 0.4833337069823203\n",
      "learning rate =  0.025717224993681984\n",
      "Initial Cost on dataset for this epoch 378 = 0.4833337069823203\n",
      "Error on this batch = 0.4694698669101962\n",
      "Error on this batch = 0.49024459843075185\n",
      "Cost on val dataset after 379 epochs is = 0.4833284122959894\n",
      "learning rate =  0.025683274846875707\n",
      "Initial Cost on dataset for this epoch 379 = 0.4833284122959894\n",
      "Error on this batch = 0.46946032792891623\n",
      "Error on this batch = 0.4902425495347596\n",
      "Cost on val dataset after 380 epochs is = 0.4833231815068068\n",
      "learning rate =  0.025649458802128853\n",
      "Initial Cost on dataset for this epoch 380 = 0.4833231815068068\n",
      "Error on this batch = 0.46945089271832063\n",
      "Error on this batch = 0.49024053692420455\n",
      "Cost on val dataset after 381 epochs is = 0.4833179917120526\n",
      "learning rate =  0.025615775978927998\n",
      "Initial Cost on dataset for this epoch 381 = 0.4833179917120526\n",
      "Error on this batch = 0.469441550332576\n",
      "Error on this batch = 0.4902385367420503\n",
      "Cost on val dataset after 382 epochs is = 0.4833128628199184\n",
      "learning rate =  0.02558222550483254\n",
      "Initial Cost on dataset for this epoch 382 = 0.4833128628199184\n",
      "Error on this batch = 0.46943232834335646\n",
      "Error on this batch = 0.49023660677605707\n",
      "Cost on val dataset after 383 epochs is = 0.4833077815507442\n",
      "learning rate =  0.0255488065153798\n",
      "Initial Cost on dataset for this epoch 383 = 0.4833077815507442\n",
      "Error on this batch = 0.46942322247286633\n",
      "Error on this batch = 0.4902346935632049\n",
      "Cost on val dataset after 384 epochs is = 0.4833027579937349\n",
      "learning rate =  0.025515518153991442\n",
      "Initial Cost on dataset for this epoch 384 = 0.4833027579937349\n",
      "Error on this batch = 0.4694142030908665\n",
      "Error on this batch = 0.4902328272795872\n",
      "Cost on val dataset after 385 epochs is = 0.4832977656981916\n",
      "learning rate =  0.025482359571881278\n",
      "Initial Cost on dataset for this epoch 385 = 0.4832977656981916\n",
      "Error on this batch = 0.4694052584874842\n",
      "Error on this batch = 0.49023097955717443\n",
      "Cost on val dataset after 386 epochs is = 0.4832928292220672\n",
      "learning rate =  0.025449329927964382\n",
      "Initial Cost on dataset for this epoch 386 = 0.4832928292220672\n",
      "Error on this batch = 0.4693964398436476\n",
      "Error on this batch = 0.4902291693669301\n",
      "Cost on val dataset after 387 epochs is = 0.4832879381758956\n",
      "learning rate =  0.025416428388767447\n",
      "Initial Cost on dataset for this epoch 387 = 0.4832879381758956\n",
      "Error on this batch = 0.4693876864462583\n",
      "Error on this batch = 0.4902273929931256\n",
      "Cost on val dataset after 388 epochs is = 0.48328309723067286\n",
      "learning rate =  0.02538365412834048\n",
      "Initial Cost on dataset for this epoch 388 = 0.48328309723067286\n",
      "Error on this batch = 0.4693790188783082\n",
      "Error on this batch = 0.4902256371063915\n",
      "Cost on val dataset after 389 epochs is = 0.48327830487561535\n",
      "learning rate =  0.02535100632816969\n",
      "Initial Cost on dataset for this epoch 389 = 0.48327830487561535\n",
      "Error on this batch = 0.4693704643414847\n",
      "Error on this batch = 0.4902239179537098\n",
      "Cost on val dataset after 390 epochs is = 0.48327355176512277\n",
      "learning rate =  0.025318484177091666\n",
      "Initial Cost on dataset for this epoch 390 = 0.48327355176512277\n",
      "Error on this batch = 0.4693619835827962\n",
      "Error on this batch = 0.49022225496592486\n",
      "Cost on val dataset after 391 epochs is = 0.4832688417374424\n",
      "learning rate =  0.02528608687120868\n",
      "Initial Cost on dataset for this epoch 391 = 0.4832688417374424\n",
      "Error on this batch = 0.4693535884575626\n",
      "Error on this batch = 0.49022060857154837\n",
      "Cost on val dataset after 392 epochs is = 0.4832641921586512\n",
      "learning rate =  0.025253813613805267\n",
      "Initial Cost on dataset for this epoch 392 = 0.4832641921586512\n",
      "Error on this batch = 0.4693452758002888\n",
      "Error on this batch = 0.49021901057859396\n",
      "Cost on val dataset after 393 epochs is = 0.4832595672316148\n",
      "learning rate =  0.025221663615265913\n",
      "Initial Cost on dataset for this epoch 393 = 0.4832595672316148\n",
      "Error on this batch = 0.46933704804654797\n",
      "Error on this batch = 0.49021742930471973\n",
      "Cost on val dataset after 394 epochs is = 0.48325499041078873\n",
      "learning rate =  0.02518963609299392\n",
      "Initial Cost on dataset for this epoch 394 = 0.48325499041078873\n",
      "Error on this batch = 0.46932887448698685\n",
      "Error on this batch = 0.49021586797074934\n",
      "Cost on val dataset after 395 epochs is = 0.4832504530509883\n",
      "learning rate =  0.02515773027133138\n",
      "Initial Cost on dataset for this epoch 395 = 0.4832504530509883\n",
      "Error on this batch = 0.46932083123939633\n",
      "Error on this batch = 0.49021432978836665\n",
      "Cost on val dataset after 396 epochs is = 0.4832459533596164\n",
      "learning rate =  0.025125945381480302\n",
      "Initial Cost on dataset for this epoch 396 = 0.4832459533596164\n",
      "Error on this batch = 0.469312847975243\n",
      "Error on this batch = 0.4902128052625737\n",
      "Cost on val dataset after 397 epochs is = 0.4832415086279322\n",
      "learning rate =  0.02509428066142478\n",
      "Initial Cost on dataset for this epoch 397 = 0.4832415086279322\n",
      "Error on this batch = 0.4693049396625109\n",
      "Error on this batch = 0.490211306080989\n",
      "Cost on val dataset after 398 epochs is = 0.48323709548841504\n",
      "learning rate =  0.025062735355854276\n",
      "Initial Cost on dataset for this epoch 398 = 0.48323709548841504\n",
      "Error on this batch = 0.4692970693564889\n",
      "Error on this batch = 0.49020982576621935\n",
      "Cost on val dataset after 399 epochs is = 0.4832327225468696\n",
      "learning rate =  0.025031308716087945\n",
      "Initial Cost on dataset for this epoch 399 = 0.4832327225468696\n",
      "Error on this batch = 0.4692893005617567\n",
      "Error on this batch = 0.4902083710433399\n",
      "Cost on val dataset after 400 epochs is = 0.4832283849091271\n",
      "learning rate =  0.025\n",
      "Initial Cost on dataset for this epoch 400 = 0.4832283849091271\n",
      "Error on this batch = 0.4692815949340452\n",
      "Error on this batch = 0.4902069270092096\n",
      "Cost on val dataset after 401 epochs is = 0.48322410006881217\n",
      "learning rate =  0.024968808471946116\n",
      "Initial Cost on dataset for this epoch 401 = 0.48322410006881217\n",
      "Error on this batch = 0.4692739749877712\n",
      "Error on this batch = 0.49020550549663294\n",
      "Cost on val dataset after 402 epochs is = 0.48321984544005675\n",
      "learning rate =  0.024937733402690822\n",
      "Initial Cost on dataset for this epoch 402 = 0.48321984544005675\n",
      "Error on this batch = 0.4692664066835555\n",
      "Error on this batch = 0.4902041099732546\n",
      "Cost on val dataset after 403 epochs is = 0.48321562904049264\n",
      "learning rate =  0.024906774069335894\n",
      "Initial Cost on dataset for this epoch 403 = 0.48321562904049264\n",
      "Error on this batch = 0.46925894952810077\n",
      "Error on this batch = 0.4902027264647582\n",
      "Cost on val dataset after 404 epochs is = 0.4832114561288912\n",
      "learning rate =  0.02487592975524973\n",
      "Initial Cost on dataset for this epoch 404 = 0.4832114561288912\n",
      "Error on this batch = 0.46925156918859856\n",
      "Error on this batch = 0.49020136145215204\n",
      "Cost on val dataset after 405 epochs is = 0.4832073183414795\n",
      "learning rate =  0.024845199749997663\n",
      "Initial Cost on dataset for this epoch 405 = 0.4832073183414795\n",
      "Error on this batch = 0.46924426277747844\n",
      "Error on this batch = 0.49020001202809826\n",
      "Cost on val dataset after 406 epochs is = 0.4832032157578831\n",
      "learning rate =  0.024814583349273254\n",
      "Initial Cost on dataset for this epoch 406 = 0.4832032157578831\n",
      "Error on this batch = 0.4692370261770293\n",
      "Error on this batch = 0.49019868878426076\n",
      "Cost on val dataset after 407 epochs is = 0.4831991481674578\n",
      "learning rate =  0.02478407985483048\n",
      "Initial Cost on dataset for this epoch 407 = 0.4831991481674578\n",
      "Error on this batch = 0.4692298974336419\n",
      "Error on this batch = 0.4901973827307225\n",
      "Cost on val dataset after 408 epochs is = 0.483195109318534\n",
      "learning rate =  0.02475368857441686\n",
      "Initial Cost on dataset for this epoch 408 = 0.483195109318534\n",
      "Error on this batch = 0.4692228009911031\n",
      "Error on this batch = 0.49019609464345154\n",
      "Cost on val dataset after 409 epochs is = 0.4831911088350433\n",
      "learning rate =  0.024723408821707437\n",
      "Initial Cost on dataset for this epoch 409 = 0.4831911088350433\n",
      "Error on this batch = 0.4692158210553864\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-24531ede0cda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mX_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mY_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mfm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-ccdbdd7d4cd8>\u001b[0m in \u001b[0;36mforward_prop\u001b[0;34m(data, theta)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0;31m#print(\"relu\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0mfm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelu_act\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mfm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "costs = []\n",
    "epoch = 1\n",
    "start = time.time()\n",
    "#cost_init = cost_total(X_train, theta, train_class_enc, m) #Validation loss not giving much info\n",
    "cost_init = cost_total(X_valid, theta, valid_class_enc, X_valid.shape[0]) #Validation loss not giving much info\n",
    "costs.append(cost_init)\n",
    "early_stop= 0\n",
    "while(True):\n",
    "    count = 0\n",
    "    lr = lr0/(np.power(epoch, 1/2))\n",
    "    #if(lr < 0.001): lr = 0.001\n",
    "    print(\"learning rate = \", lr)\n",
    "\n",
    "    print(\"Initial Cost on dataset for this epoch {} = {}\".format(epoch, cost_init))\n",
    "\n",
    "    for b in mini_batch:\n",
    "        X_b = b[0]\n",
    "        Y_b = b[1]\n",
    "        fm = forward_prop(X_b, theta)\n",
    "        delta = [None]*len(fm)\n",
    "\n",
    "        if (count % 60 == 0):\n",
    "            print(\"Error on this batch = \"+str(cost_total(X_b, theta, Y_b, batch_size)))\n",
    "        #Backward Propagation\n",
    "\n",
    "        for l in range(len(fm)-1, 0, -1):\n",
    "            if (l == len(fm)-1):\n",
    "                delta[l] = ((1/batch_size)*(Y_b - fm[l])*fm[l]*(1-fm[l]))\n",
    "                #delta[l] = ((1/batch_size)*((Y_b/fm[l])-((1-Y_b)/(1-fm[l])))*fm[l]*(1-fm[l]))\n",
    "                #print(\"delta for last layer=\",delta[l].shape)\n",
    "            else:\n",
    "                delta[l]=np.dot(delta[l+1], theta[l].T)*deriv_relu(fm[l])\n",
    "                #print(\"delta for hidden layer=\",delta[l])\n",
    "\n",
    "        for t in range(len(theta)):\n",
    "            theta[t] += lr*np.dot(fm[t].T, delta[t+1])\n",
    "        \n",
    "        count+=1\n",
    "    epoch+=1 #Number of epochs\n",
    "    #ite+=1\n",
    "\n",
    "    #cost_final = cost_total(X_train, theta, train_class_enc, m)\n",
    "    cost_final = cost_total(X_valid, theta, valid_class_enc, X_valid.shape[0])\n",
    "    if(epoch%10==0): costs.append(cost_final)\n",
    "    print(\"Cost on val dataset after {} epochs is = {}\".format(epoch, cost_final))\n",
    "    \n",
    "    if ((cost_final-cost_init) > 0):\n",
    "        early_stop +=1\n",
    "    else:\n",
    "        early_stop=0\n",
    "\n",
    "    if (early_stop == 30):\n",
    "        print(\"cost initial= {} , cost final={} , change in cost= {}\".format(cost_init,cost_final, cost_final-cost_init))\n",
    "        break\n",
    "\n",
    "    cost_init = cost_final\n",
    "    \n",
    "    \n",
    "epochs.append(epoch)\n",
    "train_time.append(time.time()-start)\n",
    "train_accuracy.append(calc_accuracy(X_train, theta, train_class_enc))\n",
    "valid_accuracy.append(calc_accuracy(X_valid, theta, valid_class_enc))\n",
    "test_accuracy.append(calc_accuracy(X_test, theta, test_actual_class_enc))\n",
    "print(\"\\n------------------------------------------------------------------------------\")\n",
    "print(\"The stats for number of units in the hidden layer arch= {} are as below:\".format(arch))\n",
    "print(\"------------------------------------------------------------------------------\")\n",
    "print(\"The number of epochs = {:2.3f}\".format(epochs[-1]))\n",
    "print(\"The training time = {:2.3f}sec\".format(train_time[-1]))\n",
    "print(\"The training accuracy is = {:2.3f}%\".format(train_accuracy[-1]))\n",
    "print(\"The validation accuracy is = {:2.3f}%\".format(valid_accuracy[-1]))\n",
    "print(\"The test accuracy is = {:2.3f}%\".format(test_accuracy[-1]))\n",
    "print(\"------------------------------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.8461538461538463"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_accuracy(X_test, theta, test_actual_class_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_title(\"Validation Loss with Epochs with ReLU\")\n",
    "x = np.arange(0,len(costs[1:]))\n",
    "ax.plot(x, costs[1:], marker='o', label='Loss')\n",
    "ax.set_xlabel(\"number of epochs\")\n",
    "ax.set_ylabel(\"Val Loss\")\n",
    "\n",
    "plt.legend()\n",
    "plt.savefig(\"plots/partd/relu_val_adapt_sqrt.png\", dpi=1000, bbox_inches='tight')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_title(\"Loss with Epochs\")\n",
    "x = np.arange(0,len(costs[1:]))\n",
    "ax.plot(x, costs[1:], marker='o', label='Train Accuracy')\n",
    "ax.set_xlabel(\"number of epochs\")\n",
    "ax.set_ylabel(\"Train Loss\")\n",
    "\n",
    "plt.legend()\n",
    "#plt.savefig(\"accuracy_HiddenUnit_val20per.png\", dpi=1000, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
