{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------Reading the Data-------------------------\n",
      "----------------Data Reading completed-------------------\n",
      "The total number of training samples = 11050\n",
      "The total number of validation samples = 1950\n",
      "The number of features = 784\n"
     ]
    }
   ],
   "source": [
    "print(\"----------------Reading the Data-------------------------\")\n",
    "PATH = os.getcwd()\n",
    "os.chdir('Alphabets/')\n",
    "\n",
    "X_train = pd.read_csv('train.csv', sep=',', header=None, index_col=False)\n",
    "X_test = pd.read_csv('test.csv', sep=',', header=None, index_col=False)\n",
    "np.random.shuffle(X_train.to_numpy())\n",
    "train_class = X_train[X_train.columns[-1]]\n",
    "test_actual_class = X_test[X_test.columns[-1]]\n",
    "\n",
    "X_train = X_train.drop(X_train.columns[-1], axis=1)\n",
    "X_test = X_test.drop(X_test.columns[-1], axis=1)\n",
    "\n",
    "print(\"----------------Data Reading completed-------------------\")\n",
    "\n",
    "os.chdir('../')\n",
    "\n",
    "X_train = X_train/255\n",
    "X_test = X_test/255\n",
    "\n",
    "m = X_train.shape[0] # Number of Training Samples\n",
    "\n",
    "X_valid = X_train.iloc[(int(0.85*m)):]\n",
    "valid_class = train_class[(int(0.85*m)):]\n",
    "X_train = X_train.iloc[0:int(0.85*m)]\n",
    "train_class = train_class[0:int(0.85*m)]\n",
    "\n",
    "\n",
    "m = X_train.shape[0] # Number of Training Samples\n",
    "n = X_train.shape[1] # Number of input features\n",
    "\n",
    "print(\"The total number of training samples = {}\".format(m))\n",
    "print(\"The total number of validation samples = {}\".format(X_valid.shape[0]))\n",
    "\n",
    "\n",
    "print(\"The number of features = {}\".format(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Perform 1-hot encoding of class labels------------\n"
     ]
    }
   ],
   "source": [
    "#To get the one hot encoding of each label\n",
    "print(\"--------Perform 1-hot encoding of class labels------------\")\n",
    "\n",
    "train_class_enc = pd.get_dummies(train_class).to_numpy()\n",
    "valid_class_enc = pd.get_dummies(valid_class).to_numpy()\n",
    "test_actual_class_enc = pd.get_dummies(test_actual_class).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add the intercept term to the data samples both in training and test dataset\n",
    "X_train = np.hstack((np.ones((m,1)),X_train.to_numpy()))\n",
    "X_valid = np.hstack((np.ones((X_valid.shape[0],1)), X_valid.to_numpy()))\n",
    "X_test = np.hstack((np.ones((X_test.shape[0],1)),X_test.to_numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "arch_test = [1,5,10,50,100]\n",
    "arch = [arch_test[3]] #means one hidden layer with 2 perceptrons \n",
    "batch_size = 100 # Mini-Batch Size\n",
    "r = np.max(train_class) + 1 # Default value of the number of classes = 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of mini-batches formed is = 111\n"
     ]
    }
   ],
   "source": [
    "#Mini-Batch formation\n",
    "mini_batch = [(X_train[i:i+batch_size,:], train_class_enc[i:i+batch_size]) for i in range(0, m, batch_size)]\n",
    "print(\"The number of mini-batches formed is = {}\".format(len(mini_batch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Theta Initialization \n",
    "#np.random.seed(1)\n",
    "def theta_init(arch=[50]):\n",
    "    theta = []\n",
    "    for i in range(len(arch)+1):\n",
    "        if i == 0:\n",
    "            dim0=n+1\n",
    "            dim1=arch[i]\n",
    "        elif (i == len(arch)):\n",
    "            dim0=arch[i-1]\n",
    "            dim1 = r\n",
    "        else:\n",
    "            dim0=arch[i-1]\n",
    "            dim1= arch[i]\n",
    "        theta.append(np.random.normal(0,0.01, (dim0,dim1)))\n",
    "        #theta.append(0.01*(2*np.random.random((dim0, dim1))-1))\n",
    "        #theta.append(np.zeros((dim0, dim1)))\n",
    "        #theta.append(0.01*np.random.standard_normal((dim0, dim1)))\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_act(x):\n",
    "    return np.maximum(0.0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deriv_relu(x):\n",
    "    #x[x<=0] = -0.01\n",
    "    x[x<=0] = 0\n",
    "    x[x>0] = 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu_act(x):\n",
    "    np.where(x > 0, x, x * 0.1) \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deriv_leakyRelu(x):\n",
    "    x[x<=0] = 0.1\n",
    "    x[x>0] = 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softplus(x):\n",
    "    return np.log(1+np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deriv_softplus(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(data, theta):\n",
    "    fm = []\n",
    "    fm.append(data)\n",
    "    for l in range(len(theta)):\n",
    "        if (l != len(theta)-1):\n",
    "            #print(\"relu\")\n",
    "            fm.append(relu_act(np.dot(fm[l], theta[l])))\n",
    "        else:\n",
    "            fm.append(activation(np.dot(fm[l], theta[l])))\n",
    "            #print(\"sigmoid output\")\n",
    "    return fm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 26)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18.02184920921566"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta = theta_init([100, 100, 100])\n",
    "print(theta[3].shape)\n",
    "fm = forward_prop(X_train, theta)\n",
    "cost_total(X_train, theta, train_class_enc, m)\n",
    "cross_entropy_loss(X_train, theta, train_class_enc, m)\n",
    "#fm = forward_prop(X_train, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_total(X, theta, Y, m):\n",
    "    fm = forward_prop(X, theta)\n",
    "    cost = (1/(2*m))*np.sum((Y-fm[-1])**2)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(X, theta, Y, m):\n",
    "    fm = forward_prop(X, theta)\n",
    "    cost = -(1/m)*(np.sum(((Y*np.log(fm[-1]))+((1-Y)*(np.log(1-fm[-1]))))))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(data, theta, actual_class):\n",
    "    pred_class = forward_prop(data, theta)\n",
    "    test_pred_class = pred_class[-1]\n",
    "    for i in range(len(test_pred_class)):\n",
    "        test_pred_class[i][test_pred_class[i] == np.max(test_pred_class[i])] = 1\n",
    "        test_pred_class[i][test_pred_class[i] != np.max(test_pred_class[i])] = 0\n",
    "\n",
    "\n",
    "    test_acc = 0\n",
    "    for i in range(len(actual_class)):\n",
    "        if (np.array_equal(test_pred_class[i], actual_class[i])):\n",
    "            test_acc+=1\n",
    "    test_acc /= data.shape[0]\n",
    "\n",
    "    #print(\"The Test Accuracy of the model = {}%\".format(test_acc*100))\n",
    "    return (test_acc*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = []\n",
    "train_accuracy = []\n",
    "valid_accuracy =[]\n",
    "test_accuracy = []\n",
    "train_time = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(785, 100) (100, 100) (100, 26)\n"
     ]
    }
   ],
   "source": [
    "arch=[100, 100]\n",
    "lr=0.01\n",
    "lr0=1\n",
    "theta = theta_init(arch)\n",
    "print(theta[0].shape, theta[1].shape, theta[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 1 = 18.022212607287205\n",
      "Error on this batch = 18.022340589175023\n",
      "Error on this batch = 4.507582102855168\n",
      "Cost on val dataset after 2 epochs is = 4.579513618580161\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 2 = 4.579513618580161\n",
      "Error on this batch = 4.560816741516135\n",
      "Error on this batch = 4.4212779493554\n",
      "Cost on val dataset after 3 epochs is = 4.4713270478509815\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 3 = 4.4713270478509815\n",
      "Error on this batch = 4.468472414258326\n",
      "Error on this batch = 4.351008599611259\n",
      "Cost on val dataset after 4 epochs is = 4.364629871642738\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 4 = 4.364629871642738\n",
      "Error on this batch = 4.367008916720074\n",
      "Error on this batch = 4.290835486460032\n",
      "Cost on val dataset after 5 epochs is = 4.290024576021215\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 5 = 4.290024576021215\n",
      "Error on this batch = 4.291489159802669\n",
      "Error on this batch = 4.262185492657781\n",
      "Cost on val dataset after 6 epochs is = 4.257869849052016\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 6 = 4.257869849052016\n",
      "Error on this batch = 4.2573075637719135\n",
      "Error on this batch = 4.351034325812906\n",
      "Cost on val dataset after 7 epochs is = 4.247919037348393\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 7 = 4.247919037348393\n",
      "Error on this batch = 4.24657063313123\n",
      "Error on this batch = 4.289573131363395\n",
      "Cost on val dataset after 8 epochs is = 4.244819710548801\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 8 = 4.244819710548801\n",
      "Error on this batch = 4.242953254157152\n",
      "Error on this batch = 4.240934392848735\n",
      "Cost on val dataset after 9 epochs is = 4.243933487508386\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 9 = 4.243933487508386\n",
      "Error on this batch = 4.242065211453236\n",
      "Error on this batch = 4.269177358517222\n",
      "Cost on val dataset after 10 epochs is = 4.242677084646524\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 10 = 4.242677084646524\n",
      "Error on this batch = 4.240702909073376\n",
      "Error on this batch = 4.258658951449845\n",
      "Cost on val dataset after 11 epochs is = 4.242075749982173\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 11 = 4.242075749982173\n",
      "Error on this batch = 4.239925902730149\n",
      "Error on this batch = 4.27315689351279\n",
      "Cost on val dataset after 12 epochs is = 4.241420809791759\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 12 = 4.241420809791759\n",
      "Error on this batch = 4.239358596267206\n",
      "Error on this batch = 4.245022131467638\n",
      "Cost on val dataset after 13 epochs is = 4.240804879805468\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 13 = 4.240804879805468\n",
      "Error on this batch = 4.2388311637809\n",
      "Error on this batch = 4.238016987546803\n",
      "Cost on val dataset after 14 epochs is = 4.2403850442882005\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 14 = 4.2403850442882005\n",
      "Error on this batch = 4.2381772911977\n",
      "Error on this batch = 4.25554621774172\n",
      "Cost on val dataset after 15 epochs is = 4.239606625729071\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 15 = 4.239606625729071\n",
      "Error on this batch = 4.237617699637485\n",
      "Error on this batch = 4.236331038908165\n",
      "Cost on val dataset after 16 epochs is = 4.238988818572984\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 16 = 4.238988818572984\n",
      "Error on this batch = 4.236819466403423\n",
      "Error on this batch = 4.245439671212575\n",
      "Cost on val dataset after 17 epochs is = 4.238162622222707\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 17 = 4.238162622222707\n",
      "Error on this batch = 4.236228489310555\n",
      "Error on this batch = 4.241294212915922\n",
      "Cost on val dataset after 18 epochs is = 4.237190856172958\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 18 = 4.237190856172958\n",
      "Error on this batch = 4.235094808121581\n",
      "Error on this batch = 4.239736100146426\n",
      "Cost on val dataset after 19 epochs is = 4.2361570794922345\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 19 = 4.2361570794922345\n",
      "Error on this batch = 4.234272102482653\n",
      "Error on this batch = 4.2344239344878245\n",
      "Cost on val dataset after 20 epochs is = 4.234904014696899\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 20 = 4.234904014696899\n",
      "Error on this batch = 4.232790329687779\n",
      "Error on this batch = 4.241816193537525\n",
      "Cost on val dataset after 21 epochs is = 4.233402887229416\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 21 = 4.233402887229416\n",
      "Error on this batch = 4.231548027115803\n",
      "Error on this batch = 4.2322620802909166\n",
      "Cost on val dataset after 22 epochs is = 4.231618313825978\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 22 = 4.231618313825978\n",
      "Error on this batch = 4.229580743703529\n",
      "Error on this batch = 4.23756362929981\n",
      "Cost on val dataset after 23 epochs is = 4.2294892274771\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 23 = 4.2294892274771\n",
      "Error on this batch = 4.227744086444186\n",
      "Error on this batch = 4.229701555212715\n",
      "Cost on val dataset after 24 epochs is = 4.226742892487041\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 24 = 4.226742892487041\n",
      "Error on this batch = 4.2249239776710406\n",
      "Error on this batch = 4.225803053951499\n",
      "Cost on val dataset after 25 epochs is = 4.223577489116142\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 25 = 4.223577489116142\n",
      "Error on this batch = 4.222082951378674\n",
      "Error on this batch = 4.2241998082794705\n",
      "Cost on val dataset after 26 epochs is = 4.219545088719062\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 26 = 4.219545088719062\n",
      "Error on this batch = 4.21798163946339\n",
      "Error on this batch = 4.213689883272791\n",
      "Cost on val dataset after 27 epochs is = 4.214858647767749\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 27 = 4.214858647767749\n",
      "Error on this batch = 4.213498623094198\n",
      "Error on this batch = 4.209486872390417\n",
      "Cost on val dataset after 28 epochs is = 4.209532652762343\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 28 = 4.209532652762343\n",
      "Error on this batch = 4.208282098745713\n",
      "Error on this batch = 4.203765937378534\n",
      "Cost on val dataset after 29 epochs is = 4.203286950552377\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 29 = 4.203286950552377\n",
      "Error on this batch = 4.202184140884611\n",
      "Error on this batch = 4.197708050683887\n",
      "Cost on val dataset after 30 epochs is = 4.195914809816103\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 30 = 4.195914809816103\n",
      "Error on this batch = 4.195017258053672\n",
      "Error on this batch = 4.190404770625586\n",
      "Cost on val dataset after 31 epochs is = 4.187217895741935\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 31 = 4.187217895741935\n",
      "Error on this batch = 4.186613350620414\n",
      "Error on this batch = 4.181710375919236\n",
      "Cost on val dataset after 32 epochs is = 4.176896303522835\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 32 = 4.176896303522835\n",
      "Error on this batch = 4.1766629979677266\n",
      "Error on this batch = 4.171226215114428\n",
      "Cost on val dataset after 33 epochs is = 4.16441027969071\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 33 = 4.16441027969071\n",
      "Error on this batch = 4.164659022220752\n",
      "Error on this batch = 4.158826823610196\n",
      "Cost on val dataset after 34 epochs is = 4.149660181673087\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 34 = 4.149660181673087\n",
      "Error on this batch = 4.150388383776775\n",
      "Error on this batch = 4.1443132630869295\n",
      "Cost on val dataset after 35 epochs is = 4.132290263673414\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 35 = 4.132290263673414\n",
      "Error on this batch = 4.133522363003504\n",
      "Error on this batch = 4.127268044708329\n",
      "Cost on val dataset after 36 epochs is = 4.111924371935639\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 36 = 4.111924371935639\n",
      "Error on this batch = 4.11370284713832\n",
      "Error on this batch = 4.1073697818664865\n",
      "Cost on val dataset after 37 epochs is = 4.088027185117193\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 37 = 4.088027185117193\n",
      "Error on this batch = 4.090437725833514\n",
      "Error on this batch = 4.084215194238259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on val dataset after 38 epochs is = 4.06052048951086\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 38 = 4.06052048951086\n",
      "Error on this batch = 4.063677946945376\n",
      "Error on this batch = 4.057635291595049\n",
      "Cost on val dataset after 39 epochs is = 4.02908895151962\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 39 = 4.02908895151962\n",
      "Error on this batch = 4.03294001837721\n",
      "Error on this batch = 4.02715795830891\n",
      "Cost on val dataset after 40 epochs is = 3.993448653042206\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 40 = 3.993448653042206\n",
      "Error on this batch = 3.99790605218716\n",
      "Error on this batch = 3.992600373852436\n",
      "Cost on val dataset after 41 epochs is = 3.953778374226138\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 41 = 3.953778374226138\n",
      "Error on this batch = 3.9585390313166116\n",
      "Error on this batch = 3.954123286368515\n",
      "Cost on val dataset after 42 epochs is = 3.9103510594347557\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 42 = 3.9103510594347557\n",
      "Error on this batch = 3.9150809127284423\n",
      "Error on this batch = 3.9118375366703595\n",
      "Cost on val dataset after 43 epochs is = 3.8638314423192384\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 43 = 3.8638314423192384\n",
      "Error on this batch = 3.8681362074498074\n",
      "Error on this batch = 3.8662047617866597\n",
      "Cost on val dataset after 44 epochs is = 3.8151477974895562\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 44 = 3.8151477974895562\n",
      "Error on this batch = 3.8188279604005833\n",
      "Error on this batch = 3.818316592552543\n",
      "Cost on val dataset after 45 epochs is = 3.7656362791590876\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 45 = 3.7656362791590876\n",
      "Error on this batch = 3.768290497991637\n",
      "Error on this batch = 3.7692130056369524\n",
      "Cost on val dataset after 46 epochs is = 3.7164755052789724\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 46 = 3.7164755052789724\n",
      "Error on this batch = 3.7180928115534124\n",
      "Error on this batch = 3.719873450064662\n",
      "Cost on val dataset after 47 epochs is = 3.668727750982209\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 47 = 3.668727750982209\n",
      "Error on this batch = 3.6692554205085015\n",
      "Error on this batch = 3.671395921662666\n",
      "Cost on val dataset after 48 epochs is = 3.6230351770283087\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 48 = 3.6230351770283087\n",
      "Error on this batch = 3.6225393584752785\n",
      "Error on this batch = 3.624375119110074\n",
      "Cost on val dataset after 49 epochs is = 3.5798309851298384\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 49 = 3.5798309851298384\n",
      "Error on this batch = 3.578563309880292\n",
      "Error on this batch = 3.579490146876768\n",
      "Cost on val dataset after 50 epochs is = 3.539303021305202\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 50 = 3.539303021305202\n",
      "Error on this batch = 3.5373251582790566\n",
      "Error on this batch = 3.537046302437897\n",
      "Cost on val dataset after 51 epochs is = 3.5014163272337964\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 51 = 3.5014163272337964\n",
      "Error on this batch = 3.498713880972499\n",
      "Error on this batch = 3.4970463549958914\n",
      "Cost on val dataset after 52 epochs is = 3.466074059727981\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 52 = 3.466074059727981\n",
      "Error on this batch = 3.4625899456028324\n",
      "Error on this batch = 3.4594971658767215\n",
      "Cost on val dataset after 53 epochs is = 3.4330480078130616\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 53 = 3.4330480078130616\n",
      "Error on this batch = 3.4288031679409148\n",
      "Error on this batch = 3.424183219851109\n",
      "Cost on val dataset after 54 epochs is = 3.4021529625561917\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 54 = 3.4021529625561917\n",
      "Error on this batch = 3.3972616643871105\n",
      "Error on this batch = 3.390931414300009\n",
      "Cost on val dataset after 55 epochs is = 3.3732051576158284\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 55 = 3.3732051576158284\n",
      "Error on this batch = 3.3675427301298635\n",
      "Error on this batch = 3.359707893808097\n",
      "Cost on val dataset after 56 epochs is = 3.346001407022249\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 56 = 3.346001407022249\n",
      "Error on this batch = 3.339511111490152\n",
      "Error on this batch = 3.330238457935617\n",
      "Cost on val dataset after 57 epochs is = 3.3203920881967353\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 57 = 3.3203920881967353\n",
      "Error on this batch = 3.3129967995879994\n",
      "Error on this batch = 3.302291536059524\n",
      "Cost on val dataset after 58 epochs is = 3.2961964947729854\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 58 = 3.2961964947729854\n",
      "Error on this batch = 3.2879100253515006\n",
      "Error on this batch = 3.2759057969022756\n",
      "Cost on val dataset after 59 epochs is = 3.273287335440754\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 59 = 3.273287335440754\n",
      "Error on this batch = 3.264109782398038\n",
      "Error on this batch = 3.2508558429088636\n",
      "Cost on val dataset after 60 epochs is = 3.251580682378565\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 60 = 3.251580682378565\n",
      "Error on this batch = 3.241562847920665\n",
      "Error on this batch = 3.2270256764685095\n",
      "Cost on val dataset after 61 epochs is = 3.230960410487211\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 61 = 3.230960410487211\n",
      "Error on this batch = 3.220127548183216\n",
      "Error on this batch = 3.2042916234613803\n",
      "Cost on val dataset after 62 epochs is = 3.2113301951538977\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 62 = 3.2113301951538977\n",
      "Error on this batch = 3.1997680373942585\n",
      "Error on this batch = 3.1825464056140356\n",
      "Cost on val dataset after 63 epochs is = 3.1926170156045397\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 63 = 3.1926170156045397\n",
      "Error on this batch = 3.180487565245726\n",
      "Error on this batch = 3.161690078126229\n",
      "Cost on val dataset after 64 epochs is = 3.1747566337507944\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 64 = 3.1747566337507944\n",
      "Error on this batch = 3.1621050308313214\n",
      "Error on this batch = 3.1416704422363813\n",
      "Cost on val dataset after 65 epochs is = 3.15766427414844\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 65 = 3.15766427414844\n",
      "Error on this batch = 3.1446177557070563\n",
      "Error on this batch = 3.1225527604214576\n",
      "Cost on val dataset after 66 epochs is = 3.141279416941688\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 66 = 3.141279416941688\n",
      "Error on this batch = 3.127901638588373\n",
      "Error on this batch = 3.104153257920517\n",
      "Cost on val dataset after 67 epochs is = 3.1255573232338727\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 67 = 3.1255573232338727\n",
      "Error on this batch = 3.111920862670201\n",
      "Error on this batch = 3.086418489383056\n",
      "Cost on val dataset after 68 epochs is = 3.110458531370795\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 68 = 3.110458531370795\n",
      "Error on this batch = 3.0966787154193414\n",
      "Error on this batch = 3.0693773632192847\n",
      "Cost on val dataset after 69 epochs is = 3.0959009329391707\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 69 = 3.0959009329391707\n",
      "Error on this batch = 3.082046220915205\n",
      "Error on this batch = 3.052849702795053\n",
      "Cost on val dataset after 70 epochs is = 3.0818176197644975\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 70 = 3.0818176197644975\n",
      "Error on this batch = 3.06792929691828\n",
      "Error on this batch = 3.036857418574754\n",
      "Cost on val dataset after 71 epochs is = 3.068190166831675\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 71 = 3.068190166831675\n",
      "Error on this batch = 3.054278429982213\n",
      "Error on this batch = 3.021270964192013\n",
      "Cost on val dataset after 72 epochs is = 3.054953724574064\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 72 = 3.054953724574064\n",
      "Error on this batch = 3.0411232731881137\n",
      "Error on this batch = 3.0060288028342668\n",
      "Cost on val dataset after 73 epochs is = 3.0421214103791385\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 73 = 3.0421214103791385\n",
      "Error on this batch = 3.028341634587225\n",
      "Error on this batch = 2.991175815263907\n",
      "Cost on val dataset after 74 epochs is = 3.0296972839136758\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 74 = 3.0296972839136758\n",
      "Error on this batch = 3.0159659398346945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on this batch = 2.9767148051036703\n",
      "Cost on val dataset after 75 epochs is = 3.017596001952521\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 75 = 3.017596001952521\n",
      "Error on this batch = 3.0039473524170344\n",
      "Error on this batch = 2.9626451767028166\n",
      "Cost on val dataset after 76 epochs is = 3.0058001019309524\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 76 = 3.0058001019309524\n",
      "Error on this batch = 2.9922955581612234\n",
      "Error on this batch = 2.9488535848643527\n",
      "Cost on val dataset after 77 epochs is = 2.994237315005919\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 77 = 2.994237315005919\n",
      "Error on this batch = 2.9808412335339405\n",
      "Error on this batch = 2.935354189285323\n",
      "Cost on val dataset after 78 epochs is = 2.9828857904364683\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 78 = 2.9828857904364683\n",
      "Error on this batch = 2.9696577740126515\n",
      "Error on this batch = 2.922092888373153\n",
      "Cost on val dataset after 79 epochs is = 2.971772567675635\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 79 = 2.971772567675635\n",
      "Error on this batch = 2.9586464345004506\n",
      "Error on this batch = 2.909061093203676\n",
      "Cost on val dataset after 80 epochs is = 2.9608501217772476\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 80 = 2.9608501217772476\n",
      "Error on this batch = 2.9477822143899743\n",
      "Error on this batch = 2.896236726178697\n",
      "Cost on val dataset after 81 epochs is = 2.950136604605216\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 81 = 2.950136604605216\n",
      "Error on this batch = 2.9371087788353134\n",
      "Error on this batch = 2.8836419066253143\n",
      "Cost on val dataset after 82 epochs is = 2.939620578994221\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 82 = 2.939620578994221\n",
      "Error on this batch = 2.926641865076241\n",
      "Error on this batch = 2.8711859020078885\n",
      "Cost on val dataset after 83 epochs is = 2.92929085257698\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 83 = 2.92929085257698\n",
      "Error on this batch = 2.916287095627016\n",
      "Error on this batch = 2.858869765760511\n",
      "Cost on val dataset after 84 epochs is = 2.919128866106156\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 84 = 2.919128866106156\n",
      "Error on this batch = 2.9060567582952825\n",
      "Error on this batch = 2.8466488462560506\n",
      "Cost on val dataset after 85 epochs is = 2.9091151313632366\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 85 = 2.9091151313632366\n",
      "Error on this batch = 2.8960036449285695\n",
      "Error on this batch = 2.8346291410217304\n",
      "Cost on val dataset after 86 epochs is = 2.8992649829959842\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 86 = 2.8992649829959842\n",
      "Error on this batch = 2.8860796984392905\n",
      "Error on this batch = 2.8228141739860506\n",
      "Cost on val dataset after 87 epochs is = 2.8895660142387722\n",
      "learning rate =  0.01\n",
      "Initial Cost on dataset for this epoch 87 = 2.8895660142387722\n",
      "Error on this batch = 2.876238636661554\n",
      "Error on this batch = 2.811045567357023\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-380-d9bd1c19e61a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mtheta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mcount\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "costs = []\n",
    "epoch = 1\n",
    "start = time.time()\n",
    "#cost_init = cross_entropy_loss(X_train, theta, train_class_enc, m) #Validation loss not giving much info\n",
    "cost_init = cross_entropy_loss(X_valid, theta, valid_class_enc, X_valid.shape[0]) #Validation loss not giving much info\n",
    "costs.append(cost_init)\n",
    "early_stop= 0\n",
    "while(True):\n",
    "    count = 0\n",
    "    #lr = lr0/(np.power(epoch, 1/3))\n",
    "    #if(lr < 0.001): lr = 0.001\n",
    "    print(\"learning rate = \", lr)\n",
    "\n",
    "    print(\"Initial Cost on dataset for this epoch {} = {}\".format(epoch, cost_init))\n",
    "\n",
    "    for b in mini_batch:\n",
    "        X_b = b[0]\n",
    "        Y_b = b[1]\n",
    "        fm = forward_prop(X_b, theta)\n",
    "        delta = [None]*len(fm)\n",
    "\n",
    "        if (count % 60 == 0):\n",
    "            print(\"Error on this batch = \"+str(cross_entropy_loss(X_b, theta, Y_b, batch_size)))\n",
    "        #Backward Propagation\n",
    "\n",
    "        for l in range(len(fm)-1, 0, -1):\n",
    "            if (l == len(fm)-1):\n",
    "                #delta[l] = ((1/batch_size)*(Y_b - fm[l])*fm[l]*(1-fm[l]))\n",
    "                delta[l] = ((1/batch_size)*((Y_b/fm[l])-((1-Y_b)/(1-fm[l])))*fm[l]*(1-fm[l]))\n",
    "                #print(\"delta for last layer=\",delta[l].shape)\n",
    "            else:\n",
    "                delta[l]=np.dot(delta[l+1], theta[l].T)*deriv_relu(fm[l])\n",
    "                #print(\"delta for hidden layer=\",delta[l])\n",
    "\n",
    "        for t in range(len(theta)):\n",
    "            theta[t] += lr*np.dot(fm[t].T, delta[t+1])\n",
    "        \n",
    "        count+=1\n",
    "    epoch+=1 #Number of epochs\n",
    "    #ite+=1\n",
    "\n",
    "    #cost_final = cross_entropy_loss(X_train, theta, train_class_enc, m)\n",
    "    cost_final = cross_entropy_loss(X_valid, theta, valid_class_enc, X_valid.shape[0])\n",
    "    if(epoch%10==0): costs.append(cost_final)\n",
    "    print(\"Cost on val dataset after {} epochs is = {}\".format(epoch, cost_final))\n",
    "    \n",
    "    if ((cost_final-cost_init) > 0):\n",
    "        early_stop +=1\n",
    "    else:\n",
    "        early_stop=0\n",
    "\n",
    "    if (early_stop == 30):\n",
    "        print(\"cost initial= {} , cost final={} , change in cost= {}\".format(cost_init,cost_final, cost_final-cost_init))\n",
    "        break\n",
    "\n",
    "    cost_init = cost_final\n",
    "    \n",
    "    \n",
    "epochs.append(epoch)\n",
    "train_time.append(time.time()-start)\n",
    "train_accuracy.append(calc_accuracy(X_train, theta, train_class_enc))\n",
    "valid_accuracy.append(calc_accuracy(X_valid, theta, valid_class_enc))\n",
    "test_accuracy.append(calc_accuracy(X_test, theta, test_actual_class_enc))\n",
    "print(\"\\n------------------------------------------------------------------------------\")\n",
    "print(\"The stats for number of units in the hidden layer arch= {} are as below:\".format(arch))\n",
    "print(\"------------------------------------------------------------------------------\")\n",
    "print(\"The number of epochs = {:2.3f}\".format(epochs[-1]))\n",
    "print(\"The training time = {:2.3f}sec\".format(train_time[-1]))\n",
    "print(\"The training accuracy is = {:2.3f}%\".format(train_accuracy[-1]))\n",
    "print(\"The validation accuracy is = {:2.3f}%\".format(valid_accuracy[-1]))\n",
    "print(\"The test accuracy is = {:2.3f}%\".format(test_accuracy[-1]))\n",
    "print(\"------------------------------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38.30769230769231"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_accuracy(X_test, theta, test_actual_class_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_title(\"Train Loss with Epochs with ReLU\")\n",
    "x = np.arange(0,len(costs[1:]))\n",
    "ax.plot(x, costs[1:], marker='o', label='Train Loss')\n",
    "ax.set_xlabel(\"number of epochs\")\n",
    "ax.set_ylabel(\"Train Loss\")\n",
    "\n",
    "plt.legend()\n",
    "#plt.savefig(\"relu_trainloss.png\", dpi=1000, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_title(\"Loss with Epochs\")\n",
    "x = np.arange(0,len(costs[1:]))\n",
    "ax.plot(x, costs[1:], marker='o', label='Train Accuracy')\n",
    "ax.set_xlabel(\"number of epochs\")\n",
    "ax.set_ylabel(\"Train Loss\")\n",
    "\n",
    "plt.legend()\n",
    "#plt.savefig(\"accuracy_HiddenUnit_val20per.png\", dpi=1000, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
