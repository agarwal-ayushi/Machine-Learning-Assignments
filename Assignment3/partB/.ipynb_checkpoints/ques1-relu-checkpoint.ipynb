{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------Reading the Data-------------------------\n",
      "----------------Data Reading completed-------------------\n",
      "The total number of training samples = 13000\n",
      "The number of features = 784\n"
     ]
    }
   ],
   "source": [
    "print(\"----------------Reading the Data-------------------------\")\n",
    "PATH = os.getcwd()\n",
    "os.chdir('Alphabets/')\n",
    "\n",
    "X_train = pd.read_csv('train.csv', sep=',', header=None, index_col=False)\n",
    "X_test = pd.read_csv('test.csv', sep=',', header=None, index_col=False)\n",
    "np.random.shuffle(X_train.to_numpy())\n",
    "train_class = X_train[X_train.columns[-1]]\n",
    "test_actual_class = X_test[X_test.columns[-1]]\n",
    "\n",
    "X_train = X_train.drop(X_train.columns[-1], axis=1)\n",
    "X_test = X_test.drop(X_test.columns[-1], axis=1)\n",
    "\n",
    "print(\"----------------Data Reading completed-------------------\")\n",
    "\n",
    "os.chdir('../')\n",
    "\n",
    "X_train = X_train/255\n",
    "X_test = X_test/255\n",
    "\n",
    "m = X_train.shape[0] # Number of Training Samples\n",
    "\n",
    "# X_valid = X_train.iloc[(int(0.85*m)):]\n",
    "# valid_class = train_class[(int(0.85*m)):]\n",
    "# X_train = X_train.iloc[0:int(0.85*m)]\n",
    "# train_class = train_class[0:int(0.85*m)]\n",
    "\n",
    "\n",
    "m = X_train.shape[0] # Number of Training Samples\n",
    "n = X_train.shape[1] # Number of input features\n",
    "\n",
    "print(\"The total number of training samples = {}\".format(m))\n",
    "#print(\"The total number of validation samples = {}\".format(X_valid.shape[0]))\n",
    "\n",
    "print(\"The number of features = {}\".format(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Perform 1-hot encoding of class labels------------\n"
     ]
    }
   ],
   "source": [
    "#To get the one hot encoding of each label\n",
    "print(\"--------Perform 1-hot encoding of class labels------------\")\n",
    "\n",
    "train_class_enc = pd.get_dummies(train_class).to_numpy()\n",
    "#valid_class_enc = pd.get_dummies(valid_class).to_numpy()\n",
    "test_actual_class_enc = pd.get_dummies(test_actual_class).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add the intercept term to the data samples both in training and test dataset\n",
    "X_train = np.hstack((np.ones((m,1)),X_train.to_numpy()))\n",
    "#X_valid = np.hstack((np.ones((X_valid.shape[0],1)), X_valid.to_numpy()))\n",
    "X_test = np.hstack((np.ones((X_test.shape[0],1)),X_test.to_numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "arch_test = [1,5,10,50,100]\n",
    "arch = [arch_test[3]] #means one hidden layer with 2 perceptrons \n",
    "batch_size = 100 # Mini-Batch Size\n",
    "r = np.max(train_class) + 1 # Default value of the number of classes = 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of mini-batches formed is = 130\n"
     ]
    }
   ],
   "source": [
    "#Mini-Batch formation\n",
    "mini_batch = [(X_train[i:i+batch_size,:], train_class_enc[i:i+batch_size]) for i in range(0, m, batch_size)]\n",
    "print(\"The number of mini-batches formed is = {}\".format(len(mini_batch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Theta Initialization \n",
    "np.random.seed(1)\n",
    "def theta_init(arch=[50]):\n",
    "    theta = []\n",
    "    for i in range(len(arch)+1):\n",
    "        if i == 0:\n",
    "            dim0=n+1\n",
    "            dim1=arch[i]\n",
    "        elif (i == len(arch)):\n",
    "            dim0=arch[i-1]\n",
    "            dim1 = r\n",
    "        else:\n",
    "            dim0=arch[i-1]\n",
    "            dim1= arch[i]\n",
    "\n",
    "        theta.append(0.01*(2*np.random.random((dim0, dim1))-1))\n",
    "        #theta.append(np.zeros((dim0, dim1)))\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_act(x):\n",
    "#     x[x<=0] = 0.01*x[x<=0]\n",
    "#     return x\n",
    "    return np.maximum(0.0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softplus(x):\n",
    "    return np.log(1+np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deriv_softplus(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deriv_relu(x):\n",
    "    #x[x<=0] = -0.01\n",
    "    x[x<=0] = 0\n",
    "    x[x>0] = 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.31326169 0.31326169 0.69314718]\n",
      " [0.07888973 0.69314718 3.04858735]]\n",
      "[[0.73105858 0.26894142 0.5       ]\n",
      " [0.07585818 0.5        0.95257413]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1,-1,0], [-2.5, 0, 3]])\n",
    "print(softplus(a))\n",
    "print(deriv_softplus(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(data, theta):\n",
    "    fm = []\n",
    "    fm.append(data)\n",
    "    for l in range(len(theta)):\n",
    "        if (l != len(theta)-1):\n",
    "            #print(\"relu\")\n",
    "            fm.append(relu_act(np.dot(fm[l], theta[l])))\n",
    "        else:\n",
    "            fm.append(activation(np.dot(fm[l], theta[l])))\n",
    "            #print(\"sigmoid output\")\n",
    "    return fm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 26)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.2500020553563362"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta = theta_init([100, 100, 100])\n",
    "print(theta[3].shape)\n",
    "cost_total(X_train, theta, train_class_enc, m)\n",
    "#fm = forward_prop(X_train, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_total(X, theta, Y, m):\n",
    "    fm = forward_prop(X, theta)\n",
    "    cost = (1/(2*m))*np.sum((Y-fm[-1])**2)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(data, theta, actual_class):\n",
    "    pred_class = forward_prop(data, theta)\n",
    "    test_pred_class = pred_class[-1]\n",
    "    for i in range(len(test_pred_class)):\n",
    "        test_pred_class[i][test_pred_class[i] == np.max(test_pred_class[i])] = 1\n",
    "        test_pred_class[i][test_pred_class[i] != np.max(test_pred_class[i])] = 0\n",
    "\n",
    "\n",
    "    test_acc = 0\n",
    "    for i in range(len(actual_class)):\n",
    "        if (np.array_equal(test_pred_class[i], actual_class[i])):\n",
    "            test_acc+=1\n",
    "    test_acc /= data.shape[0]\n",
    "\n",
    "    #print(\"The Test Accuracy of the model = {}%\".format(test_acc*100))\n",
    "    return (test_acc*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = []\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "train_time = []\n",
    "valid_accuracy=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(785, 100) (100, 100) (100, 26)\n"
     ]
    }
   ],
   "source": [
    "arch=[100, 100]\n",
    "lr0=0.1\n",
    "#lr = 0.01\n",
    "theta = theta_init(arch)\n",
    "print(theta[0].shape, theta[1].shape, theta[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate =  0.1\n",
      "Initial Cost on Val dataset for this epoch 1 = 3.250017957732519\n",
      "Error on this batch = 3.2500253817448868\n",
      "Error on this batch = 0.4919501188513419\n",
      "Error on this batch = 0.4903316497963743\n",
      "Cost on val dataset after 2 epochs is = 0.4903708976190351\n",
      "learning rate =  0.07937005259840997\n",
      "Initial Cost on Val dataset for this epoch 2 = 0.4903708976190351\n",
      "Error on this batch = 0.49049580932528425\n",
      "Error on this batch = 0.4899073077641535\n",
      "Error on this batch = 0.48945233396025756\n",
      "Cost on val dataset after 3 epochs is = 0.4898093881712603\n",
      "learning rate =  0.06933612743506348\n",
      "Initial Cost on Val dataset for this epoch 3 = 0.4898093881712603\n",
      "Error on this batch = 0.48955150738505465\n",
      "Error on this batch = 0.4891148272167014\n",
      "Error on this batch = 0.48905755435741655\n",
      "Cost on val dataset after 4 epochs is = 0.4895032889081915\n",
      "learning rate =  0.06299605249474366\n",
      "Initial Cost on Val dataset for this epoch 4 = 0.4895032889081915\n",
      "Error on this batch = 0.48896389900758863\n",
      "Error on this batch = 0.4885872486782199\n",
      "Error on this batch = 0.4887967999480486\n",
      "Cost on val dataset after 5 epochs is = 0.4892653740666817\n",
      "learning rate =  0.05848035476425733\n",
      "Initial Cost on Val dataset for this epoch 5 = 0.4892653740666817\n",
      "Error on this batch = 0.48852493143919845\n",
      "Error on this batch = 0.4881755608999074\n",
      "Error on this batch = 0.48858163467092786\n",
      "Cost on val dataset after 6 epochs is = 0.48905465959087807\n",
      "learning rate =  0.05503212081491045\n",
      "Initial Cost on Val dataset for this epoch 6 = 0.48905465959087807\n",
      "Error on this batch = 0.4881643279220893\n",
      "Error on this batch = 0.4878267160332051\n",
      "Error on this batch = 0.4883839814877493\n",
      "Cost on val dataset after 7 epochs is = 0.48885741624866286\n",
      "learning rate =  0.05227579585747103\n",
      "Initial Cost on Val dataset for this epoch 7 = 0.48885741624866286\n",
      "Error on this batch = 0.4878499805209295\n",
      "Error on this batch = 0.4875176522310251\n",
      "Error on this batch = 0.4881910736675464\n",
      "Cost on val dataset after 8 epochs is = 0.4886667559972993\n",
      "learning rate =  0.05\n",
      "Initial Cost on Val dataset for this epoch 8 = 0.4886667559972993\n",
      "Error on this batch = 0.4875660035898801\n",
      "Error on this batch = 0.4872353935458428\n",
      "Error on this batch = 0.48799732062354084\n",
      "Cost on val dataset after 9 epochs is = 0.4884783869713226\n",
      "learning rate =  0.048074985676913616\n",
      "Initial Cost on Val dataset for this epoch 9 = 0.4884783869713226\n",
      "Error on this batch = 0.48730179238282745\n",
      "Error on this batch = 0.4869734057818165\n",
      "Error on this batch = 0.48779932262127473\n",
      "Cost on val dataset after 10 epochs is = 0.48828979567795144\n",
      "learning rate =  0.04641588833612779\n",
      "Initial Cost on Val dataset for this epoch 10 = 0.48828979567795144\n",
      "Error on this batch = 0.48705121142022917\n",
      "Error on this batch = 0.48672681550557356\n",
      "Error on this batch = 0.48759339260977624\n",
      "Cost on val dataset after 11 epochs is = 0.4880985524700613\n",
      "learning rate =  0.044964431302260924\n",
      "Initial Cost on Val dataset for this epoch 11 = 0.4880985524700613\n",
      "Error on this batch = 0.48681144502195006\n",
      "Error on this batch = 0.4864905304490632\n",
      "Error on this batch = 0.48737769242871054\n",
      "Cost on val dataset after 12 epochs is = 0.4879024730968132\n",
      "learning rate =  0.04367902323681495\n",
      "Initial Cost on Val dataset for this epoch 12 = 0.4879024730968132\n",
      "Error on this batch = 0.48657821990852657\n",
      "Error on this batch = 0.48626396318637144\n",
      "Error on this batch = 0.48715299708428517\n",
      "Cost on val dataset after 13 epochs is = 0.4877001235244252\n",
      "learning rate =  0.04252903702829902\n",
      "Initial Cost on Val dataset for this epoch 13 = 0.4877001235244252\n",
      "Error on this batch = 0.4863492134757991\n",
      "Error on this batch = 0.48604475807968456\n",
      "Error on this batch = 0.4869180297738596\n",
      "Cost on val dataset after 14 epochs is = 0.487490512726291\n",
      "learning rate =  0.04149132666831218\n",
      "Initial Cost on Val dataset for this epoch 14 = 0.487490512726291\n",
      "Error on this batch = 0.48612281228432097\n",
      "Error on this batch = 0.48583151384643586\n",
      "Error on this batch = 0.4866728029307788\n",
      "Cost on val dataset after 15 epochs is = 0.48727290351668967\n",
      "learning rate =  0.040548013303822676\n",
      "Initial Cost on Val dataset for this epoch 15 = 0.48727290351668967\n",
      "Error on this batch = 0.4858977621037904\n",
      "Error on this batch = 0.48562212126833837\n",
      "Error on this batch = 0.48641689483100053\n",
      "Cost on val dataset after 16 epochs is = 0.4870470597368561\n",
      "learning rate =  0.039685026299204985\n",
      "Initial Cost on Val dataset for this epoch 16 = 0.4870470597368561\n",
      "Error on this batch = 0.48567269696609644\n",
      "Error on this batch = 0.48541464749159247\n",
      "Error on this batch = 0.48615045423113307\n",
      "Cost on val dataset after 17 epochs is = 0.4868128220655325\n",
      "learning rate =  0.038891111873282036\n",
      "Initial Cost on Val dataset for this epoch 17 = 0.4868128220655325\n",
      "Error on this batch = 0.4854460277422662\n",
      "Error on this batch = 0.48520771444908223\n",
      "Error on this batch = 0.4858744418871925\n",
      "Cost on val dataset after 18 epochs is = 0.4865704422296346\n",
      "learning rate =  0.0381571414184444\n",
      "Initial Cost on Val dataset for this epoch 18 = 0.4865704422296346\n",
      "Error on this batch = 0.48521895681438215\n",
      "Error on this batch = 0.48500112058193184\n",
      "Error on this batch = 0.48559035432644165\n",
      "Cost on val dataset after 19 epochs is = 0.48632036471725026\n",
      "learning rate =  0.03747561767843155\n",
      "Initial Cost on Val dataset for this epoch 19 = 0.48632036471725026\n",
      "Error on this batch = 0.48499006222814983\n",
      "Error on this batch = 0.4847933253221379\n",
      "Error on this batch = 0.4853005262800488\n",
      "Cost on val dataset after 20 epochs is = 0.4860634437289247\n",
      "learning rate =  0.036840314986403874\n",
      "Initial Cost on Val dataset for this epoch 20 = 0.4860634437289247\n",
      "Error on this batch = 0.4847603560754293\n",
      "Error on this batch = 0.4845821704499166\n",
      "Error on this batch = 0.48500696381362435\n",
      "Cost on val dataset after 21 epochs is = 0.48580047631680484\n",
      "learning rate =  0.036246012433429745\n",
      "Initial Cost on Val dataset for this epoch 21 = 0.48580047631680484\n",
      "Error on this batch = 0.484528726482789\n",
      "Error on this batch = 0.4843671645822022\n",
      "Error on this batch = 0.4847115314934809\n",
      "Cost on val dataset after 22 epochs is = 0.48553290607512595\n",
      "learning rate =  0.035688292775180416\n",
      "Initial Cost on Val dataset for this epoch 22 = 0.48553290607512595\n",
      "Error on this batch = 0.48429691273591147\n",
      "Error on this batch = 0.48414714441102374\n",
      "Error on this batch = 0.48441676861210836\n",
      "Cost on val dataset after 23 epochs is = 0.4852622250215546\n",
      "learning rate =  0.03516338869169593\n",
      "Initial Cost on Val dataset for this epoch 23 = 0.4852622250215546\n",
      "Error on this batch = 0.48406392528176695\n",
      "Error on this batch = 0.4839206957434689\n",
      "Error on this batch = 0.4841246147079508\n",
      "Cost on val dataset after 24 epochs is = 0.4849900702817222\n",
      "learning rate =  0.03466806371753174\n",
      "Initial Cost on Val dataset for this epoch 24 = 0.4849900702817222\n",
      "Error on this batch = 0.4838302952784555\n",
      "Error on this batch = 0.4836887435170389\n",
      "Error on this batch = 0.483837805591891\n",
      "Cost on val dataset after 25 epochs is = 0.48471823455648766\n",
      "learning rate =  0.03419951893353394\n",
      "Initial Cost on Val dataset for this epoch 25 = 0.48471823455648766\n",
      "Error on this batch = 0.48359862349613614\n",
      "Error on this batch = 0.48345357472067535\n",
      "Error on this batch = 0.4835583220447004\n",
      "Cost on val dataset after 26 epochs is = 0.48444864157147954\n",
      "learning rate =  0.03375531905895819\n",
      "Initial Cost on Val dataset for this epoch 26 = 0.48444864157147954\n",
      "Error on this batch = 0.48337058911974906\n",
      "Error on this batch = 0.48321753286666613\n",
      "Error on this batch = 0.4832875749299922\n",
      "Cost on val dataset after 27 epochs is = 0.4841832969194412\n",
      "learning rate =  0.03333333333333333\n",
      "Initial Cost on Val dataset for this epoch 27 = 0.4841832969194412\n",
      "Error on this batch = 0.48314685602909974\n",
      "Error on this batch = 0.4829836552678068\n",
      "Error on this batch = 0.4830277634848229\n",
      "Cost on val dataset after 28 epochs is = 0.4839243198210093\n",
      "learning rate =  0.03293168780041748\n",
      "Initial Cost on Val dataset for this epoch 28 = 0.4839243198210093\n",
      "Error on this batch = 0.48292794132955236\n",
      "Error on this batch = 0.4827531384774845\n",
      "Error on this batch = 0.4827804708231407\n",
      "Cost on val dataset after 29 epochs is = 0.4836734177428633\n",
      "learning rate =  0.03254872647376677\n",
      "Initial Cost on Val dataset for this epoch 29 = 0.4836734177428633\n",
      "Error on this batch = 0.4827154235858339\n",
      "Error on this batch = 0.48252836368911234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on this batch = 0.48254651257228204\n",
      "Cost on val dataset after 30 epochs is = 0.4834322141025892\n",
      "learning rate =  0.03218297948685433\n",
      "Initial Cost on Val dataset for this epoch 30 = 0.4834322141025892\n",
      "Error on this batch = 0.482510823510965\n",
      "Error on this batch = 0.4823119971800023\n",
      "Error on this batch = 0.48232652384877456\n",
      "Cost on val dataset after 31 epochs is = 0.4832020618108614\n",
      "learning rate =  0.03183313678457734\n",
      "Initial Cost on Val dataset for this epoch 31 = 0.4832020618108614\n",
      "Error on this batch = 0.4823151969820505\n",
      "Error on this batch = 0.48210588706983715\n",
      "Error on this batch = 0.4821216627261577\n",
      "Cost on val dataset after 32 epochs is = 0.4829840886784467\n",
      "learning rate =  0.03149802624737183\n",
      "Initial Cost on Val dataset for this epoch 32 = 0.4829840886784467\n",
      "Error on this batch = 0.482130130222071\n",
      "Error on this batch = 0.48191197405490976\n",
      "Error on this batch = 0.4819318181222753\n",
      "Cost on val dataset after 33 epochs is = 0.48277902338366363\n",
      "learning rate =  0.031176595388187203\n",
      "Initial Cost on Val dataset for this epoch 33 = 0.48277902338366363\n",
      "Error on this batch = 0.4819564822717057\n",
      "Error on this batch = 0.4817315204190996\n",
      "Error on this batch = 0.4817573067116539\n",
      "Cost on val dataset after 34 epochs is = 0.4825873979910277\n",
      "learning rate =  0.03086789594993042\n",
      "Initial Cost on Val dataset for this epoch 34 = 0.4825873979910277\n",
      "Error on this batch = 0.4817951880712364\n",
      "Error on this batch = 0.4815652197707819\n",
      "Error on this batch = 0.48159777740864107\n",
      "Cost on val dataset after 35 epochs is = 0.4824094507081116\n",
      "learning rate =  0.03057107087328799\n",
      "Initial Cost on Val dataset for this epoch 35 = 0.4824094507081116\n",
      "Error on this batch = 0.4816466106745925\n",
      "Error on this batch = 0.4814136504340252\n",
      "Error on this batch = 0.4814530739937791\n",
      "Cost on val dataset after 36 epochs is = 0.48224522184598245\n",
      "learning rate =  0.030285343213868998\n",
      "Initial Cost on Val dataset for this epoch 36 = 0.48224522184598245\n",
      "Error on this batch = 0.4815110754779519\n",
      "Error on this batch = 0.48127733803431233\n",
      "Error on this batch = 0.4813228903931563\n",
      "Cost on val dataset after 37 epochs is = 0.4820944766374818\n",
      "learning rate =  0.03001000667185618\n",
      "Initial Cost on Val dataset for this epoch 37 = 0.4820944766374818\n",
      "Error on this batch = 0.4813891248920059\n",
      "Error on this batch = 0.4811559135717807\n",
      "Error on this batch = 0.4812064048926027\n",
      "Cost on val dataset after 38 epochs is = 0.4819568060247095\n",
      "learning rate =  0.02974441746295015\n",
      "Initial Cost on Val dataset for this epoch 38 = 0.4819568060247095\n",
      "Error on this batch = 0.481280202578961\n",
      "Error on this batch = 0.48104913836237134\n",
      "Error on this batch = 0.4811027166965866\n",
      "Cost on val dataset after 39 epochs is = 0.4818316230310401\n",
      "learning rate =  0.02948798731084674\n",
      "Initial Cost on Val dataset for this epoch 39 = 0.4818316230310401\n",
      "Error on this batch = 0.48118388214765556\n",
      "Error on this batch = 0.4809555996241437\n",
      "Error on this batch = 0.481011138836995\n",
      "Cost on val dataset after 40 epochs is = 0.48171828773268516\n",
      "learning rate =  0.029240177382128665\n",
      "Initial Cost on Val dataset for this epoch 40 = 0.48171828773268516\n",
      "Error on this batch = 0.48109948003760045\n",
      "Error on this batch = 0.4808747003867353\n",
      "Error on this batch = 0.480931095216567\n",
      "Cost on val dataset after 41 epochs is = 0.48161597830304276\n",
      "learning rate =  0.029000493016762666\n",
      "Initial Cost on Val dataset for this epoch 41 = 0.48161597830304276\n",
      "Error on this batch = 0.48102595941365994\n",
      "Error on this batch = 0.480805759574862\n",
      "Error on this batch = 0.4808612686756547\n",
      "Cost on val dataset after 42 epochs is = 0.4815238890567596\n",
      "learning rate =  0.028768479133239405\n",
      "Initial Cost on Val dataset for this epoch 42 = 0.4815238890567596\n",
      "Error on this batch = 0.48096255614093725\n",
      "Error on this batch = 0.48074769728385114\n",
      "Error on this batch = 0.4808009116629166\n",
      "Cost on val dataset after 43 epochs is = 0.48144121198204703\n",
      "learning rate =  0.02854371620818945\n",
      "Initial Cost on Val dataset for this epoch 43 = 0.48144121198204703\n",
      "Error on this batch = 0.48090862398821116\n",
      "Error on this batch = 0.48069914167858424\n",
      "Error on this batch = 0.4807492700632781\n",
      "Cost on val dataset after 44 epochs is = 0.4813671401040767\n",
      "learning rate =  0.02832581674713524\n",
      "Initial Cost on Val dataset for this epoch 44 = 0.4813671401040767\n",
      "Error on this batch = 0.4808629791084758\n",
      "Error on this batch = 0.48065916456766317\n",
      "Error on this batch = 0.48070551916188864\n",
      "Cost on val dataset after 45 epochs is = 0.48130089805014614\n",
      "learning rate =  0.02811442217672498\n",
      "Initial Cost on Val dataset for this epoch 45 = 0.48130089805014614\n",
      "Error on this batch = 0.4808246843741759\n",
      "Error on this batch = 0.48062699236409145\n",
      "Error on this batch = 0.48066872466203053\n",
      "Cost on val dataset after 46 epochs is = 0.4812417428733449\n",
      "learning rate =  0.02790920009998241\n",
      "Initial Cost on Val dataset for this epoch 46 = 0.4812417428733449\n",
      "Error on this batch = 0.48079290359608756\n",
      "Error on this batch = 0.4806015561421934\n",
      "Error on this batch = 0.4806383473678676\n",
      "Cost on val dataset after 47 epochs is = 0.4811889940583074\n",
      "learning rate =  0.02770984186529621\n",
      "Initial Cost on Val dataset for this epoch 47 = 0.4811889940583074\n",
      "Error on this batch = 0.4807668856639066\n",
      "Error on this batch = 0.4805819620432623\n",
      "Error on this batch = 0.4806135835772403\n",
      "Cost on val dataset after 48 epochs is = 0.4811420048497595\n",
      "learning rate =  0.027516060407455228\n",
      "Initial Cost on Val dataset for this epoch 48 = 0.4811420048497595\n",
      "Error on this batch = 0.4807458119789556\n",
      "Error on this batch = 0.48056745015555796\n",
      "Error on this batch = 0.4805938747945557\n",
      "Cost on val dataset after 49 epochs is = 0.4811002032153939\n",
      "learning rate =  0.027327588325319847\n",
      "Initial Cost on Val dataset for this epoch 49 = 0.4811002032153939\n",
      "Error on this batch = 0.48072901580589844\n",
      "Error on this batch = 0.4805572608625866\n",
      "Error on this batch = 0.4805787608299197\n",
      "Cost on val dataset after 50 epochs is = 0.4810630405382452\n",
      "learning rate =  0.02714417616594907\n",
      "Initial Cost on Val dataset for this epoch 50 = 0.4810630405382452\n",
      "Error on this batch = 0.48071582007113867\n",
      "Error on this batch = 0.4805507429366354\n",
      "Error on this batch = 0.4805675478445802\n",
      "Cost on val dataset after 51 epochs is = 0.4810300305118039\n",
      "learning rate =  0.02696559088937193\n",
      "Initial Cost on Val dataset for this epoch 51 = 0.4810300305118039\n",
      "Error on this batch = 0.4807057199088237\n",
      "Error on this batch = 0.48054733117852366\n",
      "Error on this batch = 0.48055979753516215\n",
      "Cost on val dataset after 52 epochs is = 0.4810007198955164\n",
      "learning rate =  0.02679161449185622\n",
      "Initial Cost on Val dataset for this epoch 52 = 0.4810007198955164\n",
      "Error on this batch = 0.4806982226556849\n",
      "Error on this batch = 0.48054647381232735\n",
      "Error on this batch = 0.4805550452194413\n",
      "Cost on val dataset after 53 epochs is = 0.4809747127693261\n",
      "learning rate =  0.026622042768611638\n",
      "Initial Cost on Val dataset for this epoch 53 = 0.4809747127693261\n",
      "Error on this batch = 0.4806928726021914\n",
      "Error on this batch = 0.4805478020050501\n",
      "Error on this batch = 0.480552860873241\n",
      "Cost on val dataset after 54 epochs is = 0.4809516522583304\n",
      "learning rate =  0.026456684199469994\n",
      "Initial Cost on Val dataset for this epoch 54 = 0.4809516522583304\n",
      "Error on this batch = 0.48068931878008103\n",
      "Error on this batch = 0.4805509603912808\n",
      "Error on this batch = 0.4805528865725286\n",
      "Cost on val dataset after 55 epochs is = 0.480931213271166\n",
      "learning rate =  0.026295358943292955\n",
      "Initial Cost on Val dataset for this epoch 55 = 0.480931213271166\n",
      "Error on this batch = 0.48068723203612607\n",
      "Error on this batch = 0.4805556118484274\n",
      "Error on this batch = 0.4805547789749138\n",
      "Cost on val dataset after 56 epochs is = 0.4809131127259409\n",
      "learning rate =  0.026137897928735516\n",
      "Initial Cost on Val dataset for this epoch 56 = 0.4809131127259409\n",
      "Error on this batch = 0.48068635780496655\n",
      "Error on this batch = 0.4805614214464905\n",
      "Error on this batch = 0.48055824217768434\n",
      "Cost on val dataset after 57 epochs is = 0.48089709496675054\n",
      "learning rate =  0.025984142030594473\n",
      "Initial Cost on Val dataset for this epoch 57 = 0.48089709496675054\n",
      "Error on this batch = 0.4806864647085453\n",
      "Error on this batch = 0.4805681480189249\n",
      "Error on this batch = 0.48056300563066295\n",
      "Cost on val dataset after 58 epochs is = 0.48088292575135033\n",
      "learning rate =  0.025833941322341274\n",
      "Initial Cost on Val dataset for this epoch 58 = 0.48088292575135033\n",
      "Error on this batch = 0.48068735555519176\n",
      "Error on this batch = 0.4805755764619318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on this batch = 0.48056880484778286\n",
      "Cost on val dataset after 59 epochs is = 0.4808703999129515\n",
      "learning rate =  0.025687154396613652\n",
      "Initial Cost on Val dataset for this epoch 59 = 0.4808703999129515\n",
      "Error on this batch = 0.48068885605801065\n",
      "Error on this batch = 0.4805835165655346\n",
      "Error on this batch = 0.48057540931338255\n",
      "Cost on val dataset after 60 epochs is = 0.4808593355978327\n",
      "learning rate =  0.02554364774645177\n",
      "Initial Cost on Val dataset for this epoch 60 = 0.4808593355978327\n",
      "Error on this batch = 0.4806908282356692\n",
      "Error on this batch = 0.4805917989498298\n",
      "Error on this batch = 0.4805826335312142\n",
      "Cost on val dataset after 61 epochs is = 0.48084956371229737\n",
      "learning rate =  0.02540329520093663\n",
      "Initial Cost on Val dataset for this epoch 61 = 0.48084956371229737\n",
      "Error on this batch = 0.48069316331178075\n",
      "Error on this batch = 0.4806002946404058\n",
      "Error on this batch = 0.48059029528217795\n",
      "Cost on val dataset after 62 epochs is = 0.4808409357354812\n",
      "learning rate =  0.02526597740964283\n",
      "Initial Cost on Val dataset for this epoch 62 = 0.4808409357354812\n",
      "Error on this batch = 0.48069575923297847\n",
      "Error on this batch = 0.4806088819021413\n",
      "Error on this batch = 0.4805982435728579\n",
      "Cost on val dataset after 63 epochs is = 0.48083332070707835\n",
      "learning rate =  0.025131581370971795\n",
      "Initial Cost on Val dataset for this epoch 63 = 0.48083332070707835\n",
      "Error on this batch = 0.4806985374816039\n",
      "Error on this batch = 0.480617464161088\n",
      "Error on this batch = 0.4806063541859149\n",
      "Cost on val dataset after 64 epochs is = 0.4808266002582606\n",
      "learning rate =  0.025000000000000005\n",
      "Initial Cost on Val dataset for this epoch 64 = 0.4808266002582606\n",
      "Error on this batch = 0.4807014241987501\n",
      "Error on this batch = 0.4806259616682743\n",
      "Error on this batch = 0.48061451656782184\n",
      "Cost on val dataset after 65 epochs is = 0.48082066946655555\n",
      "learning rate =  0.024871131731971627\n",
      "Initial Cost on Val dataset for this epoch 65 = 0.48082066946655555\n",
      "Error on this batch = 0.4807043681927189\n",
      "Error on this batch = 0.4806343179528075\n",
      "Error on this batch = 0.48062263588190357\n",
      "Cost on val dataset after 66 epochs is = 0.48081543427266066\n",
      "learning rate =  0.02474488015799764\n",
      "Initial Cost on Val dataset for this epoch 66 = 0.48081543427266066\n",
      "Error on this batch = 0.48070732916631004\n",
      "Error on this batch = 0.4806424700530452\n",
      "Error on this batch = 0.48063064217240653\n",
      "Cost on val dataset after 67 epochs is = 0.48081081142573073\n",
      "learning rate =  0.02462115368990136\n",
      "Initial Cost on Val dataset for this epoch 67 = 0.48081081142573073\n",
      "Error on this batch = 0.4807102686688687\n",
      "Error on this batch = 0.48065037792601256\n",
      "Error on this batch = 0.4806384840006453\n",
      "Cost on val dataset after 68 epochs is = 0.4808067274015393\n",
      "learning rate =  0.024499865251482233\n",
      "Initial Cost on Val dataset for this epoch 68 = 0.4808067274015393\n",
      "Error on this batch = 0.48071316234234224\n",
      "Error on this batch = 0.48065800698919214\n",
      "Error on this batch = 0.4806461072267986\n",
      "Cost on val dataset after 69 epochs is = 0.4808031149695296\n",
      "learning rate =  0.02438093199376099\n",
      "Initial Cost on Val dataset for this epoch 69 = 0.4808031149695296\n",
      "Error on this batch = 0.4807159784498029\n",
      "Error on this batch = 0.4806653351666723\n",
      "Error on this batch = 0.48065347474253095\n",
      "Cost on val dataset after 70 epochs is = 0.48079991530604005\n",
      "learning rate =  0.02426427503202587\n",
      "Initial Cost on Val dataset for this epoch 70 = 0.48079991530604005\n",
      "Error on this batch = 0.48071870241266207\n",
      "Error on this batch = 0.48067234121757224\n",
      "Error on this batch = 0.4806605562912642\n",
      "Cost on val dataset after 71 epochs is = 0.48079707793082366\n",
      "learning rate =  0.02414981920272584\n",
      "Initial Cost on Val dataset for this epoch 71 = 0.48079707793082366\n",
      "Error on this batch = 0.48072132020105346\n",
      "Error on this batch = 0.48067902154289044\n",
      "Error on this batch = 0.48066733907527104\n",
      "Cost on val dataset after 72 epochs is = 0.4807945573912108\n",
      "learning rate =  0.024037492838456808\n",
      "Initial Cost on Val dataset for this epoch 72 = 0.4807945573912108\n",
      "Error on this batch = 0.4807238249351159\n",
      "Error on this batch = 0.480685373249587\n",
      "Error on this batch = 0.4806738098790588\n",
      "Cost on val dataset after 73 epochs is = 0.4807923146011717\n",
      "learning rate =  0.023927227559463728\n",
      "Initial Cost on Val dataset for this epoch 73 = 0.4807923146011717\n",
      "Error on this batch = 0.4807262206907031\n",
      "Error on this batch = 0.4806913887138285\n",
      "Error on this batch = 0.4806799604174909\n",
      "Cost on val dataset after 74 epochs is = 0.48079031381214055\n",
      "learning rate =  0.023818958080238595\n",
      "Initial Cost on Val dataset for this epoch 74 = 0.48079031381214055\n",
      "Error on this batch = 0.48072850134924566\n",
      "Error on this batch = 0.4806970707131674\n",
      "Error on this batch = 0.48068579065646233\n",
      "Cost on val dataset after 75 epochs is = 0.48078852538371114\n",
      "learning rate =  0.023712622029933756\n",
      "Initial Cost on Val dataset for this epoch 75 = 0.48078852538371114\n",
      "Error on this batch = 0.4807306675041606\n",
      "Error on this batch = 0.4807024235853076\n",
      "Error on this batch = 0.4806913021744058\n",
      "Cost on val dataset after 76 epochs is = 0.4807869227434747\n",
      "learning rate =  0.023608159785434173\n",
      "Initial Cost on Val dataset for this epoch 76 = 0.4807869227434747\n",
      "Error on this batch = 0.48073271362726644\n",
      "Error on this batch = 0.4807074576320027\n",
      "Error on this batch = 0.4806965013993118\n",
      "Cost on val dataset after 77 epochs is = 0.4807854830919759\n",
      "learning rate =  0.02350551431604272\n",
      "Initial Cost on Val dataset for this epoch 77 = 0.4807854830919759\n",
      "Error on this batch = 0.480734643648991\n",
      "Error on this batch = 0.4807121847938739\n",
      "Error on this batch = 0.4807013965762829\n",
      "Cost on val dataset after 78 epochs is = 0.4807841855184799\n",
      "learning rate =  0.023404631038831517\n",
      "Initial Cost on Val dataset for this epoch 78 = 0.4807841855184799\n",
      "Error on this batch = 0.48073645993363584\n",
      "Error on this batch = 0.4807166149208862\n",
      "Error on this batch = 0.4807059965257841\n",
      "Cost on val dataset after 79 epochs is = 0.48078301200309814\n",
      "learning rate =  0.023305457683800566\n",
      "Initial Cost on Val dataset for this epoch 79 = 0.48078301200309814\n",
      "Error on this batch = 0.48073816670278363\n",
      "Error on this batch = 0.480720762339983\n",
      "Error on this batch = 0.48071031629130956\n",
      "Cost on val dataset after 80 epochs is = 0.4807819473670471\n",
      "learning rate =  0.0232079441680639\n",
      "Initial Cost on Val dataset for this epoch 80 = 0.4807819473670471\n",
      "Error on this batch = 0.48073976849836914\n",
      "Error on this batch = 0.4807246400597785\n",
      "Error on this batch = 0.48071436138351575\n",
      "Cost on val dataset after 81 epochs is = 0.4807809782581174\n",
      "learning rate =  0.023112042478354494\n",
      "Initial Cost on Val dataset for this epoch 81 = 0.4807809782581174\n",
      "Error on this batch = 0.4807412710340283\n",
      "Error on this batch = 0.4807282602502812\n",
      "Error on this batch = 0.4807181486093964\n",
      "Cost on val dataset after 82 epochs is = 0.480780092709706\n",
      "learning rate =  0.023017706561202743\n",
      "Initial Cost on Val dataset for this epoch 82 = 0.480780092709706\n",
      "Error on this batch = 0.48074267810032084\n",
      "Error on this batch = 0.4807316359868364\n",
      "Error on this batch = 0.4807216885042277\n",
      "Cost on val dataset after 83 epochs is = 0.4807792794509047\n",
      "learning rate =  0.02292489222020055\n",
      "Initial Cost on Val dataset for this epoch 83 = 0.4807792794509047\n",
      "Error on this batch = 0.48074400005697443\n",
      "Error on this batch = 0.48073477731555175\n",
      "Error on this batch = 0.4807249874359252\n",
      "Cost on val dataset after 84 epochs is = 0.4807785289869234\n",
      "learning rate =  0.022833557019814713\n",
      "Initial Cost on Val dataset for this epoch 84 = 0.4807785289869234\n",
      "Error on this batch = 0.48074523844985256\n",
      "Error on this batch = 0.48073770250839193\n",
      "Error on this batch = 0.48072805608932234\n",
      "Cost on val dataset after 85 epochs is = 0.4807778310971659\n",
      "learning rate =  0.022743660195259533\n",
      "Initial Cost on Val dataset for this epoch 85 = 0.4807778310971659\n",
      "Error on this batch = 0.4807463997561051\n",
      "Error on this batch = 0.48074042249899523\n",
      "Error on this batch = 0.48073089696573806\n",
      "Cost on val dataset after 86 epochs is = 0.480777172441376\n",
      "learning rate =  0.02265516256798084\n",
      "Initial Cost on Val dataset for this epoch 86 = 0.480777172441376\n",
      "Error on this batch = 0.4807474783054433\n",
      "Error on this batch = 0.4807429253958387\n",
      "Error on this batch = 0.4807335200057408\n",
      "Cost on val dataset after 87 epochs is = 0.4807765455573464\n",
      "learning rate =  0.022568026466341165\n",
      "Initial Cost on Val dataset for this epoch 87 = 0.4807765455573464\n",
      "Error on this batch = 0.4807484808833586\n",
      "Error on this batch = 0.48074523395392943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on this batch = 0.48073596001760927\n",
      "Cost on val dataset after 88 epochs is = 0.4807759469609745\n",
      "learning rate =  0.022482215651130462\n",
      "Initial Cost on Val dataset for this epoch 88 = 0.4807759469609745\n",
      "Error on this batch = 0.48074941823631817\n",
      "Error on this batch = 0.4807473728091627\n",
      "Error on this batch = 0.48073823371511104\n",
      "Cost on val dataset after 89 epochs is = 0.4807753856194715\n",
      "learning rate =  0.02239769524555751\n",
      "Initial Cost on Val dataset for this epoch 89 = 0.4807753856194715\n",
      "Error on this batch = 0.4807502955783265\n",
      "Error on this batch = 0.4807493651152882\n",
      "Error on this batch = 0.4807403613570289\n",
      "Cost on val dataset after 90 epochs is = 0.48077487115758294\n",
      "learning rate =  0.022314431669405655\n",
      "Initial Cost on Val dataset for this epoch 90 = 0.48077487115758294\n",
      "Error on this batch = 0.4807511291222314\n",
      "Error on this batch = 0.4807512225450306\n",
      "Error on this batch = 0.4807423452030794\n",
      "Cost on val dataset after 91 epochs is = 0.480774396723011\n",
      "learning rate =  0.022232392577061857\n",
      "Initial Cost on Val dataset for this epoch 91 = 0.480774396723011\n",
      "Error on this batch = 0.4807519249293854\n",
      "Error on this batch = 0.4807529541486412\n",
      "Error on this batch = 0.48074419350478936\n",
      "Cost on val dataset after 92 epochs is = 0.48077395675449924\n",
      "learning rate =  0.022151546799151527\n",
      "Initial Cost on Val dataset for this epoch 92 = 0.48077395675449924\n",
      "Error on this batch = 0.48075266949314077\n",
      "Error on this batch = 0.48075456685863494\n",
      "Error on this batch = 0.4807459120637057\n",
      "Cost on val dataset after 93 epochs is = 0.4807735440513247\n",
      "learning rate =  0.022071864287532612\n",
      "Initial Cost on Val dataset for this epoch 93 = 0.4807735440513247\n",
      "Error on this batch = 0.4807533657304828\n",
      "Error on this batch = 0.48075606833278783\n",
      "Error on this batch = 0.4807475071524566\n",
      "Cost on val dataset after 94 epochs is = 0.4807731551560082\n",
      "learning rate =  0.021993316063421826\n",
      "Initial Cost on Val dataset for this epoch 94 = 0.4807731551560082\n",
      "Error on this batch = 0.4807540180508042\n",
      "Error on this batch = 0.4807574638824222\n",
      "Error on this batch = 0.4807489882843239\n",
      "Cost on val dataset after 95 epochs is = 0.48077278674590557\n",
      "learning rate =  0.021915874168443504\n",
      "Initial Cost on Val dataset for this epoch 95 = 0.48077278674590557\n",
      "Error on this batch = 0.48075462795696683\n",
      "Error on this batch = 0.4807587596292573\n",
      "Error on this batch = 0.48075036163509366\n",
      "Cost on val dataset after 96 epochs is = 0.48077243707019507\n",
      "learning rate =  0.021839511618407473\n",
      "Initial Cost on Val dataset for this epoch 96 = 0.48077243707019507\n",
      "Error on this batch = 0.48075519907980235\n",
      "Error on this batch = 0.4807599607977386\n",
      "Error on this batch = 0.4807516350348266\n",
      "Cost on val dataset after 97 epochs is = 0.4807721037855264\n",
      "learning rate =  0.02176420235963729\n",
      "Initial Cost on Val dataset for this epoch 97 = 0.4807721037855264\n",
      "Error on this batch = 0.48075573484508477\n",
      "Error on this batch = 0.48076107211575664\n",
      "Error on this batch = 0.48075281658092395\n",
      "Cost on val dataset after 98 epochs is = 0.4807717855422747\n",
      "learning rate =  0.021689921227683307\n",
      "Initial Cost on Val dataset for this epoch 98 = 0.4807717855422747\n",
      "Error on this batch = 0.4807562382625327\n",
      "Error on this batch = 0.4807620992992554\n",
      "Error on this batch = 0.48075390957722425\n",
      "Cost on val dataset after 99 epochs is = 0.48077148089619487\n",
      "learning rate =  0.0216166439082676\n",
      "Initial Cost on Val dataset for this epoch 99 = 0.48077148089619487\n",
      "Error on this batch = 0.48075670954699645\n",
      "Error on this batch = 0.48076304875860054\n",
      "Error on this batch = 0.48075492044228546\n",
      "Cost on val dataset after 100 epochs is = 0.48077118833618987\n",
      "learning rate =  0.02154434690031884\n",
      "Initial Cost on Val dataset for this epoch 100 = 0.48077118833618987\n",
      "Error on this batch = 0.48075715157954063\n",
      "Error on this batch = 0.48076392570030707\n",
      "Error on this batch = 0.48075585458773235\n",
      "Cost on val dataset after 101 epochs is = 0.4807709069401677\n",
      "learning rate =  0.021473007480965672\n",
      "Initial Cost on Val dataset for this epoch 101 = 0.4807709069401677\n",
      "Error on this batch = 0.4807575667095375\n",
      "Error on this batch = 0.4807647364370701\n",
      "Error on this batch = 0.480756717589435\n",
      "Cost on val dataset after 102 epochs is = 0.4807706357614071\n",
      "learning rate =  0.02140260367236655\n",
      "Initial Cost on Val dataset for this epoch 102 = 0.4807706357614071\n",
      "Error on this batch = 0.48075795726783965\n",
      "Error on this batch = 0.4807654855291129\n",
      "Error on this batch = 0.48075751466946226\n",
      "Cost on val dataset after 103 epochs is = 0.48077037423864666\n",
      "learning rate =  0.021333114210262797\n",
      "Initial Cost on Val dataset for this epoch 103 = 0.48077037423864666\n",
      "Error on this batch = 0.4807583230137078\n",
      "Error on this batch = 0.48076617735798155\n",
      "Error on this batch = 0.48075824981112775\n",
      "Cost on val dataset after 104 epochs is = 0.4807701215114048\n",
      "learning rate =  0.02126451851414951\n",
      "Initial Cost on Val dataset for this epoch 104 = 0.4807701215114048\n",
      "Error on this batch = 0.48075866816887813\n",
      "Error on this batch = 0.48076681528168635\n",
      "Error on this batch = 0.48075892718678975\n",
      "Cost on val dataset after 105 epochs is = 0.48076987676443844\n",
      "learning rate =  0.021196796658966535\n",
      "Initial Cost on Val dataset for this epoch 105 = 0.48076987676443844\n",
      "Error on this batch = 0.4807589932089614\n",
      "Error on this batch = 0.4807674050515463\n",
      "Error on this batch = 0.48075955115176877\n",
      "Cost on val dataset after 106 epochs is = 0.48076963934086375\n",
      "learning rate =  0.02112992934821826\n",
      "Initial Cost on Val dataset for this epoch 106 = 0.48076963934086375\n",
      "Error on this batch = 0.4807592977848936\n",
      "Error on this batch = 0.48076795076574175\n",
      "Error on this batch = 0.48076012599061485\n",
      "Cost on val dataset after 107 epochs is = 0.48076940919670275\n",
      "learning rate =  0.02106389788843754\n",
      "Initial Cost on Val dataset for this epoch 107 = 0.48076940919670275\n",
      "Error on this batch = 0.48075958335105257\n",
      "Error on this batch = 0.48076845422418296\n",
      "Error on this batch = 0.48076065372423715\n",
      "Cost on val dataset after 108 epochs is = 0.48076918560764753\n",
      "learning rate =  0.020998684164914554\n",
      "Initial Cost on Val dataset for this epoch 108 = 0.48076918560764753\n",
      "Error on this batch = 0.48075985217839545\n",
      "Error on this batch = 0.48076891756351087\n",
      "Error on this batch = 0.4807611376103661\n",
      "Cost on val dataset after 109 epochs is = 0.4807689679429917\n",
      "learning rate =  0.020934270618616926\n",
      "Initial Cost on Val dataset for this epoch 109 = 0.4807689679429917\n",
      "Error on this batch = 0.48076010462883634\n",
      "Error on this batch = 0.48076934351662387\n",
      "Error on this batch = 0.4807615808980353\n",
      "Cost on val dataset after 110 epochs is = 0.48076875584540424\n",
      "learning rate =  0.02087064022423232\n",
      "Initial Cost on Val dataset for this epoch 110 = 0.48076875584540424\n",
      "Error on this batch = 0.4807603422813273\n",
      "Error on this batch = 0.4807697353014222\n",
      "Error on this batch = 0.48076198702488215\n",
      "Cost on val dataset after 111 epochs is = 0.48076854922728945\n",
      "learning rate =  0.020807776469269252\n",
      "Initial Cost on Val dataset for this epoch 111 = 0.48076854922728945\n",
      "Error on this batch = 0.4807605654702245\n",
      "Error on this batch = 0.48077009514239505\n",
      "Error on this batch = 0.48076235854646643\n",
      "Cost on val dataset after 112 epochs is = 0.4807683475061234\n",
      "learning rate =  0.02074566333415609\n",
      "Initial Cost on Val dataset for this epoch 112 = 0.4807683475061234\n",
      "Error on this batch = 0.48076077570863507\n",
      "Error on this batch = 0.4807704253100355\n",
      "Error on this batch = 0.48076269648656755\n",
      "Cost on val dataset after 113 epochs is = 0.480768150687341\n",
      "learning rate =  0.02068428527328215\n",
      "Initial Cost on Val dataset for this epoch 113 = 0.480768150687341\n",
      "Error on this batch = 0.48076097343270574\n",
      "Error on this batch = 0.48077072655554537\n",
      "Error on this batch = 0.48076300543955475\n",
      "Cost on val dataset after 114 epochs is = 0.48076795828809754\n",
      "learning rate =  0.02062362719692839\n",
      "Initial Cost on Val dataset for this epoch 114 = 0.48076795828809754\n",
      "Error on this batch = 0.4807611590186932\n",
      "Error on this batch = 0.48077100191349603\n",
      "Error on this batch = 0.48076328764382664\n",
      "Cost on val dataset after 115 epochs is = 0.48076776992904857\n",
      "learning rate =  0.020563674454038526\n",
      "Initial Cost on Val dataset for this epoch 115 = 0.48076776992904857\n",
      "Error on this batch = 0.4807613341008679\n",
      "Error on this batch = 0.48077125345004285\n",
      "Error on this batch = 0.48076354304098773\n",
      "Cost on val dataset after 116 epochs is = 0.480767585329138\n",
      "learning rate =  0.020504412815784637\n",
      "Initial Cost on Val dataset for this epoch 116 = 0.480767585329138\n",
      "Error on this batch = 0.480761498136852\n",
      "Error on this batch = 0.4807714822994346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on this batch = 0.4807637727615071\n",
      "Cost on val dataset after 117 epochs is = 0.4807674045039631\n",
      "learning rate =  0.02044582845988404\n",
      "Initial Cost on Val dataset for this epoch 117 = 0.4807674045039631\n",
      "Error on this batch = 0.4807616522751488\n",
      "Error on this batch = 0.4807716887379621\n",
      "Error on this batch = 0.48076397986828284\n",
      "Cost on val dataset after 118 epochs is = 0.48076722691766477\n",
      "learning rate =  0.020387907955627036\n",
      "Initial Cost on Val dataset for this epoch 118 = 0.48076722691766477\n",
      "Error on this batch = 0.48076179707920674\n",
      "Error on this batch = 0.48077187468064725\n",
      "Error on this batch = 0.4807641662212184\n",
      "Cost on val dataset after 119 epochs is = 0.48076705258692487\n",
      "learning rate =  0.020330638249577593\n",
      "Initial Cost on Val dataset for this epoch 119 = 0.48076705258692487\n",
      "Error on this batch = 0.4807619327847806\n",
      "Error on this batch = 0.48077204242972554\n",
      "Error on this batch = 0.4807643331256265\n",
      "Cost on val dataset after 120 epochs is = 0.48076688137468937\n",
      "learning rate =  0.020274006651911338\n",
      "Initial Cost on Val dataset for this epoch 120 = 0.48076688137468937\n",
      "Error on this batch = 0.4807620591722129\n",
      "Error on this batch = 0.48077219328785836\n",
      "Error on this batch = 0.48076448068795735\n",
      "Cost on val dataset after 121 epochs is = 0.48076671303705626\n",
      "learning rate =  0.020218000823357417\n",
      "Initial Cost on Val dataset for this epoch 121 = 0.48076671303705626\n",
      "Error on this batch = 0.4807621765184843\n",
      "Error on this batch = 0.4807723275545081\n",
      "Error on this batch = 0.48076461141273613\n",
      "Cost on val dataset after 122 epochs is = 0.48076654752154585\n",
      "learning rate =  0.020162608762712763\n",
      "Initial Cost on Val dataset for this epoch 122 = 0.48076654752154585\n",
      "Error on this batch = 0.480762286875084\n",
      "Error on this batch = 0.48077244756763565\n",
      "Error on this batch = 0.48076472660074737\n",
      "Cost on val dataset after 123 epochs is = 0.4807663848185516\n",
      "learning rate =  0.020107818794899247\n",
      "Initial Cost on Val dataset for this epoch 123 = 0.4807663848185516\n",
      "Error on this batch = 0.4807623897779568\n",
      "Error on this batch = 0.4807725534506565\n",
      "Error on this batch = 0.4807648270312788\n",
      "Cost on val dataset after 124 epochs is = 0.4807662247955047\n",
      "learning rate =  0.020053619559535895\n",
      "Initial Cost on Val dataset for this epoch 124 = 0.4807662247955047\n",
      "Error on this batch = 0.4807624858613059\n",
      "Error on this batch = 0.48077264744876447\n",
      "Error on this batch = 0.4807649144867719\n",
      "Cost on val dataset after 125 epochs is = 0.48076606746211575\n",
      "learning rate =  0.020000000000000004\n",
      "Initial Cost on Val dataset for this epoch 125 = 0.48076606746211575\n",
      "Error on this batch = 0.48076257665261507\n",
      "Error on this batch = 0.4807727293789402\n",
      "Error on this batch = 0.4807649892973156\n",
      "Cost on val dataset after 126 epochs is = 0.48076591258153856\n",
      "learning rate =  0.019946949352952516\n",
      "Initial Cost on Val dataset for this epoch 126 = 0.48076591258153856\n",
      "Error on this batch = 0.4807626617516608\n",
      "Error on this batch = 0.48077279958565045\n",
      "Error on this batch = 0.4807650529557067\n",
      "Cost on val dataset after 127 epochs is = 0.4807657600131754\n",
      "learning rate =  0.019894457138304456\n",
      "Initial Cost on Val dataset for this epoch 127 = 0.4807657600131754\n",
      "Error on this batch = 0.48076274029710064\n",
      "Error on this batch = 0.4807728591713689\n",
      "Error on this batch = 0.4807651055169352\n",
      "Cost on val dataset after 128 epochs is = 0.4807656093987161\n",
      "learning rate =  0.019842513149602496\n",
      "Initial Cost on Val dataset for this epoch 128 = 0.4807656093987161\n",
      "Error on this batch = 0.48076281199136434\n",
      "Error on this batch = 0.4807729088445365\n",
      "Error on this batch = 0.48076514773662354\n",
      "Cost on val dataset after 129 epochs is = 0.4807654609331517\n",
      "learning rate =  0.019791107444813105\n",
      "Initial Cost on Val dataset for this epoch 129 = 0.4807654609331517\n",
      "Error on this batch = 0.4807628770734013\n",
      "Error on this batch = 0.48077294908284757\n",
      "Error on this batch = 0.48076518144966357\n",
      "Cost on val dataset after 130 epochs is = 0.48076531447524234\n",
      "learning rate =  0.019740230337485714\n",
      "Initial Cost on Val dataset for this epoch 130 = 0.48076531447524234\n",
      "Error on this batch = 0.4807629359028775\n",
      "Error on this batch = 0.48077298054651907\n",
      "Error on this batch = 0.48076520558013075\n",
      "Cost on val dataset after 131 epochs is = 0.4807651699885127\n",
      "learning rate =  0.019689872388276634\n",
      "Initial Cost on Val dataset for this epoch 131 = 0.4807651699885127\n",
      "Error on this batch = 0.48076298943112367\n",
      "Error on this batch = 0.4807730047986868\n",
      "Error on this batch = 0.4807652215440018\n",
      "Cost on val dataset after 132 epochs is = 0.48076502731252935\n",
      "learning rate =  0.019640024396816242\n",
      "Initial Cost on Val dataset for this epoch 132 = 0.48076502731252935\n",
      "Error on this batch = 0.48076303677840015\n",
      "Error on this batch = 0.48077302129804317\n",
      "Error on this batch = 0.48076522977829994\n",
      "Cost on val dataset after 133 epochs is = 0.48076488617636404\n",
      "learning rate =  0.0195906773939032\n",
      "Initial Cost on Val dataset for this epoch 133 = 0.48076488617636404\n",
      "Error on this batch = 0.4807630795862231\n",
      "Error on this batch = 0.4807730300911395\n",
      "Error on this batch = 0.4807652299229412\n",
      "Cost on val dataset after 134 epochs is = 0.4807647466147059\n",
      "learning rate =  0.019541822634010067\n",
      "Initial Cost on Val dataset for this epoch 134 = 0.4807647466147059\n",
      "Error on this batch = 0.480763117850327\n",
      "Error on this batch = 0.4807730314309484\n",
      "Error on this batch = 0.4807652234279363\n",
      "Cost on val dataset after 135 epochs is = 0.4807646086911734\n",
      "learning rate =  0.019493451588085776\n",
      "Initial Cost on Val dataset for this epoch 135 = 0.4807646086911734\n",
      "Error on this batch = 0.48076315220278243\n",
      "Error on this batch = 0.48077302569616\n",
      "Error on this batch = 0.4807652104439069\n",
      "Cost on val dataset after 136 epochs is = 0.4807644722955813\n",
      "learning rate =  0.019445555936641018\n",
      "Initial Cost on Val dataset for this epoch 136 = 0.4807644722955813\n",
      "Error on this batch = 0.48076318047606675\n",
      "Error on this batch = 0.48077301381033777\n",
      "Error on this batch = 0.48076519188494005\n",
      "Cost on val dataset after 137 epochs is = 0.48076433740864566\n",
      "learning rate =  0.01939812756310351\n",
      "Initial Cost on Val dataset for this epoch 137 = 0.48076433740864566\n",
      "Error on this batch = 0.4807632040065324\n",
      "Error on this batch = 0.48077299645151256\n",
      "Error on this batch = 0.4807651675334057\n",
      "Cost on val dataset after 138 epochs is = 0.4807642039353129\n",
      "learning rate =  0.019351158547430667\n",
      "Initial Cost on Val dataset for this epoch 138 = 0.4807642039353129\n",
      "Error on this batch = 0.4807632233081655\n",
      "Error on this batch = 0.48077297383109713\n",
      "Error on this batch = 0.4807651377920242\n",
      "Cost on val dataset after 139 epochs is = 0.48076407189820036\n",
      "learning rate =  0.019304641159967956\n",
      "Initial Cost on Val dataset for this epoch 139 = 0.48076407189820036\n",
      "Error on this batch = 0.480763239111201\n",
      "Error on this batch = 0.48077294730097875\n",
      "Error on this batch = 0.4807651037037317\n",
      "Cost on val dataset after 140 epochs is = 0.48076394119226296\n",
      "learning rate =  0.01925856785554179\n",
      "Initial Cost on Val dataset for this epoch 140 = 0.48076394119226296\n",
      "Error on this batch = 0.48076325172297063\n",
      "Error on this batch = 0.48077291589195215\n",
      "Error on this batch = 0.4807650659717807\n",
      "Cost on val dataset after 141 epochs is = 0.48076381180157474\n",
      "learning rate =  0.01921293126777635\n",
      "Initial Cost on Val dataset for this epoch 141 = 0.48076381180157474\n",
      "Error on this batch = 0.4807632614775302\n",
      "Error on this batch = 0.4807728796803548\n",
      "Error on this batch = 0.4807650250654879\n",
      "Cost on val dataset after 142 epochs is = 0.480763683655284\n",
      "learning rate =  0.01916772420362441\n",
      "Initial Cost on Val dataset for this epoch 142 = 0.480763683655284\n",
      "Error on this batch = 0.48076326816662396\n",
      "Error on this batch = 0.4807728386329501\n",
      "Error on this batch = 0.48076497996636763\n",
      "Cost on val dataset after 143 epochs is = 0.4807635564863588\n",
      "learning rate =  0.01912293963810262\n",
      "Initial Cost on Val dataset for this epoch 143 = 0.4807635564863588\n",
      "Error on this batch = 0.48076327080695974\n",
      "Error on this batch = 0.4807727931958278\n",
      "Error on this batch = 0.48076492976237617\n",
      "Cost on val dataset after 144 epochs is = 0.4807634303328556\n",
      "learning rate =  0.0190785707092222\n",
      "Initial Cost on Val dataset for this epoch 144 = 0.4807634303328556\n",
      "Error on this batch = 0.48076327007223313\n",
      "Error on this batch = 0.48077274340678755\n",
      "Error on this batch = 0.4807648756948728\n",
      "Cost on val dataset after 145 epochs is = 0.4807633053859531\n",
      "learning rate =  0.01903461071310655\n",
      "Initial Cost on Val dataset for this epoch 145 = 0.4807633053859531\n",
      "Error on this batch = 0.48076326645860873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on this batch = 0.4807726903504817\n",
      "Error on this batch = 0.48076481845650637\n",
      "Cost on val dataset after 146 epochs is = 0.48076318144726304\n",
      "learning rate =  0.01899105309928761\n",
      "Initial Cost on Val dataset for this epoch 146 = 0.48076318144726304\n",
      "Error on this batch = 0.4807632596271292\n",
      "Error on this batch = 0.4807726336280034\n",
      "Error on this batch = 0.4807647584892811\n",
      "Cost on val dataset after 147 epochs is = 0.4807630585065383\n",
      "learning rate =  0.018947891466173296\n",
      "Initial Cost on Val dataset for this epoch 147 = 0.4807630585065383\n",
      "Error on this batch = 0.48076324914559365\n",
      "Error on this batch = 0.4807725735055536\n",
      "Error on this batch = 0.48076469625298046\n",
      "Cost on val dataset after 148 epochs is = 0.48076293648921575\n",
      "learning rate =  0.018905119556678594\n",
      "Initial Cost on Val dataset for this epoch 148 = 0.48076293648921575\n",
      "Error on this batch = 0.4807632352902181\n",
      "Error on this batch = 0.4807725094408305\n",
      "Error on this batch = 0.4807646316498513\n",
      "Cost on val dataset after 149 epochs is = 0.4807628152966166\n",
      "learning rate =  0.01886273125401342\n",
      "Initial Cost on Val dataset for this epoch 149 = 0.4807628152966166\n",
      "Error on this batch = 0.4807632193880586\n",
      "Error on this batch = 0.48077244186829926\n",
      "Error on this batch = 0.48076456538119333\n",
      "Cost on val dataset after 150 epochs is = 0.4807626951266416\n",
      "learning rate =  0.018820720577620572\n",
      "Initial Cost on Val dataset for this epoch 150 = 0.4807626951266416\n",
      "Error on this batch = 0.48076320066821965\n",
      "Error on this batch = 0.48077237150029506\n",
      "Error on this batch = 0.4807644966039106\n",
      "Cost on val dataset after 151 epochs is = 0.48076257590570753\n",
      "learning rate =  0.018779081679257494\n",
      "Initial Cost on Val dataset for this epoch 151 = 0.48076257590570753\n",
      "Error on this batch = 0.4807631792101275\n",
      "Error on this batch = 0.4807722992296081\n",
      "Error on this batch = 0.4807644262162353\n",
      "Cost on val dataset after 152 epochs is = 0.4807624575456397\n",
      "learning rate =  0.01873780883921578\n",
      "Initial Cost on Val dataset for this epoch 152 = 0.4807624575456397\n",
      "Error on this batch = 0.4807631552291862\n",
      "Error on this batch = 0.4807722256710665\n",
      "Error on this batch = 0.48076435415240704\n",
      "Cost on val dataset after 153 epochs is = 0.48076233994584466\n",
      "learning rate =  0.018696896462672787\n",
      "Initial Cost on Val dataset for this epoch 153 = 0.48076233994584466\n",
      "Error on this batch = 0.48076312904939533\n",
      "Error on this batch = 0.48077215010916163\n",
      "Error on this batch = 0.48076428036079405\n",
      "Cost on val dataset after 154 epochs is = 0.48076222306487776\n",
      "learning rate =  0.018656339076169894\n",
      "Initial Cost on Val dataset for this epoch 154 = 0.48076222306487776\n",
      "Error on this batch = 0.4807631008168896\n",
      "Error on this batch = 0.48077207240366987\n",
      "Error on this batch = 0.48076420507678763\n",
      "Cost on val dataset after 155 epochs is = 0.48076210694953436\n",
      "learning rate =  0.018616131324212123\n",
      "Initial Cost on Val dataset for this epoch 155 = 0.48076210694953436\n",
      "Error on this batch = 0.4807630707202884\n",
      "Error on this batch = 0.4807719932655665\n",
      "Error on this batch = 0.4807641284693275\n",
      "Cost on val dataset after 156 epochs is = 0.48076199156205845\n",
      "learning rate =  0.018576267965984362\n",
      "Initial Cost on Val dataset for this epoch 156 = 0.48076199156205845\n",
      "Error on this batch = 0.4807630387635872\n",
      "Error on this batch = 0.48077191275273456\n",
      "Error on this batch = 0.4807640505975695\n",
      "Cost on val dataset after 157 epochs is = 0.48076187685144406\n",
      "learning rate =  0.018536743872179295\n",
      "Initial Cost on Val dataset for this epoch 157 = 0.48076187685144406\n",
      "Error on this batch = 0.4807630056089745\n",
      "Error on this batch = 0.4807718312576531\n",
      "Error on this batch = 0.48076397073092497\n",
      "Cost on val dataset after 158 epochs is = 0.48076176280084076\n",
      "learning rate =  0.018497554021932684\n",
      "Initial Cost on Val dataset for this epoch 158 = 0.48076176280084076\n",
      "Error on this batch = 0.48076297044467575\n",
      "Error on this batch = 0.4807717478762817\n",
      "Error on this batch = 0.4807638887882804\n",
      "Cost on val dataset after 159 epochs is = 0.4807616494366856\n",
      "learning rate =  0.018458693499861668\n",
      "Initial Cost on Val dataset for this epoch 159 = 0.4807616494366856\n",
      "Error on this batch = 0.48076293316969043\n",
      "Error on this batch = 0.4807716628917396\n",
      "Error on this batch = 0.4807638054054273\n",
      "Cost on val dataset after 160 epochs is = 0.48076153684102463\n",
      "learning rate =  0.018420157493201937\n",
      "Initial Cost on Val dataset for this epoch 160 = 0.48076153684102463\n",
      "Error on this batch = 0.48076289434850794\n",
      "Error on this batch = 0.4807715765206443\n",
      "Error on this batch = 0.4807637206561\n",
      "Cost on val dataset after 161 epochs is = 0.4807614248259625\n",
      "learning rate =  0.01838194128904002\n",
      "Initial Cost on Val dataset for this epoch 161 = 0.4807614248259625\n",
      "Error on this batch = 0.4807628533229771\n",
      "Error on this batch = 0.48077148867262964\n",
      "Error on this batch = 0.48076363494807683\n",
      "Cost on val dataset after 162 epochs is = 0.4807613135165359\n",
      "learning rate =  0.018344040271636816\n",
      "Initial Cost on Val dataset for this epoch 162 = 0.4807613135165359\n",
      "Error on this batch = 0.48076280958874096\n",
      "Error on this batch = 0.4807714003423598\n",
      "Error on this batch = 0.48076354825433654\n",
      "Cost on val dataset after 163 epochs is = 0.480761202839204\n",
      "learning rate =  0.018306449919838923\n",
      "Initial Cost on Val dataset for this epoch 163 = 0.480761202839204\n",
      "Error on this batch = 0.4807627634045993\n",
      "Error on this batch = 0.48077131236739257\n",
      "Error on this batch = 0.48076346094489064\n",
      "Cost on val dataset after 164 epochs is = 0.4807610927886888\n",
      "learning rate =  0.01826916580457428\n",
      "Initial Cost on Val dataset for this epoch 164 = 0.4807610927886888\n",
      "Error on this batch = 0.48076271576052654\n",
      "Error on this batch = 0.4807712242454227\n",
      "Error on this batch = 0.4807633738036548\n",
      "Cost on val dataset after 165 epochs is = 0.4807609833817724\n",
      "learning rate =  0.018232183586428963\n",
      "Initial Cost on Val dataset for this epoch 165 = 0.4807609833817724\n",
      "Error on this batch = 0.4807626669245808\n",
      "Error on this batch = 0.4807711358238564\n",
      "Error on this batch = 0.48076328640647475\n",
      "Cost on val dataset after 166 epochs is = 0.4807608744599058\n",
      "learning rate =  0.018195499013301978\n",
      "Initial Cost on Val dataset for this epoch 166 = 0.4807608744599058\n",
      "Error on this batch = 0.4807626167542972\n",
      "Error on this batch = 0.48077104738833143\n",
      "Error on this batch = 0.48076319875590273\n",
      "Cost on val dataset after 167 epochs is = 0.4807607661721824\n",
      "learning rate =  0.018159107918135086\n",
      "Initial Cost on Val dataset for this epoch 167 = 0.4807607661721824\n",
      "Error on this batch = 0.4807625647116185\n",
      "Error on this batch = 0.48077095889308513\n",
      "Error on this batch = 0.4807631105045853\n",
      "Cost on val dataset after 168 epochs is = 0.48076065831885145\n",
      "learning rate =  0.018123006216714872\n",
      "Initial Cost on Val dataset for this epoch 168 = 0.48076065831885145\n",
      "Error on this batch = 0.48076251198735387\n"
     ]
    }
   ],
   "source": [
    "costs = []\n",
    "epoch = 1\n",
    "start = time.time()\n",
    "cost_init = cost_total(X_train, theta, train_class_enc, m) #Validation loss not giving much info\n",
    "costs.append(cost_init)\n",
    "while(True):\n",
    "    count = 0\n",
    "    lr = lr0/(np.power(epoch, 1/3))\n",
    "    #if(lr < 0.001): lr = 0.001\n",
    "    print(\"learning rate = \", lr)\n",
    "\n",
    "    print(\"Initial Cost on Val dataset for this epoch {} = {}\".format(epoch, cost_init))\n",
    "\n",
    "    for b in mini_batch:\n",
    "        X_b = b[0]\n",
    "        Y_b = b[1]\n",
    "        fm = forward_prop(X_b, theta)\n",
    "        delta = [None]*len(fm)\n",
    "\n",
    "        if (count % 60 == 0):\n",
    "            print(\"Error on this batch = \"+str(cost_total(X_b, theta, Y_b, batch_size)))\n",
    "        #Backward Propagation\n",
    "\n",
    "        for l in range(len(fm)-1, 0, -1):\n",
    "            if (l == len(fm)-1):\n",
    "                delta[l] = ((1/batch_size)*(Y_b - fm[l])*fm[l]*(1-fm[l]))\n",
    "                #print(\"delta for last layer=\",delta[l])\n",
    "            else:\n",
    "                delta[l]=(np.dot(delta[l+1], theta[l].T)*deriv_relu(fm[l]))\n",
    "                #print(\"delta for hidden layer=\",np.mean(delta[l]))\n",
    "\n",
    "        for t in range(len(theta)):\n",
    "            theta[t] += lr*np.dot(fm[t].T, delta[t+1]) \n",
    "        \n",
    "        count+=1\n",
    "    epoch+=1 #Number of epochs\n",
    "    #ite+=1\n",
    "\n",
    "    cost_final = cost_total(X_train, theta, train_class_enc, m)\n",
    "    if(epoch%10==0): costs.append(cost_final)\n",
    "    print(\"Cost on val dataset after {} epochs is = {}\".format(epoch, cost_final))\n",
    "    if (abs(cost_final-cost_init) < 1e-07):\n",
    "        print(\"cost initial= {} , cost final={} , change in cost= {}\".format(cost_init,cost_final, cost_final-cost_init))\n",
    "        break\n",
    "    cost_init = cost_final\n",
    "epochs.append(epoch)\n",
    "train_time.append(time.time()-start)\n",
    "train_accuracy.append(calc_accuracy(X_train, theta, train_class_enc))\n",
    "#valid_accuracy.append(calc_accuracy(X_valid, theta, valid_class_enc))\n",
    "test_accuracy.append(calc_accuracy(X_test, theta, test_actual_class_enc))\n",
    "print(\"\\n------------------------------------------------------------------------------\")\n",
    "print(\"The stats for number of units in the hidden layer arch= {} are as below:\".format(arch))\n",
    "print(\"------------------------------------------------------------------------------\")\n",
    "print(\"The number of epochs = {:2.3f}\".format(epochs[-1]))\n",
    "print(\"The training time = {:2.3f}sec\".format(train_time[-1]))\n",
    "print(\"The training accuracy is = {:2.3f}%\".format(train_accuracy[-1]))\n",
    "#print(\"The validation accuracy is = {:2.3f}%\".format(valid_accuracy[-1]))\n",
    "print(\"The test accuracy is = {:2.3f}%\".format(test_accuracy[-1]))\n",
    "print(\"------------------------------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.430769230769231"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_accuracy(X_test, theta, test_actual_class_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_title(\"Loss with Epochs\")\n",
    "x = np.arange(0,len(costs))\n",
    "ax.plot(x, costs, marker='o', label='Train Accuracy')\n",
    "ax.set_xlabel(\"number of epochs\")\n",
    "ax.set_ylabel(\"Train Loss\")\n",
    "\n",
    "plt.legend()\n",
    "#plt.savefig(\"accuracy_HiddenUnit_val20per.png\", dpi=1000, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5.831782109502939,\n",
       " 3.065637409541897,\n",
       " 1.8247308746788262,\n",
       " 1.5705342932734632,\n",
       " 1.33022343238924,\n",
       " 1.1285249107258797,\n",
       " 1.0643045095099792,\n",
       " 1.035211733200288,\n",
       " 1.018805714862765,\n",
       " 6.376557034638974,\n",
       " 1.422657448017669,\n",
       " 1.4211259379043941,\n",
       " 1.419297906735625,\n",
       " 1.4176605209953743,\n",
       " 1.4166161084913558,\n",
       " 1.416185709207088,\n",
       " 1.4152965281422567,\n",
       " 1.4146839345350188,\n",
       " 1.4144074662583,\n",
       " 1.4142618066732653,\n",
       " 1.4140713474500968,\n",
       " 1.4134148478899253,\n",
       " 1.4131559125054791,\n",
       " 1.412979612601874,\n",
       " 1.4128589818155581,\n",
       " 1.4127652801754333,\n",
       " 1.412630967544444,\n",
       " 1.412496660264301,\n",
       " 1.4123353187531043]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
