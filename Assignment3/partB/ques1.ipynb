{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------Reading the Data-------------------------\n",
      "----------------Data Reading completed-------------------\n",
      "----------------Preprocessing the data-------------------\n",
      "The total number of training samples = 11050\n",
      "The total number of validation samples = 1950\n",
      "The number of features = 784\n"
     ]
    }
   ],
   "source": [
    "print(\"----------------Reading the Data-------------------------\")\n",
    "PATH = os.getcwd()\n",
    "os.chdir('Alphabets/')\n",
    "\n",
    "X_train = pd.read_csv('train.csv', sep=',', header=None, index_col=False)\n",
    "X_test = pd.read_csv('test.csv', sep=',', header=None, index_col=False)\n",
    "\n",
    "np.random.shuffle(X_train.to_numpy()) #Randomly shuffle the training data for batching\n",
    "\n",
    "train_class = X_train[X_train.columns[-1]]\n",
    "test_actual_class = X_test[X_test.columns[-1]]\n",
    "\n",
    "X_train = X_train.drop(X_train.columns[-1], axis=1)\n",
    "X_test = X_test.drop(X_test.columns[-1], axis=1)\n",
    "\n",
    "print(\"----------------Data Reading completed-------------------\")\n",
    "\n",
    "os.chdir('../')\n",
    "\n",
    "print(\"----------------Preprocessing the data-------------------\")\n",
    "X_train = X_train/255\n",
    "X_test = X_test/255\n",
    "\n",
    "m = X_train.shape[0] # Number of Training Samples\n",
    "\n",
    "#Separating 15% of the training Data as Validation Dataset\n",
    "X_valid = X_train.iloc[(int(0.85*m)):]\n",
    "valid_class = train_class[(int(0.85*m)):]\n",
    "X_train = X_train.iloc[0:int(0.85*m)]\n",
    "train_class = train_class[0:int(0.85*m)]\n",
    "\n",
    "\n",
    "m = X_train.shape[0] # Number of Training Samples\n",
    "n = X_train.shape[1] # Number of input features\n",
    "\n",
    "print(\"The total number of training samples = {}\".format(m))\n",
    "print(\"The total number of validation samples = {}\".format(X_valid.shape[0]))\n",
    "print(\"The number of features = {}\".format(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Perform 1-hot encoding of class labels------------\n",
      "--------------------Done----------------------------------\n"
     ]
    }
   ],
   "source": [
    "#To get the one hot encoding of each label\n",
    "print(\"--------Perform 1-hot encoding of class labels------------\")\n",
    "\n",
    "train_class_enc = pd.get_dummies(train_class).to_numpy()\n",
    "valid_class_enc = pd.get_dummies(valid_class).to_numpy()\n",
    "test_actual_class_enc = pd.get_dummies(test_actual_class).to_numpy()\n",
    "print(\"--------------------Done----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Adding the intercept term in the dataset as bias------------\n",
      "-----------------------------Done----------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Add the intercept term to the data samples both in training and test dataset\n",
    "print(\"--------Adding the intercept term in the dataset as bias------------\")\n",
    "X_train = np.hstack((np.ones((m,1)),X_train.to_numpy()))\n",
    "X_valid = np.hstack((np.ones((X_valid.shape[0],1)), X_valid.to_numpy()))\n",
    "X_test = np.hstack((np.ones((X_test.shape[0],1)),X_test.to_numpy()))\n",
    "print(\"-----------------------------Done----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------Forming mini-batches of size 5---------------------\n",
      "The number of mini-batches formed is = 2210\n"
     ]
    }
   ],
   "source": [
    "#Mini-Batch formation\n",
    "\n",
    "batch_size = 5 # Mini-Batch Size\n",
    "\n",
    "print(\"----------------Forming mini-batches of size {}---------------------\".format(batch_size))\n",
    "mini_batch = [(X_train[i:i+batch_size,:], train_class_enc[i:i+batch_size]) for i in range(0, m, batch_size)]\n",
    "print(\"The number of mini-batches formed is = {}\".format(len(mini_batch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Theta Initialization \n",
    "\n",
    "def theta_init(n, r, arch=[50], mode='normal'):\n",
    "    theta = []\n",
    "    for i in range(len(arch)+1):\n",
    "        if i == 0:\n",
    "            dim0=n+1\n",
    "            dim1=arch[i]\n",
    "        elif (i == len(arch)):\n",
    "            dim0=arch[i-1]+1\n",
    "            dim1 = r\n",
    "        else:\n",
    "            dim0=arch[i-1]+1\n",
    "            dim1= arch[i]\n",
    "        if (mode=='normal'):\n",
    "            theta.append(np.random.normal(0,0.05, (dim0,dim1)))\n",
    "        elif(mode=='random'):\n",
    "            theta.append(2*np.random.random((dim0, dim1))-1)\n",
    "        elif(mode=='uniform'):\n",
    "            theta.append(np.random.uniform(-0.05,0.05, (dim0,dim1)))\n",
    "            \n",
    "        #theta.append(np.zeros((dim0, dim1)))\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid activation function\n",
    "def activation(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "#ReLU Activation Function\n",
    "def relu_act(x):\n",
    "    return np.maximum(0.0, x)\n",
    "\n",
    "#Derivative of ReLU activation Function\n",
    "def deriv_relu(x):\n",
    "    x[x<=0] = 0\n",
    "    x[x>0] = 1\n",
    "    return x\n",
    "\n",
    "def softplus(x):\n",
    "    return np.log(1+np.exp(x))\n",
    "\n",
    "def deriv_softplus(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward propagation\n",
    "\n",
    "def forward_prop(data, theta, act_fn='sigmoid'):\n",
    "    fm = []\n",
    "    fm.append(data)\n",
    "    #Sigmoid Activation Function \n",
    "    if (act_fn == 'sigmoid'):\n",
    "        for l in range(len(theta)):\n",
    "            fm.append(activation(np.dot(fm[l], theta[l])))\n",
    "            if (l!=len(theta)-1):\n",
    "                fm[l+1]=np.hstack((np.ones((fm[l+1].shape[0],1)),fm[l+1])) #Add intercept term as bias for each layer\n",
    "    #ReLU Activation Function \n",
    "    elif(act_fn == 'relu'):\n",
    "        for l in range(len(theta)):\n",
    "            if (l != len(theta)-1):\n",
    "                fm.append(relu_act(np.dot(fm[l], theta[l])))\n",
    "                fm[l+1]=np.hstack((np.ones((fm[l+1].shape[0],1)),fm[l+1])) #Add intercept term as bias for each layer\n",
    "            else:\n",
    "                fm.append(activation(np.dot(fm[l], theta[l])))\n",
    "    #Softplus Activation Function       \n",
    "    elif(act_fn == 'softplus'):\n",
    "        for l in range(len(theta)):\n",
    "            if (l != len(theta)-1):\n",
    "                fm.append(softplus(np.dot(fm[l], theta[l])))\n",
    "                fm[l+1]=np.hstack((np.ones((fm[l+1].shape[0],1)),fm[l+1])) #Add intercept term as bias for each layer\n",
    "            else:\n",
    "                fm.append(activation(np.dot(fm[l], theta[l])))\n",
    "    return fm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward propagation\n",
    "def backward_prop(fm, Y_b, theta, batch_size, act_fn='sigmoid', cost_fn='sqr_error'):\n",
    "    delta = [None]*len(fm)\n",
    "    for l in range(len(fm)-1, 0, -1):\n",
    "        if (l == len(fm)-1):\n",
    "            if (cost_fn=='entropy'):\n",
    "                delta[l] = ((1/batch_size)*((Y_b/fm[l])-((1-Y_b)/(1-fm[l])))*fm[l]*(1-fm[l])) #Entropy Loss\n",
    "            else:\n",
    "                delta[l] = ((1/batch_size)*(Y_b - fm[l])*fm[l]*(1-fm[l])) #MSE\n",
    "        elif (l+1 == len(fm)-1):\n",
    "            if (act_fn == 'sigmoid'):\n",
    "                delta[l]=(np.dot(delta[l+1], theta[l].T)*fm[l]*(1-fm[l]))\n",
    "            elif(act_fn == 'relu'):\n",
    "                delta[l]=np.dot(delta[l+1], theta[l].T)*deriv_relu(fm[l])\n",
    "            elif(act_fn=='softplus'):\n",
    "                delta[l]=np.dot(delta[l+1], theta[l].T)*deriv_softplus(fm[l])\n",
    "        else:\n",
    "            if (act_fn == 'sigmoid'):\n",
    "                delta[l]=(np.dot(delta[l+1][:,1:], theta[l].T)*fm[l]*(1-fm[l])) #To remove delta corresponding to bias node\n",
    "            elif(act_fn == 'relu'):\n",
    "                delta[l]=np.dot(delta[l+1][:,1:], theta[l].T)*deriv_relu(fm[l])\n",
    "            elif(act_fn=='softplus'):\n",
    "                delta[l]=np.dot(delta[l+1][:,1:], theta[l].T)*deriv_softplus(fm[l])\n",
    "    return delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cost Function \n",
    "def cost_total(X, theta, Y, m, act_fn='sigmoid', cost_fn='sqr_error'):\n",
    "    fm = forward_prop(X, theta, act_fn)\n",
    "    if (cost_fn == 'sqr_error'):\n",
    "        cost = (1/(2*m))*np.sum((Y-fm[-1])**2) #MSE\n",
    "    else:\n",
    "        cost = -(1/m)*(np.sum(((Y*np.log(fm[-1]))+((1-Y)*(np.log(1-fm[-1])))))) #Cross Entropy\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(data, theta, actual_class, act_fn='sigmoid'):\n",
    "    pred_class = forward_prop(data, theta, act_fn)\n",
    "    test_pred_class = pred_class[-1]\n",
    "    for i in range(len(test_pred_class)):\n",
    "        test_pred_class[i][test_pred_class[i] == np.max(test_pred_class[i])] = 1 #Choose one with max probability\n",
    "        test_pred_class[i][test_pred_class[i] != np.max(test_pred_class[i])] = 0\n",
    "\n",
    "\n",
    "    test_acc = 0\n",
    "    for i in range(len(actual_class)):\n",
    "        if (np.array_equal(test_pred_class[i], actual_class[i])):\n",
    "            test_acc+=1\n",
    "    test_acc /= data.shape[0]\n",
    "\n",
    "    return (test_acc*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART AB - One Hidden Layer Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(mini_batch, X_valid, valid_class_enc, theta, lr, act_fn='sigmoid', lr_mode='constant', cost_fn='sqr_error'):\n",
    "    lr0=lr\n",
    "    epoch = 1 # Number of epochs\n",
    "    early_stop=0 #Early stop count of iteration\n",
    "    \n",
    "    cost_init = cost_total(X_valid, theta, valid_class_enc, X_valid.shape[0], act_fn, cost_fn)\n",
    "    \n",
    "    while(True):\n",
    "        count_batch = 0\n",
    "        print(\"Initial Cost on Val dataset for this epoch {} = {}\".format(epoch, cost_init))\n",
    "        \n",
    "        if(lr_mode == \"adaptive\"):\n",
    "            lr = lr0/(np.power(epoch, 1/4))\n",
    "            print(\"learning rate for this epoch = \", lr)\n",
    "        \n",
    "        for b in mini_batch:\n",
    "            X_b = b[0] \n",
    "            Y_b = b[1]\n",
    "            \n",
    "            #Forward Propagation\n",
    "            fm = forward_prop(X_b, theta, act_fn)\n",
    "            \n",
    "            if (count_batch % 60 == 0):\n",
    "                print(\"Error on this batch = \"+str(cost_total(X_b, theta, Y_b, batch_size, act_fn, cost_fn)))\n",
    "                    \n",
    "            #Backward Propagation\n",
    "            delta = [None]*len(fm)\n",
    "            delta = backward_prop(fm, Y_b, theta, batch_size, act_fn, cost_fn)\n",
    "\n",
    "            #Theta Update\n",
    "            for t in range(len(theta)):\n",
    "                if (t == len(theta)-1):\n",
    "                    theta[t] += lr*np.dot(fm[t].T, delta[t+1]) \n",
    "                else:\n",
    "                    theta[t] += lr*np.dot(fm[t].T, delta[t+1])[:,1:]\n",
    "            print(\"Delta shape = \", delta[1].shape, delta[2].shape)\n",
    "            print(\"theta shape = \", theta[0].shape, theta[1].shape)\n",
    "            count_batch+=1\n",
    "            break\n",
    "        epoch+=1 #Number of epochs\n",
    "                \n",
    "        cost_final = cost_total(X_valid, theta, valid_class_enc, X_valid.shape[0], act_fn, cost_fn)\n",
    "        print(\"Cost on val dataset after {} epochs is = {}\".format(epoch, cost_final))\n",
    "        \n",
    "        #Stopping criteria for sigmoid - when Validation loss stops decreasing beyond a threshold for 10 epochs\n",
    "        if (act_fn =='sigmoid'):\n",
    "            if (abs(cost_final-cost_init) < 1e-05):\n",
    "                early_stop +=1\n",
    "            else:\n",
    "                early_stop=0\n",
    "            if (early_stop == 10):\n",
    "                print(\"cost initial= {} , cost final={} , change in cost= {}\".format(cost_init,cost_final, cost_final-cost_init))\n",
    "                break\n",
    "        \n",
    "        #Stopping criteria for relu - when Validation loss increases continuously for 10 epochs\n",
    "        elif(act_fn=='relu' or act_fn=='softplus'):\n",
    "            if ((cost_final-cost_init) > 0):\n",
    "                early_stop +=1\n",
    "            else:\n",
    "                early_stop=0\n",
    "            if (early_stop == 10):\n",
    "                print(\"cost initial= {} , cost final={} , change in cost= {}\".format(cost_init,cost_final, cost_final-cost_init))\n",
    "                break\n",
    "        \n",
    "        cost_init = cost_final\n",
    "        break\n",
    "        \n",
    "    return epoch, theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy(arch_test, train_accuracy, test_accuracy, valid_accuracy):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_title(\"Accuracy with number of hidden units \\n in one hidden layer network\")\n",
    "    ax.plot(arch_test, train_accuracy, marker='o', label='Train Accuracy')\n",
    "    ax.plot(arch_test, valid_accuracy, marker='o',label='Validation Accuracy')\n",
    "    ax.plot(arch_test, test_accuracy, marker='o', label='Test Accuracy')\n",
    "    ax.set_xlabel(\"number of hidden units\")\n",
    "    ax.set_ylabel(\"Accuracy (%)\")\n",
    "\n",
    "    plt.legend()\n",
    "    #plt.savefig(\"plots/partc/accuracy_normal_lr0.5_cube.png\", dpi=1000, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_epoch(arch_test, epochs, train_time):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(211)\n",
    "    plt.title(\"Epochs/Time with number of hidden units \\n in one hidden layer network\")\n",
    "    ax.plot(arch_test, epochs, c='b', marker='o', label='#epochs')\n",
    "    ax.set_xlabel(\"number of hidden units\")\n",
    "    ax.set_ylabel(\"Epochs\")\n",
    "    ax.legend()\n",
    "\n",
    "    ax1 = fig.add_subplot(212)\n",
    "    ax1.plot(arch_test, train_time, c='b', marker='o', label='train time')\n",
    "    ax1.set_xlabel(\"number of hidden units\")\n",
    "    ax1.set_ylabel(\"train time(sec)\")\n",
    "    plt.legend()\n",
    "    #plt.savefig(\"plots/partc/epochs_time_normal_lr0.5_cube.png\", dpi=1000, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_test = [1,5,10,50,100] # Specifically for part a and b\n",
    "#arch = [50] #means one hidden layer with 50 perceptrons (DEFAULT)\n",
    "r = np.max(train_class) + 1 # Default value of the number of classes = 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the network with 1 hidden layer with 1 units\n",
      "The parameters of the layers are of the shape:\n",
      "theta between layer 0 and layer 1 is (785, 1)\n",
      "theta between layer 1 and layer 2 is (2, 26)\n",
      "Initial Cost on Val dataset for this epoch 1 = 3.2366244790899774\n",
      "Error on this batch = 3.234730100306655\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-2889cf22b261>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_class_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-5f3666d4a2cf>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(mini_batch, X_valid, valid_class_enc, theta, lr, act_fn, lr_mode, cost_fn)\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                     \u001b[0mtheta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Delta shape = \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"theta shape = \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mcount_batch\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "epochs = []\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "valid_accuracy = []\n",
    "train_time = []\n",
    "\n",
    "lr=0.5\n",
    "\n",
    "for i in range(len(arch_test)):\n",
    "    #Choose between normal or random. Normal gives better results\n",
    "    theta = theta_init(n, r, [arch_test[i]], 'normal')\n",
    "    #print(theta[0].shape, theta[1].shape, theta[2].shape)\n",
    "    print(\"Training the network with {} hidden layer with {} units\".format(len([arch_test[i]]), arch_test[i]))\n",
    "    print(\"The parameters of the layers are of the shape:\")\n",
    "\n",
    "    for j in range(len(theta)):\n",
    "        print(\"theta between layer {} and layer {} is {}\".format(j, j+1,theta[j].shape))\n",
    "\n",
    "    start = time.time()\n",
    "    epoch, theta = training(mini_batch, X_valid, valid_class_enc, theta, lr, 'sigmoid')\n",
    "    \n",
    "    epochs.append(epoch)\n",
    "    train_time.append(time.time()-start)\n",
    "    train_accuracy.append(calc_accuracy(X_train, theta, train_class_enc))\n",
    "    valid_accuracy.append(calc_accuracy(X_valid, theta, valid_class_enc))\n",
    "    test_accuracy.append(calc_accuracy(X_test, theta, test_actual_class_enc))\n",
    "    print(\"\\n------------------------------------------------------------------------------\")\n",
    "    print(\"The stats for number of units in the hidden layer = {} are as below:\".format(arch_test[i]))\n",
    "    print(\"------------------------------------------------------------------------------\")\n",
    "    print(\"The number of epochs = {}\".format(epochs[-1]))\n",
    "    print(\"The training time = {:2.3f}sec\".format(train_time[-1]))\n",
    "    print(\"The training accuracy is = {:2.3f}%\".format(train_accuracy[-1]))\n",
    "    print(\"The validation accuracy is = {:2.3f}%\".format(valid_accuracy[-1]))\n",
    "    print(\"The test accuracy is = {:2.3f}%\".format(test_accuracy[-1]))\n",
    "    print(\"------------------------------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------Plotting Graphs for Part B - Fixed LR - One Hidden Layer ------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"------------------Plotting Graphs for Part B - Fixed LR - One Hidden Layer ------------------\")\n",
    "plot_accuracy(arch_test, train_accuracy, test_accuracy, valid_accuracy)\n",
    "plot_epoch(arch_test, epochs, train_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C - Adaptive Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the network with 1 hidden layer with 1 units\n",
      "The parameters of the layers are of the shape:\n",
      "theta between layer 0 and layer 1 is (785, 1)\n",
      "theta between layer 1 and layer 2 is (2, 26)\n",
      "Initial Cost on Val dataset for this epoch 1 = 3.2597392406061334\n",
      "learning rate for this epoch =  0.5\n",
      "Error on this batch = 3.258361740537336\n",
      "Error on this batch = 0.5819904510458613\n",
      "Cost on val dataset after 2 epochs is = 0.5204024792332181\n",
      "Initial Cost on Val dataset for this epoch 2 = 0.5204024792332181\n",
      "learning rate for this epoch =  0.4204482076268573\n",
      "Error on this batch = 0.5202182476249874\n",
      "Error on this batch = 0.50212506489877\n",
      "Cost on val dataset after 3 epochs is = 0.4949624359871587\n",
      "Initial Cost on Val dataset for this epoch 3 = 0.4949624359871587\n",
      "learning rate for this epoch =  0.37991784282579627\n",
      "Error on this batch = 0.4948485111676423\n",
      "Error on this batch = 0.49057920223495843\n",
      "Cost on val dataset after 4 epochs is = 0.48822364573631355\n",
      "Initial Cost on Val dataset for this epoch 4 = 0.48822364573631355\n",
      "learning rate for this epoch =  0.35355339059327373\n",
      "Error on this batch = 0.48812951795626336\n",
      "Error on this batch = 0.4864411017911279\n",
      "Cost on val dataset after 5 epochs is = 0.4853560069995764\n",
      "Initial Cost on Val dataset for this epoch 5 = 0.4853560069995764\n",
      "learning rate for this epoch =  0.334370152488211\n",
      "Error on this batch = 0.4852702521139584\n",
      "Error on this batch = 0.48444401232249745\n",
      "Cost on val dataset after 6 epochs is = 0.48385197513995554\n",
      "Initial Cost on Val dataset for this epoch 6 = 0.48385197513995554\n",
      "learning rate for this epoch =  0.3194715521231362\n",
      "Error on this batch = 0.4837703251632127\n",
      "Error on this batch = 0.4833204755751648\n",
      "Cost on val dataset after 7 epochs is = 0.4829630768751136\n",
      "Initial Cost on Val dataset for this epoch 7 = 0.4829630768751136\n",
      "learning rate for this epoch =  0.3073940764756322\n",
      "Error on this batch = 0.4828835239700297\n",
      "Error on this batch = 0.48262590249868903\n",
      "Cost on val dataset after 8 epochs is = 0.48239537342820915\n",
      "Initial Cost on Val dataset for this epoch 8 = 0.48239537342820915\n",
      "learning rate for this epoch =  0.29730177875068026\n",
      "Error on this batch = 0.48231683908040085\n",
      "Error on this batch = 0.48216814886298764\n",
      "Cost on val dataset after 9 epochs is = 0.4820125168039131\n",
      "Initial Cost on Val dataset for this epoch 9 = 0.4820125168039131\n",
      "learning rate for this epoch =  0.2886751345948129\n",
      "Error on this batch = 0.48193437077273643\n",
      "Error on this batch = 0.48185217896092314\n",
      "Cost on val dataset after 10 epochs is = 0.48174369797047717\n",
      "Initial Cost on Val dataset for this epoch 10 = 0.48174369797047717\n",
      "learning rate for this epoch =  0.28117066259517454\n",
      "Error on this batch = 0.4816655508392964\n",
      "Error on this batch = 0.4816263026976067\n",
      "Cost on val dataset after 11 epochs is = 0.48154900226022057\n",
      "Initial Cost on Val dataset for this epoch 11 = 0.48154900226022057\n",
      "learning rate for this epoch =  0.2745502433880562\n",
      "Error on this batch = 0.48147060441885525\n",
      "Error on this batch = 0.48146034510635366\n",
      "Cost on val dataset after 12 epochs is = 0.4814044829327383\n",
      "Initial Cost on Val dataset for this epoch 12 = 0.4814044829327383\n",
      "learning rate for this epoch =  0.2686424829558855\n",
      "Error on this batch = 0.48132567079928623\n",
      "Error on this batch = 0.48133569922196545\n",
      "Cost on val dataset after 13 epochs is = 0.4812950513242689\n",
      "Initial Cost on Val dataset for this epoch 13 = 0.4812950513242689\n",
      "learning rate for this epoch =  0.2633201939239633\n",
      "Error on this batch = 0.4812157166894792\n",
      "Error on this batch = 0.4812403789143144\n",
      "Cost on val dataset after 14 epochs is = 0.48121081643960084\n",
      "Initial Cost on Val dataset for this epoch 14 = 0.48121081643960084\n",
      "learning rate for this epoch =  0.2584865769785853\n",
      "Error on this batch = 0.48113088802007736\n",
      "Error on this batch = 0.48116638272522755\n",
      "Cost on val dataset after 15 epochs is = 0.4811450796186406\n",
      "Initial Cost on Val dataset for this epoch 15 = 0.4811450796186406\n",
      "learning rate for this epoch =  0.25406637407730737\n",
      "Error on this batch = 0.4810645114585555\n",
      "Error on this batch = 0.4811082084759502\n",
      "Cost on val dataset after 16 epochs is = 0.48109317858907674\n",
      "Initial Cost on Val dataset for this epoch 16 = 0.48109317858907674\n",
      "learning rate for this epoch =  0.25\n",
      "Error on this batch = 0.4810119424985157\n",
      "Error on this batch = 0.48106197673483236\n",
      "Cost on val dataset after 17 epochs is = 0.4810517922426216\n",
      "Initial Cost on Val dataset for this epoch 17 = 0.4810517922426216\n",
      "learning rate for this epoch =  0.24623953025272619\n",
      "Error on this batch = 0.4809698727154488\n",
      "Error on this batch = 0.48102489308147767\n",
      "Cost on val dataset after 18 epochs is = 0.48101850719097006\n",
      "Initial Cost on Val dataset for this epoch 18 = 0.48101850719097006\n",
      "learning rate for this epoch =  0.24274588585366172\n",
      "Error on this batch = 0.4809358978999215\n",
      "Error on this batch = 0.4809949070962916\n",
      "Cost on val dataset after 19 epochs is = 0.4809915390568528\n",
      "Initial Cost on Val dataset for this epoch 19 = 0.4809915390568528\n",
      "learning rate for this epoch =  0.23948681272178735\n",
      "Error on this batch = 0.4809082403824344\n",
      "Error on this batch = 0.48097048982628543\n",
      "Cost on val dataset after 20 epochs is = 0.4809695483947119\n",
      "Initial Cost on Val dataset for this epoch 20 = 0.4809695483947119\n",
      "learning rate for this epoch =  0.23643540225079396\n",
      "Error on this batch = 0.48088556564955726\n",
      "Error on this batch = 0.48095048489487513\n",
      "Cost on val dataset after 21 epochs is = 0.480951516224597\n",
      "Initial Cost on Val dataset for this epoch 21 = 0.480951516224597\n",
      "learning rate for this epoch =  0.23356898886410005\n",
      "Error on this batch = 0.48086685835643295\n",
      "Error on this batch = 0.4809340066729284\n",
      "Cost on val dataset after 22 epochs is = 0.48093665811171704\n",
      "Initial Cost on Val dataset for this epoch 22 = 0.48093665811171704\n",
      "learning rate for this epoch =  0.2308683154720513\n",
      "Error on this batch = 0.48085133674238\n",
      "Error on this batch = 0.4809203692673546\n",
      "Cost on val dataset after 23 epochs is = 0.48092436375149034\n",
      "Initial Cost on Val dataset for this epoch 23 = 0.48092436375149034\n",
      "learning rate for this epoch =  0.22831689274836564\n",
      "Error on this batch = 0.4808383924561351\n",
      "Error on this batch = 0.4809090361332837\n",
      "Cost on val dataset after 24 epochs is = 0.48091415378257\n",
      "Initial Cost on Val dataset for this epoch 24 = 0.48091415378257\n",
      "learning rate for this epoch =  0.22590050090246122\n",
      "Error on this batch = 0.480827547543312\n",
      "Error on this batch = 0.48089958375872416\n",
      "Cost on val dataset after 25 epochs is = 0.48090564845372946\n",
      "Initial Cost on Val dataset for this epoch 25 = 0.48090564845372946\n",
      "learning rate for this epoch =  0.22360679774997896\n",
      "Error on this batch = 0.4808184232417862\n",
      "Error on this batch = 0.48089167512098707\n",
      "Cost on val dataset after 26 epochs is = 0.4808985445840222\n",
      "Initial Cost on Val dataset for this epoch 26 = 0.4808985445840222\n",
      "learning rate for this epoch =  0.2214250071345737\n",
      "Error on this batch = 0.4808107170377391\n",
      "Error on this batch = 0.48088504003672217\n",
      "Cost on val dataset after 27 epochs is = 0.48089259841351206\n",
      "Initial Cost on Val dataset for this epoch 27 = 0.48089259841351206\n",
      "learning rate for this epoch =  0.21934566882541542\n",
      "Error on this batch = 0.48080418558877314\n",
      "Error on this batch = 0.48087946044557656\n",
      "Cost on val dataset after 28 epochs is = 0.4808876126959658\n",
      "Initial Cost on Val dataset for this epoch 28 = 0.4808876126959658\n",
      "learning rate for this epoch =  0.2173604359724957\n",
      "Error on this batch = 0.48079863187182853\n",
      "Error on this batch = 0.4808747592713812\n",
      "Cost on val dataset after 29 epochs is = 0.4808834268849919\n",
      "Initial Cost on Val dataset for this epoch 29 = 0.4808834268849919\n",
      "learning rate for this epoch =  0.215461909729453\n",
      "Error on this batch = 0.4807938954118523\n",
      "Error on this batch = 0.4808707919088892\n",
      "Cost on val dataset after 30 epochs is = 0.48087990960226196\n",
      "Initial Cost on Val dataset for this epoch 30 = 0.48087990960226196\n",
      "learning rate for this epoch =  0.21364350319811704\n",
      "Error on this batch = 0.48078984478306425\n",
      "Error on this batch = 0.4808674396588134\n",
      "Cost on val dataset after 31 epochs is = 0.48087695280725723\n",
      "Initial Cost on Val dataset for this epoch 31 = 0.48087695280725723\n",
      "learning rate for this epoch =  0.21189932870751083\n",
      "Error on this batch = 0.4807863718045745\n",
      "Error on this batch = 0.4808646046234041\n",
      "Cost on val dataset after 32 epochs is = 0.4808744672481807\n",
      "Initial Cost on Val dataset for this epoch 32 = 0.4808744672481807\n",
      "learning rate for this epoch =  0.21022410381342865\n",
      "Error on this batch = 0.48078338701169443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on this batch = 0.4808622057072821\n",
      "Cost on val dataset after 33 epochs is = 0.48087237888632844\n",
      "Initial Cost on Val dataset for this epoch 33 = 0.48087237888632844\n",
      "learning rate for this epoch =  0.2086130724305753\n",
      "Error on this batch = 0.4807808160964955\n",
      "Error on this batch = 0.48086017546199744\n",
      "Cost on val dataset after 34 epochs is = 0.4808706260663819\n",
      "cost initial= 0.48087237888632844 , cost final=0.4808706260663819 , change in cost= -1.7528199465211003e-06\n",
      "\n",
      "------------------------------------------------------------------------------\n",
      "The stats for number of units in the hidden layer = 1 are as below:\n",
      "------------------------------------------------------------------------------\n",
      "The number of epochs = 34\n",
      "The training time = 2.163sec\n",
      "The training accuracy is = 3.973%\n",
      "The validation accuracy is = 3.128%\n",
      "The test accuracy is = 3.846%\n",
      "------------------------------------------------------------------------------\n",
      "\n",
      "Training the network with 1 hidden layer with 5 units\n",
      "The parameters of the layers are of the shape:\n",
      "theta between layer 0 and layer 1 is (785, 5)\n",
      "theta between layer 1 and layer 2 is (6, 26)\n",
      "Initial Cost on Val dataset for this epoch 1 = 3.2884481219596324\n",
      "learning rate for this epoch =  0.5\n",
      "Error on this batch = 3.287007203952446\n",
      "Error on this batch = 0.49897808664779675\n",
      "Cost on val dataset after 2 epochs is = 0.4865578565525861\n",
      "Initial Cost on Val dataset for this epoch 2 = 0.4865578565525861\n",
      "learning rate for this epoch =  0.4204482076268573\n",
      "Error on this batch = 0.4862483057750571\n",
      "Error on this batch = 0.483406225275497\n",
      "Cost on val dataset after 3 epochs is = 0.4822794022366817\n",
      "Initial Cost on Val dataset for this epoch 3 = 0.4822794022366817\n",
      "learning rate for this epoch =  0.37991784282579627\n",
      "Error on this batch = 0.4821105944181454\n",
      "Error on this batch = 0.481714140829764\n",
      "Cost on val dataset after 4 epochs is = 0.4814090111840232\n",
      "Initial Cost on Val dataset for this epoch 4 = 0.4814090111840232\n",
      "learning rate for this epoch =  0.35355339059327373\n",
      "Error on this batch = 0.48127966463242666\n",
      "Error on this batch = 0.4812444781063708\n",
      "Cost on val dataset after 5 epochs is = 0.481122525673021\n",
      "Initial Cost on Val dataset for this epoch 5 = 0.481122525673021\n",
      "learning rate for this epoch =  0.334370152488211\n",
      "Error on this batch = 0.4810097464237582\n",
      "Error on this batch = 0.48106822884843936\n",
      "Cost on val dataset after 6 epochs is = 0.48100651870635475\n",
      "Initial Cost on Val dataset for this epoch 6 = 0.48100651870635475\n",
      "learning rate for this epoch =  0.3194715521231362\n",
      "Error on this batch = 0.48090187501931086\n",
      "Error on this batch = 0.48099115573123585\n",
      "Cost on val dataset after 7 epochs is = 0.4809538867390374\n",
      "Initial Cost on Val dataset for this epoch 7 = 0.4809538867390374\n",
      "learning rate for this epoch =  0.3073940764756322\n",
      "Error on this batch = 0.4808535234604673\n",
      "Error on this batch = 0.48095419451914956\n",
      "Cost on val dataset after 8 epochs is = 0.4809283100849405\n",
      "Initial Cost on Val dataset for this epoch 8 = 0.4809283100849405\n",
      "learning rate for this epoch =  0.29730177875068026\n",
      "Error on this batch = 0.4808302445842416\n",
      "Error on this batch = 0.48093534590460507\n",
      "Cost on val dataset after 9 epochs is = 0.48091534528442004\n",
      "Initial Cost on Val dataset for this epoch 9 = 0.48091534528442004\n",
      "learning rate for this epoch =  0.2886751345948129\n",
      "Error on this batch = 0.4808184860964096\n",
      "Error on this batch = 0.48092529834701436\n",
      "Cost on val dataset after 10 epochs is = 0.48090861380937544\n",
      "Initial Cost on Val dataset for this epoch 10 = 0.48090861380937544\n",
      "learning rate for this epoch =  0.28117066259517454\n",
      "Error on this batch = 0.4808123368784131\n",
      "Error on this batch = 0.4809197503883323\n",
      "Cost on val dataset after 11 epochs is = 0.4809050848901941\n",
      "Initial Cost on Val dataset for this epoch 11 = 0.4809050848901941\n",
      "learning rate for this epoch =  0.2745502433880562\n",
      "Error on this batch = 0.48080902775299666\n",
      "Error on this batch = 0.4809165860814039\n",
      "Cost on val dataset after 12 epochs is = 0.4809032404331069\n",
      "Initial Cost on Val dataset for this epoch 12 = 0.4809032404331069\n",
      "learning rate for this epoch =  0.2686424829558855\n",
      "Error on this batch = 0.48080719352933543\n",
      "Error on this batch = 0.4809147150402417\n",
      "Cost on val dataset after 13 epochs is = 0.48090229032710086\n",
      "Initial Cost on Val dataset for this epoch 13 = 0.48090229032710086\n",
      "learning rate for this epoch =  0.2633201939239633\n",
      "Error on this batch = 0.4808061359894963\n",
      "Error on this batch = 0.48091355551189163\n",
      "Cost on val dataset after 14 epochs is = 0.4809018120424333\n",
      "Initial Cost on Val dataset for this epoch 14 = 0.4809018120424333\n",
      "learning rate for this epoch =  0.2584865769785853\n",
      "Error on this batch = 0.48080548867964235\n",
      "Error on this batch = 0.4809127890621804\n",
      "Cost on val dataset after 15 epochs is = 0.4809015756488622\n",
      "Initial Cost on Val dataset for this epoch 15 = 0.4809015756488622\n",
      "learning rate for this epoch =  0.25406637407730737\n",
      "Error on this batch = 0.4808050560204363\n",
      "Error on this batch = 0.4809122382454738\n",
      "Cost on val dataset after 16 epochs is = 0.4809014550381529\n",
      "Initial Cost on Val dataset for this epoch 16 = 0.4809014550381529\n",
      "learning rate for this epoch =  0.25\n",
      "Error on this batch = 0.48080473277100655\n",
      "Error on this batch = 0.4809118031282054\n",
      "Cost on val dataset after 17 epochs is = 0.48090138126616333\n",
      "Initial Cost on Val dataset for this epoch 17 = 0.48090138126616333\n",
      "learning rate for this epoch =  0.24623953025272619\n",
      "Error on this batch = 0.48080446236080576\n",
      "Error on this batch = 0.4809114272702392\n",
      "Cost on val dataset after 18 epochs is = 0.4809013173256754\n",
      "Initial Cost on Val dataset for this epoch 18 = 0.4809013173256754\n",
      "learning rate for this epoch =  0.24274588585366172\n",
      "Error on this batch = 0.4808042147651926\n",
      "Error on this batch = 0.48091107900608093\n",
      "Cost on val dataset after 19 epochs is = 0.4809012441937833\n",
      "cost initial= 0.4809013173256754 , cost final=0.4809012441937833 , change in cost= -7.313189215318872e-08\n",
      "\n",
      "------------------------------------------------------------------------------\n",
      "The stats for number of units in the hidden layer = 5 are as below:\n",
      "------------------------------------------------------------------------------\n",
      "The number of epochs = 19\n",
      "The training time = 1.215sec\n",
      "The training accuracy is = 3.928%\n",
      "The validation accuracy is = 2.974%\n",
      "The test accuracy is = 3.800%\n",
      "------------------------------------------------------------------------------\n",
      "\n",
      "Training the network with 1 hidden layer with 10 units\n",
      "The parameters of the layers are of the shape:\n",
      "theta between layer 0 and layer 1 is (785, 10)\n",
      "theta between layer 1 and layer 2 is (11, 26)\n",
      "Initial Cost on Val dataset for this epoch 1 = 3.2708585206102443\n",
      "learning rate for this epoch =  0.5\n",
      "Error on this batch = 3.2741713120071916\n",
      "Error on this batch = 0.4871291320862221\n",
      "Cost on val dataset after 2 epochs is = 0.4825887531052519\n",
      "Initial Cost on Val dataset for this epoch 2 = 0.4825887531052519\n",
      "learning rate for this epoch =  0.4204482076268573\n",
      "Error on this batch = 0.48228837867881197\n",
      "Error on this batch = 0.48153559050390415\n",
      "Cost on val dataset after 3 epochs is = 0.48129831723234257\n",
      "Initial Cost on Val dataset for this epoch 3 = 0.48129831723234257\n",
      "learning rate for this epoch =  0.37991784282579627\n",
      "Error on this batch = 0.48112272119109706\n",
      "Error on this batch = 0.48112804393300146\n",
      "Cost on val dataset after 4 epochs is = 0.4811132453323031\n",
      "Initial Cost on Val dataset for this epoch 4 = 0.4811132453323031\n",
      "learning rate for this epoch =  0.35355339059327373\n",
      "Error on this batch = 0.48097114574172867\n",
      "Error on this batch = 0.48104845399481494\n",
      "Cost on val dataset after 5 epochs is = 0.48106821345786166\n",
      "Initial Cost on Val dataset for this epoch 5 = 0.48106821345786166\n",
      "learning rate for this epoch =  0.334370152488211\n",
      "Error on this batch = 0.4809389924731349\n",
      "Error on this batch = 0.48102578802783086\n",
      "Cost on val dataset after 6 epochs is = 0.4810531489557464\n",
      "Initial Cost on Val dataset for this epoch 6 = 0.4810531489557464\n",
      "learning rate for this epoch =  0.3194715521231362\n",
      "Error on this batch = 0.4809296492132114\n",
      "Error on this batch = 0.4810168451397722\n",
      "Cost on val dataset after 7 epochs is = 0.4810461328688657\n",
      "Initial Cost on Val dataset for this epoch 7 = 0.4810461328688657\n",
      "learning rate for this epoch =  0.3073940764756322\n",
      "Error on this batch = 0.48092540137423767\n",
      "Error on this batch = 0.4810118236828902\n",
      "Cost on val dataset after 8 epochs is = 0.48104156658626607\n",
      "Initial Cost on Val dataset for this epoch 8 = 0.48104156658626607\n",
      "learning rate for this epoch =  0.29730177875068026\n",
      "Error on this batch = 0.48092228385566727\n",
      "Error on this batch = 0.48100806636253474\n",
      "Cost on val dataset after 9 epochs is = 0.48103781503906357\n",
      "Initial Cost on Val dataset for this epoch 9 = 0.48103781503906357\n",
      "learning rate for this epoch =  0.2886751345948129\n",
      "Error on this batch = 0.48091937178044564\n",
      "Error on this batch = 0.4810047768107806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on val dataset after 10 epochs is = 0.4810343673084456\n",
      "Initial Cost on Val dataset for this epoch 10 = 0.4810343673084456\n",
      "learning rate for this epoch =  0.28117066259517454\n",
      "Error on this batch = 0.480916481605657\n",
      "Error on this batch = 0.4810017030440847\n",
      "Cost on val dataset after 11 epochs is = 0.48103106224571907\n",
      "Initial Cost on Val dataset for this epoch 11 = 0.48103106224571907\n",
      "learning rate for this epoch =  0.2745502433880562\n",
      "Error on this batch = 0.4809136069741128\n",
      "Error on this batch = 0.4809987635196389\n",
      "Cost on val dataset after 12 epochs is = 0.4810278525830765\n",
      "Initial Cost on Val dataset for this epoch 12 = 0.4810278525830765\n",
      "learning rate for this epoch =  0.2686424829558855\n",
      "Error on this batch = 0.48091077379551955\n",
      "Error on this batch = 0.48099593164119336\n",
      "Cost on val dataset after 13 epochs is = 0.48102472781311595\n",
      "Initial Cost on Val dataset for this epoch 13 = 0.48102472781311595\n",
      "learning rate for this epoch =  0.2633201939239633\n",
      "Error on this batch = 0.48090800522828164\n",
      "Error on this batch = 0.4809931982501489\n",
      "Cost on val dataset after 14 epochs is = 0.4810216880620939\n",
      "Initial Cost on Val dataset for this epoch 14 = 0.4810216880620939\n",
      "learning rate for this epoch =  0.2584865769785853\n",
      "Error on this batch = 0.4809053158580807\n",
      "Error on this batch = 0.4809905592176925\n",
      "Cost on val dataset after 15 epochs is = 0.4810187353161667\n",
      "Initial Cost on Val dataset for this epoch 15 = 0.4810187353161667\n",
      "learning rate for this epoch =  0.25406637407730737\n",
      "Error on this batch = 0.48090271294141435\n",
      "Error on this batch = 0.48098801144207665\n",
      "Cost on val dataset after 16 epochs is = 0.481015870738236\n",
      "cost initial= 0.4810187353161667 , cost final=0.481015870738236 , change in cost= -2.8645779306946118e-06\n",
      "\n",
      "------------------------------------------------------------------------------\n",
      "The stats for number of units in the hidden layer = 10 are as below:\n",
      "------------------------------------------------------------------------------\n",
      "The number of epochs = 16\n",
      "The training time = 1.121sec\n",
      "The training accuracy is = 3.973%\n",
      "The validation accuracy is = 3.128%\n",
      "The test accuracy is = 3.846%\n",
      "------------------------------------------------------------------------------\n",
      "\n",
      "Training the network with 1 hidden layer with 50 units\n",
      "The parameters of the layers are of the shape:\n",
      "theta between layer 0 and layer 1 is (785, 50)\n",
      "theta between layer 1 and layer 2 is (51, 26)\n",
      "Initial Cost on Val dataset for this epoch 1 = 3.432333393911887\n",
      "learning rate for this epoch =  0.5\n",
      "Error on this batch = 3.4325365973151007\n",
      "Error on this batch = 0.4816799445840069\n",
      "Cost on val dataset after 2 epochs is = 0.4811310837562912\n",
      "Initial Cost on Val dataset for this epoch 2 = 0.4811310837562912\n",
      "learning rate for this epoch =  0.4204482076268573\n",
      "Error on this batch = 0.4809476615658362\n",
      "Error on this batch = 0.4809422897462403\n",
      "Cost on val dataset after 3 epochs is = 0.4807636732506059\n",
      "Initial Cost on Val dataset for this epoch 3 = 0.4807636732506059\n",
      "learning rate for this epoch =  0.37991784282579627\n",
      "Error on this batch = 0.4805579543179439\n",
      "Error on this batch = 0.48059490344520767\n",
      "Cost on val dataset after 4 epochs is = 0.4804193978131324\n",
      "Initial Cost on Val dataset for this epoch 4 = 0.4804193978131324\n",
      "learning rate for this epoch =  0.35355339059327373\n",
      "Error on this batch = 0.4801582422429723\n",
      "Error on this batch = 0.4802392062749708\n",
      "Cost on val dataset after 5 epochs is = 0.4800511215770093\n",
      "Initial Cost on Val dataset for this epoch 5 = 0.4800511215770093\n",
      "learning rate for this epoch =  0.334370152488211\n",
      "Error on this batch = 0.47972384140699026\n",
      "Error on this batch = 0.47984843821733036\n",
      "Cost on val dataset after 6 epochs is = 0.47964409516789963\n",
      "Initial Cost on Val dataset for this epoch 6 = 0.47964409516789963\n",
      "learning rate for this epoch =  0.3194715521231362\n",
      "Error on this batch = 0.4792375703832661\n",
      "Error on this batch = 0.47940489424889743\n",
      "Cost on val dataset after 7 epochs is = 0.4791833203572708\n",
      "Initial Cost on Val dataset for this epoch 7 = 0.4791833203572708\n",
      "learning rate for this epoch =  0.3073940764756322\n",
      "Error on this batch = 0.4786788211408088\n",
      "Error on this batch = 0.47889015503073795\n",
      "Cost on val dataset after 8 epochs is = 0.4786507559620688\n",
      "Initial Cost on Val dataset for this epoch 8 = 0.4786507559620688\n",
      "learning rate for this epoch =  0.29730177875068026\n",
      "Error on this batch = 0.4780216704689768\n",
      "Error on this batch = 0.4782816185270896\n",
      "Cost on val dataset after 9 epochs is = 0.4780232840770061\n",
      "Initial Cost on Val dataset for this epoch 9 = 0.4780232840770061\n",
      "learning rate for this epoch =  0.2886751345948129\n",
      "Error on this batch = 0.47723168551668765\n",
      "Error on this batch = 0.47754984435306524\n",
      "Cost on val dataset after 10 epochs is = 0.4772701879242917\n",
      "Initial Cost on Val dataset for this epoch 10 = 0.4772701879242917\n",
      "learning rate for this epoch =  0.28117066259517454\n",
      "Error on this batch = 0.47626156576623163\n",
      "Error on this batch = 0.4766556855187335\n",
      "Cost on val dataset after 11 epochs is = 0.47634988591143557\n",
      "Initial Cost on Val dataset for this epoch 11 = 0.47634988591143557\n",
      "learning rate for this epoch =  0.2745502433880562\n",
      "Error on this batch = 0.4750455502404155\n",
      "Error on this batch = 0.4755475168242492\n",
      "Cost on val dataset after 12 epochs is = 0.47520634368775766\n",
      "Initial Cost on Val dataset for this epoch 12 = 0.47520634368775766\n",
      "learning rate for this epoch =  0.2686424829558855\n",
      "Error on this batch = 0.4734939713144921\n",
      "Error on this batch = 0.4741611553074326\n",
      "Cost on val dataset after 13 epochs is = 0.4737676446554184\n",
      "Initial Cost on Val dataset for this epoch 13 = 0.4737676446554184\n",
      "learning rate for this epoch =  0.2633201939239633\n",
      "Error on this batch = 0.47149412389500284\n",
      "Error on this batch = 0.4724304214649544\n",
      "Cost on val dataset after 14 epochs is = 0.47195409791279475\n",
      "Initial Cost on Val dataset for this epoch 14 = 0.47195409791279475\n",
      "learning rate for this epoch =  0.2584865769785853\n",
      "Error on this batch = 0.4689336897037091\n",
      "Error on this batch = 0.47032276689406943\n",
      "Cost on val dataset after 15 epochs is = 0.46970751620475243\n",
      "Initial Cost on Val dataset for this epoch 15 = 0.46970751620475243\n",
      "learning rate for this epoch =  0.25406637407730737\n",
      "Error on this batch = 0.46576695565961984\n",
      "Error on this batch = 0.4678987161231362\n",
      "Cost on val dataset after 16 epochs is = 0.4670359010446531\n",
      "Initial Cost on Val dataset for this epoch 16 = 0.4670359010446531\n",
      "learning rate for this epoch =  0.25\n",
      "Error on this batch = 0.4620969720624404\n",
      "Error on this batch = 0.46532695197889995\n",
      "Cost on val dataset after 17 epochs is = 0.464021845059292\n",
      "Initial Cost on Val dataset for this epoch 17 = 0.464021845059292\n",
      "learning rate for this epoch =  0.24623953025272619\n",
      "Error on this batch = 0.4581600911514769\n",
      "Error on this batch = 0.4627814979206925\n",
      "Cost on val dataset after 18 epochs is = 0.46076414932386217\n",
      "Initial Cost on Val dataset for this epoch 18 = 0.46076414932386217\n",
      "learning rate for this epoch =  0.24274588585366172\n",
      "Error on this batch = 0.45417984434454356\n",
      "Error on this batch = 0.46032196918175217\n",
      "Cost on val dataset after 19 epochs is = 0.4573220654570584\n",
      "Initial Cost on Val dataset for this epoch 19 = 0.4573220654570584\n",
      "learning rate for this epoch =  0.23948681272178735\n",
      "Error on this batch = 0.4502565516045523\n",
      "Error on this batch = 0.45789243563585535\n",
      "Cost on val dataset after 20 epochs is = 0.4537063371775303\n",
      "Initial Cost on Val dataset for this epoch 20 = 0.4537063371775303\n",
      "learning rate for this epoch =  0.23643540225079396\n",
      "Error on this batch = 0.44637960828556944\n",
      "Error on this batch = 0.4553908357385549\n",
      "Cost on val dataset after 21 epochs is = 0.4498809827432542\n",
      "Initial Cost on Val dataset for this epoch 21 = 0.4498809827432542\n",
      "learning rate for this epoch =  0.23356898886410005\n",
      "Error on this batch = 0.4424763731836791\n",
      "Error on this batch = 0.45271310577580975\n",
      "Cost on val dataset after 22 epochs is = 0.4457636796851843\n",
      "Initial Cost on Val dataset for this epoch 22 = 0.4457636796851843\n",
      "learning rate for this epoch =  0.2308683154720513\n",
      "Error on this batch = 0.4384397880204572\n",
      "Error on this batch = 0.4497566987384009\n",
      "Cost on val dataset after 23 epochs is = 0.44124868546201085\n",
      "Initial Cost on Val dataset for this epoch 23 = 0.44124868546201085\n",
      "learning rate for this epoch =  0.22831689274836564\n",
      "Error on this batch = 0.4341545959475549\n",
      "Error on this batch = 0.446416657977873\n",
      "Cost on val dataset after 24 epochs is = 0.4362664469062646\n",
      "Initial Cost on Val dataset for this epoch 24 = 0.4362664469062646\n",
      "learning rate for this epoch =  0.22590050090246122\n",
      "Error on this batch = 0.42954866601813585\n",
      "Error on this batch = 0.44260964294386973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on val dataset after 25 epochs is = 0.4308390192203968\n",
      "Initial Cost on Val dataset for this epoch 25 = 0.4308390192203968\n",
      "learning rate for this epoch =  0.22360679774997896\n",
      "Error on this batch = 0.42463729820746243\n",
      "Error on this batch = 0.43832162491402576\n",
      "Cost on val dataset after 26 epochs is = 0.42504557291164574\n",
      "Initial Cost on Val dataset for this epoch 26 = 0.42504557291164574\n",
      "learning rate for this epoch =  0.2214250071345737\n",
      "Error on this batch = 0.41948296891911485\n",
      "Error on this batch = 0.4336120664941014\n",
      "Cost on val dataset after 27 epochs is = 0.4189259015501969\n",
      "Initial Cost on Val dataset for this epoch 27 = 0.4189259015501969\n",
      "learning rate for this epoch =  0.21934566882541542\n",
      "Error on this batch = 0.41410075649129957\n",
      "Error on this batch = 0.42855988913067267\n",
      "Cost on val dataset after 28 epochs is = 0.4124580820502143\n",
      "Initial Cost on Val dataset for this epoch 28 = 0.4124580820502143\n",
      "learning rate for this epoch =  0.2173604359724957\n",
      "Error on this batch = 0.4084357983657928\n",
      "Error on this batch = 0.4232254193501368\n",
      "Cost on val dataset after 29 epochs is = 0.4056320408135874\n",
      "Initial Cost on Val dataset for this epoch 29 = 0.4056320408135874\n",
      "learning rate for this epoch =  0.215461909729453\n",
      "Error on this batch = 0.40243147395154466\n",
      "Error on this batch = 0.41766227080205\n",
      "Cost on val dataset after 30 epochs is = 0.3985280787403657\n",
      "Initial Cost on Val dataset for this epoch 30 = 0.3985280787403657\n",
      "learning rate for this epoch =  0.21364350319811704\n",
      "Error on this batch = 0.3961066517103239\n",
      "Error on this batch = 0.4119454112895529\n",
      "Cost on val dataset after 31 epochs is = 0.3913116596827842\n",
      "Initial Cost on Val dataset for this epoch 31 = 0.3913116596827842\n",
      "learning rate for this epoch =  0.21189932870751083\n",
      "Error on this batch = 0.38956152100226404\n",
      "Error on this batch = 0.4061651498650477\n",
      "Cost on val dataset after 32 epochs is = 0.3841479375294367\n",
      "Initial Cost on Val dataset for this epoch 32 = 0.3841479375294367\n",
      "learning rate for this epoch =  0.21022410381342865\n",
      "Error on this batch = 0.3829106141068073\n",
      "Error on this batch = 0.4003895451292566\n",
      "Cost on val dataset after 33 epochs is = 0.37713647538413114\n",
      "Initial Cost on Val dataset for this epoch 33 = 0.37713647538413114\n",
      "learning rate for this epoch =  0.2086130724305753\n",
      "Error on this batch = 0.3762285631961899\n",
      "Error on this batch = 0.3946462297225676\n",
      "Cost on val dataset after 34 epochs is = 0.3703158763048405\n",
      "Initial Cost on Val dataset for this epoch 34 = 0.3703158763048405\n",
      "learning rate for this epoch =  0.20706193828327601\n",
      "Error on this batch = 0.3695526017227866\n",
      "Error on this batch = 0.38893691824383825\n",
      "Cost on val dataset after 35 epochs is = 0.363696920896734\n",
      "Initial Cost on Val dataset for this epoch 35 = 0.363696920896734\n",
      "learning rate for this epoch =  0.20556680845025985\n",
      "Error on this batch = 0.36290704158974485\n",
      "Error on this batch = 0.38325869695957127\n",
      "Cost on val dataset after 36 epochs is = 0.3572840682426129\n",
      "Initial Cost on Val dataset for this epoch 36 = 0.3572840682426129\n",
      "learning rate for this epoch =  0.20412414523193154\n",
      "Error on this batch = 0.35631648867177335\n",
      "Error on this batch = 0.3776148268342326\n",
      "Cost on val dataset after 37 epochs is = 0.3510805700531517\n",
      "Initial Cost on Val dataset for this epoch 37 = 0.3510805700531517\n",
      "learning rate for this epoch =  0.2027307249193849\n",
      "Error on this batch = 0.34980576099513727\n",
      "Error on this batch = 0.37201522949372284\n",
      "Cost on val dataset after 38 epochs is = 0.34508652730861644\n",
      "Initial Cost on Val dataset for this epoch 38 = 0.34508652730861644\n",
      "learning rate for this epoch =  0.20138360231828867\n",
      "Error on this batch = 0.34339573777919435\n",
      "Error on this batch = 0.3664727145341491\n",
      "Cost on val dataset after 39 epochs is = 0.33929686598166353\n",
      "Initial Cost on Val dataset for this epoch 39 = 0.33929686598166353\n",
      "learning rate for this epoch =  0.20008008009612496\n",
      "Error on this batch = 0.337100787007826\n",
      "Error on this batch = 0.36099940221391025\n",
      "Cost on val dataset after 40 epochs is = 0.333701218172429\n",
      "Initial Cost on Val dataset for this epoch 40 = 0.333701218172429\n",
      "learning rate for this epoch =  0.19881768219176266\n",
      "Error on this batch = 0.3309285557474891\n",
      "Error on this batch = 0.3556048000752023\n",
      "Cost on val dataset after 41 epochs is = 0.32828516910117317\n",
      "Initial Cost on Val dataset for this epoch 41 = 0.32828516910117317\n",
      "learning rate for this epoch =  0.19759413066220238\n",
      "Error on this batch = 0.3248810929359412\n",
      "Error on this batch = 0.3502952675461174\n",
      "Cost on val dataset after 42 epochs is = 0.32303195285227887\n",
      "Initial Cost on Val dataset for this epoch 42 = 0.32303195285227887\n",
      "learning rate for this epoch =  0.1964073254502565\n",
      "Error on this batch = 0.3189563889464066\n",
      "Error on this batch = 0.3450742156125314\n",
      "Cost on val dataset after 43 epochs is = 0.3179240479034149\n",
      "Initial Cost on Val dataset for this epoch 43 = 0.3179240479034149\n",
      "learning rate for this epoch =  0.19525532664475806\n",
      "Error on this batch = 0.3131498990343821\n",
      "Error on this batch = 0.3399425827039322\n",
      "Cost on val dataset after 44 epochs is = 0.312944474355369\n",
      "Initial Cost on Val dataset for this epoch 44 = 0.312944474355369\n",
      "learning rate for this epoch =  0.19413633887611162\n",
      "Error on this batch = 0.3074559005231848\n",
      "Error on this batch = 0.33489937630132505\n",
      "Cost on val dataset after 45 epochs is = 0.30807777092616734\n",
      "Initial Cost on Val dataset for this epoch 45 = 0.30807777092616734\n",
      "learning rate for this epoch =  0.19304869754804485\n",
      "Error on this batch = 0.30186862570488304\n",
      "Error on this batch = 0.3299422142463464\n",
      "Cost on val dataset after 46 epochs is = 0.30331068690659263\n",
      "Initial Cost on Val dataset for this epoch 46 = 0.30331068690659263\n",
      "learning rate for this epoch =  0.19199085665396748\n",
      "Error on this batch = 0.29638313241749925\n",
      "Error on this batch = 0.32506785851854825\n",
      "Cost on val dataset after 47 epochs is = 0.298632630290758\n",
      "Initial Cost on Val dataset for this epoch 47 = 0.298632630290758\n",
      "learning rate for this epoch =  0.1909613779654767\n",
      "Error on this batch = 0.29099589245704865\n",
      "Error on this batch = 0.3202727521439309\n",
      "Cost on val dataset after 48 epochs is = 0.2940359041972479\n",
      "Initial Cost on Val dataset for this epoch 48 = 0.2940359041972479\n",
      "learning rate for this epoch =  0.18995892141289814\n",
      "Error on this batch = 0.28570510911013847\n",
      "Error on this batch = 0.31555356674710483\n",
      "Cost on val dataset after 49 epochs is = 0.28951575160758114\n",
      "Initial Cost on Val dataset for this epoch 49 = 0.28951575160758114\n",
      "learning rate for this epoch =  0.1889822365046136\n",
      "Error on this batch = 0.2805108047662898\n",
      "Error on this batch = 0.3109077441397881\n",
      "Cost on val dataset after 50 epochs is = 0.28507021812111544\n",
      "Initial Cost on Val dataset for this epoch 50 = 0.28507021812111544\n",
      "learning rate for this epoch =  0.1880301546543197\n",
      "Error on this batch = 0.2754147314218986\n",
      "Error on this batch = 0.3063339750757709\n",
      "Cost on val dataset after 51 epochs is = 0.2806998412857163\n",
      "Initial Cost on Val dataset for this epoch 51 = 0.2806998412857163\n",
      "learning rate for this epoch =  0.18710158230410626\n",
      "Error on this batch = 0.2704201494271501\n",
      "Error on this batch = 0.30183252803740573\n",
      "Cost on val dataset after 52 epochs is = 0.2764071895136236\n",
      "Initial Cost on Val dataset for this epoch 52 = 0.2764071895136236\n",
      "learning rate for this epoch =  0.1861954947469912\n",
      "Error on this batch = 0.26553150651935603\n",
      "Error on this batch = 0.29740535439025245\n",
      "Cost on val dataset after 53 epochs is = 0.2721962986790607\n",
      "Initial Cost on Val dataset for this epoch 53 = 0.2721962986790607\n",
      "learning rate for this epoch =  0.18531093056582568\n",
      "Error on this batch = 0.2607540449326325\n",
      "Error on this batch = 0.29305595673736945\n",
      "Cost on val dataset after 54 epochs is = 0.26807207434981095\n",
      "Initial Cost on Val dataset for this epoch 54 = 0.26807207434981095\n",
      "learning rate for this epoch =  0.18444698661672027\n",
      "Error on this batch = 0.25609337166318663\n",
      "Error on this batch = 0.28878907421783295\n",
      "Cost on val dataset after 55 epochs is = 0.26403972905986395\n",
      "Initial Cost on Val dataset for this epoch 55 = 0.26403972905986395\n",
      "learning rate for this epoch =  0.1836028134946796\n",
      "Error on this batch = 0.25155503474827695\n",
      "Error on this batch = 0.28461026798535094\n",
      "Cost on val dataset after 56 epochs is = 0.260104306920193\n",
      "Initial Cost on Val dataset for this epoch 56 = 0.260104306920193\n",
      "learning rate for this epoch =  0.18277761142725618\n",
      "Error on this batch = 0.2471441449870821\n",
      "Error on this batch = 0.2805254788636504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on val dataset after 57 epochs is = 0.2562703223868598\n",
      "Initial Cost on Val dataset for this epoch 57 = 0.2562703223868598\n",
      "learning rate for this epoch =  0.18197062654897384\n",
      "Error on this batch = 0.2428650682544209\n",
      "Error on this batch = 0.27654060452233153\n",
      "Cost on val dataset after 58 epochs is = 0.2525415173157063\n",
      "Initial Cost on Val dataset for this epoch 58 = 0.2525415173157063\n",
      "learning rate for this epoch =  0.1811811475152165\n",
      "Error on this batch = 0.23872119841991782\n",
      "Error on this batch = 0.27266112603794185\n",
      "Cost on val dataset after 59 epochs is = 0.2489207258471806\n",
      "Initial Cost on Val dataset for this epoch 59 = 0.2489207258471806\n",
      "learning rate for this epoch =  0.180408502419387\n",
      "Error on this batch = 0.2347148121852778\n",
      "Error on this batch = 0.26889180438382276\n",
      "Cost on val dataset after 60 epochs is = 0.24540982945864656\n",
      "Initial Cost on Val dataset for this epoch 60 = 0.24540982945864656\n",
      "learning rate for this epoch =  0.17965205598154213\n",
      "Error on this batch = 0.2308470039286113\n",
      "Error on this batch = 0.2652364597550986\n",
      "Cost on val dataset after 61 epochs is = 0.24200978179168514\n",
      "Initial Cost on Val dataset for this epoch 61 = 0.24200978179168514\n",
      "learning rate for this epoch =  0.1789112069805131\n",
      "Error on this batch = 0.2271176962536032\n",
      "Error on this batch = 0.26169783771717897\n",
      "Cost on val dataset after 62 epochs is = 0.23872068256831466\n",
      "Initial Cost on Val dataset for this epoch 62 = 0.23872068256831466\n",
      "learning rate for this epoch =  0.1781853859048144\n",
      "Error on this batch = 0.223525717769585\n",
      "Error on this batch = 0.2582775572141501\n",
      "Cost on val dataset after 63 epochs is = 0.23554188129348447\n",
      "Initial Cost on Val dataset for this epoch 63 = 0.23554188129348447\n",
      "learning rate for this epoch =  0.17747405280050266\n",
      "Error on this batch = 0.22006893429145458\n",
      "Error on this batch = 0.25497612860412494\n",
      "Cost on val dataset after 64 epochs is = 0.23247209419226306\n",
      "Initial Cost on Val dataset for this epoch 64 = 0.23247209419226306\n",
      "learning rate for this epoch =  0.17677669529663687\n",
      "Error on this batch = 0.21674441517421145\n",
      "Error on this batch = 0.2517930260865378\n",
      "Cost on val dataset after 65 epochs is = 0.22950952150388837\n",
      "Initial Cost on Val dataset for this epoch 65 = 0.22950952150388837\n",
      "learning rate for this epoch =  0.1760928267911618\n",
      "Error on this batch = 0.21354861450600449\n",
      "Error on this batch = 0.24872679792135757\n",
      "Cost on val dataset after 66 epochs is = 0.2266519562395852\n",
      "Initial Cost on Val dataset for this epoch 66 = 0.2266519562395852\n",
      "learning rate for this epoch =  0.17542198478193427\n",
      "Error on this batch = 0.21047754790996578\n",
      "Error on this batch = 0.2457751989096115\n",
      "Cost on val dataset after 67 epochs is = 0.2238968792329648\n",
      "Initial Cost on Val dataset for this epoch 67 = 0.2238968792329648\n",
      "learning rate for this epoch =  0.1747637293292756\n",
      "Error on this batch = 0.20752694935584123\n",
      "Error on this batch = 0.24293533186869462\n",
      "Cost on val dataset after 68 epochs is = 0.22124153839141797\n",
      "Initial Cost on Val dataset for this epoch 68 = 0.22124153839141797\n",
      "learning rate for this epoch =  0.1741176416378927\n",
      "Error on this batch = 0.20469239775101677\n",
      "Error on this batch = 0.24020378764649594\n",
      "Cost on val dataset after 69 epochs is = 0.21868301233824405\n",
      "Initial Cost on Val dataset for this epoch 69 = 0.21868301233824405\n",
      "learning rate for this epoch =  0.1734833227472955\n",
      "Error on this batch = 0.20196940908731598\n",
      "Error on this batch = 0.2375767761365586\n",
      "Cost on val dataset after 70 epochs is = 0.2162182601296517\n",
      "Initial Cost on Val dataset for this epoch 70 = 0.2162182601296517\n",
      "learning rate for this epoch =  0.1728603923209705\n",
      "Error on this batch = 0.19935349552417517\n",
      "Error on this batch = 0.23505024349422715\n",
      "Cost on val dataset after 71 epochs is = 0.2138441595394646\n",
      "Initial Cost on Val dataset for this epoch 71 = 0.2138441595394646\n",
      "learning rate for this epoch =  0.17224848752556968\n",
      "Error on this batch = 0.19684019713280174\n",
      "Error on this batch = 0.23261997312002922\n",
      "Cost on val dataset after 72 epochs is = 0.2115575366609635\n",
      "Initial Cost on Val dataset for this epoch 72 = 0.2115575366609635\n",
      "learning rate for this epoch =  0.17164726199225983\n",
      "Error on this batch = 0.19442509455972556\n",
      "Error on this batch = 0.23028166985332327\n",
      "Cost on val dataset after 73 epochs is = 0.20935518941848594\n",
      "Initial Cost on Val dataset for this epoch 73 = 0.20935518941848594\n",
      "learning rate for this epoch =  0.17105638485316074\n",
      "Error on this batch = 0.19210381144267508\n",
      "Error on this batch = 0.22803102815812223\n",
      "Cost on val dataset after 74 epochs is = 0.20723390714784076\n",
      "Initial Cost on Val dataset for this epoch 74 = 0.20723390714784076\n",
      "learning rate for this epoch =  0.1704755398464977\n",
      "Error on this batch = 0.18987201429189157\n",
      "Error on this batch = 0.2258637859045264\n",
      "Cost on val dataset after 75 epochs is = 0.20519048782652283\n",
      "Initial Cost on Val dataset for this epoch 75 = 0.20519048782652283\n",
      "learning rate for this epoch =  0.16990442448471224\n",
      "Error on this batch = 0.18772541531668374\n",
      "Error on this batch = 0.22377576573193644\n",
      "Cost on val dataset after 76 epochs is = 0.20322175393255068\n",
      "Initial Cost on Val dataset for this epoch 76 = 0.20322175393255068\n",
      "learning rate for this epoch =  0.16934274928032858\n",
      "Error on this batch = 0.1856597810408293\n",
      "Error on this batch = 0.22176290603780924\n",
      "Cost on val dataset after 77 epochs is = 0.20132456738075996\n",
      "Initial Cost on Val dataset for this epoch 77 = 0.20132456738075996\n",
      "learning rate for this epoch =  0.16879023702486318\n",
      "Error on this batch = 0.1836709471478741\n",
      "Error on this batch = 0.21982128348900964\n",
      "Cost on val dataset after 78 epochs is = 0.19949584358848352\n",
      "Initial Cost on Val dataset for this epoch 78 = 0.19949584358848352\n",
      "learning rate for this epoch =  0.16824662211650757\n",
      "Error on this batch = 0.18175483824539002\n",
      "Error on this batch = 0.21794712870633112\n",
      "Cost on val dataset after 79 epochs is = 0.19773256448060017\n",
      "Initial Cost on Val dataset for this epoch 79 = 0.19773256448060017\n",
      "learning rate for this epoch =  0.1677116499327062\n",
      "Error on this batch = 0.17990749028653827\n",
      "Error on this batch = 0.21613683650168888\n",
      "Cost on val dataset after 80 epochs is = 0.1960317901452003\n",
      "Initial Cost on Val dataset for this epoch 80 = 0.1960317901452003\n",
      "learning rate for this epoch =  0.1671850762441055\n",
      "Error on this batch = 0.17812507316581522\n",
      "Error on this batch = 0.21438697179581348\n",
      "Cost on val dataset after 81 epochs is = 0.19439066886223938\n",
      "Initial Cost on Val dataset for this epoch 81 = 0.19439066886223938\n",
      "learning rate for this epoch =  0.16666666666666666\n",
      "Error on this batch = 0.17640391131104877\n",
      "Error on this batch = 0.2126942721306928\n",
      "Cost on val dataset after 82 epochs is = 0.1928064453070953\n",
      "Initial Cost on Val dataset for this epoch 82 = 0.1928064453070953\n",
      "learning rate for this epoch =  0.16615619614902008\n",
      "Error on this batch = 0.1747405006873027\n",
      "Error on this batch = 0.2110556475179529\n",
      "Cost on val dataset after 83 epochs is = 0.19127646684031566\n",
      "Initial Cost on Val dataset for this epoch 83 = 0.19127646684031566\n",
      "learning rate for this epoch =  0.16565344849239508\n",
      "Error on this batch = 0.17313152130409287\n",
      "Error on this batch = 0.20946817822626687\n",
      "Cost on val dataset after 84 epochs is = 0.18979818790466185\n",
      "Initial Cost on Val dataset for this epoch 84 = 0.18979818790466185\n",
      "learning rate for this epoch =  0.16515821590069035\n",
      "Error on this batch = 0.17157384493302552\n",
      "Error on this batch = 0.20792911099998526\n",
      "Cost on val dataset after 85 epochs is = 0.18836917264256248\n",
      "Initial Cost on Val dataset for this epoch 85 = 0.18836917264256248\n",
      "learning rate for this epoch =  0.164670298558459\n",
      "Error on this batch = 0.17006453822333392\n",
      "Error on this batch = 0.20643585411058638\n",
      "Cost on val dataset after 86 epochs is = 0.18698709591296175\n",
      "Initial Cost on Val dataset for this epoch 86 = 0.18698709591296175\n",
      "learning rate for this epoch =  0.16418950423477013\n",
      "Error on this batch = 0.16860086172577776\n",
      "Error on this batch = 0.20498597156725054\n",
      "Cost on val dataset after 87 epochs is = 0.18564974292526984\n",
      "Initial Cost on Val dataset for this epoch 87 = 0.18564974292526984\n",
      "learning rate for this epoch =  0.16371564791108048\n",
      "Error on this batch = 0.16718026551314652\n",
      "Error on this batch = 0.20357717674961373\n",
      "Cost on val dataset after 88 epochs is = 0.18435500772307953\n",
      "Initial Cost on Val dataset for this epoch 88 = 0.18435500772307953\n",
      "learning rate for this epoch =  0.16324855143140263\n",
      "Error on this batch = 0.16580038214733506\n",
      "Error on this batch = 0.20220732567256433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on val dataset after 89 epochs is = 0.1831008907469176\n",
      "Initial Cost on Val dataset for this epoch 89 = 0.1831008907469176\n",
      "learning rate for this epoch =  0.1627880431731981\n",
      "Error on this batch = 0.1644590177224032\n",
      "Error on this batch = 0.2008744100484762\n",
      "Cost on val dataset after 90 epochs is = 0.18188549568929502\n",
      "Initial Cost on Val dataset for this epoch 90 = 0.18188549568929502\n",
      "learning rate for this epoch =  0.16233395773754944\n",
      "Error on this batch = 0.16315414164117506\n",
      "Error on this batch = 0.1995765502755002\n",
      "Cost on val dataset after 91 epochs is = 0.1807070258318478\n",
      "Initial Cost on Val dataset for this epoch 91 = 0.1807070258318478\n",
      "learning rate for this epoch =  0.16188613565728216\n",
      "Error on this batch = 0.16188387568468293\n",
      "Error on this batch = 0.19831198845053932\n",
      "Cost on val dataset after 92 epochs is = 0.17956378002748996\n",
      "Initial Cost on Val dataset for this epoch 92 = 0.17956378002748996\n",
      "learning rate for this epoch =  0.161444423121811\n",
      "Error on this batch = 0.16064648282711702\n",
      "Error on this batch = 0.19707908148136583\n",
      "Cost on val dataset after 93 epochs is = 0.17845414846318913\n",
      "Initial Cost on Val dataset for this epoch 93 = 0.17845414846318913\n",
      "learning rate for this epoch =  0.16100867171758368\n",
      "Error on this batch = 0.15944035614575244\n",
      "Error on this batch = 0.19587629435312134\n",
      "Cost on val dataset after 94 epochs is = 0.17737660831314783\n",
      "Initial Cost on Val dataset for this epoch 94 = 0.17737660831314783\n",
      "learning rate for this epoch =  0.160578738183079\n",
      "Error on this batch = 0.15826400808244476\n",
      "Error on this batch = 0.19470219358930854\n",
      "Cost on val dataset after 95 epochs is = 0.17632971936895095\n",
      "Initial Cost on Val dataset for this epoch 95 = 0.17632971936895095\n",
      "learning rate for this epoch =  0.16015448417739933\n",
      "Error on this batch = 0.157116060233993\n",
      "Error on this batch = 0.19355544093556445\n",
      "Cost on val dataset after 96 epochs is = 0.1753121197131221\n",
      "Initial Cost on Val dataset for this epoch 96 = 0.1753121197131221\n",
      "learning rate for this epoch =  0.1597357760615681\n",
      "Error on this batch = 0.15599523378379318\n",
      "Error on this batch = 0.19243478728534505\n",
      "Cost on val dataset after 97 epochs is = 0.17432252148562263\n",
      "Initial Cost on Val dataset for this epoch 97 = 0.17432252148562263\n",
      "learning rate for this epoch =  0.15932248469171095\n",
      "Error on this batch = 0.15490034063607966\n",
      "Error on this batch = 0.19133906685958885\n",
      "Cost on val dataset after 98 epochs is = 0.17335970677895657\n",
      "Initial Cost on Val dataset for this epoch 98 = 0.17335970677895657\n",
      "learning rate for this epoch =  0.15891448522335927\n",
      "Error on this batch = 0.15383027527520374\n",
      "Error on this batch = 0.19026719164702158\n",
      "Cost on val dataset after 99 epochs is = 0.17242252368640992\n",
      "Initial Cost on Val dataset for this epoch 99 = 0.17242252368640992\n",
      "learning rate for this epoch =  0.15851165692617153\n",
      "Error on this batch = 0.1527840073439937\n",
      "Error on this batch = 0.18921814610765836\n",
      "Cost on val dataset after 100 epochs is = 0.17150988251918428\n",
      "Initial Cost on Val dataset for this epoch 100 = 0.17150988251918428\n",
      "learning rate for this epoch =  0.15811388300841897\n",
      "Error on this batch = 0.15176057491540115\n",
      "Error on this batch = 0.188190982138979\n",
      "Cost on val dataset after 101 epochs is = 0.1706207522014064\n",
      "Initial Cost on Val dataset for this epoch 101 = 0.1706207522014064\n",
      "learning rate for this epoch =  0.1577210504506286\n",
      "Error on this batch = 0.150759078418597\n",
      "Error on this batch = 0.18718481430198047\n",
      "Cost on val dataset after 102 epochs is = 0.16975415684684683\n",
      "Initial Cost on Val dataset for this epoch 102 = 0.16975415684684683\n",
      "learning rate for this epoch =  0.1573330498478208\n",
      "Error on this batch = 0.14977867517288895\n",
      "Error on this batch = 0.18619881530267116\n",
      "Cost on val dataset after 103 epochs is = 0.16890917251735263\n",
      "Initial Cost on Val dataset for this epoch 103 = 0.16890917251735263\n",
      "learning rate for this epoch =  0.15694977525981785\n",
      "Error on this batch = 0.14881857447897168\n",
      "Error on this batch = 0.18523221172343227\n",
      "Cost on val dataset after 104 epochs is = 0.16808492416020843\n",
      "Initial Cost on Val dataset for this epoch 104 = 0.16808492416020843\n",
      "learning rate for this epoch =  0.15657112406913673\n",
      "Error on this batch = 0.14787803321601936\n",
      "Error on this batch = 0.1842842799979143\n",
      "Cost on val dataset after 105 epochs is = 0.16728058271966734\n",
      "Initial Cost on Val dataset for this epoch 105 = 0.16728058271966734\n",
      "learning rate for this epoch =  0.1561969968460128\n",
      "Error on this batch = 0.14695635189413403\n",
      "Error on this batch = 0.1833543426226693\n",
      "Cost on val dataset after 106 epochs is = 0.16649536241654844\n",
      "Initial Cost on Val dataset for this epoch 106 = 0.16649536241654844\n",
      "learning rate for this epoch =  0.15582729722013278\n",
      "Error on this batch = 0.14605287111400547\n",
      "Error on this batch = 0.1824417645984596\n",
      "Cost on val dataset after 107 epochs is = 0.16572851818892667\n",
      "Initial Cost on Val dataset for this epoch 107 = 0.16572851818892667\n",
      "learning rate for this epoch =  0.15546193175868359\n",
      "Error on this batch = 0.14516696838881957\n",
      "Error on this batch = 0.18154595009406777\n",
      "Cost on val dataset after 108 epochs is = 0.16497934328643565\n",
      "Initial Cost on Val dataset for this epoch 108 = 0.16497934328643565\n",
      "learning rate for this epoch =  0.15510080985034994\n",
      "Error on this batch = 0.1442980552870949\n",
      "Error on this batch = 0.1806663393254005\n",
      "Cost on val dataset after 109 epochs is = 0.16424716701045833\n",
      "Initial Cost on Val dataset for this epoch 109 = 0.16424716701045833\n",
      "learning rate for this epoch =  0.1547438435949191\n",
      "Error on this batch = 0.14344557485896478\n",
      "Error on this batch = 0.17980240564269043\n",
      "Cost on val dataset after 110 epochs is = 0.16353135259243465\n",
      "Initial Cost on Val dataset for this epoch 110 = 0.16353135259243465\n",
      "learning rate for this epoch =  0.1543909476981724\n",
      "Error on this batch = 0.1426089993122608\n",
      "Error on this batch = 0.17895365281861572\n",
      "Cost on val dataset after 111 epochs is = 0.16283129520260298\n",
      "Initial Cost on Val dataset for this epoch 111 = 0.16283129520260298\n",
      "learning rate for this epoch =  0.15404203937176525\n",
      "Error on this batch = 0.14178782790846708\n",
      "Error on this batch = 0.17811961253015626\n",
      "Cost on val dataset after 112 epochs is = 0.1621464200816825\n",
      "Initial Cost on Val dataset for this epoch 112 = 0.1621464200816825\n",
      "learning rate for this epoch =  0.1536970382378161\n",
      "Error on this batch = 0.14098158505211925\n",
      "Error on this batch = 0.17729984202696955\n",
      "Cost on val dataset after 113 epochs is = 0.16147618078825543\n",
      "Initial Cost on Val dataset for this epoch 113 = 0.16147618078825543\n",
      "learning rate for this epoch =  0.15335586623794323\n",
      "Error on this batch = 0.14018981855046442\n",
      "Error on this batch = 0.17649392197899227\n",
      "Cost on val dataset after 114 epochs is = 0.16082005755490217\n",
      "Initial Cost on Val dataset for this epoch 114 = 0.16082005755490217\n",
      "learning rate for this epoch =  0.1530184475465045\n",
      "Error on this batch = 0.1394120980231623\n",
      "Error on this batch = 0.1757014544958545\n",
      "Cost on val dataset after 115 epochs is = 0.16017755574646392\n",
      "Initial Cost on Val dataset for this epoch 115 = 0.16017755574646392\n",
      "learning rate for this epoch =  0.15268470848781107\n",
      "Error on this batch = 0.1386480134444733\n",
      "Error on this batch = 0.1749220613105401\n",
      "Cost on val dataset after 116 epochs is = 0.15954820441413747\n",
      "Initial Cost on Val dataset for this epoch 116 = 0.15954820441413747\n",
      "learning rate for this epoch =  0.1523545774571\n",
      "Error on this batch = 0.13789717380276206\n",
      "Error on this batch = 0.17415538211954698\n",
      "Cost on val dataset after 117 epochs is = 0.15893155493944153\n",
      "Initial Cost on Val dataset for this epoch 117 = 0.15893155493944153\n",
      "learning rate for this epoch =  0.15202798484506466\n",
      "Error on this batch = 0.13715920586424868\n",
      "Error on this batch = 0.1734010730716087\n",
      "Cost on val dataset after 118 epochs is = 0.1583271797624249\n",
      "Initial Cost on Val dataset for this epoch 118 = 0.1583271797624249\n",
      "learning rate for this epoch =  0.15170486296575364\n",
      "Error on this batch = 0.13643375302978547\n",
      "Error on this batch = 0.17265880539685402\n",
      "Cost on val dataset after 119 epochs is = 0.15773467118881065\n",
      "Initial Cost on Val dataset for this epoch 119 = 0.15773467118881065\n",
      "learning rate for this epoch =  0.1513851459876605\n",
      "Error on this batch = 0.13572047427504427\n",
      "Error on this batch = 0.17192826416810852\n",
      "Cost on val dataset after 120 epochs is = 0.15715364027108109\n",
      "Initial Cost on Val dataset for this epoch 120 = 0.15715364027108109\n",
      "learning rate for this epoch =  0.1510687698678384\n",
      "Error on this batch = 0.13501904316588642\n",
      "Error on this batch = 0.1712091471859125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on val dataset after 121 epochs is = 0.15658371575880936\n",
      "Initial Cost on Val dataset for this epoch 121 = 0.15658371575880936\n",
      "learning rate for this epoch =  0.15075567228888181\n",
      "Error on this batch = 0.13432914694187997\n",
      "Error on this batch = 0.1705011639787388\n",
      "Cost on val dataset after 122 epochs is = 0.15602454311382913\n",
      "Initial Cost on Val dataset for this epoch 122 = 0.15602454311382913\n",
      "learning rate for this epoch =  0.1504457925986288\n",
      "Error on this batch = 0.1336504856619447\n",
      "Error on this batch = 0.16980403490986434\n",
      "Cost on val dataset after 123 epochs is = 0.15547578358610503\n",
      "Initial Cost on Val dataset for this epoch 123 = 0.15547578358610503\n",
      "learning rate for this epoch =  0.15013907175244492\n",
      "Error on this batch = 0.1329827714069663\n",
      "Error on this batch = 0.1691174903823866\n",
      "Cost on val dataset after 124 epochs is = 0.15493711334642557\n",
      "Initial Cost on Val dataset for this epoch 124 = 0.15493711334642557\n",
      "learning rate for this epoch =  0.1498354522579582\n",
      "Error on this batch = 0.13232572753494506\n",
      "Error on this batch = 0.16844127013397622\n",
      "Cost on val dataset after 125 epochs is = 0.15440822267228207\n",
      "Initial Cost on Val dataset for this epoch 125 = 0.15440822267228207\n",
      "learning rate for this epoch =  0.14953487812212204\n",
      "Error on this batch = 0.13167908798485053\n",
      "Error on this batch = 0.16777512261313463\n",
      "Cost on val dataset after 126 epochs is = 0.15388881518352826\n",
      "Initial Cost on Val dataset for this epoch 126 = 0.15388881518352826\n",
      "learning rate for this epoch =  0.14923729480049114\n",
      "Error on this batch = 0.13104259662585588\n",
      "Error on this batch = 0.16711880442896515\n",
      "Cost on val dataset after 127 epochs is = 0.15337860712463156\n",
      "Initial Cost on Val dataset for this epoch 127 = 0.15337860712463156\n",
      "learning rate for this epoch =  0.14894264914859962\n",
      "Error on this batch = 0.13041600664903913\n",
      "Error on this batch = 0.1664720798667736\n",
      "Cost on val dataset after 128 epochs is = 0.1528773266905303\n",
      "Initial Cost on Val dataset for this epoch 128 = 0.1528773266905303\n",
      "learning rate for this epoch =  0.14865088937534013\n",
      "Error on this batch = 0.12979907999897874\n",
      "Error on this batch = 0.16583472046217584\n",
      "Cost on val dataset after 129 epochs is = 0.15238471339330342\n",
      "Initial Cost on Val dataset for this epoch 129 = 0.15238471339330342\n",
      "learning rate for this epoch =  0.14836196499824542\n",
      "Error on this batch = 0.12919158684294452\n",
      "Error on this batch = 0.16520650462680064\n",
      "Cost on val dataset after 130 epochs is = 0.151900517467039\n",
      "Initial Cost on Val dataset for this epoch 130 = 0.151900517467039\n",
      "learning rate for this epoch =  0.14807582680058123\n",
      "Error on this batch = 0.12859330507560787\n",
      "Error on this batch = 0.164587217319126\n",
      "Cost on val dataset after 131 epochs is = 0.15142449930845708\n",
      "Initial Cost on Val dataset for this epoch 131 = 0.15142449930845708\n",
      "learning rate for this epoch =  0.14779242679016388\n",
      "Error on this batch = 0.12800401985737137\n",
      "Error on this batch = 0.16397664975446405\n",
      "Cost on val dataset after 132 epochs is = 0.15095642895100075\n",
      "Initial Cost on Val dataset for this epoch 132 = 0.15095642895100075\n",
      "learning rate for this epoch =  0.1475117181598202\n",
      "Error on this batch = 0.12742352318455946\n",
      "Error on this batch = 0.163374599148605\n",
      "Cost on val dataset after 133 epochs is = 0.15049608557025784\n",
      "Initial Cost on Val dataset for this epoch 133 = 0.15049608557025784\n",
      "learning rate for this epoch =  0.147233655249413\n",
      "Error on this batch = 0.12685161348982227\n",
      "Error on this batch = 0.16278086849013587\n",
      "Cost on val dataset after 134 epochs is = 0.150043257018716\n",
      "Initial Cost on Val dataset for this epoch 134 = 0.150043257018716\n",
      "learning rate for this epoch =  0.1469581935093583\n",
      "Error on this batch = 0.1262880952711922\n",
      "Error on this batch = 0.16219526633695083\n",
      "Cost on val dataset after 135 epochs is = 0.14959773938798257\n",
      "Initial Cost on Val dataset for this epoch 135 = 0.14959773938798257\n",
      "learning rate for this epoch =  0.14668528946556555\n",
      "Error on this batch = 0.12573277874830205\n",
      "Error on this batch = 0.16161760663296357\n",
      "Cost on val dataset after 136 epochs is = 0.14915933659672503\n",
      "Initial Cost on Val dataset for this epoch 136 = 0.14915933659672503\n",
      "learning rate for this epoch =  0.14641490068573487\n",
      "Error on this batch = 0.12518547954432738\n",
      "Error on this batch = 0.16104770854150827\n",
      "Cost on val dataset after 137 epochs is = 0.14872786000269927\n",
      "Initial Cost on Val dataset for this epoch 137 = 0.14872786000269927\n",
      "learning rate for this epoch =  0.14614698574694937\n",
      "Error on this batch = 0.12464601839225964\n",
      "Error on this batch = 0.16048539629236971\n",
      "Cost on val dataset after 138 epochs is = 0.14830312803734316\n",
      "Initial Cost on Val dataset for this epoch 138 = 0.14830312803734316\n",
      "learning rate for this epoch =  0.145881504204504\n",
      "Error on this batch = 0.12411422086415361\n",
      "Error on this batch = 0.1599304990398083\n",
      "Cost on val dataset after 139 epochs is = 0.1478849658615094\n",
      "Initial Cost on Val dataset for this epoch 139 = 0.1478849658615094\n",
      "learning rate for this epoch =  0.14561841656191457\n",
      "Error on this batch = 0.12358991712202379\n",
      "Error on this batch = 0.15938285072934172\n",
      "Cost on val dataset after 140 epochs is = 0.1474732050410074\n",
      "Initial Cost on Val dataset for this epoch 140 = 0.1474732050410074\n",
      "learning rate for this epoch =  0.14535768424205484\n",
      "Error on this batch = 0.1230729416890926\n",
      "Error on this batch = 0.15884228997140656\n",
      "Cost on val dataset after 141 epochs is = 0.14706768324070935\n",
      "Initial Cost on Val dataset for this epoch 141 = 0.14706768324070935\n",
      "learning rate for this epoch =  0.14509926955937089\n",
      "Error on this batch = 0.1225631332401213\n",
      "Error on this batch = 0.1583086599203509\n",
      "Cost on val dataset after 142 epochs is = 0.14666824393605934\n",
      "Initial Cost on Val dataset for this epoch 142 = 0.14666824393605934\n",
      "learning rate for this epoch =  0.1448431356931257\n",
      "Error on this batch = 0.12206033440958247\n",
      "Error on this batch = 0.15778180815750087\n",
      "Cost on val dataset after 143 epochs is = 0.14627473614089687\n",
      "Initial Cost on Val dataset for this epoch 143 = 0.14627473614089687\n",
      "learning rate for this epoch =  0.14458924666162856\n",
      "Error on this batch = 0.12156439161646039\n",
      "Error on this batch = 0.15726158657730513\n",
      "Cost on val dataset after 144 epochs is = 0.14588701415058047\n",
      "Initial Cost on Val dataset for this epoch 144 = 0.14588701415058047\n",
      "learning rate for this epoch =  0.14433756729740646\n",
      "Error on this batch = 0.12107515490449759\n",
      "Error on this batch = 0.15674785127578317\n",
      "Cost on val dataset after 145 epochs is = 0.1455049372994619\n",
      "Initial Cost on Val dataset for this epoch 145 = 0.1455049372994619\n",
      "learning rate for this epoch =  0.14408806322327672\n",
      "Error on this batch = 0.12059247779673878\n",
      "Error on this batch = 0.15624046244070117\n",
      "Cost on val dataset after 146 epochs is = 0.1451283697318222\n",
      "Initial Cost on Val dataset for this epoch 146 = 0.1451283697318222\n",
      "learning rate for this epoch =  0.14384070082928266\n",
      "Error on this batch = 0.12011621716325817\n",
      "Error on this batch = 0.1557392842430617\n",
      "Cost on val dataset after 147 epochs is = 0.1447571801854401\n",
      "Initial Cost on Val dataset for this epoch 147 = 0.1447571801854401\n",
      "learning rate for this epoch =  0.1435954472504545\n",
      "Error on this batch = 0.11964623310099565\n",
      "Error on this batch = 0.15524418472963286\n",
      "Cost on val dataset after 148 epochs is = 0.1443912417870173\n",
      "Initial Cost on Val dataset for this epoch 148 = 0.1443912417870173\n",
      "learning rate for this epoch =  0.1433522703453617\n",
      "Error on this batch = 0.11918238882466799\n",
      "Error on this batch = 0.15475503571635546\n",
      "Cost on val dataset after 149 epochs is = 0.14403043185873252\n",
      "Initial Cost on Val dataset for this epoch 149 = 0.14403043185873252\n",
      "learning rate for this epoch =  0.1431111386754225\n",
      "Error on this batch = 0.11872455056776442\n",
      "Error on this batch = 0.15427171268255765\n",
      "Cost on val dataset after 150 epochs is = 0.14367463173524697\n",
      "Initial Cost on Val dataset for this epoch 150 = 0.14367463173524697\n",
      "learning rate for this epoch =  0.14287202148493997\n",
      "Error on this batch = 0.11827258749268303\n",
      "Error on this batch = 0.153794094665981\n",
      "Cost on val dataset after 151 epochs is = 0.1433237265905249\n",
      "Initial Cost on Val dataset for this epoch 151 = 0.1433237265905249\n",
      "learning rate for this epoch =  0.14263488868183333\n",
      "Error on this batch = 0.11782637160911051\n",
      "Error on this batch = 0.15332206415867397\n",
      "Cost on val dataset after 152 epochs is = 0.14297760527387435\n",
      "Initial Cost on Val dataset for this epoch 152 = 0.14297760527387435\n",
      "learning rate for this epoch =  0.14239971081903682\n",
      "Error on this batch = 0.1173857776997989\n",
      "Error on this batch = 0.15285550700385206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on val dataset after 153 epochs is = 0.14263616015465203\n",
      "Initial Cost on Val dataset for this epoch 153 = 0.14263616015465203\n",
      "learning rate for this epoch =  0.14216645907653844\n",
      "Error on this batch = 0.11695068325294208\n",
      "Error on this batch = 0.15239431229385045\n",
      "Cost on val dataset after 154 epochs is = 0.14229928697511143\n",
      "Initial Cost on Val dataset for this epoch 154 = 0.14229928697511143\n",
      "learning rate for this epoch =  0.14193510524403224\n",
      "Error on this batch = 0.11652096840040599\n",
      "Error on this batch = 0.15193837226931325\n",
      "Cost on val dataset after 155 epochs is = 0.14196688471090552\n",
      "Initial Cost on Val dataset for this epoch 155 = 0.14196688471090552\n",
      "learning rate for this epoch =  0.1417056217041599\n",
      "Error on this batch = 0.11609651586111898\n",
      "Error on this batch = 0.1514875822197722\n",
      "Cost on val dataset after 156 epochs is = 0.14163885543878865\n",
      "Initial Cost on Val dataset for this epoch 156 = 0.14163885543878865\n",
      "learning rate for this epoch =  0.14147798141631754\n",
      "Error on this batch = 0.11567721088897823\n",
      "Error on this batch = 0.15104184038577048\n",
      "Cost on val dataset after 157 epochs is = 0.1413151042110896\n",
      "Initial Cost on Val dataset for this epoch 157 = 0.1413151042110896\n",
      "learning rate for this epoch =  0.14125215790100537\n",
      "Error on this batch = 0.11526294122468014\n",
      "Error on this batch = 0.15060104786268286\n",
      "Cost on val dataset after 158 epochs is = 0.14099553893655462\n",
      "Initial Cost on Val dataset for this epoch 158 = 0.14099553893655462\n",
      "learning rate for this epoch =  0.14102812522469854\n",
      "Error on this batch = 0.11485359705093118\n",
      "Error on this batch = 0.1501651085063765\n",
      "Cost on val dataset after 159 epochs is = 0.14068007026718665\n",
      "Initial Cost on Val dataset for this epoch 159 = 0.14068007026718665\n",
      "learning rate for this epoch =  0.1408058579852188\n",
      "Error on this batch = 0.11444907095054463\n",
      "Error on this batch = 0.14973392884084555\n",
      "Cost on val dataset after 160 epochs is = 0.14036861149072824\n",
      "Initial Cost on Val dataset for this epoch 160 = 0.14036861149072824\n",
      "learning rate for this epoch =  0.14058533129758727\n",
      "Error on this batch = 0.11404925786697515\n",
      "Error on this batch = 0.14930741796793962\n",
      "Cost on val dataset after 161 epochs is = 0.14006107842845955\n",
      "Initial Cost on Val dataset for this epoch 161 = 0.14006107842845955\n",
      "learning rate for this epoch =  0.14036652078033962\n",
      "Error on this batch = 0.11365405506688772\n",
      "Error on this batch = 0.14888548747929142\n",
      "Cost on val dataset after 162 epochs is = 0.1397573893380031\n",
      "Initial Cost on Val dataset for this epoch 162 = 0.1397573893380031\n",
      "learning rate for this epoch =  0.14014940254228575\n",
      "Error on this batch = 0.11326336210440083\n",
      "Error on this batch = 0.14846805137053345\n",
      "Cost on val dataset after 163 epochs is = 0.13945746482084573\n",
      "Initial Cost on Val dataset for this epoch 163 = 0.13945746482084573\n",
      "learning rate for this epoch =  0.13993395316969692\n",
      "Error on this batch = 0.11287708078668388\n",
      "Error on this batch = 0.1480550259578784\n",
      "Cost on val dataset after 164 epochs is = 0.13916122773430706\n",
      "Initial Cost on Val dataset for this epoch 164 = 0.13916122773430706\n",
      "learning rate for this epoch =  0.13972014971390403\n",
      "Error on this batch = 0.11249511514062716\n",
      "Error on this batch = 0.1476463297971212\n",
      "Cost on val dataset after 165 epochs is = 0.13886860310770102\n",
      "Initial Cost on Val dataset for this epoch 165 = 0.13886860310770102\n",
      "learning rate for this epoch =  0.13950796967929133\n",
      "Error on this batch = 0.11211737138033892\n",
      "Error on this batch = 0.14724188360510657\n",
      "Cost on val dataset after 166 epochs is = 0.1385795180624517\n",
      "Initial Cost on Val dataset for this epoch 166 = 0.1385795180624517\n",
      "learning rate for this epoch =  0.13929739101167085\n",
      "Error on this batch = 0.11174375787525609\n",
      "Error on this batch = 0.1468416101836904\n",
      "Cost on val dataset after 167 epochs is = 0.13829390173594103\n",
      "Initial Cost on Val dataset for this epoch 167 = 0.13829390173594103\n",
      "learning rate for this epoch =  0.13908839208702292\n",
      "Error on this batch = 0.11137418511868799\n",
      "Error on this batch = 0.14644543434621046\n",
      "Cost on val dataset after 168 epochs is = 0.1380116852088782\n",
      "Initial Cost on Val dataset for this epoch 168 = 0.1380116852088782\n",
      "learning rate for this epoch =  0.13888095170058953\n",
      "Error on this batch = 0.11100856569663861\n",
      "Error on this batch = 0.1460532828464688\n",
      "Cost on val dataset after 169 epochs is = 0.1377328014359958\n",
      "Initial Cost on Val dataset for this epoch 169 = 0.1377328014359958\n",
      "learning rate for this epoch =  0.1386750490563073\n",
      "Error on this batch = 0.11064681425678007\n",
      "Error on this batch = 0.14566508431021644\n",
      "Cost on val dataset after 170 epochs is = 0.13745718517988745\n",
      "Initial Cost on Val dataset for this epoch 170 = 0.13745718517988745\n",
      "learning rate for this epoch =  0.13847066375656708\n",
      "Error on this batch = 0.11028884747747299\n",
      "Error on this batch = 0.14528076916912108\n",
      "Cost on val dataset after 171 epochs is = 0.13718477294781506\n",
      "Initial Cost on Val dataset for this epoch 171 = 0.13718477294781506\n",
      "learning rate for this epoch =  0.1382677757922894\n",
      "Error on this batch = 0.10993458403675031\n",
      "Error on this batch = 0.14490026959718857\n",
      "Cost on val dataset after 172 epochs is = 0.1369155029313232\n",
      "Initial Cost on Val dataset for this epoch 172 = 0.1369155029313232\n",
      "learning rate for this epoch =  0.1380663655333028\n",
      "Error on this batch = 0.1095839445812008\n",
      "Error on this batch = 0.14452351944960096\n",
      "Cost on val dataset after 173 epochs is = 0.136649314948509\n",
      "Initial Cost on Val dataset for this epoch 173 = 0.136649314948509\n",
      "learning rate for this epoch =  0.13786641371901512\n",
      "Error on this batch = 0.10923685169470383\n",
      "Error on this batch = 0.1441504542039263\n",
      "Cost on val dataset after 174 epochs is = 0.13638615038880458\n",
      "Initial Cost on Val dataset for this epoch 174 = 0.13638615038880458\n",
      "learning rate for this epoch =  0.13766790144936686\n",
      "Error on this batch = 0.10889322986698372\n",
      "Error on this batch = 0.1437810109036504\n",
      "Cost on val dataset after 175 epochs is = 0.13612595216013806\n",
      "Initial Cost on Val dataset for this epoch 175 = 0.13612595216013806\n",
      "learning rate for this epoch =  0.1374708101760565\n",
      "Error on this batch = 0.108553005461963\n",
      "Error on this batch = 0.1434151281039736\n",
      "Cost on val dataset after 176 epochs is = 0.1358686646383478\n",
      "Initial Cost on Val dataset for this epoch 176 = 0.1358686646383478\n",
      "learning rate for this epoch =  0.1372751216940281\n",
      "Error on this batch = 0.10821610668590693\n",
      "Error on this batch = 0.14305274581981367\n",
      "Cost on val dataset after 177 epochs is = 0.13561423361873076\n",
      "Initial Cost on Val dataset for this epoch 177 = 0.13561423361873076\n",
      "learning rate for this epoch =  0.1370808181332119\n",
      "Error on this batch = 0.10788246355535924\n",
      "Error on this batch = 0.14269380547594995\n",
      "Cost on val dataset after 178 epochs is = 0.13536260626961533\n",
      "Initial Cost on Val dataset for this epoch 178 = 0.13536260626961533\n",
      "learning rate for this epoch =  0.13688788195050916\n",
      "Error on this batch = 0.10755200786487942\n",
      "Error on this batch = 0.14233824985924345\n",
      "Cost on val dataset after 179 epochs is = 0.13511373108785293\n",
      "Initial Cost on Val dataset for this epoch 179 = 0.13511373108785293\n",
      "learning rate for this epoch =  0.13669629592201246\n",
      "Error on this batch = 0.10722467315459724\n",
      "Error on this batch = 0.14198602307286456\n",
      "Cost on val dataset after 180 epochs is = 0.13486755785613097\n",
      "Initial Cost on Val dataset for this epoch 180 = 0.13486755785613097\n",
      "learning rate for this epoch =  0.13650604313545334\n",
      "Error on this batch = 0.10690039467760556\n",
      "Error on this batch = 0.14163707049245847\n",
      "Cost on val dataset after 181 epochs is = 0.13462403760201552\n",
      "Initial Cost on Val dataset for this epoch 181 = 0.13462403760201552\n",
      "learning rate for this epoch =  0.13631710698286975\n",
      "Error on this batch = 0.10657910936721883\n",
      "Error on this batch = 0.14129133872417768\n",
      "Cost on val dataset after 182 epochs is = 0.13438312255863544\n",
      "Initial Cost on Val dataset for this epoch 182 = 0.13438312255863544\n",
      "learning rate for this epoch =  0.1361294711534851\n",
      "Error on this batch = 0.10626075580412572\n",
      "Error on this batch = 0.14094877556451205\n",
      "Cost on val dataset after 183 epochs is = 0.13414476612692788\n",
      "Initial Cost on Val dataset for this epoch 183 = 0.13414476612692788\n",
      "learning rate for this epoch =  0.13594311962679215\n",
      "Error on this batch = 0.10594527418346926\n",
      "Error on this batch = 0.14060932996184466\n",
      "Cost on val dataset after 184 epochs is = 0.13390892283936748\n",
      "Initial Cost on Val dataset for this epoch 184 = 0.13390892283936748\n",
      "learning rate for this epoch =  0.13575803666583477\n",
      "Error on this batch = 0.10563260628188842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on this batch = 0.14027295197966397\n",
      "Cost on val dataset after 185 epochs is = 0.13367554832510678\n",
      "Initial Cost on Val dataset for this epoch 185 = 0.13367554832510678\n",
      "learning rate for this epoch =  0.13557420681068058\n",
      "Error on this batch = 0.10532269542455634\n",
      "Error on this batch = 0.13993959276136345\n",
      "Cost on val dataset after 186 epochs is = 0.13344459927646046\n",
      "Initial Cost on Val dataset for this epoch 186 = 0.13344459927646046\n",
      "learning rate for this epoch =  0.13539161487207826\n",
      "Error on this batch = 0.10501548645225192\n",
      "Error on this batch = 0.13960920449656003\n",
      "Cost on val dataset after 187 epochs is = 0.13321603341666818\n",
      "Initial Cost on Val dataset for this epoch 187 = 0.13321603341666818\n",
      "learning rate for this epoch =  0.13521024592529318\n",
      "Error on this batch = 0.10471092568850063\n",
      "Error on this batch = 0.13928174038886554\n",
      "Cost on val dataset after 188 epochs is = 0.13298980946887634\n",
      "Initial Cost on Val dataset for this epoch 188 = 0.13298980946887634\n",
      "learning rate for this epoch =  0.1350300853041159\n",
      "Error on this batch = 0.10440896090682071\n",
      "Error on this batch = 0.13895715462504535\n",
      "Cost on val dataset after 189 epochs is = 0.13276588712628068\n",
      "Initial Cost on Val dataset for this epoch 189 = 0.13276588712628068\n",
      "learning rate for this epoch =  0.13485111859503687\n",
      "Error on this batch = 0.10410954129811022\n",
      "Error on this batch = 0.13863540234550129\n",
      "Cost on val dataset after 190 epochs is = 0.1325442270233771\n",
      "Initial Cost on Val dataset for this epoch 190 = 0.1325442270233771\n",
      "learning rate for this epoch =  0.13467333163158285\n",
      "Error on this batch = 0.10381261743820912\n",
      "Error on this batch = 0.13831643961601772\n",
      "Cost on val dataset after 191 epochs is = 0.1323247907082682\n",
      "Initial Cost on Val dataset for this epoch 191 = 0.1323247907082682\n",
      "learning rate for this epoch =  0.13449671048880912\n",
      "Error on this batch = 0.10351814125566947\n",
      "Error on this batch = 0.13800022340071025\n",
      "Cost on val dataset after 192 epochs is = 0.13210754061597887\n",
      "Initial Cost on Val dataset for this epoch 192 = 0.13210754061597887\n",
      "learning rate for this epoch =  0.13432124147794275\n",
      "Error on this batch = 0.10322606599976589\n",
      "Error on this batch = 0.1376867115361211\n",
      "Cost on val dataset after 193 epochs is = 0.1318924400427347\n",
      "Initial Cost on Val dataset for this epoch 193 = 0.1318924400427347\n",
      "learning rate for this epoch =  0.1341469111411715\n",
      "Error on this batch = 0.10293634620877586\n",
      "Error on this batch = 0.13737586270640445\n",
      "Cost on val dataset after 194 epochs is = 0.13167945312116036\n",
      "Initial Cost on Val dataset for this epoch 194 = 0.13167945312116036\n",
      "learning rate for this epoch =  0.13397370624657456\n",
      "Error on this batch = 0.10264893767855936\n",
      "Error on this batch = 0.1370676364195491\n",
      "Cost on val dataset after 195 epochs is = 0.13146854479635756\n",
      "Initial Cost on Val dataset for this epoch 195 = 0.13146854479635756\n",
      "learning rate for this epoch =  0.13380161378318955\n",
      "Error on this batch = 0.10236379743146336\n",
      "Error on this batch = 0.13676199298458766\n",
      "Cost on val dataset after 196 epochs is = 0.13125968080282402\n",
      "Initial Cost on Val dataset for this epoch 196 = 0.13125968080282402\n",
      "learning rate for this epoch =  0.1336306209562122\n",
      "Error on this batch = 0.10208088368557697\n",
      "Error on this batch = 0.13645889348974233\n",
      "Cost on val dataset after 197 epochs is = 0.13105282764217688\n",
      "Initial Cost on Val dataset for this epoch 197 = 0.13105282764217688\n",
      "learning rate for this epoch =  0.13346071518232402\n",
      "Error on this batch = 0.10180015582436017\n",
      "Error on this batch = 0.13615829978146154\n",
      "Cost on val dataset after 198 epochs is = 0.13084795256164627\n",
      "Initial Cost on Val dataset for this epoch 198 = 0.13084795256164627\n",
      "learning rate for this epoch =  0.13329188408514428\n",
      "Error on this batch = 0.10152157436666706\n",
      "Error on this batch = 0.13586017444430162\n",
      "Cost on val dataset after 199 epochs is = 0.13064502353330637\n",
      "Initial Cost on Val dataset for this epoch 199 = 0.13064502353330637\n",
      "learning rate for this epoch =  0.13312411549080203\n",
      "Error on this batch = 0.10124510093718328\n",
      "Error on this batch = 0.13556448078161137\n",
      "Cost on val dataset after 200 epochs is = 0.13044400923401278\n",
      "Initial Cost on Val dataset for this epoch 200 = 0.13044400923401278\n",
      "learning rate for this epoch =  0.13295739742362472\n",
      "Error on this batch = 0.10097069823729492\n",
      "Error on this batch = 0.13527118279697872\n",
      "Cost on val dataset after 201 epochs is = 0.13024487902601675\n",
      "Initial Cost on Val dataset for this epoch 201 = 0.13024487902601675\n",
      "learning rate for this epoch =  0.13279171810193946\n",
      "Error on this batch = 0.10069833001640481\n",
      "Error on this batch = 0.1349802451764002\n",
      "Cost on val dataset after 202 epochs is = 0.13004760293822887\n",
      "Initial Cost on Val dataset for this epoch 202 = 0.13004760293822887\n",
      "learning rate for this epoch =  0.13262706593398382\n",
      "Error on this batch = 0.10042796104371032\n",
      "Error on this batch = 0.1346916332711373\n",
      "Cost on val dataset after 203 epochs is = 0.12985215164810485\n",
      "Initial Cost on Val dataset for this epoch 203 = 0.12985215164810485\n",
      "learning rate for this epoch =  0.13246342951392248\n",
      "Error on this batch = 0.10015955708045468\n",
      "Error on this batch = 0.1344053130812235\n",
      "Cost on val dataset after 204 epochs is = 0.1296584964641292\n",
      "Initial Cost on Val dataset for this epoch 204 = 0.1296584964641292\n",
      "learning rate for this epoch =  0.13230079761796648\n",
      "Error on this batch = 0.09989308485266288\n",
      "Error on this batch = 0.1341212512395896\n",
      "Cost on val dataset after 205 epochs is = 0.12946660930887183\n",
      "Initial Cost on Val dataset for this epoch 205 = 0.12946660930887183\n",
      "learning rate for this epoch =  0.13213915920059222\n",
      "Error on this batch = 0.0996285120243709\n",
      "Error on this batch = 0.1338394149967754\n",
      "Cost on val dataset after 206 epochs is = 0.1292764627025957\n",
      "Initial Cost on Val dataset for this epoch 206 = 0.1292764627025957\n",
      "learning rate for this epoch =  0.13197850339085695\n",
      "Error on this batch = 0.09936580717135664\n",
      "Error on this batch = 0.13355977220619808\n",
      "Cost on val dataset after 207 epochs is = 0.12908802974739322\n",
      "Initial Cost on Val dataset for this epoch 207 = 0.12908802974739322\n",
      "learning rate for this epoch =  0.1318188194888078\n",
      "Error on this batch = 0.09910493975537797\n",
      "Error on this batch = 0.13328229130994854\n",
      "Cost on val dataset after 208 epochs is = 0.12890128411183105\n",
      "Initial Cost on Val dataset for this epoch 208 = 0.12890128411183105\n",
      "learning rate for this epoch =  0.13166009696198164\n",
      "Error on this batch = 0.09884588009892376\n",
      "Error on this batch = 0.13300694132508956\n",
      "Cost on val dataset after 209 epochs is = 0.12871620001608364\n",
      "Initial Cost on Val dataset for this epoch 209 = 0.12871620001608364\n",
      "learning rate for this epoch =  0.1315023254419931\n",
      "Error on this batch = 0.09858859936048069\n",
      "Error on this batch = 0.13273369183042977\n",
      "Cost on val dataset after 210 epochs is = 0.12853275221753654\n",
      "Initial Cost on Val dataset for this epoch 210 = 0.12853275221753654\n",
      "learning rate for this epoch =  0.1313454947212079\n",
      "Error on this batch = 0.09833306951031912\n",
      "Error on this batch = 0.13246251295375017\n",
      "Cost on val dataset after 211 epochs is = 0.12835091599684206\n",
      "Initial Cost on Val dataset for this epoch 211 = 0.12835091599684206\n",
      "learning rate for this epoch =  0.13118959474949932\n",
      "Error on this batch = 0.09807926330679917\n",
      "Error on this batch = 0.13219337535946\n",
      "Cost on val dataset after 212 epochs is = 0.1281706671444096\n",
      "Initial Cost on Val dataset for this epoch 212 = 0.1281706671444096\n",
      "learning rate for this epoch =  0.1310346156310848\n",
      "Error on this batch = 0.09782715427319721\n",
      "Error on this batch = 0.1319262502366611\n",
      "Cost on val dataset after 213 epochs is = 0.12799198194731512\n",
      "Initial Cost on Val dataset for this epoch 213 = 0.12799198194731512\n",
      "learning rate for this epoch =  0.13088054762144102\n",
      "Error on this batch = 0.09757671667505259\n",
      "Error on this batch = 0.13166110928760041\n",
      "Cost on val dataset after 214 epochs is = 0.1278148371766134\n",
      "Initial Cost on Val dataset for this epoch 214 = 0.1278148371766134\n",
      "learning rate for this epoch =  0.13072738112429463\n",
      "Error on this batch = 0.09732792549803293\n",
      "Error on this batch = 0.13139792471649148\n",
      "Cost on val dataset after 215 epochs is = 0.1276392100750393\n",
      "Initial Cost on Val dataset for this epoch 215 = 0.1276392100750393\n",
      "learning rate for this epoch =  0.1305751066886864\n",
      "Error on this batch = 0.09708075642631574\n",
      "Error on this batch = 0.13113666921868744\n",
      "Cost on val dataset after 216 epochs is = 0.12746507834508247\n",
      "Initial Cost on Val dataset for this epoch 216 = 0.12746507834508247\n",
      "learning rate for this epoch =  0.13042371500610728\n",
      "Error on this batch = 0.09683518582148332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on this batch = 0.13087731597018853\n",
      "Cost on val dataset after 217 epochs is = 0.12729242013742334\n",
      "Initial Cost on Val dataset for this epoch 217 = 0.12729242013742334\n",
      "learning rate for this epoch =  0.13027319690770342\n",
      "Error on this batch = 0.09659119070192762\n",
      "Error on this batch = 0.13061983861746781\n",
      "Cost on val dataset after 218 epochs is = 0.1271212140397166\n",
      "Initial Cost on Val dataset for this epoch 218 = 0.1271212140397166\n",
      "learning rate for this epoch =  0.13012354336154894\n",
      "Error on this batch = 0.09634874872276009\n",
      "Error on this batch = 0.13036421126760067\n",
      "Cost on val dataset after 219 epochs is = 0.12695143906570966\n",
      "Initial Cost on Val dataset for this epoch 219 = 0.12695143906570966\n",
      "learning rate for this epoch =  0.12997474546998408\n",
      "Error on this batch = 0.09610783815622219\n",
      "Error on this batch = 0.13011040847868408\n",
      "Cost on val dataset after 220 epochs is = 0.126783074644685\n",
      "Initial Cost on Val dataset for this epoch 220 = 0.126783074644685\n",
      "learning rate for this epoch =  0.12982679446701692\n",
      "Error on this batch = 0.09586843787259092\n",
      "Error on this batch = 0.12985840525053155\n",
      "Cost on val dataset after 221 epochs is = 0.1266161006112141\n",
      "Initial Cost on Val dataset for this epoch 221 = 0.1266161006112141\n",
      "learning rate for this epoch =  0.12967968171578698\n",
      "Error on this batch = 0.09563052732157318\n",
      "Error on this batch = 0.12960817701563232\n",
      "Cost on val dataset after 222 epochs is = 0.12645049719521248\n",
      "Initial Cost on Val dataset for this epoch 222 = 0.12645049719521248\n",
      "learning rate for this epoch =  0.12953339870608896\n",
      "Error on this batch = 0.09539408651418356\n",
      "Error on this batch = 0.1293596996303623\n",
      "Cost on val dataset after 223 epochs is = 0.12628624501228594\n",
      "Initial Cost on Val dataset for this epoch 223 = 0.12628624501228594\n",
      "learning rate for this epoch =  0.12938793705195484\n",
      "Error on this batch = 0.09515909600509759\n",
      "Error on this batch = 0.12911294936643633\n",
      "Cost on val dataset after 224 epochs is = 0.12612332505435664\n",
      "Initial Cost on Val dataset for this epoch 224 = 0.12612332505435664\n",
      "learning rate for this epoch =  0.12924328848929265\n",
      "Error on this batch = 0.0949255368754746\n",
      "Error on this batch = 0.1288679029025907\n",
      "Cost on val dataset after 225 epochs is = 0.1259617186805607\n",
      "Initial Cost on Val dataset for this epoch 225 = 0.1259617186805607\n",
      "learning rate for this epoch =  0.12909944487358055\n",
      "Error on this batch = 0.09469339071624226\n",
      "Error on this batch = 0.12862453731648657\n",
      "Cost on val dataset after 226 epochs is = 0.12580140760840744\n",
      "Initial Cost on Val dataset for this epoch 226 = 0.12580140760840744\n",
      "learning rate for this epoch =  0.1289563981776146\n",
      "Error on this batch = 0.09446263961183533\n",
      "Error on this batch = 0.12838283007682524\n",
      "Cost on val dataset after 227 epochs is = 0.12564237390519173\n",
      "Initial Cost on Val dataset for this epoch 227 = 0.12564237390519173\n",
      "learning rate for this epoch =  0.12881414048930848\n",
      "Error on this batch = 0.0942332661243811\n",
      "Error on this batch = 0.12814275903566624\n",
      "Cost on val dataset after 228 epochs is = 0.12548459997965034\n",
      "Initial Cost on Val dataset for this epoch 228 = 0.12548459997965034\n",
      "learning rate for this epoch =  0.1286726640095442\n",
      "Error on this batch = 0.09400525327832307\n",
      "Error on this batch = 0.1279043024209401\n",
      "Cost on val dataset after 229 epochs is = 0.1253280685738552\n",
      "Initial Cost on Val dataset for this epoch 229 = 0.1253280685738552\n",
      "learning rate for this epoch =  0.12853196105007209\n",
      "Error on this batch = 0.09377858454547539\n",
      "Error on this batch = 0.1276674388291488\n",
      "Cost on val dataset after 230 epochs is = 0.12517276275533457\n",
      "Initial Cost on Val dataset for this epoch 230 = 0.12517276275533457\n",
      "learning rate for this epoch =  0.12839202403145872\n",
      "Error on this batch = 0.0935532438304994\n",
      "Error on this batch = 0.12743214721824575\n",
      "Cost on val dataset after 231 epochs is = 0.1250186659094153\n",
      "Initial Cost on Val dataset for this epoch 231 = 0.1250186659094153\n",
      "learning rate for this epoch =  0.12825284548108173\n",
      "Error on this batch = 0.0933292154567939\n",
      "Error on this batch = 0.1271984069006892\n",
      "Cost on val dataset after 232 epochs is = 0.12486576173177905\n",
      "Initial Cost on Val dataset for this epoch 232 = 0.12486576173177905\n",
      "learning rate for this epoch =  0.1281144180311698\n",
      "Error on this batch = 0.09310648415279106\n",
      "Error on this batch = 0.12696619753666233\n",
      "Cost on val dataset after 233 epochs is = 0.12471403422122447\n",
      "Initial Cost on Val dataset for this epoch 233 = 0.12471403422122447\n",
      "learning rate for this epoch =  0.12797673441688712\n",
      "Error on this batch = 0.0928850350386492\n",
      "Error on this batch = 0.1267354991274543\n",
      "Cost on val dataset after 234 epochs is = 0.12456346767263037\n",
      "Initial Cost on Val dataset for this epoch 234 = 0.12456346767263037\n",
      "learning rate for this epoch =  0.12783978747446093\n",
      "Error on this batch = 0.09266485361333433\n",
      "Error on this batch = 0.1265062920089965\n",
      "Cost on val dataset after 235 epochs is = 0.1244140466701115\n",
      "Initial Cost on Val dataset for this epoch 235 = 0.1244140466701115\n",
      "learning rate for this epoch =  0.12770357013935066\n",
      "Error on this batch = 0.09244592574208145\n",
      "Error on this batch = 0.1262785568455481\n",
      "Cost on val dataset after 236 epochs is = 0.12426575608036242\n",
      "Initial Cost on Val dataset for this epoch 236 = 0.12426575608036242\n",
      "learning rate for this epoch =  0.12756807544445822\n",
      "Error on this batch = 0.09222823764422751\n",
      "Error on this batch = 0.12605227462352714\n",
      "Cost on val dataset after 237 epochs is = 0.12411858104618223\n",
      "Initial Cost on Val dataset for this epoch 237 = 0.12411858104618223\n",
      "learning rate for this epoch =  0.1274332965183777\n",
      "Error on this batch = 0.09201177588140765\n",
      "Error on this batch = 0.1258274266454808\n",
      "Cost on val dataset after 238 epochs is = 0.12397250698017531\n",
      "Initial Cost on Val dataset for this epoch 238 = 0.12397250698017531\n",
      "learning rate for this epoch =  0.12729922658368395\n",
      "Error on this batch = 0.0917965273461056\n",
      "Error on this batch = 0.1256039945241918\n",
      "Cost on val dataset after 239 epochs is = 0.12382751955862199\n",
      "Initial Cost on Val dataset for this epoch 239 = 0.12382751955862199\n",
      "learning rate for this epoch =  0.12716585895525878\n",
      "Error on this batch = 0.09158247925055114\n",
      "Error on this batch = 0.12538196017691555\n",
      "Cost on val dataset after 240 epochs is = 0.12368360471551414\n",
      "Initial Cost on Val dataset for this epoch 240 = 0.12368360471551414\n",
      "learning rate for this epoch =  0.12703318703865368\n",
      "Error on this batch = 0.091369619115955\n",
      "Error on this batch = 0.12516130581974483\n",
      "Cost on val dataset after 241 epochs is = 0.12354074863675014\n",
      "Initial Cost on Val dataset for this epoch 241 = 0.12354074863675014\n",
      "learning rate for this epoch =  0.12690120432848842\n",
      "Error on this batch = 0.09115793476207351\n",
      "Error on this batch = 0.12494201396209781\n",
      "Cost on val dataset after 242 epochs is = 0.12339893775448497\n",
      "Initial Cost on Val dataset for this epoch 242 = 0.12339893775448497\n",
      "learning rate for this epoch =  0.12676990440688446\n",
      "Error on this batch = 0.09094741429709557\n",
      "Error on this batch = 0.12472406740132583\n",
      "Cost on val dataset after 243 epochs is = 0.12325815874162992\n",
      "Initial Cost on Val dataset for this epoch 243 = 0.12325815874162992\n",
      "learning rate for this epoch =  0.12663928094193208\n",
      "Error on this batch = 0.0907380461078424\n",
      "Error on this batch = 0.12450744921743738\n",
      "Cost on val dataset after 244 epochs is = 0.12311839850649768\n",
      "Initial Cost on Val dataset for this epoch 244 = 0.12311839850649768\n",
      "learning rate for this epoch =  0.12650932768619078\n",
      "Error on this batch = 0.0905298188502737\n",
      "Error on this batch = 0.12429214276793568\n",
      "Cost on val dataset after 245 epochs is = 0.12297964418758864\n",
      "Initial Cost on Val dataset for this epoch 245 = 0.12297964418758864\n",
      "learning rate for this epoch =  0.12638003847522164\n",
      "Error on this batch = 0.09032272144029115\n",
      "Error on this batch = 0.12407813168276575\n",
      "Cost on val dataset after 246 epochs is = 0.12284188314851342\n",
      "Initial Cost on Val dataset for this epoch 246 = 0.12284188314851342\n",
      "learning rate for this epoch =  0.1262514072261512\n",
      "Error on this batch = 0.0901167430448327\n",
      "Error on this batch = 0.12386539985936888\n",
      "Cost on val dataset after 247 epochs is = 0.12270510297304821\n",
      "Initial Cost on Val dataset for this epoch 247 = 0.12270510297304821\n",
      "learning rate for this epoch =  0.12612342793626585\n",
      "Error on this batch = 0.08991187307324867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on this batch = 0.12365393145784126\n",
      "Cost on val dataset after 248 epochs is = 0.12256929146031857\n",
      "Initial Cost on Val dataset for this epoch 248 = 0.12256929146031857\n",
      "learning rate for this epoch =  0.1259960946816361\n",
      "Error on this batch = 0.08970810116895392\n",
      "Error on this batch = 0.12344371089619412\n",
      "Cost on val dataset after 249 epochs is = 0.12243443662010801\n",
      "Initial Cost on Val dataset for this epoch 249 = 0.12243443662010801\n",
      "learning rate for this epoch =  0.12586940161576976\n",
      "Error on this batch = 0.08950541720134719\n",
      "Error on this batch = 0.12323472284571314\n",
      "Cost on val dataset after 250 epochs is = 0.12230052666828745\n",
      "Initial Cost on Val dataset for this epoch 250 = 0.12230052666828745\n",
      "learning rate for this epoch =  0.12574334296829354\n",
      "Error on this batch = 0.08930381125799147\n",
      "Error on this batch = 0.12302695222641424\n",
      "Cost on val dataset after 251 epochs is = 0.12216755002236222\n",
      "Initial Cost on Val dataset for this epoch 251 = 0.12216755002236222\n",
      "learning rate for this epoch =  0.1256179130436622\n",
      "Error on this batch = 0.08910327363704805\n",
      "Error on this batch = 0.1228203842025934\n",
      "Cost on val dataset after 252 epochs is = 0.12203549529713312\n",
      "Initial Cost on Val dataset for this epoch 252 = 0.12203549529713312\n",
      "learning rate for this epoch =  0.12549310621989482\n",
      "Error on this batch = 0.08890379483995708\n",
      "Error on this batch = 0.12261500417846896\n",
      "Cost on val dataset after 253 epochs is = 0.121904351300468\n",
      "Initial Cost on Val dataset for this epoch 253 = 0.121904351300468\n",
      "learning rate for this epoch =  0.125368916947337\n",
      "Error on this batch = 0.08870536556435847\n",
      "Error on this batch = 0.1224107977939132\n",
      "Cost on val dataset after 254 epochs is = 0.1217741070291806\n",
      "Initial Cost on Val dataset for this epoch 254 = 0.1217741070291806\n",
      "learning rate for this epoch =  0.12524533974744914\n",
      "Error on this batch = 0.08850797669724603\n",
      "Error on this batch = 0.12220775092027182\n",
      "Cost on val dataset after 255 epochs is = 0.12164475166501436\n",
      "Initial Cost on Val dataset for this epoch 255 = 0.12164475166501436\n",
      "learning rate for this epoch =  0.12512236921161915\n",
      "Error on this batch = 0.08831161930834858\n",
      "Error on this batch = 0.12200584965626923\n",
      "Cost on val dataset after 256 epochs is = 0.12151627457072683\n",
      "Initial Cost on Val dataset for this epoch 256 = 0.12151627457072683\n",
      "learning rate for this epoch =  0.125\n",
      "Error on this batch = 0.08811628464373181\n",
      "Error on this batch = 0.12180508032399726\n",
      "Cost on val dataset after 257 epochs is = 0.12138866528627282\n",
      "Initial Cost on Val dataset for this epoch 257 = 0.12138866528627282\n",
      "learning rate for this epoch =  0.12487822684037092\n",
      "Error on this batch = 0.08792196411961445\n",
      "Error on this batch = 0.12160542946498605\n",
      "Cost on val dataset after 258 epochs is = 0.12126191352508314\n",
      "Initial Cost on Val dataset for this epoch 258 = 0.12126191352508314\n",
      "learning rate for this epoch =  0.12475704452702165\n",
      "Error on this batch = 0.08772864931639311\n",
      "Error on this batch = 0.12140688383635485\n",
      "Cost on val dataset after 259 epochs is = 0.1211360091704362\n",
      "Initial Cost on Val dataset for this epoch 259 = 0.1211360091704362\n",
      "learning rate for this epoch =  0.12463644791965951\n",
      "Error on this batch = 0.08753633197286956\n",
      "Error on this batch = 0.12120943040704102\n",
      "Cost on val dataset after 260 epochs is = 0.12101094227191987\n",
      "Initial Cost on Val dataset for this epoch 260 = 0.12101094227191987\n",
      "learning rate for this epoch =  0.12451643194233865\n",
      "Error on this batch = 0.0873450039806746\n",
      "Error on this batch = 0.12101305635410567\n",
      "Cost on val dataset after 261 epochs is = 0.12088670304198106\n",
      "Initial Cost on Val dataset for this epoch 261 = 0.12088670304198106\n",
      "learning rate for this epoch =  0.12439699158241055\n",
      "Error on this batch = 0.08715465737888405\n",
      "Error on this batch = 0.12081774905911437\n",
      "Cost on val dataset after 262 epochs is = 0.12076328185256037\n",
      "Initial Cost on Val dataset for this epoch 262 = 0.12076328185256037\n",
      "learning rate for this epoch =  0.12427812188949586\n",
      "Error on this batch = 0.08696528434881927\n",
      "Error on this batch = 0.1206234961045908\n",
      "Cost on val dataset after 263 epochs is = 0.12064066923180998\n",
      "Initial Cost on Val dataset for this epoch 263 = 0.12064066923180998\n",
      "learning rate for this epoch =  0.1241598179744767\n",
      "Error on this batch = 0.08677687720902924\n",
      "Error on this batch = 0.12043028527054254\n",
      "Cost on val dataset after 264 epochs is = 0.12051885586089177\n",
      "Initial Cost on Val dataset for this epoch 264 = 0.12051885586089177\n",
      "learning rate for this epoch =  0.12404207500850907\n",
      "Error on this batch = 0.08658942841044724\n",
      "Error on this batch = 0.12023810453105682\n",
      "Cost on val dataset after 265 epochs is = 0.12039783257085376\n",
      "Initial Cost on Val dataset for this epoch 265 = 0.12039783257085376\n",
      "learning rate for this epoch =  0.12392488822205483\n",
      "Error on this batch = 0.08640293053171803\n",
      "Error on this batch = 0.12004694205096485\n",
      "Cost on val dataset after 266 epochs is = 0.12027759033958312\n",
      "Initial Cost on Val dataset for this epoch 266 = 0.12027759033958312\n",
      "learning rate for this epoch =  0.12380825290393263\n",
      "Error on this batch = 0.0862173762746903\n",
      "Error on this batch = 0.11985678618257381\n",
      "Cost on val dataset after 267 epochs is = 0.12015812028883262\n",
      "Initial Cost on Val dataset for this epoch 267 = 0.12015812028883262\n",
      "learning rate for this epoch =  0.12369216440038802\n",
      "Error on this batch = 0.08603275846006941\n",
      "Error on this batch = 0.1196676254624644\n",
      "Cost on val dataset after 268 epochs is = 0.1200394136813196\n",
      "Initial Cost on Val dataset for this epoch 268 = 0.1200394136813196\n",
      "learning rate for this epoch =  0.1235766181141811\n",
      "Error on this batch = 0.08584907002322664\n",
      "Error on this batch = 0.11947944860835286\n",
      "Cost on val dataset after 269 epochs is = 0.11992146191789484\n",
      "Initial Cost on Val dataset for this epoch 269 = 0.11992146191789484\n",
      "learning rate for this epoch =  0.12346160950369273\n",
      "Error on this batch = 0.08566630401015847\n",
      "Error on this batch = 0.1192922445160162\n",
      "Cost on val dataset after 270 epochs is = 0.11980425653477957\n",
      "Initial Cost on Val dataset for this epoch 270 = 0.11980425653477957\n",
      "learning rate for this epoch =  0.12334713408204756\n",
      "Error on this batch = 0.08548445357359395\n",
      "Error on this batch = 0.11910600225627899\n",
      "Cost on val dataset after 271 epochs is = 0.1196877892008687\n",
      "Initial Cost on Val dataset for this epoch 271 = 0.1196877892008687\n",
      "learning rate for this epoch =  0.12323318741625437\n",
      "Error on this batch = 0.08530351196924403\n",
      "Error on this batch = 0.11892071107206122\n",
      "Cost on val dataset after 272 epochs is = 0.11957205171509877\n",
      "Initial Cost on Val dataset for this epoch 272 = 0.11957205171509877\n",
      "learning rate for this epoch =  0.12311976512636309\n",
      "Error on this batch = 0.08512347255218906\n",
      "Error on this batch = 0.11873636037548431\n",
      "Cost on val dataset after 273 epochs is = 0.1194570360038785\n",
      "Initial Cost on Val dataset for this epoch 273 = 0.1194570360038785\n",
      "learning rate for this epoch =  0.12300686288463769\n",
      "Error on this batch = 0.08494432877340084\n",
      "Error on this batch = 0.11855293974503639\n",
      "Cost on val dataset after 274 epochs is = 0.11934273411858044\n",
      "Initial Cost on Val dataset for this epoch 274 = 0.11934273411858044\n",
      "learning rate for this epoch =  0.12289447641474543\n",
      "Error on this batch = 0.08476607417639474\n",
      "Error on this batch = 0.1183704389227933\n",
      "Cost on val dataset after 275 epochs is = 0.11922913823309221\n",
      "Initial Cost on Val dataset for this epoch 275 = 0.11922913823309221\n",
      "learning rate for this epoch =  0.12278260149096118\n",
      "Error on this batch = 0.08458870239400831\n",
      "Error on this batch = 0.1181888478116957\n",
      "Cost on val dataset after 276 epochs is = 0.11911624064142529\n",
      "Initial Cost on Val dataset for this epoch 276 = 0.11911624064142529\n",
      "learning rate for this epoch =  0.12267123393738709\n",
      "Error on this batch = 0.08441220714530216\n",
      "Error on this batch = 0.11800815647288043\n",
      "Cost on val dataset after 277 epochs is = 0.11900403375538036\n",
      "Initial Cost on Val dataset for this epoch 277 = 0.11900403375538036\n",
      "learning rate for this epoch =  0.12256036962718717\n",
      "Error on this batch = 0.08423658223258017\n",
      "Error on this batch = 0.1178283551230653\n",
      "Cost on val dataset after 278 epochs is = 0.11889251010226755\n",
      "Initial Cost on Val dataset for this epoch 278 = 0.11889251010226755\n",
      "learning rate for this epoch =  0.12245000448183609\n",
      "Error on this batch = 0.08406182153852441\n",
      "Error on this batch = 0.11764943413198588\n",
      "Cost on val dataset after 279 epochs is = 0.1187816623226796\n",
      "Initial Cost on Val dataset for this epoch 279 = 0.1187816623226796\n",
      "learning rate for this epoch =  0.12234013447038239\n",
      "Error on this batch = 0.08388791902344261\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on this batch = 0.11747138401988325\n",
      "Cost on val dataset after 280 epochs is = 0.11867148316831756\n",
      "Initial Cost on Val dataset for this epoch 280 = 0.11867148316831756\n",
      "learning rate for this epoch =  0.12223075560872526\n",
      "Error on this batch = 0.08371486872262317\n",
      "Error on this batch = 0.11729419545504213\n",
      "Cost on val dataset after 281 epochs is = 0.11856196549986651\n",
      "Initial Cost on Val dataset for this epoch 281 = 0.11856196549986651\n",
      "learning rate for this epoch =  0.12212186395890517\n",
      "Error on this batch = 0.08354266474379603\n",
      "Error on this batch = 0.11711785925137758\n",
      "Cost on val dataset after 282 epochs is = 0.11845310228492097\n",
      "Initial Cost on Val dataset for this epoch 282 = 0.11845310228492097\n",
      "learning rate for this epoch =  0.12201345562840739\n",
      "Error on this batch = 0.0833713012646946\n",
      "Error on this batch = 0.11694236636606978\n",
      "Cost on val dataset after 283 epochs is = 0.11834488659595803\n",
      "Initial Cost on Val dataset for this epoch 283 = 0.11834488659595803\n",
      "learning rate for this epoch =  0.12190552676947876\n",
      "Error on this batch = 0.08320077253071702\n",
      "Error on this batch = 0.11676770789724569\n",
      "Cost on val dataset after 284 epochs is = 0.11823731160835714\n",
      "Initial Cost on Val dataset for this epoch 284 = 0.11823731160835714\n",
      "learning rate for this epoch =  0.12179807357845675\n",
      "Error on this batch = 0.08303107285268262\n",
      "Error on this batch = 0.11659387508170671\n",
      "Cost on val dataset after 285 epochs is = 0.11813037059846573\n",
      "Initial Cost on Val dataset for this epoch 285 = 0.11813037059846573\n",
      "learning rate for this epoch =  0.12169109229511134\n",
      "Error on this batch = 0.08286219660468107\n",
      "Error on this batch = 0.1164208592927011\n",
      "Cost on val dataset after 286 epochs is = 0.11802405694170864\n",
      "Initial Cost on Val dataset for this epoch 286 = 0.11802405694170864\n",
      "learning rate for this epoch =  0.12158457920199858\n",
      "Error on this batch = 0.08269413822201106\n",
      "Error on this batch = 0.11624865203774049\n",
      "Cost on val dataset after 287 epochs is = 0.11791836411074118\n",
      "Initial Cost on Val dataset for this epoch 287 = 0.11791836411074118\n",
      "learning rate for this epoch =  0.1214785306238262\n",
      "Error on this batch = 0.08252689219920614\n",
      "Error on this batch = 0.11607724495645957\n",
      "Cost on val dataset after 288 epochs is = 0.11781328567364395\n",
      "Initial Cost on Val dataset for this epoch 288 = 0.11781328567364395\n",
      "learning rate for this epoch =  0.12137294292683086\n",
      "Error on this batch = 0.08236045308814399\n",
      "Error on this batch = 0.11590662981851788\n",
      "Cost on val dataset after 289 epochs is = 0.11770881529215864\n",
      "Initial Cost on Val dataset for this epoch 289 = 0.11770881529215864\n",
      "learning rate for this epoch =  0.12126781251816648\n",
      "Error on this batch = 0.08219481549623762\n",
      "Error on this batch = 0.1157367985215426\n",
      "Cost on val dataset after 290 epochs is = 0.117604946719964\n",
      "Initial Cost on Val dataset for this epoch 290 = 0.117604946719964\n",
      "learning rate for this epoch =  0.121163135845304\n",
      "Error on this batch = 0.08202997408470473\n",
      "Error on this batch = 0.11556774308911223\n",
      "Cost on val dataset after 291 epochs is = 0.11750167380099028\n",
      "Initial Cost on Val dataset for this epoch 291 = 0.11750167380099028\n",
      "learning rate for this epoch =  0.12105890939544156\n",
      "Error on this batch = 0.08186592356691329\n",
      "Error on this batch = 0.11539945566877925\n",
      "Cost on val dataset after 292 epochs is = 0.11739899046777208\n",
      "Initial Cost on Val dataset for this epoch 292 = 0.11739899046777208\n",
      "learning rate for this epoch =  0.1209551296949258\n",
      "Error on this batch = 0.08170265870680064\n",
      "Error on this batch = 0.11523192853013192\n",
      "Cost on val dataset after 293 epochs is = 0.11729689073983766\n",
      "Initial Cost on Val dataset for this epoch 293 = 0.11729689073983766\n",
      "learning rate for this epoch =  0.12085179330868305\n",
      "Error on this batch = 0.08154017431736386\n",
      "Error on this batch = 0.11506515406289364\n",
      "Cost on val dataset after 294 epochs is = 0.11719536872213468\n",
      "Initial Cost on Val dataset for this epoch 294 = 0.11719536872213468\n",
      "learning rate for this epoch =  0.12074889683966107\n",
      "Error on this batch = 0.08137846525921823\n",
      "Error on this batch = 0.11489912477505929\n",
      "Cost on val dataset after 295 epochs is = 0.11709441860349067\n",
      "Initial Cost on Val dataset for this epoch 295 = 0.11709441860349067\n",
      "learning rate for this epoch =  0.12064643692828043\n",
      "Error on this batch = 0.08121752643922271\n",
      "Error on this batch = 0.11473383329106834\n",
      "Cost on val dataset after 296 epochs is = 0.11699403465510812\n",
      "Initial Cost on Val dataset for this epoch 296 = 0.11699403465510812\n",
      "learning rate for this epoch =  0.120544410251896\n",
      "Error on this batch = 0.08105735280916887\n",
      "Error on this batch = 0.11456927235001266\n",
      "Cost on val dataset after 297 epochs is = 0.11689421122909249\n",
      "Initial Cost on Val dataset for this epoch 297 = 0.11689421122909249\n",
      "learning rate for this epoch =  0.12044281352426756\n",
      "Error on this batch = 0.08089793936453196\n",
      "Error on this batch = 0.11440543480387959\n",
      "Cost on val dataset after 298 epochs is = 0.1167949427570132\n",
      "Initial Cost on Val dataset for this epoch 298 = 0.1167949427570132\n",
      "learning rate for this epoch =  0.12034164349504001\n",
      "Error on this batch = 0.08073928114328133\n",
      "Error on this batch = 0.11424231361582879\n",
      "Cost on val dataset after 299 epochs is = 0.11669622374849617\n",
      "Initial Cost on Val dataset for this epoch 299 = 0.11669622374849617\n",
      "learning rate for this epoch =  0.12024089694923273\n",
      "Error on this batch = 0.0805813732247485\n",
      "Error on this batch = 0.11407990185850203\n",
      "Cost on val dataset after 300 epochs is = 0.1165980487898471\n",
      "Initial Cost on Val dataset for this epoch 300 = 0.1165980487898471\n",
      "learning rate for this epoch =  0.12014057070673773\n",
      "Error on this batch = 0.08042421072855045\n",
      "Error on this batch = 0.113918192712366\n",
      "Cost on val dataset after 301 epochs is = 0.11650041254270548\n",
      "Initial Cost on Val dataset for this epoch 301 = 0.11650041254270548\n",
      "learning rate for this epoch =  0.1200406616218266\n",
      "Error on this batch = 0.08026778881356667\n",
      "Error on this batch = 0.11375717946408631\n",
      "Cost on val dataset after 302 epochs is = 0.11640330974272747\n",
      "Initial Cost on Val dataset for this epoch 302 = 0.11640330974272747\n",
      "learning rate for this epoch =  0.11994116658266626\n",
      "Error on this batch = 0.08011210267696688\n",
      "Error on this batch = 0.11359685550493281\n",
      "Cost on val dataset after 303 epochs is = 0.11630673519829803\n",
      "Initial Cost on Val dataset for this epoch 303 = 0.11630673519829803\n",
      "learning rate for this epoch =  0.1198420825108428\n",
      "Error on this batch = 0.07995714755328914\n",
      "Error on this batch = 0.11343721432921537\n",
      "Cost on val dataset after 304 epochs is = 0.11621068378927069\n",
      "Initial Cost on Val dataset for this epoch 304 = 0.11621068378927069\n",
      "learning rate for this epoch =  0.11974340636089367\n",
      "Error on this batch = 0.07980291871356468\n",
      "Error on this batch = 0.11327824953274894\n",
      "Cost on val dataset after 305 epochs is = 0.11611515046573496\n",
      "Initial Cost on Val dataset for this epoch 305 = 0.11611515046573496\n",
      "learning rate for this epoch =  0.11964513511984808\n",
      "Error on this batch = 0.0796494114644891\n",
      "Error on this batch = 0.11311995481134808\n",
      "Cost on val dataset after 306 epochs is = 0.11602013024681049\n",
      "Initial Cost on Val dataset for this epoch 306 = 0.11602013024681049\n",
      "learning rate for this epoch =  0.11954726580677509\n",
      "Error on this batch = 0.07949662114763709\n",
      "Error on this batch = 0.1129623239593495\n",
      "Cost on val dataset after 307 epochs is = 0.1159256182194671\n",
      "Initial Cost on Val dataset for this epoch 307 = 0.1159256182194671\n",
      "learning rate for this epoch =  0.11944979547233951\n",
      "Error on this batch = 0.07934454313871991\n",
      "Error on this batch = 0.11280535086816258\n",
      "Cost on val dataset after 308 epochs is = 0.11583160953737046\n",
      "Initial Cost on Val dataset for this epoch 308 = 0.11583160953737046\n",
      "learning rate for this epoch =  0.1193527211983654\n",
      "Error on this batch = 0.07919317284688278\n",
      "Error on this batch = 0.11264902952484648\n",
      "Cost on val dataset after 309 epochs is = 0.1157380994197526\n",
      "Initial Cost on Val dataset for this epoch 309 = 0.1157380994197526\n",
      "learning rate for this epoch =  0.11925604009740705\n",
      "Error on this batch = 0.07904250571404176\n",
      "Error on this batch = 0.1124933540107142\n",
      "Cost on val dataset after 310 epochs is = 0.11564508315030667\n",
      "Initial Cost on Val dataset for this epoch 310 = 0.11564508315030667\n",
      "learning rate for this epoch =  0.11915974931232703\n",
      "Error on this batch = 0.07889253721425739\n",
      "Error on this batch = 0.11233831849996215\n",
      "Cost on val dataset after 311 epochs is = 0.11555255607610555\n",
      "Initial Cost on Val dataset for this epoch 311 = 0.11555255607610555\n",
      "learning rate for this epoch =  0.1190638460158816\n",
      "Error on this batch = 0.07874326285314445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on this batch = 0.11218391725832491\n",
      "Cost on val dataset after 312 epochs is = 0.11546051360654346\n",
      "Initial Cost on Val dataset for this epoch 312 = 0.11546051360654346\n",
      "learning rate for this epoch =  0.11896832741031306\n",
      "Error on this batch = 0.07859467816731569\n",
      "Error on this batch = 0.11203014464175459\n",
      "Cost on val dataset after 313 epochs is = 0.11536895121230037\n",
      "Initial Cost on Val dataset for this epoch 313 = 0.11536895121230037\n",
      "learning rate for this epoch =  0.11887319072694875\n",
      "Error on this batch = 0.07844677872385844\n",
      "Error on this batch = 0.11187699509512429\n",
      "Cost on val dataset after 314 epochs is = 0.11527786442432844\n",
      "Initial Cost on Val dataset for this epoch 314 = 0.11527786442432844\n",
      "learning rate for this epoch =  0.11877843322580707\n",
      "Error on this batch = 0.07829956011984222\n",
      "Error on this batch = 0.11172446315095468\n",
      "Cost on val dataset after 315 epochs is = 0.1151872488328602\n",
      "Initial Cost on Val dataset for this epoch 315 = 0.1151872488328602\n",
      "learning rate for this epoch =  0.11868405219520976\n",
      "Error on this batch = 0.07815301798185677\n",
      "Error on this batch = 0.1115725434281637\n",
      "Cost on val dataset after 316 epochs is = 0.11509710008643759\n",
      "Initial Cost on Val dataset for this epoch 316 = 0.11509710008643759\n",
      "learning rate for this epoch =  0.11859004495140096\n",
      "Error on this batch = 0.07800714796557788\n",
      "Error on this batch = 0.11142123063083831\n",
      "Cost on val dataset after 317 epochs is = 0.11500741389096213\n",
      "Initial Cost on Val dataset for this epoch 317 = 0.11500741389096213\n",
      "learning rate for this epoch =  0.11849640883817222\n",
      "Error on this batch = 0.07786194575536116\n",
      "Error on this batch = 0.11127051954702794\n",
      "Cost on val dataset after 318 epochs is = 0.11491818600876473\n",
      "Initial Cost on Val dataset for this epoch 318 = 0.11491818600876473\n",
      "learning rate for this epoch =  0.11840314122649412\n",
      "Error on this batch = 0.07771740706386124\n",
      "Error on this batch = 0.1111204050475589\n",
      "Cost on val dataset after 319 epochs is = 0.11482941225769569\n",
      "Initial Cost on Val dataset for this epoch 319 = 0.11482941225769569\n",
      "learning rate for this epoch =  0.11831023951415345\n",
      "Error on this batch = 0.07757352763167594\n",
      "Error on this batch = 0.11097088208486973\n",
      "Cost on val dataset after 320 epochs is = 0.11474108851023371\n",
      "Initial Cost on Val dataset for this epoch 320 = 0.11474108851023371\n",
      "learning rate for this epoch =  0.11821770112539698\n",
      "Error on this batch = 0.0774303032270139\n",
      "Error on this batch = 0.11082194569186612\n",
      "Cost on val dataset after 321 epochs is = 0.114653210692614\n",
      "Initial Cost on Val dataset for this epoch 321 = 0.114653210692614\n",
      "learning rate for this epoch =  0.11812552351058042\n",
      "Error on this batch = 0.0772877296453843\n",
      "Error on this batch = 0.11067359098079557\n",
      "Cost on val dataset after 322 epochs is = 0.11456577478397462\n",
      "Initial Cost on Val dataset for this epoch 322 = 0.11456577478397462\n",
      "learning rate for this epoch =  0.11803370414582362\n",
      "Error on this batch = 0.07714580270930813\n",
      "Error on this batch = 0.11052581314214083\n",
      "Cost on val dataset after 323 epochs is = 0.11447877681552106\n",
      "Initial Cost on Val dataset for this epoch 323 = 0.11447877681552106\n",
      "learning rate for this epoch =  0.11794224053267104\n",
      "Error on this batch = 0.07700451826804938\n",
      "Error on this batch = 0.11037860744353176\n",
      "Cost on val dataset after 324 epochs is = 0.11439221286970841\n",
      "Initial Cost on Val dataset for this epoch 324 = 0.11439221286970841\n",
      "learning rate for this epoch =  0.11785113019775793\n",
      "Error on this batch = 0.07686387219736533\n",
      "Error on this batch = 0.11023196922867523\n",
      "Cost on val dataset after 325 epochs is = 0.11430607907944078\n",
      "Initial Cost on Val dataset for this epoch 325 = 0.11430607907944078\n",
      "learning rate for this epoch =  0.11776037069248181\n",
      "Error on this batch = 0.07672386039927483\n",
      "Error on this batch = 0.1100858939163023\n",
      "Cost on val dataset after 326 epochs is = 0.1142203716272876\n",
      "Initial Cost on Val dataset for this epoch 326 = 0.1142203716272876\n",
      "learning rate for this epoch =  0.11766995959267931\n",
      "Error on this batch = 0.07658447880184359\n",
      "Error on this batch = 0.10994037699913206\n",
      "Cost on val dataset after 327 epochs is = 0.11413508674471655\n",
      "Initial Cost on Val dataset for this epoch 327 = 0.11413508674471655\n",
      "learning rate for this epoch =  0.11757989449830816\n",
      "Error on this batch = 0.0764457233589857\n",
      "Error on this batch = 0.10979541404285215\n",
      "Cost on val dataset after 328 epochs is = 0.11405022071134265\n",
      "Initial Cost on Val dataset for this epoch 328 = 0.11405022071134265\n",
      "learning rate for this epoch =  0.11749017303313421\n",
      "Error on this batch = 0.0763075900502802\n",
      "Error on this batch = 0.10965100068511503\n",
      "Cost on val dataset after 329 epochs is = 0.11396576985419285\n",
      "Initial Cost on Val dataset for this epoch 329 = 0.11396576985419285\n",
      "learning rate for this epoch =  0.11740079284442367\n",
      "Error on this batch = 0.07617007488080192\n",
      "Error on this batch = 0.10950713263454954\n",
      "Cost on val dataset after 330 epochs is = 0.11388173054698676\n",
      "Initial Cost on Val dataset for this epoch 330 = 0.11388173054698676\n",
      "learning rate for this epoch =  0.11731175160264\n",
      "Error on this batch = 0.07603317388096563\n",
      "Error on this batch = 0.10936380566978755\n",
      "Cost on val dataset after 331 epochs is = 0.1137980992094319\n",
      "Initial Cost on Val dataset for this epoch 331 = 0.1137980992094319\n",
      "learning rate for this epoch =  0.11722304700114572\n",
      "Error on this batch = 0.07589688310638301\n",
      "Error on this batch = 0.10922101563850489\n",
      "Cost on val dataset after 332 epochs is = 0.113714872306534\n",
      "Initial Cost on Val dataset for this epoch 332 = 0.113714872306534\n",
      "learning rate for this epoch =  0.11713467675590904\n",
      "Error on this batch = 0.07576119863773126\n",
      "Error on this batch = 0.10907875845647647\n",
      "Cost on val dataset after 333 epochs is = 0.11363204634792186\n",
      "Initial Cost on Val dataset for this epoch 333 = 0.11363204634792186\n",
      "learning rate for this epoch =  0.11704663860521486\n",
      "Error on this batch = 0.07562611658063248\n",
      "Error on this batch = 0.10893703010664446\n",
      "Cost on val dataset after 334 epochs is = 0.11354961788718637\n",
      "Initial Cost on Val dataset for this epoch 334 = 0.11354961788718637\n",
      "learning rate for this epoch =  0.1169589303093807\n",
      "Error on this batch = 0.07549163306554382\n",
      "Error on this batch = 0.10879582663820024\n",
      "Cost on val dataset after 335 epochs is = 0.11346758352123341\n",
      "Initial Cost on Val dataset for this epoch 335 = 0.11346758352123341\n",
      "learning rate for this epoch =  0.11687154965047665\n",
      "Error on this batch = 0.0753577442476567\n",
      "Error on this batch = 0.10865514416567827\n",
      "Cost on val dataset after 336 epochs is = 0.11338593988965018\n",
      "Initial Cost on Val dataset for this epoch 336 = 0.11338593988965018\n",
      "learning rate for this epoch =  0.11678449443205002\n",
      "Error on this batch = 0.07522444630680501\n",
      "Error on this batch = 0.10851497886806272\n",
      "Cost on val dataset after 337 epochs is = 0.11330468367408511\n",
      "Initial Cost on Val dataset for this epoch 337 = 0.11330468367408511\n",
      "learning rate for this epoch =  0.11669776247885426\n",
      "Error on this batch = 0.07509173544738174\n",
      "Error on this batch = 0.10837532698790542\n",
      "Cost on val dataset after 338 epochs is = 0.11322381159764051\n",
      "Initial Cost on Val dataset for this epoch 338 = 0.11322381159764051\n",
      "learning rate for this epoch =  0.1166113516365818\n",
      "Error on this batch = 0.07495960789826278\n",
      "Error on this batch = 0.1082361848304556\n",
      "Cost on val dataset after 339 epochs is = 0.11314332042427808\n",
      "Initial Cost on Val dataset for this epoch 339 = 0.11314332042427808\n",
      "learning rate for this epoch =  0.1165252597716015\n",
      "Error on this batch = 0.07482805991273794\n",
      "Error on this batch = 0.10809754876280016\n",
      "Cost on val dataset after 340 epochs is = 0.11306320695823673\n",
      "Initial Cost on Val dataset for this epoch 340 = 0.11306320695823673\n",
      "learning rate for this epoch =  0.11643948477069971\n",
      "Error on this batch = 0.07469708776844815\n",
      "Error on this batch = 0.10795941521301479\n",
      "Cost on val dataset after 341 epochs is = 0.11298346804346276\n",
      "Initial Cost on Val dataset for this epoch 341 = 0.11298346804346276\n",
      "learning rate for this epoch =  0.11635402454082565\n",
      "Error on this batch = 0.07456668776732854\n",
      "Error on this batch = 0.1078217806693251\n",
      "Cost on val dataset after 342 epochs is = 0.11290410056305177\n",
      "Initial Cost on Val dataset for this epoch 342 = 0.11290410056305177\n",
      "learning rate for this epoch =  0.1162688770088405\n",
      "Error on this batch = 0.07443685623555676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on this batch = 0.1076846416792776\n",
      "Cost on val dataset after 343 epochs is = 0.11282510143870231\n",
      "Initial Cost on Val dataset for this epoch 343 = 0.11282510143870231\n",
      "learning rate for this epoch =  0.11618404012127041\n",
      "Error on this batch = 0.07430758952350607\n",
      "Error on this batch = 0.10754799484892004\n",
      "Cost on val dataset after 344 epochs is = 0.11274646763018073\n",
      "Initial Cost on Val dataset for this epoch 344 = 0.11274646763018073\n",
      "learning rate for this epoch =  0.11609951184406334\n",
      "Error on this batch = 0.07417888400570234\n",
      "Error on this batch = 0.10741183684199085\n",
      "Cost on val dataset after 345 epochs is = 0.11266819613479744\n",
      "Initial Cost on Val dataset for this epoch 345 = 0.11266819613479744\n",
      "learning rate for this epoch =  0.11601529016234945\n",
      "Error on this batch = 0.07405073608078525\n",
      "Error on this batch = 0.10727616437911706\n",
      "Cost on val dataset after 346 epochs is = 0.11259028398689384\n",
      "Initial Cost on Val dataset for this epoch 346 = 0.11259028398689384\n",
      "learning rate for this epoch =  0.11593137308020533\n",
      "Error on this batch = 0.0739231421714726\n",
      "Error on this batch = 0.10714097423702092\n",
      "Cost on val dataset after 347 epochs is = 0.11251272825733986\n",
      "Initial Cost on Val dataset for this epoch 347 = 0.11251272825733986\n",
      "learning rate for this epoch =  0.11584775862042163\n",
      "Error on this batch = 0.07379609872452723\n",
      "Error on this batch = 0.10700626324773428\n",
      "Cost on val dataset after 348 epochs is = 0.11243552605304187\n",
      "Initial Cost on Val dataset for this epoch 348 = 0.11243552605304187\n",
      "learning rate for this epoch =  0.11576444482427425\n",
      "Error on this batch = 0.07366960221072684\n",
      "Error on this batch = 0.1068720282978207\n",
      "Cost on val dataset after 349 epochs is = 0.11235867451646099\n",
      "Initial Cost on Val dataset for this epoch 349 = 0.11235867451646099\n",
      "learning rate for this epoch =  0.11568142975129903\n",
      "Error on this batch = 0.07354364912483545\n",
      "Error on this batch = 0.10673826632760496\n",
      "Cost on val dataset after 350 epochs is = 0.11228217082514091\n",
      "Initial Cost on Val dataset for this epoch 350 = 0.11228217082514091\n",
      "learning rate for this epoch =  0.11559871147906978\n",
      "Error on this batch = 0.07341823598557681\n",
      "Error on this batch = 0.10660497433040966\n",
      "Cost on val dataset after 351 epochs is = 0.11220601219124576\n",
      "Initial Cost on Val dataset for this epoch 351 = 0.11220601219124576\n",
      "learning rate for this epoch =  0.11551628810297963\n",
      "Error on this batch = 0.0732933593356088\n",
      "Error on this batch = 0.1064721493517985\n",
      "Cost on val dataset after 352 epochs is = 0.11213019586110705\n",
      "Initial Cost on Val dataset for this epoch 352 = 0.11213019586110705\n",
      "learning rate for this epoch =  0.11543415773602565\n",
      "Error on this batch = 0.07316901574149914\n",
      "Error on this batch = 0.10633978848882628\n",
      "Cost on val dataset after 353 epochs is = 0.11205471911478018\n",
      "Initial Cost on Val dataset for this epoch 353 = 0.11205471911478018\n",
      "learning rate for this epoch =  0.11535231850859669\n",
      "Error on this batch = 0.07304520179370154\n",
      "Error on this batch = 0.10620788888929479\n",
      "Cost on val dataset after 354 epochs is = 0.11197957926560959\n",
      "Initial Cost on Val dataset for this epoch 354 = 0.11197957926560959\n",
      "learning rate for this epoch =  0.11527076856826429\n",
      "Error on this batch = 0.07292191410653207\n",
      "Error on this batch = 0.10607644775101506\n",
      "Cost on val dataset after 355 epochs is = 0.11190477365980316\n",
      "Initial Cost on Val dataset for this epoch 355 = 0.11190477365980316\n",
      "learning rate for this epoch =  0.1151895060795769\n",
      "Error on this batch = 0.07279914931814563\n",
      "Error on this batch = 0.10594546232107505\n",
      "Cost on val dataset after 356 epochs is = 0.1118302996760145\n",
      "Initial Cost on Val dataset for this epoch 356 = 0.1118302996760145\n",
      "learning rate for this epoch =  0.11510852922385682\n",
      "Error on this batch = 0.07267690409051249\n",
      "Error on this batch = 0.10581492989511279\n",
      "Cost on val dataset after 357 epochs is = 0.11175615472493422\n",
      "Initial Cost on Val dataset for this epoch 357 = 0.11175615472493422\n",
      "learning rate for this epoch =  0.11502783619900045\n",
      "Error on this batch = 0.07255517510939356\n",
      "Error on this batch = 0.10568484781659511\n",
      "Cost on val dataset after 358 epochs is = 0.11168233624888896\n",
      "Initial Cost on Val dataset for this epoch 358 = 0.11168233624888896\n",
      "learning rate for this epoch =  0.11494742521928122\n",
      "Error on this batch = 0.07243395908431569\n",
      "Error on this batch = 0.10555521347610086\n",
      "Cost on val dataset after 359 epochs is = 0.11160884172144837\n",
      "Initial Cost on Val dataset for this epoch 359 = 0.11160884172144837\n",
      "learning rate for this epoch =  0.11486729451515557\n",
      "Error on this batch = 0.07231325274854541\n",
      "Error on this batch = 0.10542602431060961\n",
      "Cost on val dataset after 360 epochs is = 0.1115356686470399\n",
      "Initial Cost on Val dataset for this epoch 360 = 0.1115356686470399\n",
      "learning rate for this epoch =  0.11478744233307164\n",
      "Error on this batch = 0.07219305285906172\n",
      "Error on this batch = 0.10529727780279441\n",
      "Cost on val dataset after 361 epochs is = 0.11146281456057107\n",
      "Initial Cost on Val dataset for this epoch 361 = 0.11146281456057107\n",
      "learning rate for this epoch =  0.11470786693528087\n",
      "Error on this batch = 0.07207335619652713\n",
      "Error on this batch = 0.10516897148031934\n",
      "Cost on val dataset after 362 epochs is = 0.1113902770270589\n",
      "Initial Cost on Val dataset for this epoch 362 = 0.1113902770270589\n",
      "learning rate for this epoch =  0.11462856659965227\n",
      "Error on this batch = 0.07195415956525761\n",
      "Error on this batch = 0.1050411029151413\n",
      "Cost on val dataset after 363 epochs is = 0.11131805364126655\n",
      "Initial Cost on Val dataset for this epoch 363 = 0.11131805364126655\n",
      "learning rate for this epoch =  0.11454953961948931\n",
      "Error on this batch = 0.07183545979319\n",
      "Error on this batch = 0.10491366972281566\n",
      "Cost on val dataset after 364 epochs is = 0.11124614202734706\n",
      "Initial Cost on Val dataset for this epoch 364 = 0.11124614202734706\n",
      "learning rate for this epoch =  0.11447078430334955\n",
      "Error on this batch = 0.07171725373184812\n",
      "Error on this batch = 0.10478666956180639\n",
      "Cost on val dataset after 365 epochs is = 0.11117453983849312\n",
      "Initial Cost on Val dataset for this epoch 365 = 0.11117453983849312\n",
      "learning rate for this epoch =  0.11439229897486693\n",
      "Error on this batch = 0.07159953825630609\n",
      "Error on this batch = 0.10466010013279937\n",
      "Cost on val dataset after 366 epochs is = 0.11110324475659436\n",
      "Initial Cost on Val dataset for this epoch 366 = 0.11110324475659436\n",
      "learning rate for this epoch =  0.11431408197257639\n",
      "Error on this batch = 0.07148231026515\n",
      "Error on this batch = 0.1045339591780201\n",
      "Cost on val dataset after 367 epochs is = 0.1110322544918999\n",
      "Initial Cost on Val dataset for this epoch 367 = 0.1110322544918999\n",
      "learning rate for this epoch =  0.11423613164974125\n",
      "Error on this batch = 0.0713655666804365\n",
      "Error on this batch = 0.10440824448055441\n",
      "Cost on val dataset after 368 epochs is = 0.11096156678268784\n",
      "Initial Cost on Val dataset for this epoch 368 = 0.11096156678268784\n",
      "learning rate for this epoch =  0.11415844637418282\n",
      "Error on this batch = 0.07124930444764932\n",
      "Error on this batch = 0.1042829538636729\n",
      "Cost on val dataset after 369 epochs is = 0.11089117939494025\n",
      "Initial Cost on Val dataset for this epoch 369 = 0.11089117939494025\n",
      "learning rate for this epoch =  0.11408102452811265\n",
      "Error on this batch = 0.07113352053565289\n",
      "Error on this batch = 0.10415808519015883\n",
      "Cost on val dataset after 370 epochs is = 0.11082109012202401\n",
      "Initial Cost on Val dataset for this epoch 370 = 0.11082109012202401\n",
      "learning rate for this epoch =  0.11400386450796704\n",
      "Error on this batch = 0.07101821193664326\n",
      "Error on this batch = 0.1040336363616392\n",
      "Cost on val dataset after 371 epochs is = 0.11075129678437728\n",
      "Initial Cost on Val dataset for this epoch 371 = 0.11075129678437728\n",
      "learning rate for this epoch =  0.11392696472424395\n",
      "Error on this batch = 0.07090337566609606\n",
      "Error on this batch = 0.10390960531791915\n",
      "Cost on val dataset after 372 epochs is = 0.11068179722920125\n",
      "Initial Cost on Val dataset for this epoch 372 = 0.11068179722920125\n",
      "learning rate for this epoch =  0.11385032360134212\n",
      "Error on this batch = 0.07078900876271164\n",
      "Error on this batch = 0.10378599003631914\n",
      "Cost on val dataset after 373 epochs is = 0.11061258933015734\n",
      "Initial Cost on Val dataset for this epoch 373 = 0.11061258933015734\n",
      "learning rate for this epoch =  0.11377393957740255\n",
      "Error on this batch = 0.07067510828835719\n",
      "Error on this batch = 0.10366278853101597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on val dataset after 374 epochs is = 0.11054367098706898\n",
      "Initial Cost on Val dataset for this epoch 374 = 0.11054367098706898\n",
      "learning rate for this epoch =  0.11369781110415222\n",
      "Error on this batch = 0.07056167132800545\n",
      "Error on this batch = 0.10353999885238611\n",
      "Cost on val dataset after 375 epochs is = 0.11047504012562868\n",
      "Initial Cost on Val dataset for this epoch 375 = 0.11047504012562868\n",
      "learning rate for this epoch =  0.11362193664674994\n",
      "Error on this batch = 0.07044869498967085\n",
      "Error on this batch = 0.10341761908635239\n",
      "Cost on val dataset after 376 epochs is = 0.11040669469710965\n",
      "Initial Cost on Val dataset for this epoch 376 = 0.11040669469710965\n",
      "learning rate for this epoch =  0.11354631468363437\n",
      "Error on this batch = 0.070336176404342\n",
      "Error on this batch = 0.10329564735373375\n",
      "Cost on val dataset after 377 epochs is = 0.11033863267808183\n",
      "Initial Cost on Val dataset for this epoch 377 = 0.11033863267808183\n",
      "learning rate for this epoch =  0.1134709437063741\n",
      "Error on this batch = 0.07022411272591116\n",
      "Error on this batch = 0.10317408180959783\n",
      "Cost on val dataset after 378 epochs is = 0.11027085207013229\n",
      "Initial Cost on Val dataset for this epoch 378 = 0.11027085207013229\n",
      "learning rate for this epoch =  0.11339582221952002\n",
      "Error on this batch = 0.07011250113110042\n",
      "Error on this batch = 0.10305292064261642\n",
      "Cost on val dataset after 379 epochs is = 0.11020335089959002\n",
      "Initial Cost on Val dataset for this epoch 379 = 0.11020335089959002\n",
      "learning rate for this epoch =  0.11332094874045952\n",
      "Error on this batch = 0.07000133881938457\n",
      "Error on this batch = 0.10293216207442427\n",
      "Cost on val dataset after 380 epochs is = 0.11013612721725441\n",
      "Initial Cost on Val dataset for this epoch 380 = 0.11013612721725441\n",
      "learning rate for this epoch =  0.1132463217992727\n",
      "Error on this batch = 0.06989062301291092\n",
      "Error on this batch = 0.1028118043589803\n",
      "Cost on val dataset after 381 epochs is = 0.11006917909812773\n",
      "Initial Cost on Val dataset for this epoch 381 = 0.11006917909812773\n",
      "learning rate for this epoch =  0.11317193993859079\n",
      "Error on this batch = 0.06978035095641541\n",
      "Error on this batch = 0.10269184578193226\n",
      "Cost on val dataset after 382 epochs is = 0.11000250464115154\n",
      "Initial Cost on Val dataset for this epoch 382 = 0.11000250464115154\n",
      "learning rate for this epoch =  0.11309780171345626\n",
      "Error on this batch = 0.06967051991713596\n",
      "Error on this batch = 0.10257228465998437\n",
      "Cost on val dataset after 383 epochs is = 0.1099361019689461\n",
      "Initial Cost on Val dataset for this epoch 383 = 0.1099361019689461\n",
      "learning rate for this epoch =  0.11302390569118509\n",
      "Error on this batch = 0.06956112718472204\n",
      "Error on this batch = 0.10245311934026763\n",
      "Cost on val dataset after 384 epochs is = 0.10986996922755377\n",
      "Initial Cost on Val dataset for this epoch 384 = 0.10986996922755377\n",
      "learning rate for this epoch =  0.11295025045123061\n",
      "Error on this batch = 0.06945217007114142\n",
      "Error on this batch = 0.10233434819971363\n",
      "Cost on val dataset after 385 epochs is = 0.1098041045861851\n",
      "Initial Cost on Val dataset for this epoch 385 = 0.1098041045861851\n",
      "learning rate for this epoch =  0.11287683458504956\n",
      "Error on this batch = 0.0693436459105835\n",
      "Error on this batch = 0.10221596964443119\n",
      "Cost on val dataset after 386 epochs is = 0.10973850623696844\n",
      "Initial Cost on Val dataset for this epoch 386 = 0.10973850623696844\n",
      "learning rate for this epoch =  0.11280365669596971\n",
      "Error on this batch = 0.06923555205935947\n",
      "Error on this batch = 0.1020979821090865\n",
      "Cost on val dataset after 387 epochs is = 0.10967317239470213\n",
      "Initial Cost on Val dataset for this epoch 387 = 0.10967317239470213\n",
      "learning rate for this epoch =  0.11273071539905938\n",
      "Error on this batch = 0.06912788589579917\n",
      "Error on this batch = 0.10198038405628605\n",
      "Cost on val dataset after 388 epochs is = 0.10960810129660985\n",
      "Initial Cost on Val dataset for this epoch 388 = 0.10960810129660985\n",
      "learning rate for this epoch =  0.11265800932099873\n",
      "Error on this batch = 0.06902064482014517\n",
      "Error on this batch = 0.10186317397596341\n",
      "Cost on val dataset after 389 epochs is = 0.1095432912020982\n",
      "Initial Cost on Val dataset for this epoch 389 = 0.1095432912020982\n",
      "learning rate for this epoch =  0.11258553709995278\n",
      "Error on this batch = 0.06891382625444352\n",
      "Error on this batch = 0.10174635038476874\n",
      "Cost on val dataset after 390 epochs is = 0.10947874039251715\n",
      "Initial Cost on Val dataset for this epoch 390 = 0.10947874039251715\n",
      "learning rate for this epoch =  0.11251329738544609\n",
      "Error on this batch = 0.06880742764243147\n",
      "Error on this batch = 0.10162991182546229\n",
      "Cost on val dataset after 391 epochs is = 0.10941444717092269\n",
      "Initial Cost on Val dataset for this epoch 391 = 0.10941444717092269\n",
      "learning rate for this epoch =  0.11244128883823923\n",
      "Error on this batch = 0.0687014464494225\n",
      "Error on this batch = 0.10151385686631137\n",
      "Cost on val dataset after 392 epochs is = 0.10935040986184175\n",
      "Initial Cost on Val dataset for this epoch 392 = 0.10935040986184175\n",
      "learning rate for this epoch =  0.11236951013020673\n",
      "Error on this batch = 0.0685958801621881\n",
      "Error on this batch = 0.1013981841004905\n",
      "Cost on val dataset after 393 epochs is = 0.10928662681103923\n",
      "Initial Cost on Val dataset for this epoch 393 = 0.10928662681103923\n",
      "learning rate for this epoch =  0.11229795994421696\n",
      "Error on this batch = 0.06849072628883696\n",
      "Error on this batch = 0.10128289214548566\n",
      "Cost on val dataset after 394 epochs is = 0.1092230963852871\n",
      "Initial Cost on Val dataset for this epoch 394 = 0.1092230963852871\n",
      "learning rate for this epoch =  0.11222663697401325\n",
      "Error on this batch = 0.06838598235869144\n",
      "Error on this batch = 0.10116797964250239\n",
      "Cost on val dataset after 395 epochs is = 0.10915981697213514\n",
      "Initial Cost on Val dataset for this epoch 395 = 0.10915981697213514\n",
      "learning rate for this epoch =  0.11215553992409688\n",
      "Error on this batch = 0.06828164592216118\n",
      "Error on this batch = 0.10105344525587746\n",
      "Cost on val dataset after 396 epochs is = 0.1090967869796838\n",
      "Initial Cost on Val dataset for this epoch 396 = 0.1090967869796838\n",
      "learning rate for this epoch =  0.11208466750961145\n",
      "Error on this batch = 0.06817771455061414\n",
      "Error on this batch = 0.10093928767249467\n",
      "Cost on val dataset after 397 epochs is = 0.10903400483635825\n",
      "Initial Cost on Val dataset for this epoch 397 = 0.10903400483635825\n",
      "learning rate for this epoch =  0.11201401845622891\n",
      "Error on this batch = 0.06807418583624528\n",
      "Error on this batch = 0.10082550560120507\n",
      "Cost on val dataset after 398 epochs is = 0.1089714689906842\n",
      "Initial Cost on Val dataset for this epoch 398 = 0.1089714689906842\n",
      "learning rate for this epoch =  0.11194359150003692\n",
      "Error on this batch = 0.06797105739194265\n",
      "Error on this batch = 0.10071209777225068\n",
      "Cost on val dataset after 399 epochs is = 0.1089091779110652\n",
      "Initial Cost on Val dataset for this epoch 399 = 0.1089091779110652\n",
      "learning rate for this epoch =  0.11187338538742793\n",
      "Error on this batch = 0.06786832685115121\n",
      "Error on this batch = 0.10059906293669339\n",
      "Cost on val dataset after 400 epochs is = 0.1088471300855609\n",
      "Initial Cost on Val dataset for this epoch 400 = 0.1088471300855609\n",
      "learning rate for this epoch =  0.11180339887498948\n",
      "Error on this batch = 0.06776599186773423\n",
      "Error on this batch = 0.10048639986584754\n",
      "Cost on val dataset after 401 epochs is = 0.10878532402166687\n",
      "Initial Cost on Val dataset for this epoch 401 = 0.10878532402166687\n",
      "learning rate for this epoch =  0.11173363072939616\n",
      "Error on this batch = 0.06766405011583287\n",
      "Error on this batch = 0.10037410735071747\n",
      "Cost on val dataset after 402 epochs is = 0.10872375824609509\n",
      "Initial Cost on Val dataset for this epoch 402 = 0.10872375824609509\n",
      "learning rate for this epoch =  0.11166407972730269\n",
      "Error on this batch = 0.06756249928972334\n",
      "Error on this batch = 0.10026218420143965\n",
      "Cost on val dataset after 403 epochs is = 0.10866243130455605\n",
      "Initial Cost on Val dataset for this epoch 403 = 0.10866243130455605\n",
      "learning rate for this epoch =  0.1115947446552388\n",
      "Error on this batch = 0.06746133710367222\n",
      "Error on this batch = 0.10015062924672954\n",
      "Cost on val dataset after 404 epochs is = 0.10860134176154103\n",
      "Initial Cost on Val dataset for this epoch 404 = 0.10860134176154103\n",
      "learning rate for this epoch =  0.11152562430950505\n",
      "Error on this batch = 0.06736056129178984\n",
      "Error on this batch = 0.10003944133333331\n",
      "Cost on val dataset after 405 epochs is = 0.10854048820010556\n",
      "Initial Cost on Val dataset for this epoch 405 = 0.10854048820010556\n",
      "learning rate for this epoch =  0.11145671749607033\n",
      "Error on this batch = 0.06726016960788213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on this batch = 0.09992861932548465\n",
      "Cost on val dataset after 406 epochs is = 0.10847986922165376\n",
      "Initial Cost on Val dataset for this epoch 406 = 0.10847986922165376\n",
      "learning rate for this epoch =  0.1113880230304705\n",
      "Error on this batch = 0.06716015982530041\n",
      "Error on this batch = 0.09981816210436659\n",
      "Cost on val dataset after 407 epochs is = 0.10841948344572287\n",
      "Initial Cost on Val dataset for this epoch 407 = 0.10841948344572287\n",
      "learning rate for this epoch =  0.11131953973770842\n",
      "Error on this batch = 0.06706052973678978\n",
      "Error on this batch = 0.09970806856757886\n",
      "Cost on val dataset after 408 epochs is = 0.1083593295097687\n",
      "Initial Cost on Val dataset for this epoch 408 = 0.1083593295097687\n",
      "learning rate for this epoch =  0.11125126645215519\n",
      "Error on this batch = 0.06696127715433618\n",
      "Error on this batch = 0.09959833762861023\n",
      "Cost on val dataset after 409 epochs is = 0.10829940606895148\n",
      "Initial Cost on Val dataset for this epoch 409 = 0.10829940606895148\n",
      "learning rate for this epoch =  0.11118320201745278\n",
      "Error on this batch = 0.0668623999090118\n",
      "Error on this batch = 0.09948896821631649\n",
      "Cost on val dataset after 410 epochs is = 0.10823971179592196\n",
      "Initial Cost on Val dataset for this epoch 410 = 0.10823971179592196\n",
      "learning rate for this epoch =  0.11111534528641788\n",
      "Error on this batch = 0.06676389585081934\n",
      "Error on this batch = 0.09937995927440425\n",
      "Cost on val dataset after 411 epochs is = 0.10818024538060798\n",
      "Initial Cost on Val dataset for this epoch 411 = 0.10818024538060798\n",
      "learning rate for this epoch =  0.11104769512094681\n",
      "Error on this batch = 0.06666576284853509\n",
      "Error on this batch = 0.09927130976092009\n",
      "Cost on val dataset after 412 epochs is = 0.10812100553000142\n",
      "Initial Cost on Val dataset for this epoch 412 = 0.10812100553000142\n",
      "learning rate for this epoch =  0.11098025039192183\n",
      "Error on this batch = 0.06656799878955097\n",
      "Error on this batch = 0.09916301864774571\n",
      "Cost on val dataset after 413 epochs is = 0.10806199096794514\n",
      "Initial Cost on Val dataset for this epoch 413 = 0.10806199096794514\n",
      "learning rate for this epoch =  0.11091300997911864\n",
      "Error on this batch = 0.06647060157971549\n",
      "Error on this batch = 0.09905508492009915\n",
      "Cost on val dataset after 414 epochs is = 0.10800320043492027\n",
      "Initial Cost on Val dataset for this epoch 414 = 0.10800320043492027\n",
      "learning rate for this epoch =  0.11084597277111498\n",
      "Error on this batch = 0.06637356914317394\n",
      "Error on this batch = 0.0989475075760419\n",
      "Cost on val dataset after 415 epochs is = 0.10794463268783341\n",
      "Initial Cost on Val dataset for this epoch 415 = 0.10794463268783341\n",
      "learning rate for this epoch =  0.11077913766520031\n",
      "Error on this batch = 0.06627689942220769\n",
      "Error on this batch = 0.0988402856259921\n",
      "Cost on val dataset after 416 epochs is = 0.10788628649980418\n",
      "Initial Cost on Val dataset for this epoch 416 = 0.10788628649980418\n",
      "learning rate for this epoch =  0.11071250356728685\n",
      "Error on this batch = 0.0661805903770729\n",
      "Error on this batch = 0.09873341809224438\n",
      "Cost on val dataset after 417 epochs is = 0.10782816065995257\n",
      "Initial Cost on Val dataset for this epoch 417 = 0.10782816065995257\n",
      "learning rate for this epoch =  0.11064606939182157\n",
      "Error on this batch = 0.06608463998583859\n",
      "Error on this batch = 0.09862690400849551\n",
      "Cost on val dataset after 418 epochs is = 0.10777025397318624\n",
      "Initial Cost on Val dataset for this epoch 418 = 0.10777025397318624\n",
      "learning rate for this epoch =  0.11057983406169934\n",
      "Error on this batch = 0.06598904624422425\n",
      "Error on this batch = 0.09852074241937718\n",
      "Cost on val dataset after 419 epochs is = 0.10771256525998799\n",
      "Initial Cost on Val dataset for this epoch 419 = 0.10771256525998799\n",
      "learning rate for this epoch =  0.11051379650817714\n",
      "Error on this batch = 0.06589380716543705\n",
      "Error on this batch = 0.09841493237999485\n",
      "Cost on val dataset after 420 epochs is = 0.10765509335620294\n",
      "Initial Cost on Val dataset for this epoch 420 = 0.10765509335620294\n",
      "learning rate for this epoch =  0.11044795567078942\n",
      "Error on this batch = 0.06579892078000879\n",
      "Error on this batch = 0.09830947295547343\n",
      "Cost on val dataset after 421 epochs is = 0.10759783711282564\n",
      "Initial Cost on Val dataset for this epoch 421 = 0.10759783711282564\n",
      "learning rate for this epoch =  0.1103823104972644\n",
      "Error on this batch = 0.06570438513563243\n",
      "Error on this batch = 0.09820436322050993\n",
      "Cost on val dataset after 422 epochs is = 0.10754079539578712\n",
      "Initial Cost on Val dataset for this epoch 422 = 0.10754079539578712\n",
      "learning rate for this epoch =  0.11031685994344151\n",
      "Error on this batch = 0.0656101982969987\n",
      "Error on this batch = 0.09809960225893263\n",
      "Cost on val dataset after 423 epochs is = 0.10748396708574172\n",
      "Initial Cost on Val dataset for this epoch 423 = 0.10748396708574172\n",
      "learning rate for this epoch =  0.11025160297318981\n",
      "Error on this batch = 0.06551635834563271\n",
      "Error on this batch = 0.0979951891632675\n",
      "Cost on val dataset after 424 epochs is = 0.1074273510778537\n",
      "Initial Cost on Val dataset for this epoch 424 = 0.1074273510778537\n",
      "learning rate for this epoch =  0.11018653855832754\n",
      "Error on this batch = 0.06542286337973031\n",
      "Error on this batch = 0.09789112303431143\n",
      "Cost on val dataset after 425 epochs is = 0.10737094628158382\n",
      "Initial Cost on Val dataset for this epoch 425 = 0.10737094628158382\n",
      "learning rate for this epoch =  0.11012166567854233\n",
      "Error on this batch = 0.06532971151399504\n",
      "Error on this batch = 0.09778740298071269\n",
      "Cost on val dataset after 426 epochs is = 0.10731475162047559\n",
      "Initial Cost on Val dataset for this epoch 426 = 0.10731475162047559\n",
      "learning rate for this epoch =  0.11005698332131283\n",
      "Error on this batch = 0.06523690087947466\n",
      "Error on this batch = 0.09768402811855854\n",
      "Cost on val dataset after 427 epochs is = 0.10725876603194133\n",
      "Initial Cost on Val dataset for this epoch 427 = 0.10725876603194133\n",
      "learning rate for this epoch =  0.10999249048183099\n",
      "Error on this batch = 0.06514442962339857\n",
      "Error on this batch = 0.09758099757097025\n",
      "Cost on val dataset after 428 epochs is = 0.10720298846704825\n",
      "Initial Cost on Val dataset for this epoch 428 = 0.10720298846704825\n",
      "learning rate for this epoch =  0.10992818616292545\n",
      "Error on this batch = 0.06505229590901516\n",
      "Error on this batch = 0.09747831046770497\n",
      "Cost on val dataset after 429 epochs is = 0.10714741789030395\n",
      "Initial Cost on Val dataset for this epoch 429 = 0.10714741789030395\n",
      "learning rate for this epoch =  0.10986406937498579\n",
      "Error on this batch = 0.06496049791543002\n",
      "Error on this batch = 0.09737596594476564\n",
      "Cost on val dataset after 430 epochs is = 0.10709205327944224\n",
      "Initial Cost on Val dataset for this epoch 430 = 0.10709205327944224\n",
      "learning rate for this epoch =  0.10980013913588772\n",
      "Error on this batch = 0.06486903383744401\n",
      "Error on this batch = 0.09727396314401791\n",
      "Cost on val dataset after 431 epochs is = 0.1070368936252083\n",
      "Initial Cost on Val dataset for this epoch 431 = 0.1070368936252083\n",
      "learning rate for this epoch =  0.10973639447091926\n",
      "Error on this batch = 0.06477790188539252\n",
      "Error on this batch = 0.0971723012128146\n",
      "Cost on val dataset after 432 epochs is = 0.10698193793114419\n",
      "Initial Cost on Val dataset for this epoch 432 = 0.10698193793114419\n",
      "learning rate for this epoch =  0.10967283441270771\n",
      "Error on this batch = 0.06468710028498512\n",
      "Error on this batch = 0.09707097930362801\n",
      "Cost on val dataset after 433 epochs is = 0.10692718521337369\n",
      "Initial Cost on Val dataset for this epoch 433 = 0.10692718521337369\n",
      "learning rate for this epoch =  0.10960945800114749\n",
      "Error on this batch = 0.0645966272771457\n",
      "Error on this batch = 0.09696999657368935\n",
      "Cost on val dataset after 434 epochs is = 0.10687263450038766\n",
      "Initial Cost on Val dataset for this epoch 434 = 0.10687263450038766\n",
      "learning rate for this epoch =  0.10954626428332909\n",
      "Error on this batch = 0.0645064811178538\n",
      "Error on this batch = 0.09686935218463624\n",
      "Cost on val dataset after 435 epochs is = 0.1068182848328287\n",
      "Initial Cost on Val dataset for this epoch 435 = 0.1068182848328287\n",
      "learning rate for this epoch =  0.10948325231346849\n",
      "Error on this batch = 0.06441666007798637\n",
      "Error on this batch = 0.09676904530216775\n",
      "Cost on val dataset after 436 epochs is = 0.10676413526327634\n",
      "Initial Cost on Val dataset for this epoch 436 = 0.10676413526327634\n",
      "learning rate for this epoch =  0.1094204211528378\n",
      "Error on this batch = 0.06432716244316057\n",
      "Error on this batch = 0.096669075095707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on val dataset after 437 epochs is = 0.10671018485603187\n",
      "Initial Cost on Val dataset for this epoch 437 = 0.10671018485603187\n",
      "learning rate for this epoch =  0.1093577698696965\n",
      "Error on this batch = 0.06423798651357748\n",
      "Error on this batch = 0.09656944073807179\n",
      "Cost on val dataset after 438 epochs is = 0.1066564326869032\n",
      "Initial Cost on Val dataset for this epoch 438 = 0.1066564326869032\n",
      "learning rate for this epoch =  0.1092952975392236\n",
      "Error on this batch = 0.06414913060386669\n",
      "Error on this batch = 0.09647014140515277\n",
      "Cost on val dataset after 439 epochs is = 0.10660287784299015\n",
      "Initial Cost on Val dataset for this epoch 439 = 0.10660287784299015\n",
      "learning rate for this epoch =  0.10923300324345062\n",
      "Error on this batch = 0.06406059304293209\n",
      "Error on this batch = 0.09637117627559931\n",
      "Cost on val dataset after 440 epochs is = 0.10654951942246949\n",
      "Initial Cost on Val dataset for this epoch 440 = 0.10654951942246949\n",
      "learning rate for this epoch =  0.10917088607119531\n",
      "Error on this batch = 0.06397237217379856\n",
      "Error on this batch = 0.09627254453051362\n",
      "Cost on val dataset after 441 epochs is = 0.10649635653438026\n",
      "Initial Cost on Val dataset for this epoch 441 = 0.10649635653438026\n",
      "learning rate for this epoch =  0.1091089451179962\n",
      "Error on this batch = 0.06388446635345968\n",
      "Error on this batch = 0.09617424535315212\n",
      "Cost on val dataset after 442 epochs is = 0.10644338829840955\n",
      "Initial Cost on Val dataset for this epoch 442 = 0.10644338829840955\n",
      "learning rate for this epoch =  0.10904717948604793\n",
      "Error on this batch = 0.06379687395272689\n",
      "Error on this batch = 0.096076277928635\n",
      "Cost on val dataset after 443 epochs is = 0.10639061384467827\n",
      "Initial Cost on Val dataset for this epoch 443 = 0.10639061384467827\n",
      "learning rate for this epoch =  0.10898558828413735\n",
      "Error on this batch = 0.0637095933560795\n",
      "Error on this batch = 0.09597864144366351\n",
      "Cost on val dataset after 444 epochs is = 0.10633803231352733\n",
      "Initial Cost on Val dataset for this epoch 444 = 0.10633803231352733\n",
      "learning rate for this epoch =  0.10892417062758035\n",
      "Error on this batch = 0.06362262296151608\n",
      "Error on this batch = 0.09588133508624495\n",
      "Cost on val dataset after 445 epochs is = 0.10628564285530441\n",
      "Initial Cost on Val dataset for this epoch 445 = 0.10628564285530441\n",
      "learning rate for this epoch =  0.10886292563815944\n",
      "Error on this batch = 0.06353596118040718\n",
      "Error on this batch = 0.09578435804542554\n",
      "Cost on val dataset after 446 epochs is = 0.10623344463015112\n",
      "Initial Cost on Val dataset for this epoch 446 = 0.10623344463015112\n",
      "learning rate for this epoch =  0.10880185244406208\n",
      "Error on this batch = 0.06344960643734897\n",
      "Error on this batch = 0.09568770951103123\n",
      "Cost on val dataset after 447 epochs is = 0.10618143680779046\n",
      "Initial Cost on Val dataset for this epoch 447 = 0.10618143680779046\n",
      "learning rate for this epoch =  0.10874095017981981\n",
      "Error on this batch = 0.06336355717001858\n",
      "Error on this batch = 0.09559138867341592\n",
      "Cost on val dataset after 448 epochs is = 0.10612961856731536\n",
      "Initial Cost on Val dataset for this epoch 448 = 0.10612961856731536\n",
      "learning rate for this epoch =  0.10868021798624786\n",
      "Error on this batch = 0.06327781182903067\n",
      "Error on this batch = 0.09549539472321797\n",
      "Cost on val dataset after 449 epochs is = 0.10607798909697735\n",
      "Initial Cost on Val dataset for this epoch 449 = 0.10607798909697735\n",
      "learning rate for this epoch =  0.10861965501038574\n",
      "Error on this batch = 0.06319236887779489\n",
      "Error on this batch = 0.09539972685112387\n",
      "Cost on val dataset after 450 epochs is = 0.10602654759397673\n",
      "Initial Cost on Val dataset for this epoch 450 = 0.10602654759397673\n",
      "learning rate for this epoch =  0.10855926040543842\n",
      "Error on this batch = 0.0631072267923756\n",
      "Error on this batch = 0.09530438424763986\n",
      "Cost on val dataset after 451 epochs is = 0.10597529326425267\n",
      "Initial Cost on Val dataset for this epoch 451 = 0.10597529326425267\n",
      "learning rate for this epoch =  0.1084990333307181\n",
      "Error on this batch = 0.06302238406135204\n",
      "Error on this batch = 0.09520936610287155\n",
      "Cost on val dataset after 452 epochs is = 0.10592422532227502\n",
      "Initial Cost on Val dataset for this epoch 452 = 0.10592422532227502\n",
      "learning rate for this epoch =  0.10843897295158678\n",
      "Error on this batch = 0.06293783918568054\n",
      "Error on this batch = 0.0951146716063105\n",
      "Cost on val dataset after 453 epochs is = 0.10587334299083669\n",
      "Initial Cost on Val dataset for this epoch 453 = 0.10587334299083669\n",
      "learning rate for this epoch =  0.10837907843939941\n",
      "Error on this batch = 0.06285359067855781\n",
      "Error on this batch = 0.09502029994662899\n",
      "Cost on val dataset after 454 epochs is = 0.10582264550084716\n",
      "Initial Cost on Val dataset for this epoch 454 = 0.10582264550084716\n",
      "learning rate for this epoch =  0.10831934897144786\n",
      "Error on this batch = 0.0627696370652858\n",
      "Error on this batch = 0.09492625031148197\n",
      "Cost on val dataset after 455 epochs is = 0.10577213209112747\n",
      "Initial Cost on Val dataset for this epoch 455 = 0.10577213209112747\n",
      "learning rate for this epoch =  0.10825978373090529\n",
      "Error on this batch = 0.06268597688313786\n",
      "Error on this batch = 0.09483252188731665\n",
      "Cost on val dataset after 456 epochs is = 0.10572180200820581\n",
      "Initial Cost on Val dataset for this epoch 456 = 0.10572180200820581\n",
      "learning rate for this epoch =  0.10820038190677136\n",
      "Error on this batch = 0.0626026086812266\n",
      "Error on this batch = 0.09473911385918964\n",
      "Cost on val dataset after 457 epochs is = 0.10567165450611526\n",
      "Initial Cost on Val dataset for this epoch 457 = 0.10567165450611526\n",
      "learning rate for this epoch =  0.10814114269381797\n",
      "Error on this batch = 0.06251953102037291\n",
      "Error on this batch = 0.09464602541059108\n",
      "Cost on val dataset after 458 epochs is = 0.10562168884619234\n",
      "Initial Cost on Val dataset for this epoch 458 = 0.10562168884619234\n",
      "learning rate for this epoch =  0.10808206529253567\n",
      "Error on this batch = 0.06243674247297654\n",
      "Error on this batch = 0.09455325572327672\n",
      "Cost on val dataset after 459 epochs is = 0.10557190429687732\n",
      "Initial Cost on Val dataset for this epoch 459 = 0.10557190429687732\n",
      "learning rate for this epoch =  0.10802314890908067\n",
      "Error on this batch = 0.06235424162288817\n",
      "Error on this batch = 0.09446080397710674\n",
      "Cost on val dataset after 460 epochs is = 0.10552230013351602\n",
      "Initial Cost on Val dataset for this epoch 460 = 0.10552230013351602\n",
      "learning rate for this epoch =  0.10796439275522242\n",
      "Error on this batch = 0.062272027065282884\n",
      "Error on this batch = 0.09436866934989184\n",
      "Cost on val dataset after 461 epochs is = 0.10547287563816357\n",
      "Initial Cost on Val dataset for this epoch 461 = 0.10547287563816357\n",
      "learning rate for this epoch =  0.10790579604829184\n",
      "Error on this batch = 0.06219009740653521\n",
      "Error on this batch = 0.09427685101724684\n",
      "Cost on val dataset after 462 epochs is = 0.10542363009938972\n",
      "Initial Cost on Val dataset for this epoch 462 = 0.10542363009938972\n",
      "learning rate for this epoch =  0.10784735801113018\n",
      "Error on this batch = 0.06210845126409537\n",
      "Error on this batch = 0.09418534815245089\n",
      "Cost on val dataset after 463 epochs is = 0.10537456281208585\n",
      "Initial Cost on Val dataset for this epoch 463 = 0.10537456281208585\n",
      "learning rate for this epoch =  0.10778907787203826\n",
      "Error on this batch = 0.062027087266367256\n",
      "Error on this batch = 0.09409415992631485\n",
      "Cost on val dataset after 464 epochs is = 0.1053256730772746\n",
      "Initial Cost on Val dataset for this epoch 464 = 0.1053256730772746\n",
      "learning rate for this epoch =  0.1077309548647265\n",
      "Error on this batch = 0.06194600405258762\n",
      "Error on this batch = 0.09400328550705571\n",
      "Cost on val dataset after 465 epochs is = 0.10527696020192072\n",
      "Initial Cost on Val dataset for this epoch 465 = 0.10527696020192072\n",
      "learning rate for this epoch =  0.10767298822826553\n",
      "Error on this batch = 0.06186520027270687\n",
      "Error on this batch = 0.09391272406017745\n",
      "Cost on val dataset after 466 epochs is = 0.10522842349874464\n",
      "Initial Cost on Val dataset for this epoch 466 = 0.10522842349874464\n",
      "learning rate for this epoch =  0.10761517720703706\n",
      "Error on this batch = 0.061784674587271146\n",
      "Error on this batch = 0.09382247474835918\n",
      "Cost on val dataset after 467 epochs is = 0.10518006228603823\n",
      "Initial Cost on Val dataset for this epoch 467 = 0.10518006228603823\n",
      "learning rate for this epoch =  0.10755752105068565\n",
      "Error on this batch = 0.06170442566730606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on this batch = 0.09373253673134936\n",
      "Cost on val dataset after 468 epochs is = 0.10513187588748256\n",
      "Initial Cost on Val dataset for this epoch 468 = 0.10513187588748256\n",
      "learning rate for this epoch =  0.1075000190140709\n",
      "Error on this batch = 0.0616244521942015\n",
      "Error on this batch = 0.09364290916586704\n",
      "Cost on val dataset after 469 epochs is = 0.10508386363196831\n",
      "Initial Cost on Val dataset for this epoch 469 = 0.10508386363196831\n",
      "learning rate for this epoch =  0.10744267035722005\n",
      "Error on this batch = 0.06154475285959809\n",
      "Error on this batch = 0.09355359120550932\n",
      "Cost on val dataset after 470 epochs is = 0.10503602485341872\n",
      "Initial Cost on Val dataset for this epoch 470 = 0.10503602485341872\n",
      "learning rate for this epoch =  0.10738547434528128\n",
      "Error on this batch = 0.061465326365274894\n",
      "Error on this batch = 0.09346458200066521\n",
      "Cost on val dataset after 471 epochs is = 0.10498835889061499\n",
      "Initial Cost on Val dataset for this epoch 471 = 0.10498835889061499\n",
      "learning rate for this epoch =  0.10732843024847744\n",
      "Error on this batch = 0.06138617142303874\n",
      "Error on this batch = 0.09337588069843582\n",
      "Cost on val dataset after 472 epochs is = 0.10494086508702422\n",
      "Initial Cost on Val dataset for this epoch 472 = 0.10494086508702422\n",
      "learning rate for this epoch =  0.10727153734206031\n",
      "Error on this batch = 0.06130728675461456\n",
      "Error on this batch = 0.09328748644256077\n",
      "Cost on val dataset after 473 epochs is = 0.10489354279063047\n",
      "Initial Cost on Val dataset for this epoch 473 = 0.10489354279063047\n",
      "learning rate for this epoch =  0.1072147949062655\n",
      "Error on this batch = 0.061228671091537325\n",
      "Error on this batch = 0.09319939837335067\n",
      "Cost on val dataset after 474 epochs is = 0.10484639135376825\n",
      "Initial Cost on Val dataset for this epoch 474 = 0.10484639135376825\n",
      "learning rate for this epoch =  0.10715820222626746\n",
      "Error on this batch = 0.06115032317504524\n",
      "Error on this batch = 0.0931116156276254\n",
      "Cost on val dataset after 475 epochs is = 0.10479941013295907\n",
      "Initial Cost on Val dataset for this epoch 475 = 0.10479941013295907\n",
      "learning rate for this epoch =  0.1071017585921356\n",
      "Error on this batch = 0.061072241755974144\n",
      "Error on this batch = 0.09302413733865858\n",
      "Cost on val dataset after 476 epochs is = 0.104752598488751\n",
      "Initial Cost on Val dataset for this epoch 476 = 0.104752598488751\n",
      "learning rate for this epoch =  0.1070454632987902\n",
      "Error on this batch = 0.060994425594653484\n",
      "Error on this batch = 0.09293696263612748\n",
      "Cost on val dataset after 477 epochs is = 0.10470595578556117\n",
      "Initial Cost on Val dataset for this epoch 477 = 0.10470595578556117\n",
      "learning rate for this epoch =  0.10698931564595948\n",
      "Error on this batch = 0.0609168734608033\n",
      "Error on this batch = 0.09285009064606876\n",
      "Cost on val dataset after 478 epochs is = 0.10465948139152134\n",
      "Initial Cost on Val dataset for this epoch 478 = 0.10465948139152134\n",
      "learning rate for this epoch =  0.1069333149381366\n",
      "Error on this batch = 0.06083958413343262\n",
      "Error on this batch = 0.09276352049084001\n",
      "Cost on val dataset after 479 epochs is = 0.1046131746783268\n",
      "Initial Cost on Val dataset for this epoch 479 = 0.1046131746783268\n",
      "learning rate for this epoch =  0.10687746048453738\n",
      "Error on this batch = 0.06076255640073904\n",
      "Error on this batch = 0.092677251289086\n",
      "Cost on val dataset after 480 epochs is = 0.10456703502108841\n",
      "Initial Cost on Val dataset for this epoch 480 = 0.10456703502108841\n",
      "learning rate for this epoch =  0.10682175159905852\n",
      "Error on this batch = 0.06068578906000964\n",
      "Error on this batch = 0.09259128215571125\n",
      "Cost on val dataset after 481 epochs is = 0.10452106179818782\n",
      "Initial Cost on Val dataset for this epoch 481 = 0.10452106179818782\n",
      "learning rate for this epoch =  0.1067661876002362\n",
      "Error on this batch = 0.06060928091752292\n",
      "Error on this batch = 0.09250561220185688\n",
      "Cost on val dataset after 482 epochs is = 0.10447525439113621\n",
      "Initial Cost on Val dataset for this epoch 482 = 0.10447525439113621\n",
      "learning rate for this epoch =  0.1067107678112051\n",
      "Error on this batch = 0.06053303078845214\n",
      "Error on this batch = 0.09242024053488318\n",
      "Cost on val dataset after 483 epochs is = 0.10442961218443633\n",
      "Initial Cost on Val dataset for this epoch 483 = 0.10442961218443633\n",
      "learning rate for this epoch =  0.10665549155965787\n",
      "Error on this batch = 0.06045703749676969\n",
      "Error on this batch = 0.09233516625835707\n",
      "Cost on val dataset after 484 epochs is = 0.10438413456544794\n",
      "Initial Cost on Val dataset for this epoch 484 = 0.10438413456544794\n",
      "learning rate for this epoch =  0.10660035817780521\n",
      "Error on this batch = 0.06038129987515276\n",
      "Error on this batch = 0.09225038847204402\n",
      "Cost on val dataset after 485 epochs is = 0.10433882092425703\n",
      "Initial Cost on Val dataset for this epoch 485 = 0.10433882092425703\n",
      "learning rate for this epoch =  0.10654536700233612\n",
      "Error on this batch = 0.060305816764889955\n",
      "Error on this batch = 0.09216590627190538\n",
      "Cost on val dataset after 486 epochs is = 0.10429367065354792\n",
      "Initial Cost on Val dataset for this epoch 486 = 0.10429367065354792\n",
      "learning rate for this epoch =  0.10649051737437876\n",
      "Error on this batch = 0.06023058701578906\n",
      "Error on this batch = 0.09208171875009986\n",
      "Cost on val dataset after 487 epochs is = 0.10424868314847972\n",
      "Initial Cost on Val dataset for this epoch 487 = 0.10424868314847972\n",
      "learning rate for this epoch =  0.10643580863946162\n",
      "Error on this batch = 0.06015560948608632\n",
      "Error on this batch = 0.09199782499498962\n",
      "Cost on val dataset after 488 epochs is = 0.10420385780656581\n",
      "Initial Cost on Val dataset for this epoch 488 = 0.10420385780656581\n",
      "learning rate for this epoch =  0.10638124014747533\n",
      "Error on this batch = 0.06008088304235601\n",
      "Error on this batch = 0.09191422409115099\n",
      "Cost on val dataset after 489 epochs is = 0.10415919402755715\n",
      "Initial Cost on Val dataset for this epoch 489 = 0.10415919402755715\n",
      "learning rate for this epoch =  0.1063268112526345\n",
      "Error on this batch = 0.0600064065594218\n",
      "Error on this batch = 0.09183091511938919\n",
      "Cost on val dataset after 490 epochs is = 0.10411469121332928\n",
      "Initial Cost on Val dataset for this epoch 490 = 0.10411469121332928\n",
      "learning rate for this epoch =  0.10627252131344038\n",
      "Error on this batch = 0.05993217892026877\n",
      "Error on this batch = 0.09174789715675737\n",
      "Cost on val dataset after 491 epochs is = 0.10407034876777291\n",
      "Initial Cost on Val dataset for this epoch 491 = 0.10407034876777291\n",
      "learning rate for this epoch =  0.10621836969264359\n",
      "Error on this batch = 0.05985819901595664\n",
      "Error on this batch = 0.09166516927657979\n",
      "Cost on val dataset after 492 epochs is = 0.10402616609668802\n",
      "Initial Cost on Val dataset for this epoch 492 = 0.10402616609668802\n",
      "learning rate for this epoch =  0.10616435575720744\n",
      "Error on this batch = 0.059784465745533776\n",
      "Error on this batch = 0.09158273054847857\n",
      "Cost on val dataset after 493 epochs is = 0.10398214260768218\n",
      "Initial Cost on Val dataset for this epoch 493 = 0.10398214260768218\n",
      "learning rate for this epoch =  0.1061104788782716\n",
      "Error on this batch = 0.059710978015952404\n",
      "Error on this batch = 0.09150058003840475\n",
      "Cost on val dataset after 494 epochs is = 0.10393827771007193\n",
      "Initial Cost on Val dataset for this epoch 494 = 0.10393827771007193\n",
      "learning rate for this epoch =  0.10605673843111615\n",
      "Error on this batch = 0.05963773474198481\n",
      "Error on this batch = 0.09141871680867251\n",
      "Cost on val dataset after 495 epochs is = 0.10389457081478835\n",
      "Initial Cost on Val dataset for this epoch 495 = 0.10389457081478835\n",
      "learning rate for this epoch =  0.10600313379512592\n",
      "Error on this batch = 0.05956473484614021\n",
      "Error on this batch = 0.09133713991799755\n",
      "Cost on val dataset after 496 epochs is = 0.10385102133428598\n",
      "Initial Cost on Val dataset for this epoch 496 = 0.10385102133428598\n",
      "learning rate for this epoch =  0.10594966435375541\n",
      "Error on this batch = 0.05949197725858288\n",
      "Error on this batch = 0.09125584842153835\n",
      "Cost on val dataset after 497 epochs is = 0.10380762868245576\n",
      "Initial Cost on Val dataset for this epoch 497 = 0.10380762868245576\n",
      "learning rate for this epoch =  0.10589632949449389\n",
      "Error on this batch = 0.05941946091705095\n",
      "Error on this batch = 0.0911748413709411\n",
      "Cost on val dataset after 498 epochs is = 0.1037643922745413\n",
      "Initial Cost on Val dataset for this epoch 498 = 0.1037643922745413\n",
      "learning rate for this epoch =  0.10584312860883092\n",
      "Error on this batch = 0.05934718476677625\n",
      "Error on this batch = 0.09109411781438778\n",
      "Cost on val dataset after 499 epochs is = 0.10372131152705928\n",
      "Initial Cost on Val dataset for this epoch 499 = 0.10372131152705928\n",
      "learning rate for this epoch =  0.10579006109222232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on this batch = 0.05927514776040512\n",
      "Error on this batch = 0.09101367679664747\n",
      "Cost on val dataset after 500 epochs is = 0.1036783858577228\n",
      "Initial Cost on Val dataset for this epoch 500 = 0.1036783858577228\n",
      "learning rate for this epoch =  0.10573712634405641\n",
      "Error on this batch = 0.05920334885791975\n",
      "Error on this batch = 0.09093351735913015\n",
      "Cost on val dataset after 501 epochs is = 0.10363561468536918\n",
      "Initial Cost on Val dataset for this epoch 501 = 0.10363561468536918\n",
      "learning rate for this epoch =  0.1056843237676206\n",
      "Error on this batch = 0.05913178702656076\n",
      "Error on this batch = 0.09085363853994416\n",
      "Cost on val dataset after 502 epochs is = 0.10359299742989074\n",
      "Initial Cost on Val dataset for this epoch 502 = 0.10359299742989074\n",
      "learning rate for this epoch =  0.10563165277006838\n",
      "Error on this batch = 0.05906046124075026\n",
      "Error on this batch = 0.09077403937395563\n",
      "Cost on val dataset after 503 epochs is = 0.10355053351216956\n",
      "Initial Cost on Val dataset for this epoch 503 = 0.10355053351216956\n",
      "learning rate for this epoch =  0.10557911276238661\n",
      "Error on this batch = 0.058989370482016065\n",
      "Error on this batch = 0.09069471889285115\n",
      "Cost on val dataset after 504 epochs is = 0.10350822235401524\n",
      "Initial Cost on Val dataset for this epoch 504 = 0.10350822235401524\n",
      "learning rate for this epoch =  0.10552670315936317\n",
      "Error on this batch = 0.05891851373891659\n",
      "Error on this batch = 0.09061567612520254\n",
      "Cost on val dataset after 505 epochs is = 0.1034660633781068\n",
      "Initial Cost on Val dataset for this epoch 505 = 0.1034660633781068\n",
      "learning rate for this epoch =  0.105474423379555\n",
      "Error on this batch = 0.05884789000696634\n",
      "Error on this batch = 0.09053691009653445\n",
      "Cost on val dataset after 506 epochs is = 0.10342405600793772\n",
      "Initial Cost on Val dataset for this epoch 506 = 0.10342405600793772\n",
      "learning rate for this epoch =  0.10542227284525636\n",
      "Error on this batch = 0.05877749828856251\n",
      "Error on this batch = 0.09045841982939372\n",
      "Cost on val dataset after 507 epochs is = 0.10338219966776419\n",
      "Initial Cost on Val dataset for this epoch 507 = 0.10338219966776419\n",
      "learning rate for this epoch =  0.10537025098246748\n",
      "Error on this batch = 0.05870733759291206\n",
      "Error on this batch = 0.09038020434342155\n",
      "Cost on val dataset after 508 epochs is = 0.10334049378255702\n",
      "Initial Cost on Val dataset for this epoch 508 = 0.10334049378255702\n",
      "learning rate for this epoch =  0.10531835722086355\n",
      "Error on this batch = 0.05863740693595984\n",
      "Error on this batch = 0.09030226265542733\n",
      "Cost on val dataset after 509 epochs is = 0.10329893777795691\n",
      "Initial Cost on Val dataset for this epoch 509 = 0.10329893777795691\n",
      "learning rate for this epoch =  0.10526659099376405\n",
      "Error on this batch = 0.058567705340317196\n",
      "Error on this batch = 0.09022459377946455\n",
      "Cost on val dataset after 510 epochs is = 0.10325753108023258\n",
      "Initial Cost on Val dataset for this epoch 510 = 0.10325753108023258\n",
      "learning rate for this epoch =  0.10521495173810227\n",
      "Error on this batch = 0.058498231835191435\n",
      "Error on this batch = 0.09014719672690862\n",
      "Cost on val dataset after 511 epochs is = 0.10321627311624251\n",
      "Initial Cost on Val dataset for this epoch 511 = 0.10321627311624251\n",
      "learning rate for this epoch =  0.10516343889439533\n",
      "Error on this batch = 0.05842898545631594\n",
      "Error on this batch = 0.09007007050653658\n",
      "Cost on val dataset after 512 epochs is = 0.10317516331339951\n",
      "Initial Cost on Val dataset for this epoch 512 = 0.10317516331339951\n",
      "learning rate for this epoch =  0.10511205190671433\n",
      "Error on this batch = 0.05835996524588106\n",
      "Error on this batch = 0.08999321412460827\n",
      "Cost on val dataset after 513 epochs is = 0.1031342010996386\n",
      "Initial Cost on Val dataset for this epoch 513 = 0.1031342010996386\n",
      "learning rate for this epoch =  0.10506079022265488\n",
      "Error on this batch = 0.05829117025246578\n",
      "Error on this batch = 0.08991662658494917\n",
      "Cost on val dataset after 514 epochs is = 0.10309338590338771\n",
      "Initial Cost on Val dataset for this epoch 514 = 0.10309338590338771\n",
      "learning rate for this epoch =  0.10500965329330811\n",
      "Error on this batch = 0.05822259953096992\n",
      "Error on this batch = 0.08984030688903463\n",
      "Cost on val dataset after 515 epochs is = 0.10305271715354133\n",
      "Initial Cost on Val dataset for this epoch 515 = 0.10305271715354133\n",
      "learning rate for this epoch =  0.10495864057323148\n",
      "Error on this batch = 0.05815425214254698\n",
      "Error on this batch = 0.08976425403607567\n",
      "Cost on val dataset after 516 epochs is = 0.10301219427943717\n",
      "Initial Cost on Val dataset for this epoch 516 = 0.10301219427943717\n",
      "learning rate for this epoch =  0.10490775152042053\n",
      "Error on this batch = 0.058086127154537925\n",
      "Error on this batch = 0.0896884670231059\n",
      "Cost on val dataset after 517 epochs is = 0.10297181671083545\n",
      "Initial Cost on Val dataset for this epoch 517 = 0.10297181671083545\n",
      "learning rate for this epoch =  0.10485698559628044\n",
      "Error on this batch = 0.058018223640405306\n",
      "Error on this batch = 0.08961294484506949\n",
      "Cost on val dataset after 518 epochs is = 0.10293158387790095\n",
      "Initial Cost on Val dataset for this epoch 518 = 0.10293158387790095\n",
      "learning rate for this epoch =  0.10480634226559798\n",
      "Error on this batch = 0.0579505406796685\n",
      "Error on this batch = 0.08953768649491065\n",
      "Cost on val dataset after 519 epochs is = 0.10289149521118775\n",
      "Initial Cost on Val dataset for this epoch 519 = 0.10289149521118775\n",
      "learning rate for this epoch =  0.10475582099651397\n",
      "Error on this batch = 0.05788307735783883\n",
      "Error on this batch = 0.08946269096366354\n",
      "Cost on val dataset after 520 epochs is = 0.10285155014162652\n",
      "Initial Cost on Val dataset for this epoch 520 = 0.10285155014162652\n",
      "learning rate for this epoch =  0.10470542126049572\n",
      "Error on this batch = 0.05781583276635635\n",
      "Error on this batch = 0.08938795724054367\n",
      "Cost on val dataset after 521 epochs is = 0.10281174810051408\n",
      "Initial Cost on Val dataset for this epoch 521 = 0.10281174810051408\n",
      "learning rate for this epoch =  0.10465514253230984\n",
      "Error on this batch = 0.05774880600252621\n",
      "Error on this batch = 0.08931348431303975\n",
      "Cost on val dataset after 522 epochs is = 0.10277208851950591\n",
      "Initial Cost on Val dataset for this epoch 522 = 0.10277208851950591\n",
      "learning rate for this epoch =  0.10460498428999554\n",
      "Error on this batch = 0.05768199616945658\n",
      "Error on this batch = 0.08923927116700622\n",
      "Cost on val dataset after 523 epochs is = 0.1027325708306102\n",
      "Initial Cost on Val dataset for this epoch 523 = 0.1027325708306102\n",
      "learning rate for this epoch =  0.10455494601483782\n",
      "Error on this batch = 0.05761540237599654\n",
      "Error on this batch = 0.08916531678675689\n",
      "Cost on val dataset after 524 epochs is = 0.10269319446618488\n",
      "Initial Cost on Val dataset for this epoch 524 = 0.10269319446618488\n",
      "learning rate for this epoch =  0.10450502719134125\n",
      "Error on this batch = 0.05754902373667495\n",
      "Error on this batch = 0.08909162015515854\n",
      "Cost on val dataset after 525 epochs is = 0.1026539588589364\n",
      "Initial Cost on Val dataset for this epoch 525 = 0.1026539588589364\n",
      "learning rate for this epoch =  0.10445522730720382\n",
      "Error on this batch = 0.05748285937163969\n",
      "Error on this batch = 0.08901818025372556\n",
      "Cost on val dataset after 526 epochs is = 0.10261486344192054\n",
      "Initial Cost on Val dataset for this epoch 526 = 0.10261486344192054\n",
      "learning rate for this epoch =  0.10440554585329116\n",
      "Error on this batch = 0.057416908406597814\n",
      "Error on this batch = 0.08894499606271478\n",
      "Cost on val dataset after 527 epochs is = 0.10257590764854554\n",
      "Initial Cost on Val dataset for this epoch 527 = 0.10257590764854554\n",
      "learning rate for this epoch =  0.10435598232361096\n",
      "Error on this batch = 0.05735116997275603\n",
      "Error on this batch = 0.08887206656122039\n",
      "Cost on val dataset after 528 epochs is = 0.1025370909125769\n",
      "Initial Cost on Val dataset for this epoch 528 = 0.1025370909125769\n",
      "learning rate for this epoch =  0.10430653621528765\n",
      "Error on this batch = 0.05728564320676194\n",
      "Error on this batch = 0.0887993907272697\n",
      "Cost on val dataset after 529 epochs is = 0.10249841266814397\n",
      "Initial Cost on Val dataset for this epoch 529 = 0.10249841266814397\n",
      "learning rate for this epoch =  0.10425720702853739\n",
      "Error on this batch = 0.057220327250645714\n",
      "Error on this batch = 0.08872696753791852\n",
      "Cost on val dataset after 530 epochs is = 0.10245987234974845\n",
      "Initial Cost on Val dataset for this epoch 530 = 0.10245987234974845\n",
      "learning rate for this epoch =  0.10420799426664315\n",
      "Error on this batch = 0.05715522125176263\n",
      "Error on this batch = 0.08865479596934722\n",
      "Cost on val dataset after 531 epochs is = 0.10242146939227445\n",
      "Initial Cost on Val dataset for this epoch 531 = 0.10242146939227445\n",
      "learning rate for this epoch =  0.10415889743593033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on this batch = 0.05709032436273592\n",
      "Error on this batch = 0.08858287499695626\n",
      "Cost on val dataset after 532 epochs is = 0.10238320323100018\n",
      "Initial Cost on Val dataset for this epoch 532 = 0.10238320323100018\n",
      "learning rate for this epoch =  0.10410991604574225\n",
      "Error on this batch = 0.057025635741400356\n",
      "Error on this batch = 0.08851120359546219\n",
      "Cost on val dataset after 533 epochs is = 0.10234507330161101\n",
      "Initial Cost on Val dataset for this epoch 533 = 0.10234507330161101\n",
      "learning rate for this epoch =  0.10406104960841617\n",
      "Error on this batch = 0.0569611545507464\n",
      "Error on this batch = 0.08843978073899347\n",
      "Cost on val dataset after 534 epochs is = 0.10230707904021429\n",
      "Initial Cost on Val dataset for this epoch 534 = 0.10230707904021429\n",
      "learning rate for this epoch =  0.1040122976392594\n",
      "Error on this batch = 0.05689687995886493\n",
      "Error on this batch = 0.088368605401186\n",
      "Cost on val dataset after 535 epochs is = 0.10226921988335491\n",
      "Initial Cost on Val dataset for this epoch 535 = 0.10226921988335491\n",
      "learning rate for this epoch =  0.10396365965652576\n",
      "Error on this batch = 0.056832811138892436\n",
      "Error on this batch = 0.08829767655527865\n",
      "Cost on val dataset after 536 epochs is = 0.10223149526803285\n",
      "Initial Cost on Val dataset for this epoch 536 = 0.10223149526803285\n",
      "learning rate for this epoch =  0.10391513518139214\n",
      "Error on this batch = 0.056768947268957044\n",
      "Error on this batch = 0.08822699317420855\n",
      "Cost on val dataset after 537 epochs is = 0.1021939046317212\n",
      "Initial Cost on Val dataset for this epoch 537 = 0.1021939046317212\n",
      "learning rate for this epoch =  0.10386672373793533\n",
      "Error on this batch = 0.05670528753212475\n",
      "Error on this batch = 0.08815655423070588\n",
      "Cost on val dataset after 538 epochs is = 0.10215644741238596\n",
      "Initial Cost on Val dataset for this epoch 538 = 0.10215644741238596\n",
      "learning rate for this epoch =  0.10381842485310916\n",
      "Error on this batch = 0.05664183111634651\n",
      "Error on this batch = 0.08808635869738847\n",
      "Cost on val dataset after 539 epochs is = 0.10211912304850623\n",
      "Initial Cost on Val dataset for this epoch 539 = 0.10211912304850623\n",
      "learning rate for this epoch =  0.10377023805672174\n",
      "Error on this batch = 0.056578577214405894\n",
      "Error on this batch = 0.08801640554685623\n",
      "Cost on val dataset after 540 epochs is = 0.1020819309790958\n",
      "Initial Cost on Val dataset for this epoch 540 = 0.1020819309790958\n",
      "learning rate for this epoch =  0.10372216288141306\n",
      "Error on this batch = 0.05651552502386697\n",
      "Error on this batch = 0.08794669375178449\n",
      "Cost on val dataset after 541 epochs is = 0.10204487064372562\n",
      "Initial Cost on Val dataset for this epoch 541 = 0.10204487064372562\n",
      "learning rate for this epoch =  0.10367419886263263\n",
      "Error on this batch = 0.05645267374702314\n",
      "Error on this batch = 0.08787722228501768\n",
      "Cost on val dataset after 542 epochs is = 0.10200794148254672\n",
      "Initial Cost on Val dataset for this epoch 542 = 0.10200794148254672\n",
      "learning rate for this epoch =  0.10362634553861746\n",
      "Error on this batch = 0.05639002259084634\n",
      "Error on this batch = 0.08780799011966166\n",
      "Cost on val dataset after 543 epochs is = 0.10197114293631411\n",
      "Initial Cost on Val dataset for this epoch 543 = 0.10197114293631411\n",
      "learning rate for this epoch =  0.10357860245037034\n",
      "Error on this batch = 0.05632757076693671\n",
      "Error on this batch = 0.08773899622917603\n",
      "Cost on val dataset after 544 epochs is = 0.10193447444641159\n",
      "Initial Cost on Val dataset for this epoch 544 = 0.10193447444641159\n",
      "learning rate for this epoch =  0.10353096914163801\n",
      "Error on this batch = 0.05626531749147299\n",
      "Error on this batch = 0.0876702395874657\n",
      "Cost on val dataset after 545 epochs is = 0.10189793545487671\n",
      "Initial Cost on Val dataset for this epoch 545 = 0.10189793545487671\n",
      "learning rate for this epoch =  0.10348344515888994\n",
      "Error on this batch = 0.056203261985163414\n",
      "Error on this batch = 0.08760171916897154\n",
      "Cost on val dataset after 546 epochs is = 0.10186152540442653\n",
      "Initial Cost on Val dataset for this epoch 546 = 0.10186152540442653\n",
      "learning rate for this epoch =  0.10343603005129703\n",
      "Error on this batch = 0.056141403473196995\n",
      "Error on this batch = 0.08753343394876063\n",
      "Cost on val dataset after 547 epochs is = 0.10182524373848388\n",
      "Initial Cost on Val dataset for this epoch 547 = 0.10182524373848388\n",
      "learning rate for this epoch =  0.1033887233707106\n",
      "Error on this batch = 0.05607974118519536\n",
      "Error on this batch = 0.08746538290261566\n",
      "Cost on val dataset after 548 epochs is = 0.10178908990120399\n",
      "Initial Cost on Val dataset for this epoch 548 = 0.10178908990120399\n",
      "learning rate for this epoch =  0.10334152467164161\n",
      "Error on this batch = 0.056018274355165494\n",
      "Error on this batch = 0.08739756500712346\n",
      "Cost on val dataset after 549 epochs is = 0.10175306333750139\n",
      "Initial Cost on Val dataset for this epoch 549 = 0.10175306333750139\n",
      "learning rate for this epoch =  0.10329443351124007\n",
      "Error on this batch = 0.05595700222145244\n",
      "Error on this batch = 0.08732997923976296\n",
      "Cost on val dataset after 550 epochs is = 0.10171716349307738\n",
      "Initial Cost on Val dataset for this epoch 550 = 0.10171716349307738\n",
      "learning rate for this epoch =  0.10324744944927464\n",
      "Error on this batch = 0.05589592402669293\n",
      "Error on this batch = 0.08726262457899203\n",
      "Cost on val dataset after 551 epochs is = 0.10168138981444744\n",
      "Initial Cost on Val dataset for this epoch 551 = 0.10168138981444744\n",
      "learning rate for this epoch =  0.10320057204811232\n",
      "Error on this batch = 0.0558350390177693\n",
      "Error on this batch = 0.08719550000433365\n",
      "Cost on val dataset after 552 epochs is = 0.10164574174896902\n",
      "Initial Cost on Val dataset for this epoch 552 = 0.10164574174896902\n",
      "learning rate for this epoch =  0.10315380087269863\n",
      "Error on this batch = 0.05577434644576412\n",
      "Error on this batch = 0.0871286044964612\n",
      "Cost on val dataset after 553 epochs is = 0.10161021874486952\n",
      "Initial Cost on Val dataset for this epoch 553 = 0.10161021874486952\n",
      "learning rate for this epoch =  0.10310713549053749\n",
      "Error on this batch = 0.0557138455659151\n",
      "Error on this batch = 0.0870619370372826\n",
      "Cost on val dataset after 554 epochs is = 0.10157482025127416\n",
      "Initial Cost on Val dataset for this epoch 554 = 0.10157482025127416\n",
      "learning rate for this epoch =  0.10306057547167191\n",
      "Error on this batch = 0.055653535637570715\n",
      "Error on this batch = 0.08699549661002381\n",
      "Cost on val dataset after 555 epochs is = 0.1015395457182342\n",
      "Initial Cost on Val dataset for this epoch 555 = 0.1015395457182342\n",
      "learning rate for this epoch =  0.1030141203886643\n",
      "Error on this batch = 0.05559341592414618\n",
      "Error on this batch = 0.08692928219931116\n",
      "Cost on val dataset after 556 epochs is = 0.10150439459675485\n",
      "Initial Cost on Val dataset for this epoch 556 = 0.10150439459675485\n",
      "learning rate for this epoch =  0.10296776981657725\n",
      "Error on this batch = 0.05553348569307996\n",
      "Error on this batch = 0.0868632927912525\n",
      "Cost on val dataset after 557 epochs is = 0.1014693663388234\n",
      "Initial Cost on Val dataset for this epoch 557 = 0.1014693663388234\n",
      "learning rate for this epoch =  0.10292152333295448\n",
      "Error on this batch = 0.05547374421579072\n",
      "Error on this batch = 0.08679752737351809\n",
      "Cost on val dataset after 558 epochs is = 0.10143446039743717\n",
      "Initial Cost on Val dataset for this epoch 558 = 0.10143446039743717\n",
      "learning rate for this epoch =  0.10287538051780193\n",
      "Error on this batch = 0.05541419076763497\n",
      "Error on this batch = 0.08673198493541945\n",
      "Cost on val dataset after 559 epochs is = 0.10139967622663129\n",
      "Initial Cost on Val dataset for this epoch 559 = 0.10139967622663129\n",
      "learning rate for this epoch =  0.10282934095356899\n",
      "Error on this batch = 0.055354824627864695\n",
      "Error on this batch = 0.08666666446798786\n",
      "Cost on val dataset after 560 epochs is = 0.10136501328150639\n",
      "Initial Cost on Val dataset for this epoch 560 = 0.10136501328150639\n",
      "learning rate for this epoch =  0.10278340422512992\n",
      "Error on this batch = 0.05529564507958616\n",
      "Error on this batch = 0.08660156496405154\n",
      "Cost on val dataset after 561 epochs is = 0.10133047101825622\n",
      "Initial Cost on Val dataset for this epoch 561 = 0.10133047101825622\n",
      "learning rate for this epoch =  0.10273756991976561\n",
      "Error on this batch = 0.05523665140971844\n",
      "Error on this batch = 0.08653668541831179\n",
      "Cost on val dataset after 562 epochs is = 0.10129604889419477\n",
      "Initial Cost on Val dataset for this epoch 562 = 0.10129604889419477\n",
      "learning rate for this epoch =  0.10269183762714515\n",
      "Error on this batch = 0.055177842908952945\n",
      "Error on this batch = 0.08647202482741795\n",
      "Cost on val dataset after 563 epochs is = 0.10126174636778339\n",
      "Initial Cost on Val dataset for this epoch 563 = 0.10126174636778339\n",
      "learning rate for this epoch =  0.10264620693930798\n",
      "Error on this batch = 0.0551192188717131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on this batch = 0.08640758219004141\n",
      "Cost on val dataset after 564 epochs is = 0.10122756289865747\n",
      "Initial Cost on Val dataset for this epoch 564 = 0.10122756289865747\n",
      "learning rate for this epoch =  0.10260067745064594\n",
      "Error on this batch = 0.055060778596114524\n",
      "Error on this batch = 0.08634335650694837\n",
      "Cost on val dataset after 565 epochs is = 0.10119349794765294\n",
      "Initial Cost on Val dataset for this epoch 565 = 0.10119349794765294\n",
      "learning rate for this epoch =  0.10255524875788552\n",
      "Error on this batch = 0.05500252138392593\n",
      "Error on this batch = 0.08627934678107138\n",
      "Cost on val dataset after 566 epochs is = 0.10115955097683225\n",
      "Initial Cost on Val dataset for this epoch 566 = 0.10115955097683225\n",
      "learning rate for this epoch =  0.10250992046007043\n",
      "Error on this batch = 0.054944446540529754\n",
      "Error on this batch = 0.08621555201758004\n",
      "Cost on val dataset after 567 epochs is = 0.10112572144951029\n",
      "Initial Cost on Val dataset for this epoch 567 = 0.10112572144951029\n",
      "learning rate for this epoch =  0.10246469215854406\n",
      "Error on this batch = 0.05488655337488407\n",
      "Error on this batch = 0.08615197122395013\n",
      "Cost on val dataset after 568 epochs is = 0.10109200883027943\n",
      "Initial Cost on Val dataset for this epoch 568 = 0.10109200883027943\n",
      "learning rate for this epoch =  0.10241956345693246\n",
      "Error on this batch = 0.05482884119948414\n",
      "Error on this batch = 0.08608860341003194\n",
      "Cost on val dataset after 569 epochs is = 0.10105841258503477\n",
      "Initial Cost on Val dataset for this epoch 569 = 0.10105841258503477\n",
      "learning rate for this epoch =  0.1023745339611271\n",
      "Error on this batch = 0.054771309330324994\n",
      "Error on this batch = 0.08602544758811721\n",
      "Cost on val dataset after 570 epochs is = 0.10102493218099844\n",
      "Initial Cost on Val dataset for this epoch 570 = 0.10102493218099844\n",
      "learning rate for this epoch =  0.10232960327926806\n",
      "Error on this batch = 0.054713957086863906\n",
      "Error on this batch = 0.08596250277300493\n",
      "Cost on val dataset after 571 epochs is = 0.1009915670867438\n",
      "Initial Cost on Val dataset for this epoch 571 = 0.1009915670867438\n",
      "learning rate for this epoch =  0.10228477102172728\n",
      "Error on this batch = 0.05465678379198366\n",
      "Error on this batch = 0.08589976798206603\n",
      "Cost on val dataset after 572 epochs is = 0.10095831677221884\n",
      "Initial Cost on Val dataset for this epoch 572 = 0.10095831677221884\n",
      "learning rate for this epoch =  0.10224003680109194\n",
      "Error on this batch = 0.05459978877195582\n",
      "Error on this batch = 0.08583724223530671\n",
      "Cost on val dataset after 573 epochs is = 0.10092518070876957\n",
      "Initial Cost on Val dataset for this epoch 573 = 0.10092518070876957\n",
      "learning rate for this epoch =  0.10219540023214801\n",
      "Error on this batch = 0.0545429713564047\n",
      "Error on this batch = 0.08577492455543084\n",
      "Cost on val dataset after 574 epochs is = 0.1008921583691624\n",
      "Initial Cost on Val dataset for this epoch 574 = 0.1008921583691624\n",
      "learning rate for this epoch =  0.10215086093186404\n",
      "Error on this batch = 0.054486330878271545\n",
      "Error on this batch = 0.08571281396790081\n",
      "Cost on val dataset after 575 epochs is = 0.10085924922760643\n",
      "Initial Cost on Val dataset for this epoch 575 = 0.10085924922760643\n",
      "learning rate for this epoch =  0.10210641851937487\n",
      "Error on this batch = 0.05442986667377903\n",
      "Error on this batch = 0.08565090950099752\n",
      "Cost on val dataset after 576 epochs is = 0.10082645275977499\n",
      "Initial Cost on Val dataset for this epoch 576 = 0.10082645275977499\n",
      "learning rate for this epoch =  0.10206207261596577\n",
      "Error on this batch = 0.05437357808239618\n",
      "Error on this batch = 0.08558921018587903\n",
      "Cost on val dataset after 577 epochs is = 0.10079376844282674\n",
      "Initial Cost on Val dataset for this epoch 577 = 0.10079376844282674\n",
      "learning rate for this epoch =  0.10201782284505649\n",
      "Error on this batch = 0.05431746444680366\n",
      "Error on this batch = 0.08552771505663781\n",
      "Cost on val dataset after 578 epochs is = 0.10076119575542618\n",
      "Initial Cost on Val dataset for this epoch 578 = 0.10076119575542618\n",
      "learning rate for this epoch =  0.10197366883218573\n",
      "Error on this batch = 0.054261525112859206\n",
      "Error on this batch = 0.08546642315035716\n",
      "Cost on val dataset after 579 epochs is = 0.1007287341777637\n",
      "Initial Cost on Val dataset for this epoch 579 = 0.1007287341777637\n",
      "learning rate for this epoch =  0.1019296102049953\n",
      "Error on this batch = 0.0542057594295637\n",
      "Error on this batch = 0.08540533350716603\n",
      "Cost on val dataset after 580 epochs is = 0.10069638319157506\n",
      "Initial Cost on Val dataset for this epoch 580 = 0.10069638319157506\n",
      "learning rate for this epoch =  0.10188564659321496\n",
      "Error on this batch = 0.054150166749027236\n",
      "Error on this batch = 0.08534444517029303\n",
      "Cost on val dataset after 581 epochs is = 0.10066414228016016\n",
      "Initial Cost on Val dataset for this epoch 581 = 0.10066414228016016\n",
      "learning rate for this epoch =  0.10184177762864698\n",
      "Error on this batch = 0.054094746426435754\n",
      "Error on this batch = 0.0852837571861189\n",
      "Cost on val dataset after 582 epochs is = 0.10063201092840153\n",
      "Initial Cost on Val dataset for this epoch 582 = 0.10063201092840153\n",
      "learning rate for this epoch =  0.10179800294515103\n",
      "Error on this batch = 0.05403949782001765\n",
      "Error on this batch = 0.08522326860422785\n",
      "Cost on val dataset after 583 epochs is = 0.1005999886227821\n",
      "Initial Cost on Val dataset for this epoch 583 = 0.1005999886227821\n",
      "learning rate for this epoch =  0.10175432217862923\n",
      "Error on this batch = 0.05398442029101128\n",
      "Error on this batch = 0.08516297847745787\n",
      "Cost on val dataset after 584 epochs is = 0.10056807485140228\n",
      "Initial Cost on Val dataset for this epoch 584 = 0.10056807485140228\n",
      "learning rate for this epoch =  0.10171073496701123\n",
      "Error on this batch = 0.05392951320363177\n",
      "Error on this batch = 0.08510288586194965\n",
      "Cost on val dataset after 585 epochs is = 0.10053626910399668\n",
      "Initial Cost on Val dataset for this epoch 585 = 0.10053626910399668\n",
      "learning rate for this epoch =  0.10166724095023941\n",
      "Error on this batch = 0.05387477592503931\n",
      "Error on this batch = 0.08504298981719446\n",
      "Cost on val dataset after 586 epochs is = 0.10050457087195025\n",
      "Initial Cost on Val dataset for this epoch 586 = 0.10050457087195025\n",
      "learning rate for this epoch =  0.10162383977025442\n",
      "Error on this batch = 0.05382020782530668\n",
      "Error on this batch = 0.08498328940608055\n",
      "Cost on val dataset after 587 epochs is = 0.10047297964831373\n",
      "Initial Cost on Val dataset for this epoch 587 = 0.10047297964831373\n",
      "learning rate for this epoch =  0.10158053107098057\n",
      "Error on this batch = 0.0537658082773876\n",
      "Error on this batch = 0.08492378369493879\n",
      "Cost on val dataset after 588 epochs is = 0.10044149492781856\n",
      "Initial Cost on Val dataset for this epoch 588 = 0.10044149492781856\n",
      "learning rate for this epoch =  0.10153731449831156\n",
      "Error on this batch = 0.05371157665708518\n",
      "Error on this batch = 0.0848644717535867\n",
      "Cost on val dataset after 589 epochs is = 0.1004101162068913\n",
      "Initial Cost on Val dataset for this epoch 589 = 0.1004101162068913\n",
      "learning rate for this epoch =  0.1014941897000962\n",
      "Error on this batch = 0.05365751234302065\n",
      "Error on this batch = 0.08480535265537167\n",
      "Cost on val dataset after 590 epochs is = 0.10037884298366748\n",
      "Initial Cost on Val dataset for this epoch 590 = 0.10037884298366748\n",
      "learning rate for this epoch =  0.10145115632612439\n",
      "Error on this batch = 0.053603614716602264\n",
      "Error on this batch = 0.08474642547721258\n",
      "Cost on val dataset after 591 epochs is = 0.10034767475800474\n",
      "Initial Cost on Val dataset for this epoch 591 = 0.10034767475800474\n",
      "learning rate for this epoch =  0.10140821402811308\n",
      "Error on this batch = 0.05354988316199456\n",
      "Error on this batch = 0.08468768929964067\n",
      "Cost on val dataset after 592 epochs is = 0.10031661103149572\n",
      "Initial Cost on Val dataset for this epoch 592 = 0.10031661103149572\n",
      "learning rate for this epoch =  0.10136536245969245\n",
      "Error on this batch = 0.05349631706608779\n",
      "Error on this batch = 0.0846291432068391\n",
      "Cost on val dataset after 593 epochs is = 0.10028565130747992\n",
      "Initial Cost on Val dataset for this epoch 593 = 0.10028565130747992\n",
      "learning rate for this epoch =  0.10132260127639228\n",
      "Error on this batch = 0.05344291581846743\n",
      "Error on this batch = 0.0845707862866811\n",
      "Cost on val dataset after 594 epochs is = 0.10025479509105557\n",
      "Initial Cost on Val dataset for this epoch 594 = 0.10025479509105557\n",
      "learning rate for this epoch =  0.10127993013562818\n",
      "Error on this batch = 0.05338967881138434\n",
      "Error on this batch = 0.08451261763076748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on val dataset after 595 epochs is = 0.10022404188909059\n",
      "Initial Cost on Val dataset for this epoch 595 = 0.10022404188909059\n",
      "learning rate for this epoch =  0.10123734869668825\n",
      "Error on this batch = 0.05333660543972453\n",
      "Error on this batch = 0.0844546363344623\n",
      "Cost on val dataset after 596 epochs is = 0.10019339121023303\n",
      "Initial Cost on Val dataset for this epoch 596 = 0.10019339121023303\n",
      "learning rate for this epoch =  0.10119485662071964\n",
      "Error on this batch = 0.05328369510097973\n",
      "Error on this batch = 0.08439684149692823\n",
      "Cost on val dataset after 597 epochs is = 0.10016284256492114\n",
      "Initial Cost on Val dataset for this epoch 597 = 0.10016284256492114\n",
      "learning rate for this epoch =  0.10115245357071535\n",
      "Error on this batch = 0.053230947195217815\n",
      "Error on this batch = 0.08433923222116001\n",
      "Cost on val dataset after 598 epochs is = 0.1001323954653928\n",
      "Initial Cost on Val dataset for this epoch 598 = 0.1001323954653928\n",
      "learning rate for this epoch =  0.10111013921150111\n",
      "Error on this batch = 0.05317836112505345\n",
      "Error on this batch = 0.0842818076140172\n",
      "Cost on val dataset after 599 epochs is = 0.10010204942569456\n",
      "Initial Cost on Val dataset for this epoch 599 = 0.10010204942569456\n",
      "learning rate for this epoch =  0.1010679132097223\n",
      "Error on this batch = 0.0531259362956191\n",
      "Error on this batch = 0.0842245667862559\n",
      "Cost on val dataset after 600 epochs is = 0.10007180396168992\n",
      "Initial Cost on Val dataset for this epoch 600 = 0.10007180396168992\n",
      "learning rate for this epoch =  0.10102577523383117\n",
      "Error on this batch = 0.05307367211453624\n",
      "Error on this batch = 0.08416750885255893\n",
      "Cost on val dataset after 601 epochs is = 0.10004165859106737\n",
      "Initial Cost on Val dataset for this epoch 601 = 0.10004165859106737\n",
      "learning rate for this epoch =  0.10098372495407393\n",
      "Error on this batch = 0.053021567991886454\n",
      "Error on this batch = 0.08411063293156534\n",
      "Cost on val dataset after 602 epochs is = 0.1000116128333478\n",
      "Initial Cost on Val dataset for this epoch 602 = 0.1000116128333478\n",
      "learning rate for this epoch =  0.10094176204247814\n",
      "Error on this batch = 0.05296962334018313\n",
      "Error on this batch = 0.08405393814589865\n",
      "Cost on val dataset after 603 epochs is = 0.09998166620989155\n",
      "Initial Cost on Val dataset for this epoch 603 = 0.09998166620989155\n",
      "learning rate for this epoch =  0.10089988617284017\n",
      "Error on this batch = 0.05291783757434318\n",
      "Error on this batch = 0.08399742362219378\n",
      "Cost on val dataset after 604 epochs is = 0.09995181824390476\n",
      "Initial Cost on Val dataset for this epoch 604 = 0.09995181824390476\n",
      "learning rate for this epoch =  0.10085809702071269\n",
      "Error on this batch = 0.05286621011165882\n",
      "Error on this batch = 0.08394108849112358\n",
      "Cost on val dataset after 605 epochs is = 0.09992206846044546\n",
      "Initial Cost on Val dataset for this epoch 605 = 0.09992206846044546\n",
      "learning rate for this epoch =  0.10081639426339235\n",
      "Error on this batch = 0.052814740371769774\n",
      "Error on this batch = 0.08388493188742337\n",
      "Cost on val dataset after 606 epochs is = 0.09989241638642916\n",
      "Initial Cost on Val dataset for this epoch 606 = 0.09989241638642916\n",
      "learning rate for this epoch =  0.10077477757990758\n",
      "Error on this batch = 0.05276342777663563\n",
      "Error on this batch = 0.0838289529499152\n",
      "Cost on val dataset after 607 epochs is = 0.09986286155063391\n",
      "Initial Cost on Val dataset for this epoch 607 = 0.09986286155063391\n",
      "learning rate for this epoch =  0.10073324665100637\n",
      "Error on this batch = 0.05271227175050818\n",
      "Error on this batch = 0.08377315082153071\n",
      "Cost on val dataset after 608 epochs is = 0.09983340348370505\n",
      "Initial Cost on Val dataset for this epoch 608 = 0.09983340348370505\n",
      "learning rate for this epoch =  0.10069180115914433\n",
      "Error on this batch = 0.052661271719904236\n",
      "Error on this batch = 0.08371752464933291\n",
      "Cost on val dataset after 609 epochs is = 0.09980404171815939\n",
      "Initial Cost on Val dataset for this epoch 609 = 0.09980404171815939\n",
      "learning rate for this epoch =  0.1006504407884727\n",
      "Error on this batch = 0.05261042711357849\n",
      "Error on this batch = 0.08366207358453713\n",
      "Cost on val dataset after 610 epochs is = 0.09977477578838914\n",
      "Initial Cost on Val dataset for this epoch 610 = 0.09977477578838914\n",
      "learning rate for this epoch =  0.10060916522482655\n",
      "Error on this batch = 0.05255973736249666\n",
      "Error on this batch = 0.08360679678253082\n",
      "Cost on val dataset after 611 epochs is = 0.09974560523066521\n",
      "Initial Cost on Val dataset for this epoch 611 = 0.09974560523066521\n",
      "learning rate for this epoch =  0.10056797415571314\n",
      "Error on this batch = 0.052509201899808663\n",
      "Error on this batch = 0.08355169340289229\n",
      "Cost on val dataset after 612 epochs is = 0.09971652958314042\n",
      "Initial Cost on Val dataset for this epoch 612 = 0.09971652958314042\n",
      "learning rate for this epoch =  0.10052686727030014\n",
      "Error on this batch = 0.05245882016082232\n",
      "Error on this batch = 0.08349676260940864\n",
      "Cost on val dataset after 613 epochs is = 0.09968754838585206\n",
      "Initial Cost on Val dataset for this epoch 613 = 0.09968754838585206\n",
      "learning rate for this epoch =  0.10048584425940424\n",
      "Error on this batch = 0.05240859158297696\n",
      "Error on this batch = 0.08344200357009263\n",
      "Cost on val dataset after 614 epochs is = 0.09965866118072413\n",
      "Initial Cost on Val dataset for this epoch 614 = 0.09965866118072413\n",
      "learning rate for this epoch =  0.10044490481547967\n",
      "Error on this batch = 0.05235851560581738\n",
      "Error on this batch = 0.08338741545719841\n",
      "Cost on val dataset after 615 epochs is = 0.09962986751156941\n",
      "Initial Cost on Val dataset for this epoch 615 = 0.09962986751156941\n",
      "learning rate for this epoch =  0.10040404863260693\n",
      "Error on this batch = 0.05230859167096815\n",
      "Error on this batch = 0.08333299744723636\n",
      "Cost on val dataset after 616 epochs is = 0.09960116692409088\n",
      "Initial Cost on Val dataset for this epoch 616 = 0.09960116692409088\n",
      "learning rate for this epoch =  0.10036327540648149\n",
      "Error on this batch = 0.05225881922210785\n",
      "Error on this batch = 0.08327874872098728\n",
      "Cost on val dataset after 617 epochs is = 0.09957255896588309\n",
      "Initial Cost on Val dataset for this epoch 617 = 0.09957255896588309\n",
      "learning rate for this epoch =  0.10032258483440272\n",
      "Error on this batch = 0.05220919770494391\n",
      "Error on this batch = 0.08322466846351517\n",
      "Cost on val dataset after 618 epochs is = 0.09954404318643294\n",
      "Initial Cost on Val dataset for this epoch 618 = 0.09954404318643294\n",
      "learning rate for this epoch =  0.10028197661526282\n",
      "Error on this batch = 0.052159726567187335\n",
      "Error on this batch = 0.08317075586417931\n",
      "Cost on val dataset after 619 epochs is = 0.09951561913712043\n",
      "Initial Cost on Val dataset for this epoch 619 = 0.09951561913712043\n",
      "learning rate for this epoch =  0.10024145044953589\n",
      "Error on this batch = 0.05211040525852813\n",
      "Error on this batch = 0.08311701011664542\n",
      "Cost on val dataset after 620 epochs is = 0.0994872863712188\n",
      "Initial Cost on Val dataset for this epoch 620 = 0.0994872863712188\n",
      "learning rate for this epoch =  0.10020100603926707\n",
      "Error on this batch = 0.05206123323061039\n",
      "Error on this batch = 0.08306343041889601\n",
      "Cost on val dataset after 621 epochs is = 0.09945904444389468\n",
      "Initial Cost on Val dataset for this epoch 621 = 0.09945904444389468\n",
      "learning rate for this epoch =  0.1001606430880618\n",
      "Error on this batch = 0.05201220993700839\n",
      "Error on this batch = 0.08301001597323948\n",
      "Cost on val dataset after 622 epochs is = 0.09943089291220765\n",
      "Initial Cost on Val dataset for this epoch 622 = 0.09943089291220765\n",
      "learning rate for this epoch =  0.10012036130107511\n",
      "Error on this batch = 0.05196333483320228\n",
      "Error on this batch = 0.08295676598631868\n",
      "Cost on val dataset after 623 epochs is = 0.09940283133510988\n",
      "Initial Cost on Val dataset for this epoch 623 = 0.09940283133510988\n",
      "learning rate for this epoch =  0.10008016038500113\n",
      "Error on this batch = 0.05191460737655459\n",
      "Error on this batch = 0.08290367966911844\n",
      "Cost on val dataset after 624 epochs is = 0.09937485927344525\n",
      "Initial Cost on Val dataset for this epoch 624 = 0.09937485927344525\n",
      "learning rate for this epoch =  0.10004004004806248\n",
      "Error on this batch = 0.051866027026286655\n",
      "Error on this batch = 0.0828507562369723\n",
      "Cost on val dataset after 625 epochs is = 0.09934697628994825\n",
      "Initial Cost on Val dataset for this epoch 625 = 0.09934697628994825\n",
      "learning rate for this epoch =  0.1\n",
      "Error on this batch = 0.05181759324345581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on this batch = 0.08279799490956846\n",
      "Cost on val dataset after 626 epochs is = 0.09931918194924286\n",
      "Initial Cost on Val dataset for this epoch 626 = 0.09931918194924286\n",
      "learning rate for this epoch =  0.09996003995206232\n",
      "Error on this batch = 0.05176930549093245\n",
      "Error on this batch = 0.08274539491095449\n",
      "Cost on val dataset after 627 epochs is = 0.09929147581784085\n",
      "Initial Cost on Val dataset for this epoch 627 = 0.09929147581784085\n",
      "learning rate for this epoch =  0.0999201596169957\n",
      "Error on this batch = 0.051721163233377734\n",
      "Error on this batch = 0.08269295546954177\n",
      "Cost on val dataset after 628 epochs is = 0.09926385746414043\n",
      "Initial Cost on Val dataset for this epoch 628 = 0.09926385746414043\n",
      "learning rate for this epoch =  0.09988035870903386\n",
      "Error on this batch = 0.05167316593722159\n",
      "Error on this batch = 0.08264067581810874\n",
      "Cost on val dataset after 629 epochs is = 0.09923632645842399\n",
      "Initial Cost on Val dataset for this epoch 629 = 0.09923632645842399\n",
      "learning rate for this epoch =  0.09984063694388798\n",
      "Error on this batch = 0.05162531307064102\n",
      "Error on this batch = 0.0825885551938035\n",
      "Cost on val dataset after 630 epochs is = 0.09920888237285634\n",
      "Initial Cost on Val dataset for this epoch 630 = 0.09920888237285634\n",
      "learning rate for this epoch =  0.09980099403873664\n",
      "Error on this batch = 0.05157760410353898\n",
      "Error on this batch = 0.08253659283814561\n",
      "Cost on val dataset after 631 epochs is = 0.09918152478148233\n",
      "Initial Cost on Val dataset for this epoch 631 = 0.09918152478148233\n",
      "learning rate for this epoch =  0.099761429712216\n",
      "Error on this batch = 0.05153003850752349\n",
      "Error on this batch = 0.08248478799702698\n",
      "Cost on val dataset after 632 epochs is = 0.09915425326022444\n",
      "Initial Cost on Val dataset for this epoch 632 = 0.09915425326022444\n",
      "learning rate for this epoch =  0.09972194368440992\n",
      "Error on this batch = 0.05148261575588714\n",
      "Error on this batch = 0.08243313992071213\n",
      "Cost on val dataset after 633 epochs is = 0.09912706738688042\n",
      "Initial Cost on Val dataset for this epoch 633 = 0.09912706738688042\n",
      "learning rate for this epoch =  0.09968253567684038\n",
      "Error on this batch = 0.05143533532358735\n",
      "Error on this batch = 0.08238164786383781\n",
      "Cost on val dataset after 634 epochs is = 0.0990999667411205\n",
      "Initial Cost on Val dataset for this epoch 634 = 0.0990999667411205\n",
      "learning rate for this epoch =  0.09964320541245761\n",
      "Error on this batch = 0.05138819668722672\n",
      "Error on this batch = 0.08233031108541156\n",
      "Cost on val dataset after 635 epochs is = 0.0990729509044846\n",
      "Initial Cost on Val dataset for this epoch 635 = 0.0990729509044846\n",
      "learning rate for this epoch =  0.09960395261563074\n",
      "Error on this batch = 0.051341199325034045\n",
      "Error on this batch = 0.08227912884880974\n",
      "Cost on val dataset after 636 epochs is = 0.09904601946037962\n",
      "Initial Cost on Val dataset for this epoch 636 = 0.09904601946037962\n",
      "learning rate for this epoch =  0.0995647770121382\n",
      "Error on this batch = 0.05129434271684578\n",
      "Error on this batch = 0.08222810042177489\n",
      "Cost on val dataset after 637 epochs is = 0.09901917199407625\n",
      "Initial Cost on Val dataset for this epoch 637 = 0.09901917199407625\n",
      "learning rate for this epoch =  0.0995256783291583\n",
      "Error on this batch = 0.05124762634408814\n",
      "Error on this batch = 0.08217722507641234\n",
      "Cost on val dataset after 638 epochs is = 0.09899240809270615\n",
      "Initial Cost on Val dataset for this epoch 638 = 0.09899240809270615\n",
      "learning rate for this epoch =  0.09948665629526\n",
      "Error on this batch = 0.051201049689759394\n",
      "Error on this batch = 0.082126502089186\n",
      "Cost on val dataset after 639 epochs is = 0.09896572734525853\n",
      "Initial Cost on Val dataset for this epoch 639 = 0.09896572734525853\n",
      "learning rate for this epoch =  0.09944771064039355\n",
      "Error on this batch = 0.05115461223841311\n",
      "Error on this batch = 0.08207593074091377\n",
      "Cost on val dataset after 640 epochs is = 0.09893912934257741\n",
      "Initial Cost on Val dataset for this epoch 640 = 0.09893912934257741\n",
      "learning rate for this epoch =  0.09940884109588133\n",
      "Error on this batch = 0.05110831347614182\n",
      "Error on this batch = 0.08202551031676186\n",
      "Cost on val dataset after 641 epochs is = 0.09891261367735792\n",
      "Initial Cost on Val dataset for this epoch 641 = 0.09891261367735792\n",
      "learning rate for this epoch =  0.09937004739440881\n",
      "Error on this batch = 0.05106215289056104\n",
      "Error on this batch = 0.08197524010623901\n",
      "Cost on val dataset after 642 epochs is = 0.09888617994414342\n",
      "Initial Cost on Val dataset for this epoch 642 = 0.09888617994414342\n",
      "learning rate for this epoch =  0.09933132927001546\n",
      "Error on this batch = 0.051016129970794286\n",
      "Error on this batch = 0.08192511940318958\n",
      "Cost on val dataset after 643 epochs is = 0.09885982773932205\n",
      "Initial Cost on Val dataset for this epoch 643 = 0.09885982773932205\n",
      "learning rate for this epoch =  0.09929268645808582\n",
      "Error on this batch = 0.050970244207458375\n",
      "Error on this batch = 0.08187514750578627\n",
      "Cost on val dataset after 644 epochs is = 0.0988335566611235\n",
      "Initial Cost on Val dataset for this epoch 644 = 0.0988335566611235\n",
      "learning rate for this epoch =  0.09925411869534059\n",
      "Error on this batch = 0.050924495092649597\n",
      "Error on this batch = 0.08182532371652217\n",
      "Cost on val dataset after 645 epochs is = 0.0988073663096157\n",
      "Initial Cost on Val dataset for this epoch 645 = 0.0988073663096157\n",
      "learning rate for this epoch =  0.0992156257198279\n",
      "Error on this batch = 0.05087888211993032\n",
      "Error on this batch = 0.08177564734220225\n",
      "Cost on val dataset after 646 epochs is = 0.09878125628670163\n",
      "Initial Cost on Val dataset for this epoch 646 = 0.09878125628670163\n",
      "learning rate for this epoch =  0.09917720727091442\n",
      "Error on this batch = 0.05083340478431654\n",
      "Error on this batch = 0.08172611769393423\n",
      "Cost on val dataset after 647 epochs is = 0.09875522619611603\n",
      "Initial Cost on Val dataset for this epoch 647 = 0.09875522619611603\n",
      "learning rate for this epoch =  0.09913886308927693\n",
      "Error on this batch = 0.05078806258226589\n",
      "Error on this batch = 0.08167673408711888\n",
      "Cost on val dataset after 648 epochs is = 0.09872927564342218\n",
      "Initial Cost on Val dataset for this epoch 648 = 0.09872927564342218\n",
      "learning rate for this epoch =  0.09910059291689342\n",
      "Error on this batch = 0.050742855011666484\n",
      "Error on this batch = 0.08162749584143981\n",
      "Cost on val dataset after 649 epochs is = 0.0987034042360088\n",
      "Initial Cost on Val dataset for this epoch 649 = 0.0987034042360088\n",
      "learning rate for this epoch =  0.09906239649703485\n",
      "Error on this batch = 0.050697781571826306\n",
      "Error on this batch = 0.08157840228085263\n",
      "Cost on val dataset after 650 epochs is = 0.09867761158308695\n",
      "Initial Cost on Val dataset for this epoch 650 = 0.09867761158308695\n",
      "learning rate for this epoch =  0.09902427357425653\n",
      "Error on this batch = 0.05065284176346365\n",
      "Error on this batch = 0.08152945273357366\n",
      "Cost on val dataset after 651 epochs is = 0.09865189729568696\n",
      "Initial Cost on Val dataset for this epoch 651 = 0.09865189729568696\n",
      "learning rate for this epoch =  0.09898622389438974\n",
      "Error on this batch = 0.05060803508869803\n",
      "Error on this batch = 0.08148064653206813\n",
      "Cost on val dataset after 652 epochs is = 0.09862626098665551\n",
      "Initial Cost on Val dataset for this epoch 652 = 0.09862626098665551\n",
      "learning rate for this epoch =  0.09894824720453346\n",
      "Error on this batch = 0.05056336105104193\n",
      "Error on this batch = 0.08143198301303788\n",
      "Cost on val dataset after 653 epochs is = 0.09860070227065278\n",
      "Initial Cost on Val dataset for this epoch 653 = 0.09860070227065278\n",
      "learning rate for this epoch =  0.09891034325304611\n",
      "Error on this batch = 0.05051881915539345\n",
      "Error on this batch = 0.08138346151740841\n",
      "Cost on val dataset after 654 epochs is = 0.09857522076414967\n",
      "Initial Cost on Val dataset for this epoch 654 = 0.09857522076414967\n",
      "learning rate for this epoch =  0.09887251178953727\n",
      "Error on this batch = 0.05047440890802955\n",
      "Error on this batch = 0.08133508139031559\n",
      "Cost on val dataset after 655 epochs is = 0.09854981608542515\n",
      "Initial Cost on Val dataset for this epoch 655 = 0.09854981608542515\n",
      "learning rate for this epoch =  0.09883475256485971\n",
      "Error on this batch = 0.050430129816600354\n",
      "Error on this batch = 0.0812868419810921\n",
      "Cost on val dataset after 656 epochs is = 0.0985244878545637\n",
      "Initial Cost on Val dataset for this epoch 656 = 0.0985244878545637\n",
      "learning rate for this epoch =  0.09879706533110119\n",
      "Error on this batch = 0.05038598139012388\n",
      "Error on this batch = 0.0812387426432529\n",
      "Cost on val dataset after 657 epochs is = 0.09849923569345291\n",
      "Initial Cost on Val dataset for this epoch 657 = 0.09849923569345291\n",
      "learning rate for this epoch =  0.09875944984157659\n",
      "Error on this batch = 0.050341963138982086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on this batch = 0.08119078273448083\n",
      "Cost on val dataset after 658 epochs is = 0.09847405922578115\n",
      "Initial Cost on Val dataset for this epoch 658 = 0.09847405922578115\n",
      "learning rate for this epoch =  0.09872190585081982\n",
      "Error on this batch = 0.05029807457491736\n",
      "Error on this batch = 0.08114296161661139\n",
      "Cost on val dataset after 659 epochs is = 0.09844895807703555\n",
      "Initial Cost on Val dataset for this epoch 659 = 0.09844895807703555\n",
      "learning rate for this epoch =  0.09868443311457611\n",
      "Error on this batch = 0.05025431521102995\n",
      "Error on this batch = 0.08109527865561718\n",
      "Cost on val dataset after 660 epochs is = 0.0984239318744997\n",
      "Initial Cost on Val dataset for this epoch 660 = 0.0984239318744997\n",
      "learning rate for this epoch =  0.09864703138979418\n",
      "Error on this batch = 0.05021068456177634\n",
      "Error on this batch = 0.08104773322159221\n",
      "Cost on val dataset after 661 epochs is = 0.09839898024725224\n",
      "Initial Cost on Val dataset for this epoch 661 = 0.09839898024725224\n",
      "learning rate for this epoch =  0.09860970043461836\n",
      "Error on this batch = 0.05016718214296837\n",
      "Error on this batch = 0.08100032468873533\n",
      "Cost on val dataset after 662 epochs is = 0.0983741028261647\n",
      "Initial Cost on Val dataset for this epoch 662 = 0.0983741028261647\n",
      "learning rate for this epoch =  0.09857244000838114\n",
      "Error on this batch = 0.050123807471773335\n",
      "Error on this batch = 0.08095305243533364\n",
      "Cost on val dataset after 663 epochs is = 0.09834929924390032\n",
      "Initial Cost on Val dataset for this epoch 663 = 0.09834929924390032\n",
      "learning rate for this epoch =  0.09853524987159529\n",
      "Error on this batch = 0.05008056006671474\n",
      "Error on this batch = 0.08090591584374565\n",
      "Cost on val dataset after 664 epochs is = 0.09832456913491246\n",
      "Initial Cost on Val dataset for this epoch 664 = 0.09832456913491246\n",
      "learning rate for this epoch =  0.0984981297859465\n",
      "Error on this batch = 0.05003743944767404\n",
      "Error on this batch = 0.08085891430038349\n",
      "Cost on val dataset after 665 epochs is = 0.09829991213544347\n",
      "Initial Cost on Val dataset for this epoch 665 = 0.09829991213544347\n",
      "learning rate for this epoch =  0.09846107951428583\n",
      "Error on this batch = 0.04999444513589324\n",
      "Error on this batch = 0.08081204719569535\n",
      "Cost on val dataset after 666 epochs is = 0.09827532788352382\n",
      "Initial Cost on Val dataset for this epoch 666 = 0.09827532788352382\n",
      "learning rate for this epoch =  0.09842409882062221\n",
      "Error on this batch = 0.04995157665397832\n",
      "Error on this batch = 0.08076531392414749\n",
      "Cost on val dataset after 667 epochs is = 0.09825081601897105\n",
      "Initial Cost on Val dataset for this epoch 667 = 0.09825081601897105\n",
      "learning rate for this epoch =  0.09838718747011513\n",
      "Error on this batch = 0.04990883352590347\n",
      "Error on this batch = 0.08071871388420558\n",
      "Cost on val dataset after 668 epochs is = 0.09822637618338921\n",
      "Initial Cost on Val dataset for this epoch 668 = 0.09822637618338921\n",
      "learning rate for this epoch =  0.09835034522906724\n",
      "Error on this batch = 0.04986621527701624\n",
      "Error on this batch = 0.08067224647831614\n",
      "Cost on val dataset after 669 epochs is = 0.09820200802016839\n",
      "Initial Cost on Val dataset for this epoch 669 = 0.09820200802016839\n",
      "learning rate for this epoch =  0.09831357186491718\n",
      "Error on this batch = 0.04982372143404339\n",
      "Error on this batch = 0.0806259111128874\n",
      "Cost on val dataset after 670 epochs is = 0.09817771117448448\n",
      "Initial Cost on Val dataset for this epoch 670 = 0.09817771117448448\n",
      "learning rate for this epoch =  0.09827686714623232\n",
      "Error on this batch = 0.049781351525097745\n",
      "Error on this batch = 0.08057970719826991\n",
      "Cost on val dataset after 671 epochs is = 0.09815348529329897\n",
      "Initial Cost on Val dataset for this epoch 671 = 0.09815348529329897\n",
      "learning rate for this epoch =  0.09824023084270153\n",
      "Error on this batch = 0.04973910507968562\n",
      "Error on this batch = 0.08053363414873715\n",
      "Cost on val dataset after 672 epochs is = 0.0981293300253591\n",
      "Initial Cost on Val dataset for this epoch 672 = 0.0981293300253591\n",
      "learning rate for this epoch =  0.09820366272512825\n",
      "Error on this batch = 0.049696981628715324\n",
      "Error on this batch = 0.08048769138246527\n",
      "Cost on val dataset after 673 epochs is = 0.0981052450211982\n",
      "Initial Cost on Val dataset for this epoch 673 = 0.0981052450211982\n",
      "learning rate for this epoch =  0.0981671625654233\n",
      "Error on this batch = 0.04965498070450616\n",
      "Error on this batch = 0.08044187832151313\n",
      "Cost on val dataset after 674 epochs is = 0.09808122993313603\n",
      "Initial Cost on Val dataset for this epoch 674 = 0.09808122993313603\n",
      "learning rate for this epoch =  0.09813073013659797\n",
      "Error on this batch = 0.0496131018407984\n",
      "Error on this batch = 0.08039619439180186\n",
      "Cost on val dataset after 675 epochs is = 0.09805728441527951\n",
      "Initial Cost on Val dataset for this epoch 675 = 0.09805728441527951\n",
      "learning rate for this epoch =  0.09809436521275706\n",
      "Error on this batch = 0.04957134457276388\n",
      "Error on this batch = 0.08035063902309403\n",
      "Cost on val dataset after 676 epochs is = 0.09803340812352347\n",
      "Initial Cost on Val dataset for this epoch 676 = 0.09803340812352347\n",
      "learning rate for this epoch =  0.09805806756909202\n",
      "Error on this batch = 0.049529708437017275\n",
      "Error on this batch = 0.0803052116489729\n",
      "Cost on val dataset after 677 epochs is = 0.09800960071555177\n",
      "Initial Cost on Val dataset for this epoch 677 = 0.09800960071555177\n",
      "learning rate for this epoch =  0.09802183698187408\n",
      "Error on this batch = 0.049488192971628264\n",
      "Error on this batch = 0.08025991170682106\n",
      "Cost on val dataset after 678 epochs is = 0.09798586185083834\n",
      "Initial Cost on Val dataset for this epoch 678 = 0.09798586185083834\n",
      "learning rate for this epoch =  0.09798567322844758\n",
      "Error on this batch = 0.04944679771613414\n",
      "Error on this batch = 0.08021473863779915\n",
      "Cost on val dataset after 679 epochs is = 0.09796219119064863\n",
      "Initial Cost on Val dataset for this epoch 679 = 0.09796219119064863\n",
      "learning rate for this epoch =  0.09794957608722307\n",
      "Error on this batch = 0.049405522211553164\n",
      "Error on this batch = 0.0801696918868241\n",
      "Cost on val dataset after 680 epochs is = 0.097938588398041\n",
      "Initial Cost on Val dataset for this epoch 680 = 0.097938588398041\n",
      "learning rate for this epoch =  0.09791354533767088\n",
      "Error on this batch = 0.049364366000398605\n",
      "Error on this batch = 0.08012477090254748\n",
      "Cost on val dataset after 681 epochs is = 0.0979150531378685\n",
      "Initial Cost on Val dataset for this epoch 681 = 0.0979150531378685\n",
      "learning rate for this epoch =  0.09787758076031428\n",
      "Error on this batch = 0.04932332862669318\n",
      "Error on this batch = 0.08007997513733323\n",
      "Cost on val dataset after 682 epochs is = 0.09789158507678061\n",
      "Initial Cost on Val dataset for this epoch 682 = 0.09789158507678061\n",
      "learning rate for this epoch =  0.09784168213672302\n",
      "Error on this batch = 0.04928240963598423\n",
      "Error on this batch = 0.08003530404723552\n",
      "Cost on val dataset after 683 epochs is = 0.0978681838832252\n",
      "Initial Cost on Val dataset for this epoch 683 = 0.0978681838832252\n",
      "learning rate for this epoch =  0.09780584924950686\n",
      "Error on this batch = 0.04924160857535938\n",
      "Error on this batch = 0.0799907570919763\n",
      "Cost on val dataset after 684 epochs is = 0.0978448492274506\n",
      "Initial Cost on Val dataset for this epoch 684 = 0.0978448492274506\n",
      "learning rate for this epoch =  0.09777008188230901\n",
      "Error on this batch = 0.04920092499346245\n",
      "Error on this batch = 0.07994633373492267\n",
      "Cost on val dataset after 685 epochs is = 0.09782158078150782\n",
      "Initial Cost on Val dataset for this epoch 685 = 0.09782158078150782\n",
      "learning rate for this epoch =  0.09773437981979974\n",
      "Error on this batch = 0.04916035844051016\n",
      "Error on this batch = 0.07990203344306386\n",
      "Cost on val dataset after 686 epochs is = 0.09779837821925293\n",
      "Initial Cost on Val dataset for this epoch 686 = 0.09779837821925293\n",
      "learning rate for this epoch =  0.09769874284767002\n",
      "Error on this batch = 0.049119908468309055\n",
      "Error on this batch = 0.07985785568698854\n",
      "Cost on val dataset after 687 epochs is = 0.09777524121634934\n",
      "Initial Cost on Val dataset for this epoch 687 = 0.09777524121634934\n",
      "learning rate for this epoch =  0.0976631707526253\n",
      "Error on this batch = 0.04907957463027274\n",
      "Error on this batch = 0.07981379994086116\n",
      "Cost on val dataset after 688 epochs is = 0.09775216945027046\n",
      "Initial Cost on Val dataset for this epoch 688 = 0.09775216945027046\n",
      "learning rate for this epoch =  0.09762766332237903\n",
      "Error on this batch = 0.04903935648143959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on this batch = 0.07976986568239919\n",
      "Cost on val dataset after 689 epochs is = 0.09772916260030222\n",
      "Initial Cost on Val dataset for this epoch 689 = 0.09772916260030222\n",
      "learning rate for this epoch =  0.09759222034564662\n",
      "Error on this batch = 0.04899925357849062\n",
      "Error on this batch = 0.07972605239284901\n",
      "Cost on val dataset after 690 epochs is = 0.09770622034754571\n",
      "Initial Cost on Val dataset for this epoch 690 = 0.09770622034754571\n",
      "learning rate for this epoch =  0.09755684161213918\n",
      "Error on this batch = 0.04895926547976779\n",
      "Error on this batch = 0.07968235955696266\n",
      "Cost on val dataset after 691 epochs is = 0.09768334237492006\n",
      "Initial Cost on Val dataset for this epoch 691 = 0.09768334237492006\n",
      "learning rate for this epoch =  0.09752152691255746\n",
      "Error on this batch = 0.04891939174529224\n",
      "Error on this batch = 0.07963878666297367\n",
      "Cost on val dataset after 692 epochs is = 0.09766052836716498\n",
      "Initial Cost on Val dataset for this epoch 692 = 0.09766052836716498\n",
      "learning rate for this epoch =  0.09748627603858566\n",
      "Error on this batch = 0.04887963193678271\n",
      "Error on this batch = 0.07959533320257318\n",
      "Cost on val dataset after 693 epochs is = 0.09763777801084378\n",
      "Initial Cost on Val dataset for this epoch 693 = 0.09763777801084378\n",
      "learning rate for this epoch =  0.09745108878288547\n",
      "Error on this batch = 0.048839985617674504\n",
      "Error on this batch = 0.07955199867088585\n",
      "Cost on val dataset after 694 epochs is = 0.097615090994346\n",
      "Initial Cost on Val dataset for this epoch 694 = 0.097615090994346\n",
      "learning rate for this epoch =  0.09741596493909016\n",
      "Error on this batch = 0.048800452353137826\n",
      "Error on this batch = 0.0795087825664454\n",
      "Cost on val dataset after 695 epochs is = 0.09759246700789036\n",
      "Initial Cost on Val dataset for this epoch 695 = 0.09759246700789036\n",
      "learning rate for this epoch =  0.09738090430179841\n",
      "Error on this batch = 0.04876103171009675\n",
      "Error on this batch = 0.07946568439117027\n",
      "Cost on val dataset after 696 epochs is = 0.09756990574352747\n",
      "Initial Cost on Val dataset for this epoch 696 = 0.09756990574352747\n",
      "learning rate for this epoch =  0.09734590666656863\n",
      "Error on this batch = 0.048721723257247634\n",
      "Error on this batch = 0.07942270365033897\n",
      "Cost on val dataset after 697 epochs is = 0.09754740689514269\n",
      "Initial Cost on Val dataset for this epoch 697 = 0.09754740689514269\n",
      "learning rate for this epoch =  0.09731097182991302\n",
      "Error on this batch = 0.04868252656507801\n",
      "Error on this batch = 0.07937983985256535\n",
      "Cost on val dataset after 698 epochs is = 0.09752497015845872\n",
      "Initial Cost on Val dataset for this epoch 698 = 0.09752497015845872\n",
      "learning rate for this epoch =  0.09727609958929176\n",
      "Error on this batch = 0.048643441205884916\n",
      "Error on this batch = 0.07933709250977368\n",
      "Cost on val dataset after 699 epochs is = 0.09750259523103844\n",
      "Initial Cost on Val dataset for this epoch 699 = 0.09750259523103844\n",
      "learning rate for this epoch =  0.09724128974310718\n",
      "Error on this batch = 0.04860446675379319\n",
      "Error on this batch = 0.07929446113717388\n",
      "Cost on val dataset after 700 epochs is = 0.0974802818122874\n",
      "Initial Cost on Val dataset for this epoch 700 = 0.0974802818122874\n",
      "learning rate for this epoch =  0.09720654209069819\n",
      "Error on this batch = 0.04856560278477362\n",
      "Error on this batch = 0.07925194525323598\n",
      "Cost on val dataset after 701 epochs is = 0.0974580296034563\n",
      "Initial Cost on Val dataset for this epoch 701 = 0.0974580296034563\n",
      "learning rate for this epoch =  0.09717185643233449\n",
      "Error on this batch = 0.048526848876660846\n",
      "Error on this batch = 0.07920954437966522\n",
      "Cost on val dataset after 702 epochs is = 0.09743583830764352\n",
      "Initial Cost on Val dataset for this epoch 702 = 0.09743583830764352\n",
      "learning rate for this epoch =  0.09713723256921088\n",
      "Error on this batch = 0.048488204609170714\n",
      "Error on this batch = 0.07916725804137657\n",
      "Cost on val dataset after 703 epochs is = 0.09741370762979724\n",
      "Initial Cost on Val dataset for this epoch 703 = 0.09741370762979724\n",
      "learning rate for this epoch =  0.09710267030344186\n",
      "Error on this batch = 0.048449669563917636\n",
      "Error on this batch = 0.07912508576646905\n",
      "Cost on val dataset after 704 epochs is = 0.09739163727671761\n",
      "Initial Cost on Val dataset for this epoch 704 = 0.09739163727671761\n",
      "learning rate for this epoch =  0.09706816943805581\n",
      "Error on this batch = 0.048411243324431405\n",
      "Error on this batch = 0.07908302708620032\n",
      "Cost on val dataset after 705 epochs is = 0.09736962695705882\n",
      "Initial Cost on Val dataset for this epoch 705 = 0.09736962695705882\n",
      "learning rate for this epoch =  0.09703372977698975\n",
      "Error on this batch = 0.04837292547617333\n",
      "Error on this batch = 0.07904108153496085\n",
      "Cost on val dataset after 706 epochs is = 0.0973476763813308\n",
      "Initial Cost on Val dataset for this epoch 706 = 0.0973476763813308\n",
      "learning rate for this epoch =  0.09699935112508366\n",
      "Error on this batch = 0.04833471560655252\n",
      "Error on this batch = 0.07899924865024813\n",
      "Cost on val dataset after 707 epochs is = 0.09732578526190085\n",
      "Initial Cost on Val dataset for this epoch 707 = 0.09732578526190085\n",
      "learning rate for this epoch =  0.09696503328807513\n",
      "Error on this batch = 0.04829661330494073\n",
      "Error on this batch = 0.07895752797264051\n",
      "Cost on val dataset after 708 epochs is = 0.09730395331299513\n",
      "Initial Cost on Val dataset for this epoch 708 = 0.09730395331299513\n",
      "learning rate for this epoch =  0.09693077607259398\n",
      "Error on this batch = 0.04825861816268777\n",
      "Error on this batch = 0.07891591904577137\n",
      "Cost on val dataset after 709 epochs is = 0.09728218025069979\n",
      "Initial Cost on Val dataset for this epoch 709 = 0.09728218025069979\n",
      "learning rate for this epoch =  0.09689657928615694\n",
      "Error on this batch = 0.04822072977313532\n",
      "Error on this batch = 0.07887442141630288\n",
      "Cost on val dataset after 710 epochs is = 0.09726046579296194\n",
      "Initial Cost on Val dataset for this epoch 710 = 0.09726046579296194\n",
      "learning rate for this epoch =  0.09686244273716216\n",
      "Error on this batch = 0.048182947731630725\n",
      "Error on this batch = 0.07883303463389957\n",
      "Cost on val dataset after 711 epochs is = 0.0972388096595904\n",
      "Initial Cost on Val dataset for this epoch 711 = 0.0972388096595904\n",
      "learning rate for this epoch =  0.09682836623488422\n",
      "Error on this batch = 0.04814527163553987\n",
      "Error on this batch = 0.0787917582512022\n",
      "Cost on val dataset after 712 epochs is = 0.09721721157225607\n",
      "Initial Cost on Val dataset for this epoch 712 = 0.09721721157225607\n",
      "learning rate for this epoch =  0.09679434958946864\n",
      "Error on this batch = 0.048107701084259526\n",
      "Error on this batch = 0.07875059182380102\n",
      "Cost on val dataset after 713 epochs is = 0.0971956712544923\n",
      "Initial Cost on Val dataset for this epoch 713 = 0.0971956712544923\n",
      "learning rate for this epoch =  0.09676039261192684\n",
      "Error on this batch = 0.0480702356792288\n",
      "Error on this batch = 0.0787095349102096\n",
      "Cost on val dataset after 714 epochs is = 0.09717418843169451\n",
      "Initial Cost on Val dataset for this epoch 714 = 0.09717418843169451\n",
      "learning rate for this epoch =  0.09672649511413095\n",
      "Error on this batch = 0.048032875023940054\n",
      "Error on this batch = 0.07866858707183771\n",
      "Cost on val dataset after 715 epochs is = 0.09715276283111995\n",
      "Initial Cost on Val dataset for this epoch 715 = 0.09715276283111995\n",
      "learning rate for this epoch =  0.0966926569088086\n",
      "Error on this batch = 0.04799561872394879\n",
      "Error on this batch = 0.07862774787296505\n",
      "Cost on val dataset after 716 epochs is = 0.09713139418188708\n",
      "Initial Cost on Val dataset for this epoch 716 = 0.09713139418188708\n",
      "learning rate for this epoch =  0.09665887780953801\n",
      "Error on this batch = 0.04795846638688298\n",
      "Error on this batch = 0.0785870168807141\n",
      "Cost on val dataset after 717 epochs is = 0.09711008221497419\n",
      "Initial Cost on Val dataset for this epoch 717 = 0.09711008221497419\n",
      "learning rate for this epoch =  0.09662515763074277\n",
      "Error on this batch = 0.047921417622451355\n",
      "Error on this batch = 0.07854639366502345\n",
      "Cost on val dataset after 718 epochs is = 0.09708882666321839\n",
      "Initial Cost on Val dataset for this epoch 718 = 0.09708882666321839\n",
      "learning rate for this epoch =  0.09659149618768699\n",
      "Error on this batch = 0.04788447204245116\n",
      "Error on this batch = 0.07850587779862077\n",
      "Cost on val dataset after 719 epochs is = 0.09706762726131353\n",
      "Initial Cost on Val dataset for this epoch 719 = 0.09706762726131353\n",
      "learning rate for this epoch =  0.09655789329647017\n",
      "Error on this batch = 0.047847629260774606\n",
      "Error on this batch = 0.07846546885699582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on val dataset after 720 epochs is = 0.09704648374580836\n",
      "Initial Cost on Val dataset for this epoch 720 = 0.09704648374580836\n",
      "learning rate for this epoch =  0.09652434877402243\n",
      "Error on this batch = 0.04781088889341493\n",
      "Error on this batch = 0.07842516641837347\n",
      "Cost on val dataset after 721 epochs is = 0.09702539585510404\n",
      "Initial Cost on Val dataset for this epoch 721 = 0.09702539585510404\n",
      "learning rate for this epoch =  0.09649086243809947\n",
      "Error on this batch = 0.04777425055847127\n",
      "Error on this batch = 0.07838497006368639\n",
      "Cost on val dataset after 722 epochs is = 0.09700436332945124\n",
      "Initial Cost on Val dataset for this epoch 722 = 0.09700436332945124\n",
      "learning rate for this epoch =  0.09645743410727778\n",
      "Error on this batch = 0.047737713876152645\n",
      "Error on this batch = 0.07834487937654809\n",
      "Cost on val dataset after 723 epochs is = 0.0969833859109469\n",
      "Initial Cost on Val dataset for this epoch 723 = 0.0969833859109469\n",
      "learning rate for this epoch =  0.09642406360094986\n",
      "Error on this batch = 0.04770127846878122\n",
      "Error on this batch = 0.07830489394322551\n",
      "Cost on val dataset after 724 epochs is = 0.09696246334353081\n",
      "Initial Cost on Val dataset for this epoch 724 = 0.09696246334353081\n",
      "learning rate for this epoch =  0.09639075073931928\n",
      "Error on this batch = 0.04766494396079448\n",
      "Error on this batch = 0.07826501335261196\n",
      "Cost on val dataset after 725 epochs is = 0.09694159537298143\n",
      "Initial Cost on Val dataset for this epoch 725 = 0.09694159537298143\n",
      "learning rate for this epoch =  0.09635749534339606\n",
      "Error on this batch = 0.04762870997874666\n",
      "Error on this batch = 0.07822523719619963\n",
      "Cost on val dataset after 726 epochs is = 0.09692078174691177\n",
      "Initial Cost on Val dataset for this epoch 726 = 0.09692078174691177\n",
      "learning rate for this epoch =  0.0963242972349919\n",
      "Error on this batch = 0.047592576151309046\n",
      "Error on this batch = 0.07818556506805252\n",
      "Cost on val dataset after 727 epochs is = 0.09690002221476426\n",
      "Initial Cost on Val dataset for this epoch 727 = 0.09690002221476426\n",
      "learning rate for this epoch =  0.09629115623671548\n",
      "Error on this batch = 0.04755654210926941\n",
      "Error on this batch = 0.07814599656477886\n",
      "Cost on val dataset after 728 epochs is = 0.09687931652780585\n",
      "Initial Cost on Val dataset for this epoch 728 = 0.09687931652780585\n",
      "learning rate for this epoch =  0.09625807217196783\n",
      "Error on this batch = 0.047520607485530934\n",
      "Error on this batch = 0.07810653128550397\n",
      "Cost on val dataset after 729 epochs is = 0.09685866443912243\n",
      "Initial Cost on Val dataset for this epoch 729 = 0.09685866443912243\n",
      "learning rate for this epoch =  0.09622504486493763\n",
      "Error on this batch = 0.047484771915109504\n",
      "Error on this batch = 0.07806716883184282\n",
      "Cost on val dataset after 730 epochs is = 0.09683806570361267\n",
      "Initial Cost on Val dataset for this epoch 730 = 0.09683806570361267\n",
      "learning rate for this epoch =  0.09619207414059677\n",
      "Error on this batch = 0.047449035035130774\n",
      "Error on this batch = 0.07802790880787266\n",
      "Cost on val dataset after 731 epochs is = 0.09681752007798172\n",
      "Initial Cost on Val dataset for this epoch 731 = 0.09681752007798172\n",
      "learning rate for this epoch =  0.09615915982469565\n",
      "Error on this batch = 0.047413396484825884\n",
      "Error on this batch = 0.07798875082010569\n",
      "Cost on val dataset after 732 epochs is = 0.09679702732073445\n",
      "Initial Cost on Val dataset for this epoch 732 = 0.09679702732073445\n",
      "learning rate for this epoch =  0.09612630174375877\n",
      "Error on this batch = 0.047377855905526636\n",
      "Error on this batch = 0.07794969447746179\n",
      "Cost on val dataset after 733 epochs is = 0.09677658719216814\n",
      "Initial Cost on Val dataset for this epoch 733 = 0.09677658719216814\n",
      "learning rate for this epoch =  0.09609349972508012\n",
      "Error on this batch = 0.04734241294065953\n",
      "Error on this batch = 0.07791073939124112\n",
      "Cost on val dataset after 734 epochs is = 0.09675619945436507\n",
      "Initial Cost on Val dataset for this epoch 734 = 0.09675619945436507\n",
      "learning rate for this epoch =  0.09606075359671883\n",
      "Error on this batch = 0.047307067235739124\n",
      "Error on this batch = 0.07787188517509694\n",
      "Cost on val dataset after 735 epochs is = 0.09673586387118431\n",
      "Initial Cost on Val dataset for this epoch 735 = 0.09673586387118431\n",
      "learning rate for this epoch =  0.09602806318749467\n",
      "Error on this batch = 0.04727181843836047\n",
      "Error on this batch = 0.07783313144500824\n",
      "Cost on val dataset after 736 epochs is = 0.0967155802082534\n",
      "Initial Cost on Val dataset for this epoch 736 = 0.0967155802082534\n",
      "learning rate for this epoch =  0.09599542832698374\n",
      "Error on this batch = 0.04723666619819083\n",
      "Error on this batch = 0.07779447781925258\n",
      "Cost on val dataset after 737 epochs is = 0.09669534823295962\n",
      "Initial Cost on Val dataset for this epoch 737 = 0.09669534823295962\n",
      "learning rate for this epoch =  0.095962848845514\n",
      "Error on this batch = 0.047201610166960374\n",
      "Error on this batch = 0.07775592391837899\n",
      "Cost on val dataset after 738 epochs is = 0.0966751677144408\n",
      "Initial Cost on Val dataset for this epoch 738 = 0.0966751677144408\n",
      "learning rate for this epoch =  0.09593032457416101\n",
      "Error on this batch = 0.04716664999845252\n",
      "Error on this batch = 0.0777174693651808\n",
      "Cost on val dataset after 739 epochs is = 0.09665503842357573\n",
      "Initial Cost on Val dataset for this epoch 739 = 0.09665503842357573\n",
      "learning rate for this epoch =  0.09589785534474361\n",
      "Error on this batch = 0.047131785348492945\n",
      "Error on this batch = 0.07767911378466856\n",
      "Cost on val dataset after 740 epochs is = 0.0966349601329743\n",
      "Initial Cost on Val dataset for this epoch 740 = 0.0966349601329743\n",
      "learning rate for this epoch =  0.09586544098981967\n",
      "Error on this batch = 0.04709701587493838\n",
      "Error on this batch = 0.07764085680404322\n",
      "Cost on val dataset after 741 epochs is = 0.0966149326169672\n",
      "Initial Cost on Val dataset for this epoch 741 = 0.0966149326169672\n",
      "learning rate for this epoch =  0.09583308134268179\n",
      "Error on this batch = 0.047062341237664417\n",
      "Error on this batch = 0.07760269805266905\n",
      "Cost on val dataset after 742 epochs is = 0.09659495565159527\n",
      "Initial Cost on Val dataset for this epoch 742 = 0.09659495565159527\n",
      "learning rate for this epoch =  0.09580077623735313\n",
      "Error on this batch = 0.04702776109855271\n",
      "Error on this batch = 0.077564637162047\n",
      "Cost on val dataset after 743 epochs is = 0.09657502901459863\n",
      "Initial Cost on Val dataset for this epoch 743 = 0.09657502901459863\n",
      "learning rate for this epoch =  0.09576852550858327\n",
      "Error on this batch = 0.04699327512147745\n",
      "Error on this batch = 0.07752667376578788\n",
      "Cost on val dataset after 744 epochs is = 0.09655515248540521\n",
      "Initial Cost on Val dataset for this epoch 744 = 0.09655515248540521\n",
      "learning rate for this epoch =  0.09573632899184395\n",
      "Error on this batch = 0.04695888297229145\n",
      "Error on this batch = 0.07748880749958566\n",
      "Cost on val dataset after 745 epochs is = 0.09653532584511927\n",
      "Initial Cost on Val dataset for this epoch 745 = 0.09653532584511927\n",
      "learning rate for this epoch =  0.09570418652332507\n",
      "Error on this batch = 0.04692458431881119\n",
      "Error on this batch = 0.0774510380011912\n",
      "Cost on val dataset after 746 epochs is = 0.0965155488765093\n",
      "Initial Cost on Val dataset for this epoch 746 = 0.0965155488765093\n",
      "learning rate for this epoch =  0.0956720979399305\n",
      "Error on this batch = 0.046890378830801716\n",
      "Error on this batch = 0.07741336491038571\n",
      "Cost on val dataset after 747 epochs is = 0.09649582136399593\n",
      "Initial Cost on Val dataset for this epoch 747 = 0.09649582136399593\n",
      "learning rate for this epoch =  0.0956400630792741\n",
      "Error on this batch = 0.04685626617996086\n",
      "Error on this batch = 0.07737578786895438\n",
      "Cost on val dataset after 748 epochs is = 0.0964761430936393\n",
      "Initial Cost on Val dataset for this epoch 748 = 0.0964761430936393\n",
      "learning rate for this epoch =  0.09560808177967559\n",
      "Error on this batch = 0.04682224603990269\n",
      "Error on this batch = 0.07733830652066039\n",
      "Cost on val dataset after 749 epochs is = 0.09645651385312623\n",
      "Initial Cost on Val dataset for this epoch 749 = 0.09645651385312623\n",
      "learning rate for this epoch =  0.09557615388015658\n",
      "Error on this batch = 0.04678831808614088\n",
      "Error on this batch = 0.0773009205112188\n",
      "Cost on val dataset after 750 epochs is = 0.09643693343175717\n",
      "Initial Cost on Val dataset for this epoch 750 = 0.09643693343175717\n",
      "learning rate for this epoch =  0.09554427922043668\n",
      "Error on this batch = 0.046754481996071434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on this batch = 0.07726362948827074\n",
      "Cost on val dataset after 751 epochs is = 0.09641740162043283\n",
      "Initial Cost on Val dataset for this epoch 751 = 0.09641740162043283\n",
      "learning rate for this epoch =  0.0955124576409294\n",
      "Error on this batch = 0.046720737448954754\n",
      "Error on this batch = 0.07722643310135754\n",
      "Cost on val dataset after 752 epochs is = 0.09639791821164063\n",
      "Initial Cost on Val dataset for this epoch 752 = 0.09639791821164063\n",
      "learning rate for this epoch =  0.09548068898273834\n",
      "Error on this batch = 0.04668708412589766\n",
      "Error on this batch = 0.07718933100189528\n",
      "Cost on val dataset after 753 epochs is = 0.09637848299944092\n",
      "Initial Cost on Val dataset for this epoch 753 = 0.09637848299944092\n",
      "learning rate for this epoch =  0.09544897308765321\n",
      "Error on this batch = 0.04665352170983499\n",
      "Error on this batch = 0.07715232284314927\n",
      "Cost on val dataset after 754 epochs is = 0.09635909577945283\n",
      "Initial Cost on Val dataset for this epoch 754 = 0.09635909577945283\n",
      "learning rate for this epoch =  0.09541730979814601\n",
      "Error on this batch = 0.04662004988551059\n",
      "Error on this batch = 0.07711540828020888\n",
      "Cost on val dataset after 755 epochs is = 0.0963397563488402\n",
      "Initial Cost on Val dataset for this epoch 755 = 0.0963397563488402\n",
      "learning rate for this epoch =  0.09538569895736723\n",
      "Error on this batch = 0.04658666833945829\n",
      "Error on this batch = 0.07707858696996234\n",
      "Cost on val dataset after 756 epochs is = 0.09632046450629714\n",
      "Initial Cost on Val dataset for this epoch 756 = 0.09632046450629714\n",
      "learning rate for this epoch =  0.0953541404091419\n",
      "Error on this batch = 0.046553376759982355\n",
      "Error on this batch = 0.07704185857107192\n",
      "Cost on val dataset after 757 epochs is = 0.0963012200520333\n",
      "Initial Cost on Val dataset for this epoch 757 = 0.0963012200520333\n",
      "learning rate for this epoch =  0.09532263399796595\n",
      "Error on this batch = 0.04652017483713792\n",
      "Error on this batch = 0.07700522274394907\n",
      "Cost on val dataset after 758 epochs is = 0.09628202278775946\n",
      "Initial Cost on Val dataset for this epoch 758 = 0.09628202278775946\n",
      "learning rate for this epoch =  0.09529117956900235\n",
      "Error on this batch = 0.04648706226271096\n",
      "Error on this batch = 0.07696867915072997\n",
      "Cost on val dataset after 759 epochs is = 0.09626287251667229\n",
      "Initial Cost on Val dataset for this epoch 759 = 0.09626287251667229\n",
      "learning rate for this epoch =  0.09525977696807737\n",
      "Error on this batch = 0.046454038730198176\n",
      "Error on this batch = 0.07693222745525109\n",
      "Cost on val dataset after 760 epochs is = 0.09624376904343956\n",
      "Initial Cost on Val dataset for this epoch 760 = 0.09624376904343956\n",
      "learning rate for this epoch =  0.095228426041677\n",
      "Error on this batch = 0.046421103934786746\n",
      "Error on this batch = 0.07689586732302509\n",
      "Cost on val dataset after 761 epochs is = 0.09622471217418498\n",
      "Initial Cost on Val dataset for this epoch 761 = 0.09622471217418498\n",
      "learning rate for this epoch =  0.09519712663694305\n",
      "Error on this batch = 0.04638825757333384\n",
      "Error on this batch = 0.07685959842121667\n",
      "Cost on val dataset after 762 epochs is = 0.09620570171647297\n",
      "Initial Cost on Val dataset for this epoch 762 = 0.09620570171647297\n",
      "learning rate for this epoch =  0.09516587860166968\n",
      "Error on this batch = 0.04635549934434599\n",
      "Error on this batch = 0.07682342041861902\n",
      "Cost on val dataset after 763 epochs is = 0.09618673747929331\n",
      "Initial Cost on Val dataset for this epoch 763 = 0.09618673747929331\n",
      "learning rate for this epoch =  0.09513468178429965\n",
      "Error on this batch = 0.04632282894795841\n",
      "Error on this batch = 0.07678733298563005\n",
      "Cost on val dataset after 764 epochs is = 0.09616781927304587\n",
      "Initial Cost on Val dataset for this epoch 764 = 0.09616781927304587\n",
      "learning rate for this epoch =  0.0951035360339208\n",
      "Error on this batch = 0.046290246085914345\n",
      "Error on this batch = 0.07675133579422919\n",
      "Cost on val dataset after 765 epochs is = 0.09614894690952495\n",
      "Initial Cost on Val dataset for this epoch 765 = 0.09614894690952495\n",
      "learning rate for this epoch =  0.09507244120026234\n",
      "Error on this batch = 0.04625775046154406\n",
      "Error on this batch = 0.07671542851795403\n",
      "Cost on val dataset after 766 epochs is = 0.0961301202019041\n",
      "Initial Cost on Val dataset for this epoch 766 = 0.0961301202019041\n",
      "learning rate for this epoch =  0.09504139713369145\n",
      "Error on this batch = 0.046225341779744106\n",
      "Error on this batch = 0.07667961083187753\n",
      "Cost on val dataset after 767 epochs is = 0.09611133896472035\n",
      "Initial Cost on Val dataset for this epoch 767 = 0.09611133896472035\n",
      "learning rate for this epoch =  0.09501040368520958\n",
      "Error on this batch = 0.046193019746956436\n",
      "Error on this batch = 0.07664388241258538\n",
      "Cost on val dataset after 768 epochs is = 0.09609260301385869\n",
      "Initial Cost on Val dataset for this epoch 768 = 0.09609260301385869\n",
      "learning rate for this epoch =  0.09497946070644907\n",
      "Error on this batch = 0.04616078407114749\n",
      "Error on this batch = 0.0766082429381532\n",
      "Cost on val dataset after 769 epochs is = 0.0960739121665367\n",
      "Initial Cost on Val dataset for this epoch 769 = 0.0960739121665367\n",
      "learning rate for this epoch =  0.09494856804966957\n",
      "Error on this batch = 0.046128634461787464\n",
      "Error on this batch = 0.07657269208812459\n",
      "Cost on val dataset after 770 epochs is = 0.09605526624128878\n",
      "Initial Cost on Val dataset for this epoch 770 = 0.09605526624128878\n",
      "learning rate for this epoch =  0.09491772556775467\n",
      "Error on this batch = 0.046096570629829275\n",
      "Error on this batch = 0.0765372295434889\n",
      "Cost on val dataset after 771 epochs is = 0.09603666505795085\n",
      "Initial Cost on Val dataset for this epoch 771 = 0.09603666505795085\n",
      "learning rate for this epoch =  0.09488693311420834\n",
      "Error on this batch = 0.0460645922876881\n",
      "Error on this batch = 0.07650185498665939\n",
      "Cost on val dataset after 772 epochs is = 0.09601810843764462\n",
      "Initial Cost on Val dataset for this epoch 772 = 0.09601810843764462\n",
      "learning rate for this epoch =  0.0948561905431516\n",
      "Error on this batch = 0.046032699149220535\n",
      "Error on this batch = 0.07646656810145162\n",
      "Cost on val dataset after 773 epochs is = 0.09599959620276236\n",
      "Initial Cost on Val dataset for this epoch 773 = 0.09599959620276236\n",
      "learning rate for this epoch =  0.09482549770931908\n",
      "Error on this batch = 0.04600089092970422\n",
      "Error on this batch = 0.07643136857306232\n",
      "Cost on val dataset after 774 epochs is = 0.09598112817695124\n",
      "Initial Cost on Val dataset for this epoch 774 = 0.09598112817695124\n",
      "learning rate for this epoch =  0.09479485446805574\n",
      "Error on this batch = 0.04596916734581709\n",
      "Error on this batch = 0.07639625608804791\n",
      "Cost on val dataset after 775 epochs is = 0.0959627041850983\n",
      "Initial Cost on Val dataset for this epoch 775 = 0.0959627041850983\n",
      "learning rate for this epoch =  0.09476426067531338\n",
      "Error on this batch = 0.04593752811561736\n",
      "Error on this batch = 0.07636123033430385\n",
      "Cost on val dataset after 776 epochs is = 0.09594432405331488\n",
      "Initial Cost on Val dataset for this epoch 776 = 0.09594432405331488\n",
      "learning rate for this epoch =  0.0947337161876474\n",
      "Error on this batch = 0.04590597295852309\n",
      "Error on this batch = 0.07632629100104381\n",
      "Cost on val dataset after 777 epochs is = 0.09592598760892165\n",
      "Initial Cost on Val dataset for this epoch 777 = 0.09592598760892165\n",
      "learning rate for this epoch =  0.09470322086221351\n",
      "Error on this batch = 0.045874501595292216\n",
      "Error on this batch = 0.07629143777877945\n",
      "Cost on val dataset after 778 epochs is = 0.0959076946804334\n",
      "Initial Cost on Val dataset for this epoch 778 = 0.0959076946804334\n",
      "learning rate for this epoch =  0.09467277455676439\n",
      "Error on this batch = 0.045843113748002685\n",
      "Error on this batch = 0.07625667035929996\n",
      "Cost on val dataset after 779 epochs is = 0.09588944509754409\n",
      "Initial Cost on Val dataset for this epoch 779 = 0.09588944509754409\n",
      "learning rate for this epoch =  0.0946423771296465\n",
      "Error on this batch = 0.045811809140032744\n",
      "Error on this batch = 0.07622198843565232\n",
      "Cost on val dataset after 780 epochs is = 0.09587123869111194\n",
      "Initial Cost on Val dataset for this epoch 780 = 0.09587123869111194\n",
      "learning rate for this epoch =  0.09461202843979676\n",
      "Error on this batch = 0.04578058749604129\n",
      "Error on this batch = 0.07618739170212149\n",
      "Cost on val dataset after 781 epochs is = 0.09585307529314475\n",
      "Initial Cost on Val dataset for this epoch 781 = 0.09585307529314475\n",
      "learning rate for this epoch =  0.09458172834673945\n",
      "Error on this batch = 0.04574944854194847\n",
      "Error on this batch = 0.07615287985421092\n",
      "Cost on val dataset after 782 epochs is = 0.09583495473678513\n",
      "Initial Cost on Val dataset for this epoch 782 = 0.09583495473678513\n",
      "learning rate for this epoch =  0.09455147671058287\n",
      "Error on this batch = 0.045718392004916834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on this batch = 0.07611845258862336\n",
      "Cost on val dataset after 783 epochs is = 0.0958168768562961\n",
      "Initial Cost on Val dataset for this epoch 783 = 0.0958168768562961\n",
      "learning rate for this epoch =  0.09452127339201631\n",
      "Error on this batch = 0.045687417613331914\n",
      "Error on this batch = 0.07608410960324187\n",
      "Cost on val dataset after 784 epochs is = 0.09579884148704669\n",
      "Initial Cost on Val dataset for this epoch 784 = 0.09579884148704669\n",
      "learning rate for this epoch =  0.0944911182523068\n",
      "Error on this batch = 0.04565652509678378\n",
      "Error on this batch = 0.07604985059711095\n",
      "Cost on val dataset after 785 epochs is = 0.09578084846549777\n",
      "Initial Cost on Val dataset for this epoch 785 = 0.09578084846549777\n",
      "learning rate for this epoch =  0.09446101115329605\n",
      "Error on this batch = 0.04562571418604842\n",
      "Error on this batch = 0.07601567527041811\n",
      "Cost on val dataset after 786 epochs is = 0.09576289762918791\n",
      "Initial Cost on Val dataset for this epoch 786 = 0.09576289762918791\n",
      "learning rate for this epoch =  0.09443095195739727\n",
      "Error on this batch = 0.04559498461306921\n",
      "Error on this batch = 0.0759815833244756\n",
      "Cost on val dataset after 787 epochs is = 0.09574498881671954\n",
      "Initial Cost on Val dataset for this epoch 787 = 0.09574498881671954\n",
      "learning rate for this epoch =  0.09440094052759214\n",
      "Error on this batch = 0.04556433611093907\n",
      "Error on this batch = 0.07594757446170236\n",
      "Cost on val dataset after 788 epochs is = 0.09572712186774529\n",
      "Initial Cost on Val dataset for this epoch 788 = 0.09572712186774529\n",
      "learning rate for this epoch =  0.09437097672742772\n",
      "Error on this batch = 0.045533768413882444\n",
      "Error on this batch = 0.07591364838560613\n",
      "Cost on val dataset after 789 epochs is = 0.09570929662295423\n",
      "Initial Cost on Val dataset for this epoch 789 = 0.09570929662295423\n",
      "learning rate for this epoch =  0.09434106042101341\n",
      "Error on this batch = 0.04550328125723759\n",
      "Error on this batch = 0.07587980480076599\n",
      "Cost on val dataset after 790 epochs is = 0.09569151292405866\n",
      "Initial Cost on Val dataset for this epoch 790 = 0.09569151292405866\n",
      "learning rate for this epoch =  0.09431119147301793\n",
      "Error on this batch = 0.0454728743774392\n",
      "Error on this batch = 0.07584604341281499\n",
      "Cost on val dataset after 791 epochs is = 0.09567377061378089\n",
      "Initial Cost on Val dataset for this epoch 791 = 0.09567377061378089\n",
      "learning rate for this epoch =  0.09428136974866627\n",
      "Error on this batch = 0.0454425475120012\n",
      "Error on this batch = 0.07581236392842311\n",
      "Cost on val dataset after 792 epochs is = 0.09565606953584012\n",
      "Initial Cost on Val dataset for this epoch 792 = 0.09565606953584012\n",
      "learning rate for this epoch =  0.09425159511373676\n",
      "Error on this batch = 0.04541230039949969\n",
      "Error on this batch = 0.07577876605528029\n",
      "Cost on val dataset after 793 epochs is = 0.0956384095349397\n",
      "Initial Cost on Val dataset for this epoch 793 = 0.0956384095349397\n",
      "learning rate for this epoch =  0.09422186743455808\n",
      "Error on this batch = 0.04538213277955631\n",
      "Error on this batch = 0.07574524950207995\n",
      "Cost on val dataset after 794 epochs is = 0.09562079045675442\n",
      "Initial Cost on Val dataset for this epoch 794 = 0.09562079045675442\n",
      "learning rate for this epoch =  0.0941921865780063\n",
      "Error on this batch = 0.045352044392821644\n",
      "Error on this batch = 0.07571181397850263\n",
      "Cost on val dataset after 795 epochs is = 0.09560321214791814\n",
      "Initial Cost on Val dataset for this epoch 795 = 0.09560321214791814\n",
      "learning rate for this epoch =  0.09416255241150198\n",
      "Error on this batch = 0.04532203498095894\n",
      "Error on this batch = 0.07567845919519965\n",
      "Cost on val dataset after 796 epochs is = 0.09558567445601146\n",
      "Initial Cost on Val dataset for this epoch 796 = 0.09558567445601146\n",
      "learning rate for this epoch =  0.09413296480300724\n",
      "Error on this batch = 0.04529210428662804\n",
      "Error on this batch = 0.0756451848637775\n",
      "Cost on val dataset after 797 epochs is = 0.09556817722954973\n",
      "Initial Cost on Val dataset for this epoch 797 = 0.09556817722954973\n",
      "learning rate for this epoch =  0.09410342362102285\n",
      "Error on this batch = 0.04526225205346957\n",
      "Error on this batch = 0.07561199069678196\n",
      "Cost on val dataset after 798 epochs is = 0.09555072031797113\n",
      "Initial Cost on Val dataset for this epoch 798 = 0.09555072031797113\n",
      "learning rate for this epoch =  0.09407392873458542\n",
      "Error on this batch = 0.04523247802608938\n",
      "Error on this batch = 0.07557887640768284\n",
      "Cost on val dataset after 799 epochs is = 0.09553330357162507\n",
      "Initial Cost on Val dataset for this epoch 799 = 0.09553330357162507\n",
      "learning rate for this epoch =  0.09404448001326453\n",
      "Error on this batch = 0.045202781950043146\n",
      "Error on this batch = 0.07554584171085851\n",
      "Cost on val dataset after 800 epochs is = 0.09551592684176075\n",
      "Initial Cost on Val dataset for this epoch 800 = 0.09551592684176075\n",
      "learning rate for this epoch =  0.09401507732715984\n",
      "Error on this batch = 0.04517316357182103\n",
      "Error on this batch = 0.07551288632158143\n",
      "Cost on val dataset after 801 epochs is = 0.09549858998051583\n",
      "Initial Cost on Val dataset for this epoch 801 = 0.09549858998051583\n",
      "learning rate for this epoch =  0.09398572054689833\n",
      "Error on this batch = 0.04514362263883305\n",
      "Error on this batch = 0.07548000995600289\n",
      "Cost on val dataset after 802 epochs is = 0.09548129284090545\n",
      "Initial Cost on Val dataset for this epoch 802 = 0.09548129284090545\n",
      "learning rate for this epoch =  0.09395640954363149\n",
      "Error on this batch = 0.045114158899394054\n",
      "Error on this batch = 0.07544721233113888\n",
      "Cost on val dataset after 803 epochs is = 0.09546403527681145\n",
      "Initial Cost on Val dataset for this epoch 803 = 0.09546403527681145\n",
      "learning rate for this epoch =  0.09392714418903259\n",
      "Error on this batch = 0.045084772102709496\n",
      "Error on this batch = 0.07541449316485578\n",
      "Cost on val dataset after 804 epochs is = 0.09544681714297167\n",
      "Initial Cost on Val dataset for this epoch 804 = 0.09544681714297167\n",
      "learning rate for this epoch =  0.0938979243552938\n",
      "Error on this batch = 0.045055461998860846\n",
      "Error on this batch = 0.07538185217585629\n",
      "Cost on val dataset after 805 epochs is = 0.09542963829496945\n",
      "Initial Cost on Val dataset for this epoch 805 = 0.09542963829496945\n",
      "learning rate for this epoch =  0.0938687499151236\n",
      "Error on this batch = 0.04502622833879177\n",
      "Error on this batch = 0.07534928908366559\n",
      "Cost on val dataset after 806 epochs is = 0.09541249858922356\n",
      "Initial Cost on Val dataset for this epoch 806 = 0.09541249858922356\n",
      "learning rate for this epoch =  0.09383962074174393\n",
      "Error on this batch = 0.04499707087429395\n",
      "Error on this batch = 0.07531680360861798\n",
      "Cost on val dataset after 807 epochs is = 0.09539539788297813\n",
      "Initial Cost on Val dataset for this epoch 807 = 0.09539539788297813\n",
      "learning rate for this epoch =  0.09381053670888753\n",
      "Error on this batch = 0.044967989357993614\n",
      "Error on this batch = 0.07528439547184343\n",
      "Cost on val dataset after 808 epochs is = 0.09537833603429272\n",
      "Initial Cost on Val dataset for this epoch 808 = 0.09537833603429272\n",
      "learning rate for this epoch =  0.09378149769079532\n",
      "Error on this batch = 0.04493898354333796\n",
      "Error on this batch = 0.07525206439525452\n",
      "Cost on val dataset after 809 epochs is = 0.09536131290203295\n",
      "Initial Cost on Val dataset for this epoch 809 = 0.09536131290203295\n",
      "learning rate for this epoch =  0.09375250356221361\n",
      "Error on this batch = 0.0449100531845818\n",
      "Error on this batch = 0.0752198101015336\n",
      "Cost on val dataset after 810 epochs is = 0.09534432834586082\n",
      "Initial Cost on Val dataset for this epoch 810 = 0.09534432834586082\n",
      "learning rate for this epoch =  0.09372355419839151\n",
      "Error on this batch = 0.04488119803677454\n",
      "Error on this batch = 0.07518763231412023\n",
      "Cost on val dataset after 811 epochs is = 0.09532738222622568\n",
      "Initial Cost on Val dataset for this epoch 811 = 0.09532738222622568\n",
      "learning rate for this epoch =  0.09369464947507833\n",
      "Error on this batch = 0.04485241785574703\n",
      "Error on this batch = 0.07515553075719876\n",
      "Cost on val dataset after 812 epochs is = 0.09531047440435514\n",
      "Initial Cost on Val dataset for this epoch 812 = 0.09531047440435514\n",
      "learning rate for this epoch =  0.09366578926852086\n",
      "Error on this batch = 0.044823712398098986\n",
      "Error on this batch = 0.07512350515568614\n",
      "Cost on val dataset after 813 epochs is = 0.0952936047422463\n",
      "Initial Cost on Val dataset for this epoch 813 = 0.0952936047422463\n",
      "learning rate for this epoch =  0.09363697345546085\n",
      "Error on this batch = 0.04479508142118634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on this batch = 0.07509155523522011\n",
      "Cost on val dataset after 814 epochs is = 0.09527677310265706\n",
      "Initial Cost on Val dataset for this epoch 814 = 0.09527677310265706\n",
      "learning rate for this epoch =  0.09360820191313243\n",
      "Error on this batch = 0.04476652468310876\n",
      "Error on this batch = 0.07505968072214746\n",
      "Cost on val dataset after 815 epochs is = 0.09525997934909779\n",
      "Initial Cost on Val dataset for this epoch 815 = 0.09525997934909779\n",
      "learning rate for this epoch =  0.09357947451925948\n",
      "Error on this batch = 0.04473804194269745\n",
      "Error on this batch = 0.07502788134351257\n",
      "Cost on val dataset after 816 epochs is = 0.09524322334582293\n",
      "Initial Cost on Val dataset for this epoch 816 = 0.09524322334582293\n",
      "learning rate for this epoch =  0.09355079115205313\n",
      "Error on this batch = 0.04470963295950297\n",
      "Error on this batch = 0.07499615682704627\n",
      "Cost on val dataset after 817 epochs is = 0.09522650495782316\n",
      "Initial Cost on Val dataset for this epoch 817 = 0.09522650495782316\n",
      "learning rate for this epoch =  0.09352215169020917\n",
      "Error on this batch = 0.04468129749378336\n",
      "Error on this batch = 0.0749645069011547\n",
      "Cost on val dataset after 818 epochs is = 0.09520982405081732\n",
      "Initial Cost on Val dataset for this epoch 818 = 0.09520982405081732\n",
      "learning rate for this epoch =  0.09349355601290561\n",
      "Error on this batch = 0.04465303530649226\n",
      "Error on this batch = 0.07493293129490888\n",
      "Cost on val dataset after 819 epochs is = 0.0951931804912449\n",
      "Initial Cost on Val dataset for this epoch 819 = 0.0951931804912449\n",
      "learning rate for this epoch =  0.09346500399980012\n",
      "Error on this batch = 0.044624846159267345\n",
      "Error on this batch = 0.0749014297380338\n",
      "Cost on val dataset after 820 epochs is = 0.09517657414625835\n",
      "Initial Cost on Val dataset for this epoch 820 = 0.09517657414625835\n",
      "learning rate for this epoch =  0.09343649553102754\n",
      "Error on this batch = 0.04459672981441864\n",
      "Error on this batch = 0.07487000196089849\n",
      "Cost on val dataset after 821 epochs is = 0.09516000488371598\n",
      "Initial Cost on Val dataset for this epoch 821 = 0.09516000488371598\n",
      "learning rate for this epoch =  0.0934080304871974\n",
      "Error on this batch = 0.04456868603491744\n",
      "Error on this batch = 0.07483864769450571\n",
      "Cost on val dataset after 822 epochs is = 0.09514347257217462\n",
      "Initial Cost on Val dataset for this epoch 822 = 0.09514347257217462\n",
      "learning rate for this epoch =  0.09337960874939158\n",
      "Error on this batch = 0.04454071458438472\n",
      "Error on this batch = 0.07480736667048239\n",
      "Cost on val dataset after 823 epochs is = 0.0951269770808828\n",
      "Initial Cost on Val dataset for this epoch 823 = 0.0951269770808828\n",
      "learning rate for this epoch =  0.09335123019916165\n",
      "Error on this batch = 0.04451281522708032\n",
      "Error on this batch = 0.07477615862106983\n",
      "Cost on val dataset after 824 epochs is = 0.09511051827977376\n",
      "Initial Cost on Val dataset for this epoch 824 = 0.09511051827977376\n",
      "learning rate for this epoch =  0.09332289471852671\n",
      "Error on this batch = 0.04448498772789175\n",
      "Error on this batch = 0.07474502327911446\n",
      "Cost on val dataset after 825 epochs is = 0.09509409603945904\n",
      "Initial Cost on Val dataset for this epoch 825 = 0.09509409603945904\n",
      "learning rate for this epoch =  0.09329460218997072\n",
      "Error on this batch = 0.04445723185232344\n",
      "Error on this batch = 0.07471396037805875\n",
      "Cost on val dataset after 826 epochs is = 0.09507771023122179\n",
      "Initial Cost on Val dataset for this epoch 826 = 0.09507771023122179\n",
      "learning rate for this epoch =  0.09326635249644033\n",
      "Error on this batch = 0.04442954736648602\n",
      "Error on this batch = 0.07468296965193232\n",
      "Cost on val dataset after 827 epochs is = 0.09506136072701059\n",
      "Initial Cost on Val dataset for this epoch 827 = 0.09506136072701059\n",
      "learning rate for this epoch =  0.09323814552134235\n",
      "Error on this batch = 0.04440193403708552\n",
      "Error on this batch = 0.07465205083534324\n",
      "Cost on val dataset after 828 epochs is = 0.09504504739943327\n",
      "Initial Cost on Val dataset for this epoch 828 = 0.09504504739943327\n",
      "learning rate for this epoch =  0.09320998114854143\n",
      "Error on this batch = 0.044374391631413095\n",
      "Error on this batch = 0.07462120366346979\n",
      "Cost on val dataset after 829 epochs is = 0.09502877012175082\n",
      "Initial Cost on Val dataset for this epoch 829 = 0.09502877012175082\n",
      "learning rate for this epoch =  0.09318185926235777\n",
      "Error on this batch = 0.04434691991733446\n",
      "Error on this batch = 0.07459042787205204\n",
      "Cost on val dataset after 830 epochs is = 0.09501252876787171\n",
      "Initial Cost on Val dataset for this epoch 830 = 0.09501252876787171\n",
      "learning rate for this epoch =  0.09315377974756468\n",
      "Error on this batch = 0.04431951866327962\n",
      "Error on this batch = 0.07455972319738415\n",
      "Cost on val dataset after 831 epochs is = 0.09499632321234597\n",
      "Initial Cost on Val dataset for this epoch 831 = 0.09499632321234597\n",
      "learning rate for this epoch =  0.09312574248938638\n",
      "Error on this batch = 0.04429218763823275\n",
      "Error on this batch = 0.07452908937630649\n",
      "Cost on val dataset after 832 epochs is = 0.09498015333035983\n",
      "Initial Cost on Val dataset for this epoch 832 = 0.09498015333035983\n",
      "learning rate for this epoch =  0.0930977473734956\n",
      "Error on this batch = 0.044264926611721975\n",
      "Error on this batch = 0.07449852614619838\n",
      "Cost on val dataset after 833 epochs is = 0.09496401899773028\n",
      "Initial Cost on Val dataset for this epoch 833 = 0.09496401899773028\n",
      "learning rate for this epoch =  0.09306979428601131\n",
      "Error on this batch = 0.04423773535380945\n",
      "Error on this batch = 0.07446803324497057\n",
      "Cost on val dataset after 834 epochs is = 0.09494792009089961\n",
      "Initial Cost on Val dataset for this epoch 834 = 0.09494792009089961\n",
      "learning rate for this epoch =  0.0930418831134965\n",
      "Error on this batch = 0.04421061363508153\n",
      "Error on this batch = 0.07443761041105855\n",
      "Cost on val dataset after 835 epochs is = 0.09493185648693041\n",
      "Initial Cost on Val dataset for this epoch 835 = 0.09493185648693041\n",
      "learning rate for this epoch =  0.09301401374295587\n",
      "Error on this batch = 0.044183561226638836\n",
      "Error on this batch = 0.07440725738341554\n",
      "Cost on val dataset after 836 epochs is = 0.09491582806350066\n",
      "Initial Cost on Val dataset for this epoch 836 = 0.09491582806350066\n",
      "learning rate for this epoch =  0.09298618606183358\n",
      "Error on this batch = 0.04415657790008664\n",
      "Error on this batch = 0.07437697390150604\n",
      "Cost on val dataset after 837 epochs is = 0.09489983469889864\n",
      "Initial Cost on Val dataset for this epoch 837 = 0.09489983469889864\n",
      "learning rate for this epoch =  0.09295839995801103\n",
      "Error on this batch = 0.044129663427525216\n",
      "Error on this batch = 0.07434675970529961\n",
      "Cost on val dataset after 838 epochs is = 0.09488387627201827\n",
      "Initial Cost on Val dataset for this epoch 838 = 0.09488387627201827\n",
      "learning rate for this epoch =  0.09293065531980463\n",
      "Error on this batch = 0.04410281758154028\n",
      "Error on this batch = 0.07431661453526467\n",
      "Cost on val dataset after 839 epochs is = 0.09486795266235454\n",
      "Initial Cost on Val dataset for this epoch 839 = 0.09486795266235454\n",
      "learning rate for this epoch =  0.09290295203596367\n",
      "Error on this batch = 0.044076040135193624\n",
      "Error on this batch = 0.07428653813236276\n",
      "Cost on val dataset after 840 epochs is = 0.09485206374999891\n",
      "Initial Cost on Val dataset for this epoch 840 = 0.09485206374999891\n",
      "learning rate for this epoch =  0.09287528999566799\n",
      "Error on this batch = 0.044049330862013694\n",
      "Error on this batch = 0.07425653023804296\n",
      "Cost on val dataset after 841 epochs is = 0.09483620941563488\n",
      "Initial Cost on Val dataset for this epoch 841 = 0.09483620941563488\n",
      "learning rate for this epoch =  0.09284766908852593\n",
      "Error on this batch = 0.04402268953598634\n",
      "Error on this batch = 0.07422659059423635\n",
      "Cost on val dataset after 842 epochs is = 0.09482038954053382\n",
      "Initial Cost on Val dataset for this epoch 842 = 0.09482038954053382\n",
      "learning rate for this epoch =  0.09282008920457209\n",
      "Error on this batch = 0.043996115931545614\n",
      "Error on this batch = 0.07419671894335107\n",
      "Cost on val dataset after 843 epochs is = 0.09480460400655079\n",
      "Initial Cost on Val dataset for this epoch 843 = 0.09480460400655079\n",
      "learning rate for this epoch =  0.09279255023426522\n",
      "Error on this batch = 0.04396960982356474\n",
      "Error on this batch = 0.07416691502826735\n",
      "Cost on val dataset after 844 epochs is = 0.09478885269612032\n",
      "Initial Cost on Val dataset for this epoch 844 = 0.09478885269612032\n",
      "learning rate for this epoch =  0.09276505206848605\n",
      "Error on this batch = 0.04394317098734701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on this batch = 0.07413717859233268\n",
      "Cost on val dataset after 845 epochs is = 0.09477313549225258\n",
      "Initial Cost on Val dataset for this epoch 845 = 0.09477313549225258\n",
      "learning rate for this epoch =  0.09273759459853521\n",
      "Error on this batch = 0.04391679919861694\n",
      "Error on this batch = 0.07410750937935758\n",
      "Cost on val dataset after 846 epochs is = 0.09475745227852954\n",
      "Initial Cost on Val dataset for this epoch 846 = 0.09475745227852954\n",
      "learning rate for this epoch =  0.0927101777161311\n",
      "Error on this batch = 0.04389049423351121\n",
      "Error on this batch = 0.07407790713361122\n",
      "Cost on val dataset after 847 epochs is = 0.0947418029391011\n",
      "Initial Cost on Val dataset for this epoch 847 = 0.0947418029391011\n",
      "learning rate for this epoch =  0.09268280131340775\n",
      "Error on this batch = 0.043864255868570255\n",
      "Error on this batch = 0.07404837159981748\n",
      "Cost on val dataset after 848 epochs is = 0.09472618735868144\n",
      "Initial Cost on Val dataset for this epoch 848 = 0.09472618735868144\n",
      "learning rate for this epoch =  0.09265546528291284\n",
      "Error on this batch = 0.04383808388072927\n",
      "Error on this batch = 0.07401890252315116\n",
      "Cost on val dataset after 849 epochs is = 0.09471060542254552\n",
      "Initial Cost on Val dataset for this epoch 849 = 0.09471060542254552\n",
      "learning rate for this epoch =  0.09262816951760545\n",
      "Error on this batch = 0.043811978047309685\n",
      "Error on this batch = 0.07398949964923443\n",
      "Cost on val dataset after 850 epochs is = 0.09469505701652549\n",
      "Initial Cost on Val dataset for this epoch 850 = 0.09469505701652549\n",
      "learning rate for this epoch =  0.09260091391085426\n",
      "Error on this batch = 0.04378593814601068\n",
      "Error on this batch = 0.07396016272413346\n",
      "Cost on val dataset after 851 epochs is = 0.09467954202700732\n",
      "Initial Cost on Val dataset for this epoch 851 = 0.09467954202700732\n",
      "learning rate for this epoch =  0.09257369835643524\n",
      "Error on this batch = 0.04375996395490077\n",
      "Error on this batch = 0.07393089149435533\n",
      "Cost on val dataset after 852 epochs is = 0.09466406034092748\n",
      "Initial Cost on Val dataset for this epoch 852 = 0.09466406034092748\n",
      "learning rate for this epoch =  0.09254652274852981\n",
      "Error on this batch = 0.0437340552524094\n",
      "Error on this batch = 0.07390168570684516\n",
      "Cost on val dataset after 853 epochs is = 0.0946486118457697\n",
      "Initial Cost on Val dataset for this epoch 853 = 0.0946486118457697\n",
      "learning rate for this epoch =  0.0925193869817227\n",
      "Error on this batch = 0.043708211817318716\n",
      "Error on this batch = 0.07387254510898346\n",
      "Cost on val dataset after 854 epochs is = 0.09463319642956174\n",
      "Initial Cost on Val dataset for this epoch 854 = 0.09463319642956174\n",
      "learning rate for this epoch =  0.092492290951\n",
      "Error on this batch = 0.043682433428755336\n",
      "Error on this batch = 0.07384346944858355\n",
      "Cost on val dataset after 855 epochs is = 0.09461781398087236\n",
      "Initial Cost on Val dataset for this epoch 855 = 0.09461781398087236\n",
      "learning rate for this epoch =  0.09246523455174717\n",
      "Error on this batch = 0.043656719866182296\n",
      "Error on this batch = 0.07381445847388944\n",
      "Cost on val dataset after 856 epochs is = 0.09460246438880827\n",
      "Initial Cost on Val dataset for this epoch 856 = 0.09460246438880827\n",
      "learning rate for this epoch =  0.092438217679747\n",
      "Error on this batch = 0.043631070909391105\n",
      "Error on this batch = 0.07378551193357372\n",
      "Cost on val dataset after 857 epochs is = 0.0945871475430111\n",
      "Initial Cost on Val dataset for this epoch 857 = 0.0945871475430111\n",
      "learning rate for this epoch =  0.09241124023117771\n",
      "Error on this batch = 0.043605486338493656\n",
      "Error on this batch = 0.07375662957673568\n",
      "Cost on val dataset after 858 epochs is = 0.09457186333365468\n",
      "Initial Cost on Val dataset for this epoch 858 = 0.09457186333365468\n",
      "learning rate for this epoch =  0.09238430210261095\n",
      "Error on this batch = 0.043579965933914624\n",
      "Error on this batch = 0.07372781115289992\n",
      "Cost on val dataset after 859 epochs is = 0.09455661165144201\n",
      "Initial Cost on Val dataset for this epoch 859 = 0.09455661165144201\n",
      "learning rate for this epoch =  0.09235740319100984\n",
      "Error on this batch = 0.043554509476383625\n",
      "Error on this batch = 0.07369905641201457\n",
      "Cost on val dataset after 860 epochs is = 0.09454139238760258\n",
      "Initial Cost on Val dataset for this epoch 860 = 0.09454139238760258\n",
      "learning rate for this epoch =  0.09233054339372707\n",
      "Error on this batch = 0.04352911674692745\n",
      "Error on this batch = 0.07367036510445049\n",
      "Cost on val dataset after 861 epochs is = 0.09452620543388965\n",
      "Initial Cost on Val dataset for this epoch 861 = 0.09452620543388965\n",
      "learning rate for this epoch =  0.09230372260850297\n",
      "Error on this batch = 0.043503787526862896\n",
      "Error on this batch = 0.07364173698099989\n",
      "Cost on val dataset after 862 epochs is = 0.09451105068257754\n",
      "Initial Cost on Val dataset for this epoch 862 = 0.09451105068257754\n",
      "learning rate for this epoch =  0.09227694073346356\n",
      "Error on this batch = 0.0434785215977889\n",
      "Error on this batch = 0.0736131717928757\n",
      "Cost on val dataset after 863 epochs is = 0.09449592802645913\n",
      "Initial Cost on Val dataset for this epoch 863 = 0.09449592802645913\n",
      "learning rate for this epoch =  0.09225019766711869\n",
      "Error on this batch = 0.043453318741579566\n",
      "Error on this batch = 0.07358466929171095\n",
      "Cost on val dataset after 864 epochs is = 0.09448083735884315\n",
      "Initial Cost on Val dataset for this epoch 864 = 0.09448083735884315\n",
      "learning rate for this epoch =  0.09222349330836013\n",
      "Error on this batch = 0.04342817874037669\n",
      "Error on this batch = 0.0735562292295582\n",
      "Cost on val dataset after 865 epochs is = 0.09446577857355173\n",
      "Initial Cost on Val dataset for this epoch 865 = 0.09446577857355173\n",
      "learning rate for this epoch =  0.09219682755645973\n",
      "Error on this batch = 0.04340310137658277\n",
      "Error on this batch = 0.07352785135888952\n",
      "Cost on val dataset after 866 epochs is = 0.09445075156491796\n",
      "Initial Cost on Val dataset for this epoch 866 = 0.09445075156491796\n",
      "learning rate for this epoch =  0.09217020031106751\n",
      "Error on this batch = 0.04337808643285392\n",
      "Error on this batch = 0.07349953543259617\n",
      "Cost on val dataset after 867 epochs is = 0.09443575622778337\n",
      "Initial Cost on Val dataset for this epoch 867 = 0.09443575622778337\n",
      "learning rate for this epoch =  0.09214361147220981\n",
      "Error on this batch = 0.04335313369209292\n",
      "Error on this batch = 0.07347128120398899\n",
      "Cost on val dataset after 868 epochs is = 0.09442079245749568\n",
      "Initial Cost on Val dataset for this epoch 868 = 0.09442079245749568\n",
      "learning rate for this epoch =  0.09211706094028746\n",
      "Error on this batch = 0.0433282429374426\n",
      "Error on this batch = 0.0734430884267984\n",
      "Cost on val dataset after 869 epochs is = 0.09440586014990622\n",
      "Initial Cost on Val dataset for this epoch 869 = 0.09440586014990622\n",
      "learning rate for this epoch =  0.09209054861607395\n",
      "Error on this batch = 0.04330341395227892\n",
      "Error on this batch = 0.07341495685517516\n",
      "Cost on val dataset after 870 epochs is = 0.09439095920136781\n",
      "Initial Cost on Val dataset for this epoch 870 = 0.09439095920136781\n",
      "learning rate for this epoch =  0.0920640744007136\n",
      "Error on this batch = 0.04327864652020466\n",
      "Error on this batch = 0.0733868862436908\n",
      "Cost on val dataset after 871 epochs is = 0.09437608950873232\n",
      "Initial Cost on Val dataset for this epoch 871 = 0.09437608950873232\n",
      "learning rate for this epoch =  0.09203763819571976\n",
      "Error on this batch = 0.04325394042504274\n",
      "Error on this batch = 0.07335887634733866\n",
      "Cost on val dataset after 872 epochs is = 0.09436125096934858\n",
      "Initial Cost on Val dataset for this epoch 872 = 0.09436125096934858\n",
      "learning rate for this epoch =  0.09201123990297302\n",
      "Error on this batch = 0.043229295450830034\n",
      "Error on this batch = 0.07333092692153458\n",
      "Cost on val dataset after 873 epochs is = 0.09434644348105992\n",
      "Initial Cost on Val dataset for this epoch 873 = 0.09434644348105992\n",
      "learning rate for this epoch =  0.09198487942471935\n",
      "Error on this batch = 0.043204711381811124\n",
      "Error on this batch = 0.07330303772211826\n",
      "Cost on val dataset after 874 epochs is = 0.09433166694220213\n",
      "Initial Cost on Val dataset for this epoch 874 = 0.09433166694220213\n",
      "learning rate for this epoch =  0.09195855666356846\n",
      "Error on this batch = 0.04318018800243237\n",
      "Error on this batch = 0.0732752085053545\n",
      "Cost on val dataset after 875 epochs is = 0.09431692125160122\n",
      "Initial Cost on Val dataset for this epoch 875 = 0.09431692125160122\n",
      "learning rate for this epoch =  0.09193227152249185\n",
      "Error on this batch = 0.04315572509733571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on this batch = 0.07324743902793468\n",
      "Cost on val dataset after 876 epochs is = 0.0943022063085712\n",
      "Initial Cost on Val dataset for this epoch 876 = 0.0943022063085712\n",
      "learning rate for this epoch =  0.09190602390482124\n",
      "Error on this batch = 0.04313132245135311\n",
      "Error on this batch = 0.07321972904697821\n",
      "Cost on val dataset after 877 epochs is = 0.09428752201291199\n",
      "Initial Cost on Val dataset for this epoch 877 = 0.09428752201291199\n",
      "learning rate for this epoch =  0.09187981371424671\n",
      "Error on this batch = 0.04310697984950077\n",
      "Error on this batch = 0.07319207832003445\n",
      "Cost on val dataset after 878 epochs is = 0.09427286826490723\n",
      "Initial Cost on Val dataset for this epoch 878 = 0.09427286826490723\n",
      "learning rate for this epoch =  0.091853640854815\n",
      "Error on this batch = 0.04308269707697363\n",
      "Error on this batch = 0.07316448660508451\n",
      "Cost on val dataset after 879 epochs is = 0.09425824496532224\n",
      "Initial Cost on Val dataset for this epoch 879 = 0.09425824496532224\n",
      "learning rate for this epoch =  0.09182750523092773\n",
      "Error on this batch = 0.04305847391914003\n",
      "Error on this batch = 0.07313695366054336\n",
      "Cost on val dataset after 880 epochs is = 0.09424365201540183\n",
      "Initial Cost on Val dataset for this epoch 880 = 0.09424365201540183\n",
      "learning rate for this epoch =  0.0918014067473398\n",
      "Error on this batch = 0.04303431016153637\n",
      "Error on this batch = 0.07310947924526175\n",
      "Cost on val dataset after 881 epochs is = 0.09422908931686826\n",
      "Initial Cost on Val dataset for this epoch 881 = 0.09422908931686826\n",
      "learning rate for this epoch =  0.09177534530915758\n",
      "Error on this batch = 0.0430102055898621\n",
      "Error on this batch = 0.07308206311852877\n",
      "Cost on val dataset after 882 epochs is = 0.09421455677191903\n",
      "Initial Cost on Val dataset for this epoch 882 = 0.09421455677191903\n",
      "learning rate for this epoch =  0.09174932082183727\n",
      "Error on this batch = 0.04298615998997482\n",
      "Error on this batch = 0.07305470504007415\n",
      "Cost on val dataset after 883 epochs is = 0.094200054283225\n",
      "Initial Cost on Val dataset for this epoch 883 = 0.094200054283225\n",
      "learning rate for this epoch =  0.09172333319118318\n",
      "Error on this batch = 0.0429621731478854\n",
      "Error on this batch = 0.07302740477007064\n",
      "Cost on val dataset after 884 epochs is = 0.09418558175392812\n",
      "Initial Cost on Val dataset for this epoch 884 = 0.09418558175392812\n",
      "learning rate for this epoch =  0.0916973823233461\n",
      "Error on this batch = 0.042938244849753344\n",
      "Error on this batch = 0.07300016206913688\n",
      "Cost on val dataset after 885 epochs is = 0.09417113908763948\n",
      "Initial Cost on Val dataset for this epoch 885 = 0.09417113908763948\n",
      "learning rate for this epoch =  0.09167146812482159\n",
      "Error on this batch = 0.04291437488188241\n",
      "Error on this batch = 0.07297297669833999\n",
      "Cost on val dataset after 886 epochs is = 0.09415672618843708\n",
      "Initial Cost on Val dataset for this epoch 886 = 0.09415672618843708\n",
      "learning rate for this epoch =  0.09164559050244833\n",
      "Error on this batch = 0.04289056303071634\n",
      "Error on this batch = 0.0729458484191985\n",
      "Cost on val dataset after 887 epochs is = 0.094142342960864\n",
      "Initial Cost on Val dataset for this epoch 887 = 0.094142342960864\n",
      "learning rate for this epoch =  0.09161974936340654\n",
      "Error on this batch = 0.04286680908283449\n",
      "Error on this batch = 0.07291877699368532\n",
      "Cost on val dataset after 888 epochs is = 0.09412798930992607\n",
      "Initial Cost on Val dataset for this epoch 888 = 0.09412798930992607\n",
      "learning rate for this epoch =  0.09159394461521626\n",
      "Error on this batch = 0.042843112824948085\n",
      "Error on this batch = 0.07289176218423067\n",
      "Cost on val dataset after 889 epochs is = 0.09411366514108993\n",
      "Initial Cost on Val dataset for this epoch 889 = 0.09411366514108993\n",
      "learning rate for this epoch =  0.09156817616573577\n",
      "Error on this batch = 0.04281947404389632\n",
      "Error on this batch = 0.07286480375372537\n",
      "Cost on val dataset after 890 epochs is = 0.09409937036028089\n",
      "Initial Cost on Val dataset for this epoch 890 = 0.09409937036028089\n",
      "learning rate for this epoch =  0.09154244392315995\n",
      "Error on this batch = 0.042795892526642676\n",
      "Error on this batch = 0.07283790146552399\n",
      "Cost on val dataset after 891 epochs is = 0.09408510487388087\n",
      "Initial Cost on Val dataset for this epoch 891 = 0.09408510487388087\n",
      "learning rate for this epoch =  0.09151674779601875\n",
      "Error on this batch = 0.0427723680602716\n",
      "Error on this batch = 0.07281105508344816\n",
      "Cost on val dataset after 892 epochs is = 0.09407086858872626\n",
      "Initial Cost on Val dataset for this epoch 892 = 0.09407086858872626\n",
      "learning rate for this epoch =  0.0914910876931754\n",
      "Error on this batch = 0.04274890043198504\n",
      "Error on this batch = 0.07278426437179009\n",
      "Cost on val dataset after 893 epochs is = 0.09405666141210581\n",
      "Initial Cost on Val dataset for this epoch 893 = 0.09405666141210581\n",
      "learning rate for this epoch =  0.09146546352382512\n",
      "Error on this batch = 0.042725489429099435\n",
      "Error on this batch = 0.07275752909531578\n",
      "Cost on val dataset after 894 epochs is = 0.09404248325175861\n",
      "Initial Cost on Val dataset for this epoch 894 = 0.09404248325175861\n",
      "learning rate for this epoch =  0.09143987519749323\n",
      "Error on this batch = 0.042702134839042866\n",
      "Error on this batch = 0.07273084901926889\n",
      "Cost on val dataset after 895 epochs is = 0.09402833401587177\n",
      "Initial Cost on Val dataset for this epoch 895 = 0.09402833401587177\n",
      "learning rate for this epoch =  0.09141432262403383\n",
      "Error on this batch = 0.042678836449352206\n",
      "Error on this batch = 0.07270422390937405\n",
      "Cost on val dataset after 896 epochs is = 0.0940142136130784\n",
      "Initial Cost on Val dataset for this epoch 896 = 0.0940142136130784\n",
      "learning rate for this epoch =  0.09138880571362809\n",
      "Error on this batch = 0.042655594047670484\n",
      "Error on this batch = 0.07267765353184066\n",
      "Cost on val dataset after 897 epochs is = 0.09400012195245544\n",
      "Initial Cost on Val dataset for this epoch 897 = 0.09400012195245544\n",
      "learning rate for this epoch =  0.09136332437678274\n",
      "Error on this batch = 0.042632407421744775\n",
      "Error on this batch = 0.07265113765336662\n",
      "Cost on val dataset after 898 epochs is = 0.09398605894352147\n",
      "Initial Cost on Val dataset for this epoch 898 = 0.09398605894352147\n",
      "learning rate for this epoch =  0.09133787852432855\n",
      "Error on this batch = 0.042609276359423756\n",
      "Error on this batch = 0.07262467604114202\n",
      "Cost on val dataset after 899 epochs is = 0.09397202449623444\n",
      "Initial Cost on Val dataset for this epoch 899 = 0.09397202449623444\n",
      "learning rate for this epoch =  0.09131246806741879\n",
      "Error on this batch = 0.04258620064865587\n",
      "Error on this batch = 0.07259826846285304\n",
      "Cost on val dataset after 900 epochs is = 0.0939580185209896\n",
      "Initial Cost on Val dataset for this epoch 900 = 0.0939580185209896\n",
      "learning rate for this epoch =  0.09128709291752768\n",
      "Error on this batch = 0.04256318007748748\n",
      "Error on this batch = 0.07257191468668574\n",
      "Cost on val dataset after 901 epochs is = 0.09394404092861712\n",
      "Initial Cost on Val dataset for this epoch 901 = 0.09394404092861712\n",
      "learning rate for this epoch =  0.09126175298644894\n",
      "Error on this batch = 0.04254021443406126\n",
      "Error on this batch = 0.07254561448133001\n",
      "Cost on val dataset after 902 epochs is = 0.09393009163038003\n",
      "Initial Cost on Val dataset for this epoch 902 = 0.09393009163038003\n",
      "learning rate for this epoch =  0.09123644818629414\n",
      "Error on this batch = 0.04251730350661487\n",
      "Error on this batch = 0.07251936761598349\n",
      "Cost on val dataset after 903 epochs is = 0.09391617053797167\n",
      "Initial Cost on Val dataset for this epoch 903 = 0.09391617053797167\n",
      "learning rate for this epoch =  0.09121117842949143\n",
      "Error on this batch = 0.04249444708347954\n",
      "Error on this batch = 0.0724931738603555\n",
      "Cost on val dataset after 904 epochs is = 0.0939022775635137\n",
      "Initial Cost on Val dataset for this epoch 904 = 0.0939022775635137\n",
      "learning rate for this epoch =  0.09118594362878382\n",
      "Error on this batch = 0.04247164495307937\n",
      "Error on this batch = 0.07246703298467104\n",
      "Cost on val dataset after 905 epochs is = 0.09388841261955355\n",
      "Initial Cost on Val dataset for this epoch 905 = 0.09388841261955355\n",
      "learning rate for this epoch =  0.09116074369722786\n",
      "Error on this batch = 0.04244889690393025\n",
      "Error on this batch = 0.07244094475967479\n",
      "Cost on val dataset after 906 epochs is = 0.09387457561906228\n",
      "Initial Cost on Val dataset for this epoch 906 = 0.09387457561906228\n",
      "learning rate for this epoch =  0.09113557854819211\n",
      "Error on this batch = 0.0424262027246394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on this batch = 0.07241490895663515\n",
      "Cost on val dataset after 907 epochs is = 0.09386076647543204\n",
      "Initial Cost on Val dataset for this epoch 907 = 0.09386076647543204\n",
      "learning rate for this epoch =  0.09111044809535562\n",
      "Error on this batch = 0.042403562203904815\n",
      "Error on this batch = 0.07238892534734825\n",
      "Cost on val dataset after 908 epochs is = 0.09384698510247381\n",
      "Initial Cost on Val dataset for this epoch 908 = 0.09384698510247381\n",
      "learning rate for this epoch =  0.09108535225270664\n",
      "Error on this batch = 0.04238097513051527\n",
      "Error on this batch = 0.07236299370414208\n",
      "Cost on val dataset after 909 epochs is = 0.093833231414415\n",
      "Initial Cost on Val dataset for this epoch 909 = 0.093833231414415\n",
      "learning rate for this epoch =  0.09106029093454096\n",
      "Error on this batch = 0.04235844129335014\n",
      "Error on this batch = 0.07233711379988048\n",
      "Cost on val dataset after 910 epochs is = 0.0938195053258969\n",
      "Initial Cost on Val dataset for this epoch 910 = 0.0938195053258969\n",
      "learning rate for this epoch =  0.09103526405546067\n",
      "Error on this batch = 0.04233596048137972\n",
      "Error on this batch = 0.07231128540796707\n",
      "Cost on val dataset after 911 epochs is = 0.09380580675197234\n",
      "Initial Cost on Val dataset for this epoch 911 = 0.09380580675197234\n",
      "learning rate for this epoch =  0.09101027153037257\n",
      "Error on this batch = 0.042313532483665525\n",
      "Error on this batch = 0.07228550830234952\n",
      "Cost on val dataset after 912 epochs is = 0.09379213560810323\n",
      "Initial Cost on Val dataset for this epoch 912 = 0.09379213560810323\n",
      "learning rate for this epoch =  0.09098531327448692\n",
      "Error on this batch = 0.042291157089361014\n",
      "Error on this batch = 0.07225978225752351\n",
      "Cost on val dataset after 913 epochs is = 0.09377849181015795\n",
      "Initial Cost on Val dataset for this epoch 913 = 0.09377849181015795\n",
      "learning rate for this epoch =  0.09096038920331581\n",
      "Error on this batch = 0.04226883408771227\n",
      "Error on this batch = 0.07223410704853676\n",
      "Cost on val dataset after 914 epochs is = 0.09376487527440888\n",
      "Initial Cost on Val dataset for this epoch 914 = 0.09376487527440888\n",
      "learning rate for this epoch =  0.09093549923267195\n",
      "Error on this batch = 0.042246563268059215\n",
      "Error on this batch = 0.072208482450993\n",
      "Cost on val dataset after 915 epochs is = 0.09375128591752988\n",
      "Initial Cost on Val dataset for this epoch 915 = 0.09375128591752988\n",
      "learning rate for this epoch =  0.0909106432786672\n",
      "Error on this batch = 0.04222434441983662\n",
      "Error on this batch = 0.07218290824105601\n",
      "Cost on val dataset after 916 epochs is = 0.0937377236565936\n",
      "Initial Cost on Val dataset for this epoch 916 = 0.0937377236565936\n",
      "learning rate for this epoch =  0.09088582125771116\n",
      "Error on this batch = 0.04220217733257573\n",
      "Error on this batch = 0.07215738419545378\n",
      "Cost on val dataset after 917 epochs is = 0.09372418840906906\n",
      "Initial Cost on Val dataset for this epoch 917 = 0.09372418840906906\n",
      "learning rate for this epoch =  0.09086103308650982\n",
      "Error on this batch = 0.04218006179590573\n",
      "Error on this batch = 0.07213191009148215\n",
      "Cost on val dataset after 918 epochs is = 0.0937106800928188\n",
      "Initial Cost on Val dataset for this epoch 918 = 0.0937106800928188\n",
      "learning rate for this epoch =  0.09083627868206413\n",
      "Error on this batch = 0.04215799759955575\n",
      "Error on this batch = 0.07210648570700907\n",
      "Cost on val dataset after 919 epochs is = 0.09369719862609646\n",
      "Initial Cost on Val dataset for this epoch 919 = 0.09369719862609646\n",
      "learning rate for this epoch =  0.09081155796166877\n",
      "Error on this batch = 0.042135984533356804\n",
      "Error on this batch = 0.0720811108204783\n",
      "Cost on val dataset after 920 epochs is = 0.09368374392754389\n",
      "Initial Cost on Val dataset for this epoch 920 = 0.09368374392754389\n",
      "learning rate for this epoch =  0.09078687084291064\n",
      "Error on this batch = 0.04211402238724409\n",
      "Error on this batch = 0.07205578521091348\n",
      "Cost on val dataset after 921 epochs is = 0.09367031591618855\n",
      "Initial Cost on Val dataset for this epoch 921 = 0.09367031591618855\n",
      "learning rate for this epoch =  0.09076221724366758\n",
      "Error on this batch = 0.04209211095125939\n",
      "Error on this batch = 0.07203050865792171\n",
      "Cost on val dataset after 922 epochs is = 0.09365691451144077\n",
      "Initial Cost on Val dataset for this epoch 922 = 0.09365691451144077\n",
      "learning rate for this epoch =  0.09073759708210706\n",
      "Error on this batch = 0.04207025001555385\n",
      "Error on this batch = 0.07200528094169775\n",
      "Cost on val dataset after 923 epochs is = 0.09364353963309108\n",
      "Initial Cost on Val dataset for this epoch 923 = 0.09364353963309108\n",
      "learning rate for this epoch =  0.09071301027668477\n",
      "Error on this batch = 0.04204843937039067\n",
      "Error on this batch = 0.07198010184302742\n",
      "Cost on val dataset after 924 epochs is = 0.09363019120130722\n",
      "Initial Cost on Val dataset for this epoch 924 = 0.09363019120130722\n",
      "learning rate for this epoch =  0.09068845674614334\n",
      "Error on this batch = 0.042026678806148306\n",
      "Error on this batch = 0.07195497114329164\n",
      "Cost on val dataset after 925 epochs is = 0.09361686913663159\n",
      "Initial Cost on Val dataset for this epoch 925 = 0.09361686913663159\n",
      "learning rate for this epoch =  0.09066393640951106\n",
      "Error on this batch = 0.04200496811332366\n",
      "Error on this batch = 0.07192988862446989\n",
      "Cost on val dataset after 926 epochs is = 0.09360357335997829\n",
      "Initial Cost on Val dataset for this epoch 926 = 0.09360357335997829\n",
      "learning rate for this epoch =  0.09063944918610045\n",
      "Error on this batch = 0.041983307082535504\n",
      "Error on this batch = 0.07190485406914401\n",
      "Cost on val dataset after 927 epochs is = 0.09359030379263028\n",
      "Initial Cost on Val dataset for this epoch 927 = 0.09359030379263028\n",
      "learning rate for this epoch =  0.0906149949955071\n",
      "Error on this batch = 0.041961695504528225\n",
      "Error on this batch = 0.0718798672605018\n",
      "Cost on val dataset after 928 epochs is = 0.09357706035623661\n",
      "Initial Cost on Val dataset for this epoch 928 = 0.09357706035623661\n",
      "learning rate for this epoch =  0.09059057375760825\n",
      "Error on this batch = 0.04194013317017555\n",
      "Error on this batch = 0.07185492798234057\n",
      "Cost on val dataset after 929 epochs is = 0.09356384297280942\n",
      "Initial Cost on Val dataset for this epoch 929 = 0.09356384297280942\n",
      "learning rate for this epoch =  0.09056618539256157\n",
      "Error on this batch = 0.04191861987048478\n",
      "Error on this batch = 0.07183003601907048\n",
      "Cost on val dataset after 930 epochs is = 0.09355065156472106\n",
      "Initial Cost on Val dataset for this epoch 930 = 0.09355065156472106\n",
      "learning rate for this epoch =  0.09054182982080389\n",
      "Error on this batch = 0.04189715539660073\n",
      "Error on this batch = 0.07180519115571828\n",
      "Cost on val dataset after 931 epochs is = 0.09353748605470141\n",
      "Initial Cost on Val dataset for this epoch 931 = 0.09353748605470141\n",
      "learning rate for this epoch =  0.09051750696304985\n",
      "Error on this batch = 0.041875739539810504\n",
      "Error on this batch = 0.07178039317793056\n",
      "Cost on val dataset after 932 epochs is = 0.0935243463658345\n",
      "Initial Cost on Val dataset for this epoch 932 = 0.0935243463658345\n",
      "learning rate for this epoch =  0.0904932167402907\n",
      "Error on this batch = 0.041854372091547756\n",
      "Error on this batch = 0.07175564187197703\n",
      "Cost on val dataset after 933 epochs is = 0.09351123242155596\n",
      "Initial Cost on Val dataset for this epoch 933 = 0.09351123242155596\n",
      "learning rate for this epoch =  0.09046895907379304\n",
      "Error on this batch = 0.04183305284339772\n",
      "Error on this batch = 0.071730937024754\n",
      "Cost on val dataset after 934 epochs is = 0.09349814414564983\n",
      "Initial Cost on Val dataset for this epoch 934 = 0.09349814414564983\n",
      "learning rate for this epoch =  0.09044473388509751\n",
      "Error on this batch = 0.04181178158710215\n",
      "Error on this batch = 0.07170627842378748\n",
      "Cost on val dataset after 935 epochs is = 0.09348508146224564\n",
      "Initial Cost on Val dataset for this epoch 935 = 0.09348508146224564\n",
      "learning rate for this epoch =  0.0904205410960176\n",
      "Error on this batch = 0.04179055811456436\n",
      "Error on this batch = 0.07168166585723636\n",
      "Cost on val dataset after 936 epochs is = 0.09347204429581546\n",
      "Initial Cost on Val dataset for this epoch 936 = 0.09347204429581546\n",
      "learning rate for this epoch =  0.09039638062863838\n",
      "Error on this batch = 0.04176938221785475\n",
      "Error on this batch = 0.07165709911389563\n",
      "Cost on val dataset after 937 epochs is = 0.09345903257117079\n",
      "Initial Cost on Val dataset for this epoch 937 = 0.09345903257117079\n",
      "learning rate for this epoch =  0.09037225240531528\n",
      "Error on this batch = 0.041748253689216135\n",
      "Error on this batch = 0.07163257798319936\n",
      "Cost on val dataset after 938 epochs is = 0.09344604621345957\n",
      "Initial Cost on Val dataset for this epoch 938 = 0.09344604621345957\n",
      "learning rate for this epoch =  0.09034815634867288\n",
      "Error on this batch = 0.041727172321069615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on this batch = 0.07160810225522365\n",
      "Cost on val dataset after 939 epochs is = 0.09343308514816326\n",
      "Initial Cost on Val dataset for this epoch 939 = 0.09343308514816326\n",
      "learning rate for this epoch =  0.09032409238160365\n",
      "Error on this batch = 0.04170613790602018\n",
      "Error on this batch = 0.07158367172068977\n",
      "Cost on val dataset after 940 epochs is = 0.09342014930109364\n",
      "Initial Cost on Val dataset for this epoch 940 = 0.09342014930109364\n",
      "learning rate for this epoch =  0.09030006042726675\n",
      "Error on this batch = 0.041685150236863075\n",
      "Error on this batch = 0.07155928617096684\n",
      "Cost on val dataset after 941 epochs is = 0.09340723859838986\n",
      "Initial Cost on Val dataset for this epoch 941 = 0.09340723859838986\n",
      "learning rate for this epoch =  0.0902760604090869\n",
      "Error on this batch = 0.041664209106589745\n",
      "Error on this batch = 0.07153494539807467\n",
      "Cost on val dataset after 942 epochs is = 0.09339435296651531\n",
      "Initial Cost on Val dataset for this epoch 942 = 0.09339435296651531\n",
      "learning rate for this epoch =  0.09025209225075301\n",
      "Error on this batch = 0.041643314308394255\n",
      "Error on this batch = 0.0715106491946867\n",
      "Cost on val dataset after 943 epochs is = 0.09338149233225472\n",
      "Initial Cost on Val dataset for this epoch 943 = 0.09338149233225472\n",
      "learning rate for this epoch =  0.09022815587621721\n",
      "Error on this batch = 0.04162246563567978\n",
      "Error on this batch = 0.07148639735413244\n",
      "Cost on val dataset after 944 epochs is = 0.09336865662271086\n",
      "Initial Cost on Val dataset for this epoch 944 = 0.09336865662271086\n",
      "learning rate for this epoch =  0.0902042512096935\n",
      "Error on this batch = 0.0416016628820654\n",
      "Error on this batch = 0.07146218967040029\n",
      "Cost on val dataset after 945 epochs is = 0.09335584576530169\n",
      "Initial Cost on Val dataset for this epoch 945 = 0.09335584576530169\n",
      "learning rate for this epoch =  0.09018037817565662\n",
      "Error on this batch = 0.041580905841392804\n",
      "Error on this batch = 0.07143802593813994\n",
      "Cost on val dataset after 946 epochs is = 0.09334305968775714\n",
      "Initial Cost on Val dataset for this epoch 946 = 0.09334305968775714\n",
      "learning rate for this epoch =  0.09015653669884087\n",
      "Error on this batch = 0.041560194307733334\n",
      "Error on this batch = 0.0714139059526652\n",
      "Cost on val dataset after 947 epochs is = 0.09333029831811608\n",
      "Initial Cost on Val dataset for this epoch 947 = 0.09333029831811608\n",
      "learning rate for this epoch =  0.09013272670423898\n",
      "Error on this batch = 0.041539528075394995\n",
      "Error on this batch = 0.07138982950995597\n",
      "Cost on val dataset after 948 epochs is = 0.09331756158472344\n",
      "Initial Cost on Val dataset for this epoch 948 = 0.09331756158472344\n",
      "learning rate for this epoch =  0.09010894811710093\n",
      "Error on this batch = 0.04151890693892993\n",
      "Error on this batch = 0.07136579640666105\n",
      "Cost on val dataset after 949 epochs is = 0.09330484941622685\n",
      "Initial Cost on Val dataset for this epoch 949 = 0.09330484941622685\n",
      "learning rate for this epoch =  0.09008520086293277\n",
      "Error on this batch = 0.04149833069314158\n",
      "Error on this batch = 0.07134180644010017\n",
      "Cost on val dataset after 950 epochs is = 0.09329216174157375\n",
      "Initial Cost on Val dataset for this epoch 950 = 0.09329216174157375\n",
      "learning rate for this epoch =  0.09006148486749553\n",
      "Error on this batch = 0.041477799133092505\n",
      "Error on this batch = 0.07131785940826649\n",
      "Cost on val dataset after 951 epochs is = 0.09327949849000827\n",
      "Initial Cost on Val dataset for this epoch 951 = 0.09327949849000827\n",
      "learning rate for this epoch =  0.09003780005680402\n",
      "Error on this batch = 0.041457312054111774\n",
      "Error on this batch = 0.07129395510982862\n",
      "Cost on val dataset after 952 epochs is = 0.09326685959106838\n",
      "Initial Cost on Val dataset for this epoch 952 = 0.09326685959106838\n",
      "learning rate for this epoch =  0.09001414635712576\n",
      "Error on this batch = 0.041436869251802955\n",
      "Error on this batch = 0.07127009334413285\n",
      "Cost on val dataset after 953 epochs is = 0.09325424497458257\n",
      "Initial Cost on Val dataset for this epoch 953 = 0.09325424497458257\n",
      "learning rate for this epoch =  0.08999052369497976\n",
      "Error on this batch = 0.04141647052205199\n",
      "Error on this batch = 0.07124627391120515\n",
      "Cost on val dataset after 954 epochs is = 0.093241654570667\n",
      "Initial Cost on Val dataset for this epoch 954 = 0.093241654570667\n",
      "learning rate for this epoch =  0.08996693199713549\n",
      "Error on this batch = 0.041396115661035265\n",
      "Error on this batch = 0.07122249661175332\n",
      "Cost on val dataset after 955 epochs is = 0.09322908830972246\n",
      "Initial Cost on Val dataset for this epoch 955 = 0.09322908830972246\n",
      "learning rate for this epoch =  0.08994337119061177\n",
      "Error on this batch = 0.041375804465227775\n",
      "Error on this batch = 0.07119876124716883\n",
      "Cost on val dataset after 956 epochs is = 0.09321654612243134\n",
      "Initial Cost on Val dataset for this epoch 956 = 0.09321654612243134\n",
      "learning rate for this epoch =  0.08991984120267553\n",
      "Error on this batch = 0.041355536731411395\n",
      "Error on this batch = 0.07117506761952877\n",
      "Cost on val dataset after 957 epochs is = 0.09320402793975474\n",
      "Initial Cost on Val dataset for this epoch 957 = 0.09320402793975474\n",
      "learning rate for this epoch =  0.08989634196084093\n",
      "Error on this batch = 0.041335312256683264\n",
      "Error on this batch = 0.07115141553159779\n",
      "Cost on val dataset after 958 epochs is = 0.09319153369292933\n",
      "Initial Cost on Val dataset for this epoch 958 = 0.09319153369292933\n",
      "learning rate for this epoch =  0.089872873392868\n",
      "Error on this batch = 0.041315130838464297\n",
      "Error on this batch = 0.07112780478682973\n",
      "Cost on val dataset after 959 epochs is = 0.09317906331346468\n",
      "Initial Cost on Val dataset for this epoch 959 = 0.09317906331346468\n",
      "learning rate for this epoch =  0.08984943542676176\n",
      "Error on this batch = 0.041294992274507863\n",
      "Error on this batch = 0.07110423518936947\n",
      "Cost on val dataset after 960 epochs is = 0.09316661673313997\n",
      "Initial Cost on Val dataset for this epoch 960 = 0.09316661673313997\n",
      "learning rate for this epoch =  0.08982602799077107\n",
      "Error on this batch = 0.041274896362908306\n",
      "Error on this batch = 0.07108070654405457\n",
      "Cost on val dataset after 961 epochs is = 0.09315419388400141\n",
      "Initial Cost on Val dataset for this epoch 961 = 0.09315419388400141\n",
      "learning rate for this epoch =  0.08980265101338746\n",
      "Error on this batch = 0.04125484290210992\n",
      "Error on this batch = 0.071057218656417\n",
      "Cost on val dataset after 962 epochs is = 0.09314179469835918\n",
      "Initial Cost on Val dataset for this epoch 962 = 0.09314179469835918\n",
      "learning rate for this epoch =  0.0897793044233442\n",
      "Error on this batch = 0.04123483169091571\n",
      "Error on this batch = 0.07103377133268453\n",
      "Cost on val dataset after 963 epochs is = 0.09312941910878461\n",
      "Initial Cost on Val dataset for this epoch 963 = 0.09312941910878461\n",
      "learning rate for this epoch =  0.0897559881496152\n",
      "Error on this batch = 0.041214862528496446\n",
      "Error on this batch = 0.07101036437978249\n",
      "Cost on val dataset after 964 epochs is = 0.09311706704810738\n",
      "Initial Cost on Val dataset for this epoch 964 = 0.09311706704810738\n",
      "learning rate for this epoch =  0.08973270212141383\n",
      "Error on this batch = 0.041194935214399625\n",
      "Error on this batch = 0.07098699760533506\n",
      "Cost on val dataset after 965 epochs is = 0.09310473844941268\n",
      "Initial Cost on Val dataset for this epoch 965 = 0.09310473844941268\n",
      "learning rate for this epoch =  0.08970944626819201\n",
      "Error on this batch = 0.04117504954855867\n",
      "Error on this batch = 0.07096367081766679\n",
      "Cost on val dataset after 966 epochs is = 0.0930924332460384\n",
      "Initial Cost on Val dataset for this epoch 966 = 0.0930924332460384\n",
      "learning rate for this epoch =  0.0896862205196391\n",
      "Error on this batch = 0.04115520533130208\n",
      "Error on this batch = 0.07094038382580398\n",
      "Cost on val dataset after 967 epochs is = 0.0930801513715725\n",
      "Initial Cost on Val dataset for this epoch 967 = 0.0930801513715725\n",
      "learning rate for this epoch =  0.08966302480568086\n",
      "Error on this batch = 0.0411354023633627\n",
      "Error on this batch = 0.07091713643947602\n",
      "Cost on val dataset after 968 epochs is = 0.0930678927598501\n",
      "Initial Cost on Val dataset for this epoch 968 = 0.0930678927598501\n",
      "learning rate for this epoch =  0.08963985905647841\n",
      "Error on this batch = 0.04111564044588705\n",
      "Error on this batch = 0.0708939284691167\n",
      "Cost on val dataset after 969 epochs is = 0.09305565734495101\n",
      "Initial Cost on Val dataset for this epoch 969 = 0.09305565734495101\n",
      "learning rate for this epoch =  0.08961672320242715\n",
      "Error on this batch = 0.04109591938044474\n",
      "Error on this batch = 0.07087075972586539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on val dataset after 970 epochs is = 0.09304344506119694\n",
      "Initial Cost on Val dataset for this epoch 970 = 0.09304344506119694\n",
      "learning rate for this epoch =  0.08959361717415586\n",
      "Error on this batch = 0.04107623896903794\n",
      "Error on this batch = 0.07084763002156837\n",
      "Cost on val dataset after 971 epochs is = 0.0930312558431489\n",
      "Initial Cost on Val dataset for this epoch 971 = 0.0930312558431489\n",
      "learning rate for this epoch =  0.08957054090252553\n",
      "Error on this batch = 0.04105659901411074\n",
      "Error on this batch = 0.07082453916877979\n",
      "Cost on val dataset after 972 epochs is = 0.0930190896256047\n",
      "Initial Cost on Val dataset for this epoch 972 = 0.0930190896256047\n",
      "learning rate for this epoch =  0.08954749431862849\n",
      "Error on this batch = 0.04103699931855877\n",
      "Error on this batch = 0.07080148698076301\n",
      "Cost on val dataset after 973 epochs is = 0.09300694634359633\n",
      "Initial Cost on Val dataset for this epoch 973 = 0.09300694634359633\n",
      "learning rate for this epoch =  0.08952447735378725\n",
      "Error on this batch = 0.041017439685738837\n",
      "Error on this batch = 0.0707784732714916\n",
      "Cost on val dataset after 974 epochs is = 0.09299482593238746\n",
      "Initial Cost on Val dataset for this epoch 974 = 0.09299482593238746\n",
      "learning rate for this epoch =  0.08950148993955362\n",
      "Error on this batch = 0.04099791991947841\n",
      "Error on this batch = 0.07075549785565034\n",
      "Cost on val dataset after 975 epochs is = 0.09298272832747101\n",
      "Initial Cost on Val dataset for this epoch 975 = 0.09298272832747101\n",
      "learning rate for this epoch =  0.08947853200770761\n",
      "Error on this batch = 0.04097843982408543\n",
      "Error on this batch = 0.07073256054863632\n",
      "Cost on val dataset after 976 epochs is = 0.09297065346456673\n",
      "Initial Cost on Val dataset for this epoch 976 = 0.09297065346456673\n",
      "learning rate for this epoch =  0.08945560349025655\n",
      "Error on this batch = 0.04095899920435784\n",
      "Error on this batch = 0.07070966116655984\n",
      "Cost on val dataset after 977 epochs is = 0.09295860127961882\n",
      "Initial Cost on Val dataset for this epoch 977 = 0.09295860127961882\n",
      "learning rate for this epoch =  0.08943270431943395\n",
      "Error on this batch = 0.040939597865593313\n",
      "Error on this batch = 0.0706867995262454\n",
      "Cost on val dataset after 978 epochs is = 0.09294657170879358\n",
      "Initial Cost on Val dataset for this epoch 978 = 0.09294657170879358\n",
      "learning rate for this epoch =  0.08940983442769869\n",
      "Error on this batch = 0.04092023561359905\n",
      "Error on this batch = 0.07066397544523252\n",
      "Cost on val dataset after 979 epochs is = 0.09293456468847715\n",
      "Initial Cost on Val dataset for this epoch 979 = 0.09293456468847715\n",
      "learning rate for this epoch =  0.08938699374773387\n",
      "Error on this batch = 0.04090091225470143\n",
      "Error on this batch = 0.07064118874177673\n",
      "Cost on val dataset after 980 epochs is = 0.09292258015527313\n",
      "Initial Cost on Val dataset for this epoch 980 = 0.09292258015527313\n",
      "learning rate for this epoch =  0.089364182212446\n",
      "Error on this batch = 0.04088162759575576\n",
      "Error on this batch = 0.07061843923485037\n",
      "Cost on val dataset after 981 epochs is = 0.09291061804600065\n",
      "Initial Cost on Val dataset for this epoch 981 = 0.09291061804600065\n",
      "learning rate for this epoch =  0.08934139975496388\n",
      "Error on this batch = 0.04086238144415605\n",
      "Error on this batch = 0.07059572674414331\n",
      "Cost on val dataset after 982 epochs is = 0.09289867829769194\n",
      "Initial Cost on Val dataset for this epoch 982 = 0.09289867829769194\n",
      "learning rate for this epoch =  0.08931864630863778\n",
      "Error on this batch = 0.04084317360784476\n",
      "Error on this batch = 0.07057305109006388\n",
      "Cost on val dataset after 983 epochs is = 0.09288676084759041\n",
      "Initial Cost on Val dataset for this epoch 983 = 0.09288676084759041\n",
      "learning rate for this epoch =  0.08929592180703835\n",
      "Error on this batch = 0.040824003895322515\n",
      "Error on this batch = 0.07055041209373947\n",
      "Cost on val dataset after 984 epochs is = 0.09287486563314852\n",
      "Initial Cost on Val dataset for this epoch 984 = 0.09287486563314852\n",
      "learning rate for this epoch =  0.08927322618395579\n",
      "Error on this batch = 0.040804872115657885\n",
      "Error on this batch = 0.07052780957701744\n",
      "Cost on val dataset after 985 epochs is = 0.09286299259202574\n",
      "Initial Cost on Val dataset for this epoch 985 = 0.09286299259202574\n",
      "learning rate for this epoch =  0.08925055937339876\n",
      "Error on this batch = 0.040785778078497065\n",
      "Error on this batch = 0.07050524336246565\n",
      "Cost on val dataset after 986 epochs is = 0.09285114166208668\n",
      "Initial Cost on Val dataset for this epoch 986 = 0.09285114166208668\n",
      "learning rate for this epoch =  0.08922792130959359\n",
      "Error on this batch = 0.04076672159407356\n",
      "Error on this batch = 0.07048271327337313\n",
      "Cost on val dataset after 987 epochs is = 0.09283931278139912\n",
      "Initial Cost on Val dataset for this epoch 987 = 0.09283931278139912\n",
      "learning rate for this epoch =  0.08920531192698324\n",
      "Error on this batch = 0.04074770247321793\n",
      "Error on this batch = 0.07046021913375089\n",
      "Cost on val dataset after 988 epochs is = 0.0928275058882322\n",
      "Initial Cost on Val dataset for this epoch 988 = 0.0928275058882322\n",
      "learning rate for this epoch =  0.08918273116022643\n",
      "Error on this batch = 0.040728720527367396\n",
      "Error on this batch = 0.07043776076833241\n",
      "Cost on val dataset after 989 epochs is = 0.0928157209210546\n",
      "Initial Cost on Val dataset for this epoch 989 = 0.0928157209210546\n",
      "learning rate for this epoch =  0.08916017894419664\n",
      "Error on this batch = 0.04070977556857552\n",
      "Error on this batch = 0.07041533800257431\n",
      "Cost on val dataset after 990 epochs is = 0.09280395781853272\n",
      "Initial Cost on Val dataset for this epoch 990 = 0.09280395781853272\n",
      "learning rate for this epoch =  0.08913765521398127\n",
      "Error on this batch = 0.04069086740952172\n",
      "Error on this batch = 0.07039295066265681\n",
      "Cost on val dataset after 991 epochs is = 0.09279221651952918\n",
      "Initial Cost on Val dataset for this epoch 991 = 0.09279221651952918\n",
      "learning rate for this epoch =  0.08911515990488066\n",
      "Error on this batch = 0.04067199586352092\n",
      "Error on this batch = 0.07037059857548447\n",
      "Cost on val dataset after 992 epochs is = 0.09278049696310094\n",
      "Initial Cost on Val dataset for this epoch 992 = 0.09278049696310094\n",
      "learning rate for this epoch =  0.0890926929524072\n",
      "Error on this batch = 0.040653160744532936\n",
      "Error on this batch = 0.07034828156868655\n",
      "Cost on val dataset after 993 epochs is = 0.09276879908849779\n",
      "Initial Cost on Val dataset for this epoch 993 = 0.09276879908849779\n",
      "learning rate for this epoch =  0.08907025429228442\n",
      "Error on this batch = 0.040634361867172085\n",
      "Error on this batch = 0.07032599947061761\n",
      "Cost on val dataset after 994 epochs is = 0.09275712283516095\n",
      "Initial Cost on Val dataset for this epoch 994 = 0.09275712283516095\n",
      "learning rate for this epoch =  0.08904784386044612\n",
      "Error on this batch = 0.040615599046716466\n",
      "Error on this batch = 0.07030375211035808\n",
      "Cost on val dataset after 995 epochs is = 0.09274546814272146\n",
      "Initial Cost on Val dataset for this epoch 995 = 0.09274546814272146\n",
      "learning rate for this epoch =  0.08902546159303537\n",
      "Error on this batch = 0.040596872099117345\n",
      "Error on this batch = 0.07028153931771451\n",
      "Cost on val dataset after 996 epochs is = 0.09273383495099864\n",
      "Initial Cost on Val dataset for this epoch 996 = 0.09273383495099864\n",
      "learning rate for this epoch =  0.0890031074264038\n",
      "Error on this batch = 0.040578180841008456\n",
      "Error on this batch = 0.07025936092322038\n",
      "Cost on val dataset after 997 epochs is = 0.09272222319999902\n",
      "Initial Cost on Val dataset for this epoch 997 = 0.09272222319999902\n",
      "learning rate for this epoch =  0.08898078129711047\n",
      "Error on this batch = 0.040559525089715276\n",
      "Error on this batch = 0.0702372167581361\n",
      "Cost on val dataset after 998 epochs is = 0.09271063282991475\n",
      "Initial Cost on Val dataset for this epoch 998 = 0.09271063282991475\n",
      "learning rate for this epoch =  0.08895848314192122\n",
      "Error on this batch = 0.04054090466326409\n",
      "Error on this batch = 0.0702151066544498\n",
      "Cost on val dataset after 999 epochs is = 0.09269906378112244\n",
      "Initial Cost on Val dataset for this epoch 999 = 0.09269906378112244\n",
      "learning rate for this epoch =  0.08893621289780759\n",
      "Error on this batch = 0.04052231938039118\n",
      "Error on this batch = 0.07019303044487758\n",
      "Cost on val dataset after 1000 epochs is = 0.09268751599418197\n",
      "Initial Cost on Val dataset for this epoch 1000 = 0.09268751599418197\n",
      "learning rate for this epoch =  0.08891397050194613\n",
      "Error on this batch = 0.040503769060551705\n",
      "Error on this batch = 0.07017098796286395\n",
      "Cost on val dataset after 1001 epochs is = 0.09267598940983528\n",
      "Initial Cost on Val dataset for this epoch 1001 = 0.09267598940983528\n",
      "learning rate for this epoch =  0.0888917558917174\n",
      "Error on this batch = 0.04048525352392874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on this batch = 0.07014897904258209\n",
      "Cost on val dataset after 1002 epochs is = 0.09266448396900528\n",
      "Initial Cost on Val dataset for this epoch 1002 = 0.09266448396900528\n",
      "learning rate for this epoch =  0.0888695690047051\n",
      "Error on this batch = 0.04046677259144205\n",
      "Error on this batch = 0.07012700351893433\n",
      "Cost on val dataset after 1003 epochs is = 0.09265299961279483\n",
      "Initial Cost on Val dataset for this epoch 1003 = 0.09265299961279483\n",
      "learning rate for this epoch =  0.08884740977869533\n",
      "Error on this batch = 0.04044832608475691\n",
      "Error on this batch = 0.07010506122755243\n",
      "Cost on val dataset after 1004 epochs is = 0.09264153628248578\n",
      "Initial Cost on Val dataset for this epoch 1004 = 0.09264153628248578\n",
      "learning rate for this epoch =  0.0888252781516756\n",
      "Error on this batch = 0.04042991382629268\n",
      "Error on this batch = 0.07008315200479799\n",
      "Cost on val dataset after 1005 epochs is = 0.09263009391953785\n",
      "Initial Cost on Val dataset for this epoch 1005 = 0.09263009391953785\n",
      "learning rate for this epoch =  0.08880317406183406\n",
      "Error on this batch = 0.04041153563923142\n",
      "Error on this batch = 0.07006127568776259\n",
      "Cost on val dataset after 1006 epochs is = 0.09261867246558801\n",
      "Initial Cost on Val dataset for this epoch 1006 = 0.09261867246558801\n",
      "learning rate for this epoch =  0.08878109744755859\n",
      "Error on this batch = 0.040393191347526385\n",
      "Error on this batch = 0.07003943211426822\n",
      "Cost on val dataset after 1007 epochs is = 0.09260727186244937\n",
      "Initial Cost on Val dataset for this epoch 1007 = 0.09260727186244937\n",
      "learning rate for this epoch =  0.08875904824743605\n",
      "Error on this batch = 0.0403748807759103\n",
      "Error on this batch = 0.07001762112286751\n",
      "Cost on val dataset after 1008 epochs is = 0.09259589205211066\n",
      "Initial Cost on Val dataset for this epoch 1008 = 0.09259589205211066\n",
      "learning rate for this epoch =  0.08873702640025133\n",
      "Error on this batch = 0.04035660374990371\n",
      "Error on this batch = 0.06999584255284404\n",
      "Cost on val dataset after 1009 epochs is = 0.09258453297673525\n",
      "Initial Cost on Val dataset for this epoch 1009 = 0.09258453297673525\n",
      "learning rate for this epoch =  0.08871503184498658\n",
      "Error on this batch = 0.04033836009582302\n",
      "Error on this batch = 0.06997409624421243\n",
      "Cost on val dataset after 1010 epochs is = 0.0925731945786606\n",
      "Initial Cost on Val dataset for this epoch 1010 = 0.0925731945786606\n",
      "learning rate for this epoch =  0.08869306452082039\n",
      "Error on this batch = 0.04032014964078865\n",
      "Error on this batch = 0.06995238203771882\n",
      "Cost on val dataset after 1011 epochs is = 0.09256187680039774\n",
      "Initial Cost on Val dataset for this epoch 1011 = 0.09256187680039774\n",
      "learning rate for this epoch =  0.0886711243671269\n",
      "Error on this batch = 0.0403019722127328\n",
      "Error on this batch = 0.06993069977484088\n",
      "Cost on val dataset after 1012 epochs is = 0.09255057958463038\n",
      "Initial Cost on Val dataset for this epoch 1012 = 0.09255057958463038\n",
      "learning rate for this epoch =  0.08864921132347509\n",
      "Error on this batch = 0.040283827640407346\n",
      "Error on this batch = 0.06990904929778814\n",
      "Cost on val dataset after 1013 epochs is = 0.09253930287421469\n",
      "Initial Cost on Val dataset for this epoch 1013 = 0.09253930287421469\n",
      "learning rate for this epoch =  0.0886273253296278\n",
      "Error on this batch = 0.040265715753391496\n",
      "Error on this batch = 0.06988743044950199\n",
      "Cost on val dataset after 1014 epochs is = 0.0925280466121786\n",
      "Initial Cost on Val dataset for this epoch 1014 = 0.0925280466121786\n",
      "learning rate for this epoch =  0.08860546632554109\n",
      "Error on this batch = 0.04024763638209926\n",
      "Error on this batch = 0.06986584307365613\n",
      "Cost on val dataset after 1015 epochs is = 0.09251681074172151\n",
      "Initial Cost on Val dataset for this epoch 1015 = 0.09251681074172151\n",
      "learning rate for this epoch =  0.0885836342513633\n",
      "Error on this batch = 0.04022958935778693\n",
      "Error on this batch = 0.0698442870146564\n",
      "Cost on val dataset after 1016 epochs is = 0.09250559520621385\n",
      "Initial Cost on Val dataset for this epoch 1016 = 0.09250559520621385\n",
      "learning rate for this epoch =  0.08856182904743433\n",
      "Error on this batch = 0.040211574512560355\n",
      "Error on this batch = 0.06982276211764125\n",
      "Cost on val dataset after 1017 epochs is = 0.09249439994919664\n",
      "Initial Cost on Val dataset for this epoch 1017 = 0.09249439994919664\n",
      "learning rate for this epoch =  0.08854005065428476\n",
      "Error on this batch = 0.04019359167938205\n",
      "Error on this batch = 0.0698012682284816\n",
      "Cost on val dataset after 1018 epochs is = 0.09248322491438139\n",
      "Initial Cost on Val dataset for this epoch 1018 = 0.09248322491438139\n",
      "learning rate for this epoch =  0.08851829901263515\n",
      "Error on this batch = 0.04017564069207818\n",
      "Error on this batch = 0.06977980519378099\n",
      "Cost on val dataset after 1019 epochs is = 0.09247207004564964\n",
      "Initial Cost on Val dataset for this epoch 1019 = 0.09247207004564964\n",
      "learning rate for this epoch =  0.08849657406339513\n",
      "Error on this batch = 0.040157721385345485\n",
      "Error on this batch = 0.06975837286087579\n",
      "Cost on val dataset after 1020 epochs is = 0.09246093528705296\n",
      "Initial Cost on Val dataset for this epoch 1020 = 0.09246093528705296\n",
      "learning rate for this epoch =  0.08847487574766279\n",
      "Error on this batch = 0.040139833594757875\n",
      "Error on this batch = 0.06973697107783516\n",
      "Cost on val dataset after 1021 epochs is = 0.09244982058281256\n",
      "Initial Cost on Val dataset for this epoch 1021 = 0.09244982058281256\n",
      "learning rate for this epoch =  0.08845320400672366\n",
      "Error on this batch = 0.040121977156773174\n",
      "Error on this batch = 0.06971559969346111\n",
      "Cost on val dataset after 1022 epochs is = 0.09243872587731931\n",
      "Initial Cost on Val dataset for this epoch 1022 = 0.09243872587731931\n",
      "learning rate for this epoch =  0.0884315587820501\n",
      "Error on this batch = 0.04010415190873934\n",
      "Error on this batch = 0.06969425855728856\n",
      "Cost on val dataset after 1023 epochs is = 0.09242765111513368\n",
      "Initial Cost on Val dataset for this epoch 1023 = 0.09242765111513368\n",
      "learning rate for this epoch =  0.08840994001530049\n",
      "Error on this batch = 0.04008635768890095\n",
      "Error on this batch = 0.0696729475195853\n",
      "Cost on val dataset after 1024 epochs is = 0.09241659624098558\n",
      "Initial Cost on Val dataset for this epoch 1024 = 0.09241659624098558\n",
      "learning rate for this epoch =  0.08838834764831843\n",
      "Error on this batch = 0.040068594336405104\n",
      "Error on this batch = 0.0696516664313521\n",
      "Cost on val dataset after 1025 epochs is = 0.09240556119977433\n",
      "Initial Cost on Val dataset for this epoch 1025 = 0.09240556119977433\n",
      "learning rate for this epoch =  0.08836678162313201\n",
      "Error on this batch = 0.04005086169130744\n",
      "Error on this batch = 0.06963041514432253\n",
      "Cost on val dataset after 1026 epochs is = 0.0923945459365689\n",
      "Initial Cost on Val dataset for this epoch 1026 = 0.0923945459365689\n",
      "learning rate for this epoch =  0.088345241881953\n",
      "Error on this batch = 0.04003315959457807\n",
      "Error on this batch = 0.06960919351096302\n",
      "Cost on val dataset after 1027 epochs is = 0.09238355039660776\n",
      "Initial Cost on Val dataset for this epoch 1027 = 0.09238355039660776\n",
      "learning rate for this epoch =  0.0883237283671761\n",
      "Error on this batch = 0.04001548788810699\n",
      "Error on this batch = 0.0695880013844728\n",
      "Cost on val dataset after 1028 epochs is = 0.09237257452529901\n",
      "Initial Cost on Val dataset for this epoch 1028 = 0.09237257452529901\n",
      "learning rate for this epoch =  0.0883022410213782\n",
      "Error on this batch = 0.03999784641470977\n",
      "Error on this batch = 0.06956683861878364\n",
      "Cost on val dataset after 1029 epochs is = 0.09236161826822067\n",
      "Initial Cost on Val dataset for this epoch 1029 = 0.09236161826822067\n",
      "learning rate for this epoch =  0.08828077978731765\n",
      "Error on this batch = 0.03998023501813266\n",
      "Error on this batch = 0.06954570506856002\n",
      "Cost on val dataset after 1030 epochs is = 0.09235068157112068\n",
      "Initial Cost on Val dataset for this epoch 1030 = 0.09235068157112068\n",
      "learning rate for this epoch =  0.08825934460793343\n",
      "Error on this batch = 0.039962653543057974\n",
      "Error on this batch = 0.06952460058919878\n",
      "Cost on val dataset after 1031 epochs is = 0.09233976437991707\n",
      "Initial Cost on Val dataset for this epoch 1031 = 0.09233976437991707\n",
      "learning rate for this epoch =  0.08823793542634452\n",
      "Error on this batch = 0.039945101835108805\n",
      "Error on this batch = 0.06950352503682905\n",
      "Cost on val dataset after 1032 epochs is = 0.0923288666406984\n",
      "Initial Cost on Val dataset for this epoch 1032 = 0.0923288666406984\n",
      "learning rate for this epoch =  0.08821655218584905\n",
      "Error on this batch = 0.039927579740854\n",
      "Error on this batch = 0.06948247826831208\n",
      "Cost on val dataset after 1033 epochs is = 0.09231798829972375\n",
      "Initial Cost on Val dataset for this epoch 1033 = 0.09231798829972375\n",
      "learning rate for this epoch =  0.08819519482992363\n",
      "Error on this batch = 0.03991008710781276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on this batch = 0.06946146014124097\n",
      "Cost on val dataset after 1034 epochs is = 0.09230712930342323\n",
      "Initial Cost on Val dataset for this epoch 1034 = 0.09230712930342323\n",
      "learning rate for this epoch =  0.08817386330222259\n",
      "Error on this batch = 0.03989262378445911\n",
      "Error on this batch = 0.06944047051394049\n",
      "Cost on val dataset after 1035 epochs is = 0.09229628959839808\n",
      "Initial Cost on Val dataset for this epoch 1035 = 0.09229628959839808\n",
      "learning rate for this epoch =  0.08815255754657725\n",
      "Error on this batch = 0.03987518962022612\n",
      "Error on this batch = 0.0694195092454669\n",
      "Cost on val dataset after 1036 epochs is = 0.09228546913142123\n",
      "Initial Cost on Val dataset for this epoch 1036 = 0.09228546913142123\n",
      "learning rate for this epoch =  0.08813127750699522\n",
      "Error on this batch = 0.03985778446551016\n",
      "Error on this batch = 0.06939857619560746\n",
      "Cost on val dataset after 1037 epochs is = 0.0922746678494374\n",
      "Initial Cost on Val dataset for this epoch 1037 = 0.0922746678494374\n",
      "learning rate for this epoch =  0.08811002312765961\n",
      "Error on this batch = 0.039840408171674634\n",
      "Error on this batch = 0.06937767122488048\n",
      "Cost on val dataset after 1038 epochs is = 0.09226388569956373\n",
      "Initial Cost on Val dataset for this epoch 1038 = 0.09226388569956373\n",
      "learning rate for this epoch =  0.08808879435292839\n",
      "Error on this batch = 0.03982306059105404\n",
      "Error on this batch = 0.06935679419453454\n",
      "Cost on val dataset after 1039 epochs is = 0.09225312262908998\n",
      "Initial Cost on Val dataset for this epoch 1039 = 0.09225312262908998\n",
      "learning rate for this epoch =  0.08806759112733367\n",
      "Error on this batch = 0.03980574157695719\n",
      "Error on this batch = 0.06933594496654853\n",
      "Cost on val dataset after 1040 epochs is = 0.09224237858547903\n",
      "Initial Cost on Val dataset for this epoch 1040 = 0.09224237858547903\n",
      "learning rate for this epoch =  0.0880464133955809\n",
      "Error on this batch = 0.039788450983671\n",
      "Error on this batch = 0.06931512340363107\n",
      "Cost on val dataset after 1041 epochs is = 0.09223165351636739\n",
      "Initial Cost on Val dataset for this epoch 1041 = 0.09223165351636739\n",
      "learning rate for this epoch =  0.08802526110254827\n",
      "Error on this batch = 0.0397711886664634\n",
      "Error on this batch = 0.06929432936922013\n",
      "Cost on val dataset after 1042 epochs is = 0.09222094736956546\n",
      "Initial Cost on Val dataset for this epoch 1042 = 0.09222094736956546\n",
      "learning rate for this epoch =  0.0880041341932859\n",
      "Error on this batch = 0.039753954481586556\n",
      "Error on this batch = 0.06927356272748254\n",
      "Cost on val dataset after 1043 epochs is = 0.09221026009305817\n",
      "Initial Cost on Val dataset for this epoch 1043 = 0.09221026009305817\n",
      "learning rate for this epoch =  0.08798303261301525\n",
      "Error on this batch = 0.03973674828627976\n",
      "Error on this batch = 0.06925282334331367\n",
      "Cost on val dataset after 1044 epochs is = 0.09219959163500543\n",
      "Initial Cost on Val dataset for this epoch 1044 = 0.09219959163500543\n",
      "learning rate for this epoch =  0.08796195630712837\n",
      "Error on this batch = 0.03971956993877198\n",
      "Error on this batch = 0.06923211108233673\n",
      "Cost on val dataset after 1045 epochs is = 0.09218894194374254\n",
      "Initial Cost on Val dataset for this epoch 1045 = 0.09218894194374254\n",
      "learning rate for this epoch =  0.08794090522118717\n",
      "Error on this batch = 0.039702419298284525\n",
      "Error on this batch = 0.0692114258109024\n",
      "Cost on val dataset after 1046 epochs is = 0.09217831096778081\n",
      "Initial Cost on Val dataset for this epoch 1046 = 0.09217831096778081\n",
      "learning rate for this epoch =  0.08791987930092278\n",
      "Error on this batch = 0.03968529622503329\n",
      "Error on this batch = 0.06919076739608818\n",
      "Cost on val dataset after 1047 epochs is = 0.09216769865580801\n",
      "Initial Cost on Val dataset for this epoch 1047 = 0.09216769865580801\n",
      "learning rate for this epoch =  0.08789887849223484\n",
      "Error on this batch = 0.03966820058023085\n",
      "Error on this batch = 0.06917013570569779\n",
      "Cost on val dataset after 1048 epochs is = 0.0921571049566889\n",
      "Initial Cost on Val dataset for this epoch 1048 = 0.0921571049566889\n",
      "learning rate for this epoch =  0.08787790274119082\n",
      "Error on this batch = 0.03965113222608863\n",
      "Error on this batch = 0.06914953060826068\n",
      "Cost on val dataset after 1049 epochs is = 0.09214652981946572\n",
      "Initial Cost on Val dataset for this epoch 1049 = 0.09214652981946572\n",
      "learning rate for this epoch =  0.08785695199402538\n",
      "Error on this batch = 0.0396340910258185\n",
      "Error on this batch = 0.06912895197303112\n",
      "Cost on val dataset after 1050 epochs is = 0.0921359731933589\n",
      "Initial Cost on Val dataset for this epoch 1050 = 0.0921359731933589\n",
      "learning rate for this epoch =  0.08783602619713961\n",
      "Error on this batch = 0.03961707684363444\n",
      "Error on this batch = 0.0691083996699877\n",
      "Cost on val dataset after 1051 epochs is = 0.0921254350277674\n",
      "Initial Cost on Val dataset for this epoch 1051 = 0.0921254350277674\n",
      "learning rate for this epoch =  0.08781512529710042\n",
      "Error on this batch = 0.03960008954475397\n",
      "Error on this batch = 0.06908787356983269\n",
      "Cost on val dataset after 1052 epochs is = 0.0921149152722694\n",
      "Initial Cost on Val dataset for this epoch 1052 = 0.0921149152722694\n",
      "learning rate for this epoch =  0.08779424924063986\n",
      "Error on this batch = 0.03958312899539947\n",
      "Error on this batch = 0.06906737354399094\n",
      "Cost on val dataset after 1053 epochs is = 0.09210441387662274\n",
      "Initial Cost on Val dataset for this epoch 1053 = 0.09210441387662274\n",
      "learning rate for this epoch =  0.08777339797465443\n",
      "Error on this batch = 0.03956619506279915\n",
      "Error on this batch = 0.06904689946460936\n",
      "Cost on val dataset after 1054 epochs is = 0.0920939307907656\n",
      "Initial Cost on Val dataset for this epoch 1054 = 0.0920939307907656\n",
      "learning rate for this epoch =  0.08775257144620446\n",
      "Error on this batch = 0.039549287615188\n",
      "Error on this batch = 0.06902645120455608\n",
      "Cost on val dataset after 1055 epochs is = 0.092083465964817\n",
      "Initial Cost on Val dataset for this epoch 1055 = 0.092083465964817\n",
      "learning rate for this epoch =  0.08773176960251337\n",
      "Error on this batch = 0.03953240652180846\n",
      "Error on this batch = 0.06900602863741953\n",
      "Cost on val dataset after 1056 epochs is = 0.0920730193490772\n",
      "Initial Cost on Val dataset for this epoch 1056 = 0.0920730193490772\n",
      "learning rate for this epoch =  0.08771099239096714\n",
      "Error on this batch = 0.03951555165291101\n",
      "Error on this batch = 0.06898563163750747\n",
      "Cost on val dataset after 1057 epochs is = 0.0920625908940286\n",
      "Initial Cost on Val dataset for this epoch 1057 = 0.0920625908940286\n",
      "learning rate for this epoch =  0.08769023975911351\n",
      "Error on this batch = 0.039498722879754554\n",
      "Error on this batch = 0.0689652600798462\n",
      "Cost on val dataset after 1058 epochs is = 0.09205218055033597\n",
      "Initial Cost on Val dataset for this epoch 1058 = 0.09205218055033597\n",
      "learning rate for this epoch =  0.08766951165466147\n",
      "Error on this batch = 0.03948192007460648\n",
      "Error on this batch = 0.06894491384017946\n",
      "Cost on val dataset after 1059 epochs is = 0.09204178826884704\n",
      "Initial Cost on Val dataset for this epoch 1059 = 0.09204178826884704\n",
      "learning rate for this epoch =  0.08764880802548045\n",
      "Error on this batch = 0.03946514311074284\n",
      "Error on this batch = 0.06892459279496752\n",
      "Cost on val dataset after 1060 epochs is = 0.0920314140005933\n",
      "Initial Cost on Val dataset for this epoch 1060 = 0.0920314140005933\n",
      "learning rate for this epoch =  0.08762812881959987\n",
      "Error on this batch = 0.03944839186244804\n",
      "Error on this batch = 0.06890429682138612\n",
      "Cost on val dataset after 1061 epochs is = 0.09202105769679013\n",
      "Initial Cost on Val dataset for this epoch 1061 = 0.09202105769679013\n",
      "learning rate for this epoch =  0.08760747398520836\n",
      "Error on this batch = 0.03943166620501464\n",
      "Error on this batch = 0.06888402579732533\n",
      "Cost on val dataset after 1062 epochs is = 0.09201071930883768\n",
      "Initial Cost on Val dataset for this epoch 1062 = 0.09201071930883768\n",
      "learning rate for this epoch =  0.08758684347065314\n",
      "Error on this batch = 0.03941496601474276\n",
      "Error on this batch = 0.06886377960138845\n",
      "Cost on val dataset after 1063 epochs is = 0.09200039878832117\n",
      "Initial Cost on Val dataset for this epoch 1063 = 0.09200039878832117\n",
      "learning rate for this epoch =  0.08756623722443944\n",
      "Error on this batch = 0.03939829116893942\n",
      "Error on this batch = 0.06884355811289095\n",
      "Cost on val dataset after 1064 epochs is = 0.09199009608701146\n",
      "Initial Cost on Val dataset for this epoch 1064 = 0.09199009608701146\n",
      "learning rate for this epoch =  0.08754565519522983\n",
      "Error on this batch = 0.0393816415459178\n",
      "Error on this batch = 0.06882336121185915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on val dataset after 1065 epochs is = 0.0919798111568656\n",
      "Initial Cost on Val dataset for this epoch 1065 = 0.0919798111568656\n",
      "learning rate for this epoch =  0.08752509733184359\n",
      "Error on this batch = 0.03936501702499607\n",
      "Error on this batch = 0.06880318877902909\n",
      "Cost on val dataset after 1066 epochs is = 0.09196954395002725\n",
      "Initial Cost on Val dataset for this epoch 1066 = 0.09196954395002725\n",
      "learning rate for this epoch =  0.0875045635832561\n",
      "Error on this batch = 0.03934841748649636\n",
      "Error on this batch = 0.06878304069584525\n",
      "Cost on val dataset after 1067 epochs is = 0.09195929441882714\n",
      "Initial Cost on Val dataset for this epoch 1067 = 0.09195929441882714\n",
      "learning rate for this epoch =  0.08748405389859822\n",
      "Error on this batch = 0.03933184281174341\n",
      "Error on this batch = 0.06876291684445915\n",
      "Cost on val dataset after 1068 epochs is = 0.09194906251578362\n",
      "Initial Cost on Val dataset for this epoch 1068 = 0.09194906251578362\n",
      "learning rate for this epoch =  0.08746356822715562\n",
      "Error on this batch = 0.039315292883063\n",
      "Error on this batch = 0.06874281710772814\n",
      "Cost on val dataset after 1069 epochs is = 0.091938848193603\n",
      "Initial Cost on Val dataset for this epoch 1069 = 0.091938848193603\n",
      "learning rate for this epoch =  0.08744310651836827\n",
      "Error on this batch = 0.03929876758378037\n",
      "Error on this batch = 0.06872274136921394\n",
      "Cost on val dataset after 1070 epochs is = 0.0919286514051801\n",
      "Initial Cost on Val dataset for this epoch 1070 = 0.0919286514051801\n",
      "learning rate for this epoch =  0.08742266872182974\n",
      "Error on this batch = 0.039282266798218377\n",
      "Error on this batch = 0.06870268951318126\n",
      "Cost on val dataset after 1071 epochs is = 0.0919184721035985\n",
      "Initial Cost on Val dataset for this epoch 1071 = 0.0919184721035985\n",
      "learning rate for this epoch =  0.08740225478728658\n",
      "Error on this batch = 0.039265790411695554\n",
      "Error on this batch = 0.06868266142459631\n",
      "Cost on val dataset after 1072 epochs is = 0.09190831024213114\n",
      "Initial Cost on Val dataset for this epoch 1072 = 0.09190831024213114\n",
      "learning rate for this epoch =  0.0873818646646378\n",
      "Error on this batch = 0.039249338310524025\n",
      "Error on this batch = 0.06866265698912542\n",
      "Cost on val dataset after 1073 epochs is = 0.09189816577424054\n",
      "Initial Cost on Val dataset for this epoch 1073 = 0.09189816577424054\n",
      "learning rate for this epoch =  0.08736149830393418\n",
      "Error on this batch = 0.0392329103820071\n",
      "Error on this batch = 0.0686426760931333\n",
      "Cost on val dataset after 1074 epochs is = 0.09188803865357924\n",
      "Initial Cost on Val dataset for this epoch 1074 = 0.09188803865357924\n",
      "learning rate for this epoch =  0.0873411556553777\n",
      "Error on this batch = 0.03921650651443701\n",
      "Error on this batch = 0.06862271862368173\n",
      "Cost on val dataset after 1075 epochs is = 0.09187792883399014\n",
      "Initial Cost on Val dataset for this epoch 1075 = 0.09187792883399014\n",
      "learning rate for this epoch =  0.087320836669321\n",
      "Error on this batch = 0.03920012659709227\n",
      "Error on this batch = 0.06860278446852776\n",
      "Cost on val dataset after 1076 epochs is = 0.09186783626950681\n",
      "Initial Cost on Val dataset for this epoch 1076 = 0.09186783626950681\n",
      "learning rate for this epoch =  0.08730054129626662\n",
      "Error on this batch = 0.03918377052023497\n",
      "Error on this batch = 0.06858287351612213\n",
      "Cost on val dataset after 1077 epochs is = 0.09185776091435384\n",
      "Initial Cost on Val dataset for this epoch 1077 = 0.09185776091435384\n",
      "learning rate for this epoch =  0.08728026948686665\n",
      "Error on this batch = 0.039167438175107885\n",
      "Error on this batch = 0.06856298565560769\n",
      "Cost on val dataset after 1078 epochs is = 0.09184770272294707\n",
      "Initial Cost on Val dataset for this epoch 1078 = 0.09184770272294707\n",
      "learning rate for this epoch =  0.0872600211919219\n",
      "Error on this batch = 0.03915112945393158\n",
      "Error on this batch = 0.06854312077681753\n",
      "Cost on val dataset after 1079 epochs is = 0.09183766164989389\n",
      "Initial Cost on Val dataset for this epoch 1079 = 0.09183766164989389\n",
      "learning rate for this epoch =  0.08723979636238147\n",
      "Error on this batch = 0.03913484424990114\n",
      "Error on this batch = 0.06852327877027332\n",
      "Cost on val dataset after 1080 epochs is = 0.09182763764999353\n",
      "Initial Cost on Val dataset for this epoch 1080 = 0.09182763764999353\n",
      "learning rate for this epoch =  0.08721959494934213\n",
      "Error on this batch = 0.039118582457183075\n",
      "Error on this batch = 0.06850345952718355\n",
      "Cost on val dataset after 1081 epochs is = 0.09181763067823714\n",
      "Initial Cost on Val dataset for this epoch 1081 = 0.09181763067823714\n",
      "learning rate for this epoch =  0.0871994169040477\n",
      "Error on this batch = 0.03910234397091168\n",
      "Error on this batch = 0.06848366293944164\n",
      "Cost on val dataset after 1082 epochs is = 0.09180764068980819\n",
      "Initial Cost on Val dataset for this epoch 1082 = 0.09180764068980819\n",
      "learning rate for this epoch =  0.08717926217788849\n",
      "Error on this batch = 0.03908612868718576\n",
      "Error on this batch = 0.06846388889962417\n",
      "Cost on val dataset after 1083 epochs is = 0.09179766764008249\n",
      "Initial Cost on Val dataset for this epoch 1083 = 0.09179766764008249\n",
      "learning rate for this epoch =  0.0871591307224008\n",
      "Error on this batch = 0.039069936503064756\n",
      "Error on this batch = 0.06844413730098886\n",
      "Cost on val dataset after 1084 epochs is = 0.0917877114846284\n",
      "Initial Cost on Val dataset for this epoch 1084 = 0.0917877114846284\n",
      "learning rate for this epoch =  0.08713902248926618\n",
      "Error on this batch = 0.03905376731656503\n",
      "Error on this batch = 0.06842440803747285\n",
      "Cost on val dataset after 1085 epochs is = 0.09177777217920703\n",
      "Initial Cost on Val dataset for this epoch 1085 = 0.09177777217920703\n",
      "learning rate for this epoch =  0.08711893743031107\n",
      "Error on this batch = 0.03903762102665602\n",
      "Error on this batch = 0.0684047010036905\n",
      "Cost on val dataset after 1086 epochs is = 0.09176784967977222\n",
      "Initial Cost on Val dataset for this epoch 1086 = 0.09176784967977222\n",
      "learning rate for this epoch =  0.08709887549750603\n",
      "Error on this batch = 0.03902149753325613\n",
      "Error on this batch = 0.0683850160949316\n",
      "Cost on val dataset after 1087 epochs is = 0.09175794394247079\n",
      "Initial Cost on Val dataset for this epoch 1087 = 0.09175794394247079\n",
      "learning rate for this epoch =  0.08707883664296534\n",
      "Error on this batch = 0.03900539673722848\n",
      "Error on this batch = 0.06836535320715921\n",
      "Cost on val dataset after 1088 epochs is = 0.09174805492364238\n",
      "Initial Cost on Val dataset for this epoch 1088 = 0.09174805492364238\n",
      "learning rate for this epoch =  0.08705882081894635\n",
      "Error on this batch = 0.03898931854037685\n",
      "Error on this batch = 0.06834571223700775\n",
      "Cost on val dataset after 1089 epochs is = 0.0917381825798197\n",
      "Initial Cost on Val dataset for this epoch 1089 = 0.0917381825798197\n",
      "learning rate for this epoch =  0.08703882797784893\n",
      "Error on this batch = 0.038973262845441135\n",
      "Error on this batch = 0.06832609308178075\n",
      "Cost on val dataset after 1090 epochs is = 0.09172832686772843\n",
      "Initial Cost on Val dataset for this epoch 1090 = 0.09172832686772843\n",
      "learning rate for this epoch =  0.08701885807221492\n",
      "Error on this batch = 0.038957229556092876\n",
      "Error on this batch = 0.06830649563944881\n",
      "Cost on val dataset after 1091 epochs is = 0.09171848774428724\n",
      "cost initial= 0.09172832686772843 , cost final=0.09171848774428724 , change in cost= -9.839123441185427e-06\n",
      "\n",
      "------------------------------------------------------------------------------\n",
      "The stats for number of units in the hidden layer = 50 are as below:\n",
      "------------------------------------------------------------------------------\n",
      "The number of epochs = 1091\n",
      "The training time = 146.514sec\n",
      "The training accuracy is = 93.937%\n",
      "The validation accuracy is = 89.436%\n",
      "The test accuracy is = 87.969%\n",
      "------------------------------------------------------------------------------\n",
      "\n",
      "Training the network with 1 hidden layer with 100 units\n",
      "The parameters of the layers are of the shape:\n",
      "theta between layer 0 and layer 1 is (785, 100)\n",
      "theta between layer 1 and layer 2 is (101, 26)\n",
      "Initial Cost on Val dataset for this epoch 1 = 3.370728911030039\n",
      "learning rate for this epoch =  0.5\n",
      "Error on this batch = 3.363770892805095\n",
      "Error on this batch = 0.48067879758586357\n",
      "Cost on val dataset after 2 epochs is = 0.4802318170924747\n",
      "Initial Cost on Val dataset for this epoch 2 = 0.4802318170924747\n",
      "learning rate for this epoch =  0.4204482076268573\n",
      "Error on this batch = 0.479842758856616\n",
      "Error on this batch = 0.47966407883254564\n",
      "Cost on val dataset after 3 epochs is = 0.47927062800767084\n",
      "Initial Cost on Val dataset for this epoch 3 = 0.47927062800767084\n",
      "learning rate for this epoch =  0.37991784282579627\n",
      "Error on this batch = 0.47884417322346323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on this batch = 0.4787282769517532\n",
      "Cost on val dataset after 4 epochs is = 0.47824564889975324\n",
      "Initial Cost on Val dataset for this epoch 4 = 0.47824564889975324\n",
      "learning rate for this epoch =  0.35355339059327373\n",
      "Error on this batch = 0.4777848800488793\n",
      "Error on this batch = 0.47770924976589857\n",
      "Cost on val dataset after 5 epochs is = 0.47709144120701924\n",
      "Initial Cost on Val dataset for this epoch 5 = 0.47709144120701924\n",
      "learning rate for this epoch =  0.334370152488211\n",
      "Error on this batch = 0.4765945876337596\n",
      "Error on this batch = 0.4765494042482638\n",
      "Cost on val dataset after 6 epochs is = 0.4757397611061292\n",
      "Initial Cost on Val dataset for this epoch 6 = 0.4757397611061292\n",
      "learning rate for this epoch =  0.3194715521231362\n",
      "Error on this batch = 0.475199805630595\n",
      "Error on this batch = 0.4751879256101107\n",
      "Cost on val dataset after 7 epochs is = 0.47410113223493877\n",
      "Initial Cost on Val dataset for this epoch 7 = 0.47410113223493877\n",
      "learning rate for this epoch =  0.3073940764756322\n",
      "Error on this batch = 0.4735058211038954\n",
      "Error on this batch = 0.4735499116307421\n",
      "Cost on val dataset after 8 epochs is = 0.472047399083848\n",
      "Initial Cost on Val dataset for this epoch 8 = 0.472047399083848\n",
      "learning rate for this epoch =  0.29730177875068026\n",
      "Error on this batch = 0.4713778960238949\n",
      "Error on this batch = 0.4715369478113701\n",
      "Cost on val dataset after 9 epochs is = 0.4693869837272503\n",
      "Initial Cost on Val dataset for this epoch 9 = 0.4693869837272503\n",
      "learning rate for this epoch =  0.2886751345948129\n",
      "Error on this batch = 0.46861618865571253\n",
      "Error on this batch = 0.4690117439247193\n",
      "Cost on val dataset after 10 epochs is = 0.46584126588197455\n",
      "Initial Cost on Val dataset for this epoch 10 = 0.46584126588197455\n",
      "learning rate for this epoch =  0.28117066259517454\n",
      "Error on this batch = 0.46493235476781425\n",
      "Error on this batch = 0.4657681639211792\n",
      "Cost on val dataset after 11 epochs is = 0.4610741375666753\n",
      "Initial Cost on Val dataset for this epoch 11 = 0.4610741375666753\n",
      "learning rate for this epoch =  0.2745502433880562\n",
      "Error on this batch = 0.4599783433030107\n",
      "Error on this batch = 0.4615071146151465\n",
      "Cost on val dataset after 12 epochs is = 0.45482226446235235\n",
      "Initial Cost on Val dataset for this epoch 12 = 0.45482226446235235\n",
      "learning rate for this epoch =  0.2686424829558855\n",
      "Error on this batch = 0.45348764352657017\n",
      "Error on this batch = 0.45590628181357146\n",
      "Cost on val dataset after 13 epochs is = 0.4469406097375683\n",
      "Initial Cost on Val dataset for this epoch 13 = 0.4469406097375683\n",
      "learning rate for this epoch =  0.2633201939239633\n",
      "Error on this batch = 0.4453773444099979\n",
      "Error on this batch = 0.44874959220882693\n",
      "Cost on val dataset after 14 epochs is = 0.43739331089450806\n",
      "Initial Cost on Val dataset for this epoch 14 = 0.43739331089450806\n",
      "learning rate for this epoch =  0.2584865769785853\n",
      "Error on this batch = 0.4357808341994803\n",
      "Error on this batch = 0.440053193384169\n",
      "Cost on val dataset after 15 epochs is = 0.4265749950714816\n",
      "Initial Cost on Val dataset for this epoch 15 = 0.4265749950714816\n",
      "learning rate for this epoch =  0.25406637407730737\n",
      "Error on this batch = 0.42525810295305483\n",
      "Error on this batch = 0.4302420203988283\n",
      "Cost on val dataset after 16 epochs is = 0.4152972452295895\n",
      "Initial Cost on Val dataset for this epoch 16 = 0.4152972452295895\n",
      "learning rate for this epoch =  0.25\n",
      "Error on this batch = 0.4146149133937853\n",
      "Error on this batch = 0.42002080164665173\n",
      "Cost on val dataset after 17 epochs is = 0.40425965620782445\n",
      "Initial Cost on Val dataset for this epoch 17 = 0.40425965620782445\n",
      "learning rate for this epoch =  0.24623953025272619\n",
      "Error on this batch = 0.4043986797227748\n",
      "Error on this batch = 0.40998380575070115\n",
      "Cost on val dataset after 18 epochs is = 0.393740234901034\n",
      "Initial Cost on Val dataset for this epoch 18 = 0.393740234901034\n",
      "learning rate for this epoch =  0.24274588585366172\n",
      "Error on this batch = 0.3947039063516128\n",
      "Error on this batch = 0.40039816421656016\n",
      "Cost on val dataset after 19 epochs is = 0.3836835866058639\n",
      "Initial Cost on Val dataset for this epoch 19 = 0.3836835866058639\n",
      "learning rate for this epoch =  0.23948681272178735\n",
      "Error on this batch = 0.3853212415768353\n",
      "Error on this batch = 0.3912893586469578\n",
      "Cost on val dataset after 20 epochs is = 0.37393747235826674\n",
      "Initial Cost on Val dataset for this epoch 20 = 0.37393747235826674\n",
      "learning rate for this epoch =  0.23643540225079396\n",
      "Error on this batch = 0.3759856531087897\n",
      "Error on this batch = 0.3825847027260326\n",
      "Cost on val dataset after 21 epochs is = 0.36438027073349943\n",
      "Initial Cost on Val dataset for this epoch 21 = 0.36438027073349943\n",
      "learning rate for this epoch =  0.23356898886410005\n",
      "Error on this batch = 0.3665046109085726\n",
      "Error on this batch = 0.37418289510517455\n",
      "Cost on val dataset after 22 epochs is = 0.35495362451780443\n",
      "Initial Cost on Val dataset for this epoch 22 = 0.35495362451780443\n",
      "learning rate for this epoch =  0.2308683154720513\n",
      "Error on this batch = 0.3567920924227019\n",
      "Error on this batch = 0.3659856073609785\n",
      "Cost on val dataset after 23 epochs is = 0.34566377928052405\n",
      "Initial Cost on Val dataset for this epoch 23 = 0.34566377928052405\n",
      "learning rate for this epoch =  0.22831689274836564\n",
      "Error on this batch = 0.34686698693594586\n",
      "Error on this batch = 0.3579210361739119\n",
      "Cost on val dataset after 24 epochs is = 0.3365657723287254\n",
      "Initial Cost on Val dataset for this epoch 24 = 0.3365657723287254\n",
      "learning rate for this epoch =  0.22590050090246122\n",
      "Error on this batch = 0.3368280127850001\n",
      "Error on this batch = 0.34995602893543265\n",
      "Cost on val dataset after 25 epochs is = 0.3277319457748633\n",
      "Initial Cost on Val dataset for this epoch 25 = 0.3277319457748633\n",
      "learning rate for this epoch =  0.22360679774997896\n",
      "Error on this batch = 0.32681021986570585\n",
      "Error on this batch = 0.3420911988178588\n",
      "Cost on val dataset after 26 epochs is = 0.319221293095659\n",
      "Initial Cost on Val dataset for this epoch 26 = 0.319221293095659\n",
      "learning rate for this epoch =  0.2214250071345737\n",
      "Error on this batch = 0.3169440827488775\n",
      "Error on this batch = 0.3343467417398063\n",
      "Cost on val dataset after 27 epochs is = 0.3110664800190103\n",
      "Initial Cost on Val dataset for this epoch 27 = 0.3110664800190103\n",
      "learning rate for this epoch =  0.21934566882541542\n",
      "Error on this batch = 0.30733452695148333\n",
      "Error on this batch = 0.3267504027338844\n",
      "Cost on val dataset after 28 epochs is = 0.30327769997832726\n",
      "Initial Cost on Val dataset for this epoch 28 = 0.30327769997832726\n",
      "learning rate for this epoch =  0.2173604359724957\n",
      "Error on this batch = 0.298057844652173\n",
      "Error on this batch = 0.31933171170904606\n",
      "Cost on val dataset after 29 epochs is = 0.2958524139334724\n",
      "Initial Cost on Val dataset for this epoch 29 = 0.2958524139334724\n",
      "learning rate for this epoch =  0.215461909729453\n",
      "Error on this batch = 0.2891658948466859\n",
      "Error on this batch = 0.3121202679413458\n",
      "Cost on val dataset after 30 epochs is = 0.28878319811616293\n",
      "Initial Cost on Val dataset for this epoch 30 = 0.28878319811616293\n",
      "learning rate for this epoch =  0.21364350319811704\n",
      "Error on this batch = 0.2806907946506973\n",
      "Error on this batch = 0.3051449469785089\n",
      "Cost on val dataset after 31 epochs is = 0.2820617564386109\n",
      "Initial Cost on Val dataset for this epoch 31 = 0.2820617564386109\n",
      "learning rate for this epoch =  0.21189932870751083\n",
      "Error on this batch = 0.2726484221031019\n",
      "Error on this batch = 0.29843252985834207\n",
      "Cost on val dataset after 32 epochs is = 0.27567999902652063\n",
      "Initial Cost on Val dataset for this epoch 32 = 0.27567999902652063\n",
      "learning rate for this epoch =  0.21022410381342865\n",
      "Error on this batch = 0.2650412436901205\n",
      "Error on this batch = 0.2920057466392179\n",
      "Cost on val dataset after 33 epochs is = 0.2696296162276538\n",
      "Initial Cost on Val dataset for this epoch 33 = 0.2696296162276538\n",
      "learning rate for this epoch =  0.2086130724305753\n",
      "Error on this batch = 0.25786120047370126\n",
      "Error on this batch = 0.28588140981588395\n",
      "Cost on val dataset after 34 epochs is = 0.26390128880955344\n",
      "Initial Cost on Val dataset for this epoch 34 = 0.26390128880955344\n",
      "learning rate for this epoch =  0.20706193828327601\n",
      "Error on this batch = 0.25109288372398103\n",
      "Error on this batch = 0.28006930346818865\n",
      "Cost on val dataset after 35 epochs is = 0.2584841846335774\n",
      "Initial Cost on Val dataset for this epoch 35 = 0.2584841846335774\n",
      "learning rate for this epoch =  0.20556680845025985\n",
      "Error on this batch = 0.2447167006674939\n",
      "Error on this batch = 0.27457209132402977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on val dataset after 36 epochs is = 0.2533659223580663\n",
      "Initial Cost on Val dataset for this epoch 36 = 0.2533659223580663\n",
      "learning rate for this epoch =  0.20412414523193154\n",
      "Error on this batch = 0.238711536932313\n",
      "Error on this batch = 0.2693860788508927\n",
      "Cost on val dataset after 37 epochs is = 0.2485328835588622\n",
      "Initial Cost on Val dataset for this epoch 37 = 0.2485328835588622\n",
      "learning rate for this epoch =  0.2027307249193849\n",
      "Error on this batch = 0.233056573570794\n",
      "Error on this batch = 0.26450245283255475\n",
      "Cost on val dataset after 38 epochs is = 0.24397066476137078\n",
      "Initial Cost on Val dataset for this epoch 38 = 0.24397066476137078\n",
      "learning rate for this epoch =  0.20138360231828867\n",
      "Error on this batch = 0.2277322009504202\n",
      "Error on this batch = 0.2599086389855709\n",
      "Cost on val dataset after 39 epochs is = 0.23966450807873055\n",
      "Initial Cost on Val dataset for this epoch 39 = 0.23966450807873055\n",
      "learning rate for this epoch =  0.20008008009612496\n",
      "Error on this batch = 0.22272020204683793\n",
      "Error on this batch = 0.2555895469859843\n",
      "Cost on val dataset after 40 epochs is = 0.23559963649044724\n",
      "Initial Cost on Val dataset for this epoch 40 = 0.23559963649044724\n",
      "learning rate for this epoch =  0.19881768219176266\n",
      "Error on this batch = 0.2180034867206912\n",
      "Error on this batch = 0.25152860393262527\n",
      "Cost on val dataset after 41 epochs is = 0.23176148730197074\n",
      "Initial Cost on Val dataset for this epoch 41 = 0.23176148730197074\n",
      "learning rate for this epoch =  0.19759413066220238\n",
      "Error on this batch = 0.2135656584910171\n",
      "Error on this batch = 0.24770856345532838\n",
      "Cost on val dataset after 42 epochs is = 0.2281358687021009\n",
      "Initial Cost on Val dataset for this epoch 42 = 0.2281358687021009\n",
      "learning rate for this epoch =  0.1964073254502565\n",
      "Error on this batch = 0.2093906248182994\n",
      "Error on this batch = 0.24411211675173344\n",
      "Cost on val dataset after 43 epochs is = 0.22470906815490901\n",
      "Initial Cost on Val dataset for this epoch 43 = 0.22470906815490901\n",
      "learning rate for this epoch =  0.19525532664475806\n",
      "Error on this batch = 0.2054623636737709\n",
      "Error on this batch = 0.24072234083348043\n",
      "Cost on val dataset after 44 epochs is = 0.22146793252709576\n",
      "Initial Cost on Val dataset for this epoch 44 = 0.22146793252709576\n",
      "learning rate for this epoch =  0.19413633887611162\n",
      "Error on this batch = 0.20176486974366484\n",
      "Error on this batch = 0.23752301542367732\n",
      "Cost on val dataset after 45 epochs is = 0.218399929544623\n",
      "Initial Cost on Val dataset for this epoch 45 = 0.218399929544623\n",
      "learning rate for this epoch =  0.19304869754804485\n",
      "Error on this batch = 0.19828224477354894\n",
      "Error on this batch = 0.23449883377504338\n",
      "Cost on val dataset after 46 epochs is = 0.21549319341035916\n",
      "Initial Cost on Val dataset for this epoch 46 = 0.21549319341035916\n",
      "learning rate for this epoch =  0.19199085665396748\n",
      "Error on this batch = 0.1949988723626575\n",
      "Error on this batch = 0.23163552811545324\n",
      "Cost on val dataset after 47 epochs is = 0.21273655470051608\n",
      "Initial Cost on Val dataset for this epoch 47 = 0.21273655470051608\n",
      "learning rate for this epoch =  0.1909613779654767\n",
      "Error on this batch = 0.1918996194225195\n",
      "Error on this batch = 0.22891992767979644\n",
      "Cost on val dataset after 48 epochs is = 0.2101195545592473\n",
      "Initial Cost on Val dataset for this epoch 48 = 0.2101195545592473\n",
      "learning rate for this epoch =  0.18995892141289814\n",
      "Error on this batch = 0.1889700218527882\n",
      "Error on this batch = 0.22633996539291734\n",
      "Cost on val dataset after 49 epochs is = 0.2076324441519232\n",
      "Initial Cost on Val dataset for this epoch 49 = 0.2076324441519232\n",
      "learning rate for this epoch =  0.1889822365046136\n",
      "Error on this batch = 0.18619643030624616\n",
      "Error on this batch = 0.22388464743442554\n",
      "Cost on val dataset after 50 epochs is = 0.20526617131036312\n",
      "Initial Cost on Val dataset for this epoch 50 = 0.20526617131036312\n",
      "learning rate for this epoch =  0.1880301546543197\n",
      "Error on this batch = 0.18356610725273537\n",
      "Error on this batch = 0.2215439978410836\n",
      "Cost on val dataset after 51 epochs is = 0.2030123568905061\n",
      "Initial Cost on Val dataset for this epoch 51 = 0.2030123568905061\n",
      "learning rate for this epoch =  0.18710158230410626\n",
      "Error on this batch = 0.1810672768609169\n",
      "Error on this batch = 0.2193089880400556\n",
      "Cost on val dataset after 52 epochs is = 0.20086326351768255\n",
      "Initial Cost on Val dataset for this epoch 52 = 0.20086326351768255\n",
      "learning rate for this epoch =  0.1861954947469912\n",
      "Error on this batch = 0.1786891347907742\n",
      "Error on this batch = 0.21717145894904896\n",
      "Cost on val dataset after 53 epochs is = 0.19881175922765884\n",
      "Initial Cost on Val dataset for this epoch 53 = 0.19881175922765884\n",
      "learning rate for this epoch =  0.18531093056582568\n",
      "Error on this batch = 0.17642182706601856\n",
      "Error on this batch = 0.21512404121290496\n",
      "Cost on val dataset after 54 epochs is = 0.1968512781628266\n",
      "Initial Cost on Val dataset for this epoch 54 = 0.1968512781628266\n",
      "learning rate for this epoch =  0.18444698661672027\n",
      "Error on this batch = 0.17425640710638055\n",
      "Error on this batch = 0.21316007738864298\n",
      "Cost on val dataset after 55 epochs is = 0.19497578006571661\n",
      "Initial Cost on Val dataset for this epoch 55 = 0.19497578006571661\n",
      "learning rate for this epoch =  0.1836028134946796\n",
      "Error on this batch = 0.1721847788036391\n",
      "Error on this batch = 0.2112735484871849\n",
      "Cost on val dataset after 56 epochs is = 0.1931797099005366\n",
      "Initial Cost on Val dataset for this epoch 56 = 0.1931797099005366\n",
      "learning rate for this epoch =  0.18277761142725618\n",
      "Error on this batch = 0.17019963192865534\n",
      "Error on this batch = 0.20945900621960145\n",
      "Cost on val dataset after 57 epochs is = 0.19145795856772718\n",
      "Initial Cost on Val dataset for this epoch 57 = 0.19145795856772718\n",
      "learning rate for this epoch =  0.18197062654897384\n",
      "Error on this batch = 0.1682943745672349\n",
      "Error on this batch = 0.2077115115360176\n",
      "Cost on val dataset after 58 epochs is = 0.1898058253727849\n",
      "Initial Cost on Val dataset for this epoch 58 = 0.1898058253727849\n",
      "learning rate for this epoch =  0.1811811475152165\n",
      "Error on this batch = 0.16646306589749146\n",
      "Error on this batch = 0.20602657953022377\n",
      "Cost on val dataset after 59 epochs is = 0.18821898267033874\n",
      "Initial Cost on Val dataset for this epoch 59 = 0.18821898267033874\n",
      "learning rate for this epoch =  0.180408502419387\n",
      "Error on this batch = 0.16470035150780177\n",
      "Error on this batch = 0.2044001304558344\n",
      "Cost on val dataset after 60 epochs is = 0.18669344292148193\n",
      "Initial Cost on Val dataset for this epoch 60 = 0.18669344292148193\n",
      "learning rate for this epoch =  0.17965205598154213\n",
      "Error on this batch = 0.1630014026089211\n",
      "Error on this batch = 0.20282844641012696\n",
      "Cost on val dataset after 61 epochs is = 0.18522552826723243\n",
      "Initial Cost on Val dataset for this epoch 61 = 0.18522552826723243\n",
      "learning rate for this epoch =  0.1789112069805131\n",
      "Error on this batch = 0.16136185988028953\n",
      "Error on this batch = 0.2013081331482607\n",
      "Cost on val dataset after 62 epochs is = 0.18381184262384548\n",
      "Initial Cost on Val dataset for this epoch 62 = 0.18381184262384548\n",
      "learning rate for this epoch =  0.1781853859048144\n",
      "Error on this batch = 0.15977778226269948\n",
      "Error on this batch = 0.19983608646165668\n",
      "Cost on val dataset after 63 epochs is = 0.18244924623762604\n",
      "Initial Cost on Val dataset for this epoch 63 = 0.18244924623762604\n",
      "learning rate for this epoch =  0.17747405280050266\n",
      "Error on this batch = 0.15824560072240548\n",
      "Error on this batch = 0.19840946256652117\n",
      "Cost on val dataset after 64 epochs is = 0.1811348325905737\n",
      "Initial Cost on Val dataset for this epoch 64 = 0.1811348325905737\n",
      "learning rate for this epoch =  0.17677669529663687\n",
      "Error on this batch = 0.15676207682713372\n",
      "Error on this batch = 0.1970256519852165\n",
      "Cost on val dataset after 65 epochs is = 0.1798659075180016\n",
      "Initial Cost on Val dataset for this epoch 65 = 0.1798659075180016\n",
      "learning rate for this epoch =  0.1760928267911618\n",
      "Error on this batch = 0.15532426586203213\n",
      "Error on this batch = 0.19568225645296497\n",
      "Cost on val dataset after 66 epochs is = 0.1786399703810307\n",
      "Initial Cost on Val dataset for this epoch 66 = 0.1786399703810307\n",
      "learning rate for this epoch =  0.17542198478193427\n",
      "Error on this batch = 0.1539294841507801\n",
      "Error on this batch = 0.19437706843753105\n",
      "Cost on val dataset after 67 epochs is = 0.17745469712755943\n",
      "Initial Cost on Val dataset for this epoch 67 = 0.17745469712755943\n",
      "learning rate for this epoch =  0.1747637293292756\n",
      "Error on this batch = 0.15257528021763386\n",
      "Error on this batch = 0.19310805291501582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on val dataset after 68 epochs is = 0.17630792507276774\n",
      "Initial Cost on Val dataset for this epoch 68 = 0.17630792507276774\n",
      "learning rate for this epoch =  0.1741176416378927\n",
      "Error on this batch = 0.15125940941885968\n",
      "Error on this batch = 0.19187333109752353\n",
      "Cost on val dataset after 69 epochs is = 0.17519763923279572\n",
      "Initial Cost on Val dataset for this epoch 69 = 0.17519763923279572\n",
      "learning rate for this epoch =  0.1734833227472955\n",
      "Error on this batch = 0.14997981167913602\n",
      "Error on this batch = 0.190671165856325\n",
      "Cost on val dataset after 70 epochs is = 0.17412196005168065\n",
      "Initial Cost on Val dataset for this epoch 70 = 0.17412196005168065\n",
      "learning rate for this epoch =  0.1728603923209705\n",
      "Error on this batch = 0.1487345919849273\n",
      "Error on this batch = 0.18949994862623581\n",
      "Cost on val dataset after 71 epochs is = 0.17307913237087885\n",
      "Initial Cost on Val dataset for this epoch 71 = 0.17307913237087885\n",
      "learning rate for this epoch =  0.17224848752556968\n",
      "Error on this batch = 0.14752200330904133\n",
      "Error on this batch = 0.18835818761290476\n",
      "Cost on val dataset after 72 epochs is = 0.1720675155018553\n",
      "Initial Cost on Val dataset for this epoch 72 = 0.1720675155018553\n",
      "learning rate for this epoch =  0.17164726199225983\n",
      "Error on this batch = 0.1463404316661608\n",
      "Error on this batch = 0.18724449715467195\n",
      "Cost on val dataset after 73 epochs is = 0.17108557427451276\n",
      "Initial Cost on Val dataset for this epoch 73 = 0.17108557427451276\n",
      "learning rate for this epoch =  0.17105638485316074\n",
      "Error on this batch = 0.14518838302637244\n",
      "Error on this batch = 0.18615758811502758\n",
      "Cost on val dataset after 74 epochs is = 0.1701318709470048\n",
      "Initial Cost on Val dataset for this epoch 74 = 0.1701318709470048\n",
      "learning rate for this epoch =  0.1704755398464977\n",
      "Error on this batch = 0.14406447184135052\n",
      "Error on this batch = 0.18509625920110848\n",
      "Cost on val dataset after 75 epochs is = 0.16920505787518125\n",
      "Initial Cost on Val dataset for this epoch 75 = 0.16920505787518125\n",
      "learning rate for this epoch =  0.16990442448471224\n",
      "Error on this batch = 0.14296741096496848\n",
      "Error on this batch = 0.1840593891188428\n",
      "Cost on val dataset after 76 epochs is = 0.16830387085213075\n",
      "Initial Cost on Val dataset for this epoch 76 = 0.16830387085213075\n",
      "learning rate for this epoch =  0.16934274928032858\n",
      "Error on this batch = 0.14189600277603892\n",
      "Error on this batch = 0.18304592948705994\n",
      "Cost on val dataset after 77 epochs is = 0.1674271230397038\n",
      "Initial Cost on Val dataset for this epoch 77 = 0.1674271230397038\n",
      "learning rate for this epoch =  0.16879023702486318\n",
      "Error on this batch = 0.14084913133515245\n",
      "Error on this batch = 0.18205489844185815\n",
      "Cost on val dataset after 78 epochs is = 0.16657369942431405\n",
      "Initial Cost on Val dataset for this epoch 78 = 0.16657369942431405\n",
      "learning rate for this epoch =  0.16824662211650757\n",
      "Error on this batch = 0.13982575542989717\n",
      "Error on this batch = 0.18108537486943185\n",
      "Cost on val dataset after 79 epochs is = 0.16574255173861935\n",
      "Initial Cost on Val dataset for this epoch 79 = 0.16574255173861935\n",
      "learning rate for this epoch =  0.1677116499327062\n",
      "Error on this batch = 0.13882490238293485\n",
      "Error on this batch = 0.18013649321098243\n",
      "Cost on val dataset after 80 epochs is = 0.16493269379884856\n",
      "Initial Cost on Val dataset for this epoch 80 = 0.16493269379884856\n",
      "learning rate for this epoch =  0.1671850762441055\n",
      "Error on this batch = 0.13784566251543967\n",
      "Error on this batch = 0.1792074387877264\n",
      "Cost on val dataset after 81 epochs is = 0.16414319721458925\n",
      "Initial Cost on Val dataset for this epoch 81 = 0.16414319721458925\n",
      "learning rate for this epoch =  0.16666666666666666\n",
      "Error on this batch = 0.13688718417431506\n",
      "Error on this batch = 0.17829744359773603\n",
      "Cost on val dataset after 82 epochs is = 0.16337318743386484\n",
      "Initial Cost on Val dataset for this epoch 82 = 0.16337318743386484\n",
      "learning rate for this epoch =  0.16615619614902008\n",
      "Error on this batch = 0.13594866924549687\n",
      "Error on this batch = 0.17740578253966324\n",
      "Cost on val dataset after 83 epochs is = 0.16262184009140274\n",
      "Initial Cost on Val dataset for this epoch 83 = 0.16262184009140274\n",
      "learning rate for this epoch =  0.16565344849239508\n",
      "Error on this batch = 0.1350293690876767\n",
      "Error on this batch = 0.17653177002147988\n",
      "Cost on val dataset after 84 epochs is = 0.16188837763224057\n",
      "Initial Cost on Val dataset for this epoch 84 = 0.16188837763224057\n",
      "learning rate for this epoch =  0.16515821590069035\n",
      "Error on this batch = 0.1341285808311091\n",
      "Error on this batch = 0.17567475691532378\n",
      "Cost on val dataset after 85 epochs is = 0.1611720661863528\n",
      "Initial Cost on Val dataset for this epoch 85 = 0.1611720661863528\n",
      "learning rate for this epoch =  0.164670298558459\n",
      "Error on this batch = 0.13324564399498\n",
      "Error on this batch = 0.17483412782243624\n",
      "Cost on val dataset after 86 epochs is = 0.1604722126729211\n",
      "Initial Cost on Val dataset for this epoch 86 = 0.1604722126729211\n",
      "learning rate for this epoch =  0.16418950423477013\n",
      "Error on this batch = 0.1323799373842913\n",
      "Error on this batch = 0.17400929861502193\n",
      "Cost on val dataset after 87 epochs is = 0.1597881621153106\n",
      "Initial Cost on Val dataset for this epoch 87 = 0.1597881621153106\n",
      "learning rate for this epoch =  0.16371564791108048\n",
      "Error on this batch = 0.13153087623352774\n",
      "Error on this batch = 0.17319971422465957\n",
      "Cost on val dataset after 88 epochs is = 0.1591192951498562\n",
      "Initial Cost on Val dataset for this epoch 88 = 0.1591192951498562\n",
      "learning rate for this epoch =  0.16324855143140263\n",
      "Error on this batch = 0.13069790956967392\n",
      "Error on this batch = 0.17240484664962175\n",
      "Cost on val dataset after 89 epochs is = 0.1584650257132713\n",
      "Initial Cost on Val dataset for this epoch 89 = 0.1584650257132713\n",
      "learning rate for this epoch =  0.1627880431731981\n",
      "Error on this batch = 0.12988051777158102\n",
      "Error on this batch = 0.17162419315609945\n",
      "Cost on val dataset after 90 epochs is = 0.1578247988949438\n",
      "Initial Cost on Val dataset for this epoch 90 = 0.1578247988949438\n",
      "learning rate for this epoch =  0.16233395773754944\n",
      "Error on this batch = 0.12907821030637406\n",
      "Error on this batch = 0.1708572746508513\n",
      "Cost on val dataset after 91 epochs is = 0.15719808894162335\n",
      "Initial Cost on Val dataset for this epoch 91 = 0.15719808894162335\n",
      "learning rate for this epoch =  0.16188613565728216\n",
      "Error on this batch = 0.12829052362664567\n",
      "Error on this batch = 0.17010363420517977\n",
      "Cost on val dataset after 92 epochs is = 0.15658439740307936\n",
      "Initial Cost on Val dataset for this epoch 92 = 0.15658439740307936\n",
      "learning rate for this epoch =  0.161444423121811\n",
      "Error on this batch = 0.12751701921470165\n",
      "Error on this batch = 0.1693628357123677\n",
      "Cost on val dataset after 93 epochs is = 0.15598325140825237\n",
      "Initial Cost on Val dataset for this epoch 93 = 0.15598325140825237\n",
      "learning rate for this epoch =  0.16100867171758368\n",
      "Error on this batch = 0.12675728176218726\n",
      "Error on this batch = 0.16863446266277204\n",
      "Cost on val dataset after 94 epochs is = 0.15539420206225524\n",
      "Initial Cost on Val dataset for this epoch 94 = 0.15539420206225524\n",
      "learning rate for this epoch =  0.160578738183079\n",
      "Error on this batch = 0.12601091747510348\n",
      "Error on this batch = 0.16791811702266066\n",
      "Cost on val dataset after 95 epochs is = 0.1548168229553294\n",
      "Initial Cost on Val dataset for this epoch 95 = 0.1548168229553294\n",
      "learning rate for this epoch =  0.16015448417739933\n",
      "Error on this batch = 0.1252775524955818\n",
      "Error on this batch = 0.1672134182045961\n",
      "Cost on val dataset after 96 epochs is = 0.15425070877554056\n",
      "Initial Cost on Val dataset for this epoch 96 = 0.15425070877554056\n",
      "learning rate for this epoch =  0.1597357760615681\n",
      "Error on this batch = 0.12455683143287619\n",
      "Error on this batch = 0.1665200021187146\n",
      "Cost on val dataset after 97 epochs is = 0.153695474017613\n",
      "Initial Cost on Val dataset for this epoch 97 = 0.153695474017613\n",
      "learning rate for this epoch =  0.15932248469171095\n",
      "Error on this batch = 0.12384841599690093\n",
      "Error on this batch = 0.16583752029562845\n",
      "Cost on val dataset after 98 epochs is = 0.15315075178086737\n",
      "Initial Cost on Val dataset for this epoch 98 = 0.15315075178086737\n",
      "learning rate for this epoch =  0.15891448522335927\n",
      "Error on this batch = 0.1231519837283273\n",
      "Error on this batch = 0.1651656390729049\n",
      "Cost on val dataset after 99 epochs is = 0.15261619264975096\n",
      "Initial Cost on Val dataset for this epoch 99 = 0.15261619264975096\n",
      "learning rate for this epoch =  0.15851165692617153\n",
      "Error on this batch = 0.12246722681979189\n",
      "Error on this batch = 0.1645040388381536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on val dataset after 100 epochs is = 0.1520914636509267\n",
      "Initial Cost on Val dataset for this epoch 100 = 0.1520914636509267\n",
      "learning rate for this epoch =  0.15811388300841897\n",
      "Error on this batch = 0.12179385102318711\n",
      "Error on this batch = 0.16385241332269865\n",
      "Cost on val dataset after 101 epochs is = 0.15157624728133715\n",
      "Initial Cost on Val dataset for this epoch 101 = 0.15157624728133715\n",
      "learning rate for this epoch =  0.1577210504506286\n",
      "Error on this batch = 0.12113157463833028\n",
      "Error on this batch = 0.16321046894063387\n",
      "Cost on val dataset after 102 epochs is = 0.15107024060206975\n",
      "Initial Cost on Val dataset for this epoch 102 = 0.15107024060206975\n",
      "learning rate for this epoch =  0.1573330498478208\n",
      "Error on this batch = 0.12048012757855965\n",
      "Error on this batch = 0.1625779241687749\n",
      "Cost on val dataset after 103 epochs is = 0.1505731543932366\n",
      "Initial Cost on Val dataset for this epoch 103 = 0.1505731543932366\n",
      "learning rate for this epoch =  0.15694977525981785\n",
      "Error on this batch = 0.11983925050900403\n",
      "Error on this batch = 0.1619545089636327\n",
      "Cost on val dataset after 104 epochs is = 0.15008471236543644\n",
      "Initial Cost on Val dataset for this epoch 104 = 0.15008471236543644\n",
      "learning rate for this epoch =  0.15657112406913673\n",
      "Error on this batch = 0.11920869405342979\n",
      "Error on this batch = 0.16133996421206453\n",
      "Cost on val dataset after 105 epochs is = 0.14960465042369664\n",
      "Initial Cost on Val dataset for this epoch 105 = 0.14960465042369664\n",
      "learning rate for this epoch =  0.1561969968460128\n",
      "Error on this batch = 0.11858821806569977\n",
      "Error on this batch = 0.16073404121270685\n",
      "Cost on val dataset after 106 epochs is = 0.14913271598010236\n",
      "Initial Cost on Val dataset for this epoch 106 = 0.14913271598010236\n",
      "learning rate for this epoch =  0.15582729722013278\n",
      "Error on this batch = 0.11797759096199081\n",
      "Error on this batch = 0.1601365011856797\n",
      "Cost on val dataset after 107 epochs is = 0.1486686673116011\n",
      "Initial Cost on Val dataset for this epoch 107 = 0.1486686673116011\n",
      "learning rate for this epoch =  0.15546193175868359\n",
      "Error on this batch = 0.1173765891100185\n",
      "Error on this batch = 0.15954711480837927\n",
      "Cost on val dataset after 108 epochs is = 0.1482122729597372\n",
      "Initial Cost on Val dataset for this epoch 108 = 0.1482122729597372\n",
      "learning rate for this epoch =  0.15510080985034994\n",
      "Error on this batch = 0.11678499627161688\n",
      "Error on this batch = 0.15896566177544905\n",
      "Cost on val dataset after 109 epochs is = 0.14776331116931332\n",
      "Initial Cost on Val dataset for this epoch 109 = 0.14776331116931332\n",
      "learning rate for this epoch =  0.1547438435949191\n",
      "Error on this batch = 0.11620260309511864\n",
      "Error on this batch = 0.15839193038125568\n",
      "Cost on val dataset after 110 epochs is = 0.14732156936320226\n",
      "Initial Cost on Val dataset for this epoch 110 = 0.14732156936320226\n",
      "learning rate for this epoch =  0.1543909476981724\n",
      "Error on this batch = 0.11562920665408397\n",
      "Error on this batch = 0.1578257171233903\n",
      "Cost on val dataset after 111 epochs is = 0.1468868436507419\n",
      "Initial Cost on Val dataset for this epoch 111 = 0.1468868436507419\n",
      "learning rate for this epoch =  0.15404203937176525\n",
      "Error on this batch = 0.11506461002903452\n",
      "Error on this batch = 0.15726682632588193\n",
      "Cost on val dataset after 112 epochs is = 0.14645893836733784\n",
      "Initial Cost on Val dataset for this epoch 112 = 0.14645893836733784\n",
      "learning rate for this epoch =  0.1536970382378161\n",
      "Error on this batch = 0.11450862192896243\n",
      "Error on this batch = 0.15671506978094812\n",
      "Cost on val dataset after 113 epochs is = 0.14603766564307855\n",
      "Initial Cost on Val dataset for this epoch 113 = 0.14603766564307855\n",
      "learning rate for this epoch =  0.15335586623794323\n",
      "Error on this batch = 0.11396105634950583\n",
      "Error on this batch = 0.15617026640822623\n",
      "Cost on val dataset after 114 epochs is = 0.14562284499833092\n",
      "Initial Cost on Val dataset for this epoch 114 = 0.14562284499833092\n",
      "learning rate for this epoch =  0.1530184475465045\n",
      "Error on this batch = 0.11342173226481234\n",
      "Error on this batch = 0.15563224193052505\n",
      "Cost on val dataset after 115 epochs is = 0.1452143029644364\n",
      "Initial Cost on Val dataset for this epoch 115 = 0.1452143029644364\n",
      "learning rate for this epoch =  0.15268470848781107\n",
      "Error on this batch = 0.11289047335024321\n",
      "Error on this batch = 0.15510082856522175\n",
      "Cost on val dataset after 116 epochs is = 0.14481187272777044\n",
      "Initial Cost on Val dataset for this epoch 116 = 0.14481187272777044\n",
      "learning rate for this epoch =  0.1523545774571\n",
      "Error on this batch = 0.11236710773321404\n",
      "Error on this batch = 0.15457586473049786\n",
      "Cost on val dataset after 117 epochs is = 0.14441539379555318\n",
      "Initial Cost on Val dataset for this epoch 117 = 0.14441539379555318\n",
      "learning rate for this epoch =  0.15202798484506466\n",
      "Error on this batch = 0.11185146776960853\n",
      "Error on this batch = 0.15405719476567029\n",
      "Cost on val dataset after 118 epochs is = 0.14402471168192305\n",
      "Initial Cost on Val dataset for this epoch 118 = 0.14402471168192305\n",
      "learning rate for this epoch =  0.15170486296575364\n",
      "Error on this batch = 0.11134338984335092\n",
      "Error on this batch = 0.15354466866492142\n",
      "Cost on val dataset after 119 epochs is = 0.14363967761289215\n",
      "Initial Cost on Val dataset for this epoch 119 = 0.14363967761289215\n",
      "learning rate for this epoch =  0.1513851459876605\n",
      "Error on this batch = 0.11084271418686895\n",
      "Error on this batch = 0.1530381418237815\n",
      "Cost on val dataset after 120 epochs is = 0.1432601482489054\n",
      "Initial Cost on Val dataset for this epoch 120 = 0.1432601482489054\n",
      "learning rate for this epoch =  0.1510687698678384\n",
      "Error on this batch = 0.11034928472032768\n",
      "Error on this batch = 0.15253747479775046\n",
      "Cost on val dataset after 121 epochs is = 0.1428859854238178\n",
      "Initial Cost on Val dataset for this epoch 121 = 0.1428859854238178\n",
      "learning rate for this epoch =  0.15075567228888181\n",
      "Error on this batch = 0.10986294890766245\n",
      "Error on this batch = 0.15204253307248544\n",
      "Cost on val dataset after 122 epochs is = 0.14251705589919167\n",
      "Initial Cost on Val dataset for this epoch 122 = 0.14251705589919167\n",
      "learning rate for this epoch =  0.1504457925986288\n",
      "Error on this batch = 0.10938355762758274\n",
      "Error on this batch = 0.15155318684500654\n",
      "Cost on val dataset after 123 epochs is = 0.14215323113289488\n",
      "Initial Cost on Val dataset for this epoch 123 = 0.14215323113289488\n",
      "learning rate for this epoch =  0.15013907175244492\n",
      "Error on this batch = 0.10891096505786127\n",
      "Error on this batch = 0.15106931081540467\n",
      "Cost on val dataset after 124 epochs is = 0.14179438706105443\n",
      "Initial Cost on Val dataset for this epoch 124 = 0.14179438706105443\n",
      "learning rate for this epoch =  0.1498354522579582\n",
      "Error on this batch = 0.108445028571359\n",
      "Error on this batch = 0.15059078398855685\n",
      "Cost on val dataset after 125 epochs is = 0.1414404038924879\n",
      "Initial Cost on Val dataset for this epoch 125 = 0.1414404038924879\n",
      "learning rate for this epoch =  0.14953487812212204\n",
      "Error on this batch = 0.10798560864236954\n",
      "Error on this batch = 0.15011748948538156\n",
      "Cost on val dataset after 126 epochs is = 0.1410911659147975\n",
      "Initial Cost on Val dataset for this epoch 126 = 0.1410911659147975\n",
      "learning rate for this epoch =  0.14923729480049114\n",
      "Error on this batch = 0.10753256876199271\n",
      "Error on this batch = 0.14964931436318393\n",
      "Cost on val dataset after 127 epochs is = 0.1407465613113697\n",
      "Initial Cost on Val dataset for this epoch 127 = 0.1407465613113697\n",
      "learning rate for this epoch =  0.14894264914859962\n",
      "Error on this batch = 0.10708577536136783\n",
      "Error on this batch = 0.14918614944466405\n",
      "Cost on val dataset after 128 epochs is = 0.14040648198857697\n",
      "Initial Cost on Val dataset for this epoch 128 = 0.14040648198857697\n",
      "learning rate for this epoch =  0.14865088937534013\n",
      "Error on this batch = 0.1066450977417103\n",
      "Error on this batch = 0.1487278891551791\n",
      "Cost on val dataset after 129 epochs is = 0.14007082341252647\n",
      "Initial Cost on Val dataset for this epoch 129 = 0.14007082341252647\n",
      "learning rate for this epoch =  0.14836196499824542\n",
      "Error on this batch = 0.10621040801020364\n",
      "Error on this batch = 0.14827443136786733\n",
      "Cost on val dataset after 130 epochs is = 0.13973948445474751\n",
      "Initial Cost on Val dataset for this epoch 130 = 0.13973948445474751\n",
      "learning rate for this epoch =  0.14807582680058123\n",
      "Error on this batch = 0.10578158102089781\n",
      "Error on this batch = 0.14782567725626167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on val dataset after 131 epochs is = 0.13941236724625145\n",
      "Initial Cost on Val dataset for this epoch 131 = 0.13941236724625145\n",
      "learning rate for this epoch =  0.14779242679016388\n",
      "Error on this batch = 0.10535849431985927\n",
      "Error on this batch = 0.14738153115403305\n",
      "Cost on val dataset after 132 epochs is = 0.139089377039436\n",
      "Initial Cost on Val dataset for this epoch 132 = 0.139089377039436\n",
      "learning rate for this epoch =  0.1475117181598202\n",
      "Error on this batch = 0.10494102809390367\n",
      "Error on this batch = 0.1469419004215231\n",
      "Cost on val dataset after 133 epochs is = 0.1387704220773431\n",
      "Initial Cost on Val dataset for this epoch 133 = 0.1387704220773431\n",
      "learning rate for this epoch =  0.147233655249413\n",
      "Error on this batch = 0.10452906512232256\n",
      "Error on this batch = 0.1465066953187384\n",
      "Cost on val dataset after 134 epochs is = 0.13845541346981355\n",
      "Initial Cost on Val dataset for this epoch 134 = 0.13845541346981355\n",
      "learning rate for this epoch =  0.1469581935093583\n",
      "Error on this batch = 0.10412249073108817\n",
      "Error on this batch = 0.1460758288844931\n",
      "Cost on val dataset after 135 epochs is = 0.13814426507610986\n",
      "Initial Cost on Val dataset for this epoch 135 = 0.13814426507610986\n",
      "learning rate for this epoch =  0.14668528946556555\n",
      "Error on this batch = 0.10372119274908688\n",
      "Error on this batch = 0.14564921682140158\n",
      "Cost on val dataset after 136 epochs is = 0.13783689339361227\n",
      "Initial Cost on Val dataset for this epoch 136 = 0.13783689339361227\n",
      "learning rate for this epoch =  0.14641490068573487\n",
      "Error on this batch = 0.10332506146599338\n",
      "Error on this batch = 0.14522677738643489\n",
      "Cost on val dataset after 137 epochs is = 0.13753321745221492\n",
      "Initial Cost on Val dataset for this epoch 137 = 0.13753321745221492\n",
      "learning rate for this epoch =  0.14614698574694937\n",
      "Error on this batch = 0.10293398959145204\n",
      "Error on this batch = 0.14480843128676793\n",
      "Cost on val dataset after 138 epochs is = 0.13723315871407746\n",
      "Initial Cost on Val dataset for this epoch 138 = 0.13723315871407746\n",
      "learning rate for this epoch =  0.145881504204504\n",
      "Error on this batch = 0.10254787221528154\n",
      "Error on this batch = 0.14439410158065652\n",
      "Cost on val dataset after 139 epochs is = 0.13693664097840869\n",
      "Initial Cost on Val dataset for this epoch 139 = 0.13693664097840869\n",
      "learning rate for this epoch =  0.14561841656191457\n",
      "Error on this batch = 0.10216660676846345\n",
      "Error on this batch = 0.14398371358309525\n",
      "Cost on val dataset after 140 epochs is = 0.13664359029098103\n",
      "Initial Cost on Val dataset for this epoch 140 = 0.13664359029098103\n",
      "learning rate for this epoch =  0.14535768424205484\n",
      "Error on this batch = 0.10179009298471516\n",
      "Error on this batch = 0.14357719477601777\n",
      "Cost on val dataset after 141 epochs is = 0.1363539348580941\n",
      "Initial Cost on Val dataset for this epoch 141 = 0.1363539348580941\n",
      "learning rate for this epoch =  0.14509926955937089\n",
      "Error on this batch = 0.10141823286248258\n",
      "Error on this batch = 0.14317447472281322\n",
      "Cost on val dataset after 142 epochs is = 0.13606760496472486\n",
      "Initial Cost on Val dataset for this epoch 142 = 0.13606760496472486\n",
      "learning rate for this epoch =  0.1448431356931257\n",
      "Error on this batch = 0.10105093062722009\n",
      "Error on this batch = 0.1427754849869411\n",
      "Cost on val dataset after 143 epochs is = 0.1357845328966182\n",
      "Initial Cost on Val dataset for this epoch 143 = 0.1357845328966182\n",
      "learning rate for this epoch =  0.14458924666162856\n",
      "Error on this batch = 0.10068809269385096\n",
      "Error on this batch = 0.14238015905443893\n",
      "Cost on val dataset after 144 epochs is = 0.13550465286608923\n",
      "Initial Cost on Val dataset for this epoch 144 = 0.13550465286608923\n",
      "learning rate for this epoch =  0.14433756729740646\n",
      "Error on this batch = 0.10032962762932819\n",
      "Error on this batch = 0.14198843226012484\n",
      "Cost on val dataset after 145 epochs is = 0.1352279009413213\n",
      "Initial Cost on Val dataset for this epoch 145 = 0.1352279009413213\n",
      "learning rate for this epoch =  0.14408806322327672\n",
      "Error on this batch = 0.0999754461152339\n",
      "Error on this batch = 0.14160024171730765\n",
      "Cost on val dataset after 146 epochs is = 0.13495421497896062\n",
      "Initial Cost on Val dataset for this epoch 146 = 0.13495421497896062\n",
      "learning rate for this epoch =  0.14384070082928266\n",
      "Error on this batch = 0.0996254609103755\n",
      "Error on this batch = 0.1412155262508256\n",
      "Cost on val dataset after 147 epochs is = 0.13468353455981855\n",
      "Initial Cost on Val dataset for this epoch 147 = 0.13468353455981855\n",
      "learning rate for this epoch =  0.1435954472504545\n",
      "Error on this batch = 0.09927958681335167\n",
      "Error on this batch = 0.14083422633324325\n",
      "Cost on val dataset after 148 epochs is = 0.1344158009275065\n",
      "Initial Cost on Val dataset for this epoch 148 = 0.1344158009275065\n",
      "learning rate for this epoch =  0.1433522703453617\n",
      "Error on this batch = 0.09893774062507422\n",
      "Error on this batch = 0.14045628402404428\n",
      "Cost on val dataset after 149 epochs is = 0.13415095692983925\n",
      "Initial Cost on Val dataset for this epoch 149 = 0.13415095692983925\n",
      "learning rate for this epoch =  0.1431111386754225\n",
      "Error on this batch = 0.09859984111124385\n",
      "Error on this batch = 0.14008164291166617\n",
      "Cost on val dataset after 150 epochs is = 0.13388894696285203\n",
      "Initial Cost on Val dataset for this epoch 150 = 0.13388894696285203\n",
      "learning rate for this epoch =  0.14287202148493997\n",
      "Error on this batch = 0.0982658089647862\n",
      "Error on this batch = 0.13971024805823018\n",
      "Cost on val dataset after 151 epochs is = 0.13362971691728806\n",
      "Initial Cost on Val dataset for this epoch 151 = 0.13362971691728806\n",
      "learning rate for this epoch =  0.14263488868183333\n",
      "Error on this batch = 0.09793556676826327\n",
      "Error on this batch = 0.13934204594682714\n",
      "Cost on val dataset after 152 epochs is = 0.13337321412742142\n",
      "Initial Cost on Val dataset for this epoch 152 = 0.13337321412742142\n",
      "learning rate for this epoch =  0.14239971081903682\n",
      "Error on this batch = 0.09760903895628062\n",
      "Error on this batch = 0.13897698443122689\n",
      "Cost on val dataset after 153 epochs is = 0.13311938732208828\n",
      "Initial Cost on Val dataset for this epoch 153 = 0.13311938732208828\n",
      "learning rate for this epoch =  0.14216645907653844\n",
      "Error on this batch = 0.09728615177791632\n",
      "Error on this batch = 0.13861501268788584\n",
      "Cost on val dataset after 154 epochs is = 0.13286818657780866\n",
      "Initial Cost on Val dataset for this epoch 154 = 0.13286818657780866\n",
      "learning rate for this epoch =  0.14193510524403224\n",
      "Error on this batch = 0.09696683325920129\n",
      "Error on this batch = 0.13825608117013327\n",
      "Cost on val dataset after 155 epochs is = 0.13261956327388713\n",
      "Initial Cost on Val dataset for this epoch 155 = 0.13261956327388713\n",
      "learning rate for this epoch =  0.1417056217041599\n",
      "Error on this batch = 0.09665101316568322\n",
      "Error on this batch = 0.1379001415644241\n",
      "Cost on val dataset after 156 epochs is = 0.13237347004938838\n",
      "Initial Cost on Val dataset for this epoch 156 = 0.13237347004938838\n",
      "learning rate for this epoch =  0.14147798141631754\n",
      "Error on this batch = 0.09633862296510852\n",
      "Error on this batch = 0.13754714674855045\n",
      "Cost on val dataset after 157 epochs is = 0.13212986076188996\n",
      "Initial Cost on Val dataset for this epoch 157 = 0.13212986076188996\n",
      "learning rate for this epoch =  0.14125215790100537\n",
      "Error on this batch = 0.09602959579025785\n",
      "Error on this batch = 0.13719705075171085\n",
      "Cost on val dataset after 158 epochs is = 0.13188869044792\n",
      "Initial Cost on Val dataset for this epoch 158 = 0.13188869044792\n",
      "learning rate for this epoch =  0.14102812522469854\n",
      "Error on this batch = 0.09572386640197122\n",
      "Error on this batch = 0.13684980871634186\n",
      "Cost on val dataset after 159 epochs is = 0.1316499152849945\n",
      "Initial Cost on Val dataset for this epoch 159 = 0.1316499152849945\n",
      "learning rate for this epoch =  0.1408058579852188\n",
      "Error on this batch = 0.09542137115239914\n",
      "Error on this batch = 0.13650537686162018\n",
      "Cost on val dataset after 160 epochs is = 0.1314134925551727\n",
      "Initial Cost on Val dataset for this epoch 160 = 0.1314134925551727\n",
      "learning rate for this epoch =  0.14058533129758727\n",
      "Error on this batch = 0.0951220479485149\n",
      "Error on this batch = 0.1361637124485506\n",
      "Cost on val dataset after 161 epochs is = 0.13117938061005446\n",
      "Initial Cost on Val dataset for this epoch 161 = 0.13117938061005446\n",
      "learning rate for this epoch =  0.14036652078033962\n",
      "Error on this batch = 0.09482583621592339\n",
      "Error on this batch = 0.1358247737465576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on val dataset after 162 epochs is = 0.1309475388371484\n",
      "Initial Cost on Val dataset for this epoch 162 = 0.1309475388371484\n",
      "learning rate for this epoch =  0.14014940254228575\n",
      "Error on this batch = 0.09453267686300006\n",
      "Error on this batch = 0.1354885200015047\n",
      "Cost on val dataset after 163 epochs is = 0.13071792762754408\n",
      "Initial Cost on Val dataset for this epoch 163 = 0.13071792762754408\n",
      "learning rate for this epoch =  0.13993395316969692\n",
      "Error on this batch = 0.09424251224539187\n",
      "Error on this batch = 0.13515491140506847\n",
      "Cost on val dataset after 164 epochs is = 0.13049050834482376\n",
      "Initial Cost on Val dataset for this epoch 164 = 0.13049050834482376\n",
      "learning rate for this epoch =  0.13972014971390403\n",
      "Error on this batch = 0.09395528613091166\n",
      "Error on this batch = 0.13482390906539887\n",
      "Cost on val dataset after 165 epochs is = 0.13026524329515562\n",
      "Initial Cost on Val dataset for this epoch 165 = 0.13026524329515562\n",
      "learning rate for this epoch =  0.13950796967929133\n",
      "Error on this batch = 0.0936709436648541\n",
      "Error on this batch = 0.13449547497900122\n",
      "Cost on val dataset after 166 epochs is = 0.13004209569851155\n",
      "Initial Cost on Val dataset for this epoch 166 = 0.13004209569851155\n",
      "learning rate for this epoch =  0.13929739101167085\n",
      "Error on this batch = 0.0933894313357608\n",
      "Error on this batch = 0.13416957200377816\n",
      "Cost on val dataset after 167 epochs is = 0.12982102966095707\n",
      "Initial Cost on Val dataset for this epoch 167 = 0.12982102966095707\n",
      "learning rate for this epoch =  0.13908839208702292\n",
      "Error on this batch = 0.09311069694165951\n",
      "Error on this batch = 0.13384616383317435\n",
      "Cost on val dataset after 168 epochs is = 0.12960201014796363\n",
      "Initial Cost on Val dataset for this epoch 168 = 0.12960201014796363\n",
      "learning rate for this epoch =  0.13888095170058953\n",
      "Error on this batch = 0.09283468955679997\n",
      "Error on this batch = 0.13352521497136827\n",
      "Cost on val dataset after 169 epochs is = 0.12938500295869643\n",
      "Initial Cost on Val dataset for this epoch 169 = 0.12938500295869643\n",
      "learning rate for this epoch =  0.1386750490563073\n",
      "Error on this batch = 0.09256135949890824\n",
      "Error on this batch = 0.13320669070946026\n",
      "Cost on val dataset after 170 epochs is = 0.1291699747012336\n",
      "Initial Cost on Val dataset for this epoch 170 = 0.1291699747012336\n",
      "learning rate for this epoch =  0.13847066375656708\n",
      "Error on this batch = 0.0922906582969777\n",
      "Error on this batch = 0.1328905571026071\n",
      "Cost on val dataset after 171 epochs is = 0.12895689276867472\n",
      "Initial Cost on Val dataset for this epoch 171 = 0.12895689276867472\n",
      "learning rate for this epoch =  0.1382677757922894\n",
      "Error on this batch = 0.09202253865961392\n",
      "Error on this batch = 0.13257678094805708\n",
      "Cost on val dataset after 172 epochs is = 0.12874572531609974\n",
      "Initial Cost on Val dataset for this epoch 172 = 0.12874572531609974\n",
      "learning rate for this epoch =  0.1380663655333028\n",
      "Error on this batch = 0.09175695444394834\n",
      "Error on this batch = 0.1322653297640412\n",
      "Cost on val dataset after 173 epochs is = 0.12853644123834068\n",
      "Initial Cost on Val dataset for this epoch 173 = 0.12853644123834068\n",
      "learning rate for this epoch =  0.13786641371901512\n",
      "Error on this batch = 0.0914938606251335\n",
      "Error on this batch = 0.13195617176947938\n",
      "Cost on val dataset after 174 epochs is = 0.12832901014853085\n",
      "Initial Cost on Val dataset for this epoch 174 = 0.12832901014853085\n",
      "learning rate for this epoch =  0.13766790144936686\n",
      "Error on this batch = 0.09123321326643152\n",
      "Error on this batch = 0.13164927586446182\n",
      "Cost on val dataset after 175 epochs is = 0.12812340235739886\n",
      "Initial Cost on Val dataset for this epoch 175 = 0.12812340235739886\n",
      "learning rate for this epoch =  0.1374708101760565\n",
      "Error on this batch = 0.09097496948990383\n",
      "Error on this batch = 0.13134461161146793\n",
      "Cost on val dataset after 176 epochs is = 0.12791958885327528\n",
      "Initial Cost on Val dataset for this epoch 176 = 0.12791958885327528\n",
      "learning rate for this epoch =  0.1372751216940281\n",
      "Error on this batch = 0.09071908744771104\n",
      "Error on this batch = 0.13104214921728718\n",
      "Cost on val dataset after 177 epochs is = 0.12771754128278232\n",
      "Initial Cost on Val dataset for this epoch 177 = 0.12771754128278232\n",
      "learning rate for this epoch =  0.1370808181332119\n",
      "Error on this batch = 0.09046552629402797\n",
      "Error on this batch = 0.13074185951560854\n",
      "Cost on val dataset after 178 epochs is = 0.12751723193217826\n",
      "Initial Cost on Val dataset for this epoch 178 = 0.12751723193217826\n",
      "learning rate for this epoch =  0.13688788195050916\n",
      "Error on this batch = 0.09021424615757809\n",
      "Error on this batch = 0.1304437139502454\n",
      "Cost on val dataset after 179 epochs is = 0.1273186337093301\n",
      "Initial Cost on Val dataset for this epoch 179 = 0.1273186337093301\n",
      "learning rate for this epoch =  0.13669629592201246\n",
      "Error on this batch = 0.08996520811479047\n",
      "Error on this batch = 0.13014768455896622\n",
      "Cost on val dataset after 180 epochs is = 0.1271217201262884\n",
      "Initial Cost on Val dataset for this epoch 180 = 0.1271217201262884\n",
      "learning rate for this epoch =  0.13650604313545334\n",
      "Error on this batch = 0.08971837416358046\n",
      "Error on this batch = 0.12985374395790097\n",
      "Cost on val dataset after 181 epochs is = 0.12692646528244048\n",
      "Initial Cost on Val dataset for this epoch 181 = 0.12692646528244048\n",
      "learning rate for this epoch =  0.13631710698286975\n",
      "Error on this batch = 0.0894737071977539\n",
      "Error on this batch = 0.12956186532649627\n",
      "Cost on val dataset after 182 epochs is = 0.1267328438482193\n",
      "Initial Cost on Val dataset for this epoch 182 = 0.1267328438482193\n",
      "learning rate for this epoch =  0.1361294711534851\n",
      "Error on this batch = 0.08923117098203381\n",
      "Error on this batch = 0.12927202239299207\n",
      "Cost on val dataset after 183 epochs is = 0.1265408310493458\n",
      "Initial Cost on Val dataset for this epoch 183 = 0.1265408310493458\n",
      "learning rate for this epoch =  0.13594311962679215\n",
      "Error on this batch = 0.08899073012770738\n",
      "Error on this batch = 0.1289841894203949\n",
      "Cost on val dataset after 184 epochs is = 0.1263504026515837\n",
      "Initial Cost on Val dataset for this epoch 184 = 0.1263504026515837\n",
      "learning rate for this epoch =  0.13575803666583477\n",
      "Error on this batch = 0.08875235006888943\n",
      "Error on this batch = 0.12869834119292378\n",
      "Cost on val dataset after 185 epochs is = 0.12616153494598809\n",
      "Initial Cost on Val dataset for this epoch 185 = 0.12616153494598809\n",
      "learning rate for this epoch =  0.13557420681068058\n",
      "Error on this batch = 0.08851599703939822\n",
      "Error on this batch = 0.12841445300290533\n",
      "Cost on val dataset after 186 epochs is = 0.1259742047346279\n",
      "Initial Cost on Val dataset for this epoch 186 = 0.1259742047346279\n",
      "learning rate for this epoch =  0.13539161487207826\n",
      "Error on this batch = 0.08828163805023827\n",
      "Error on this batch = 0.12813250063809645\n",
      "Cost on val dataset after 187 epochs is = 0.1257883893167646\n",
      "Initial Cost on Val dataset for this epoch 187 = 0.1257883893167646\n",
      "learning rate for this epoch =  0.13521024592529318\n",
      "Error on this batch = 0.08804924086768397\n",
      "Error on this batch = 0.1278524603694135\n",
      "Cost on val dataset after 188 epochs is = 0.12560406647547068\n",
      "Initial Cost on Val dataset for this epoch 188 = 0.12560406647547068\n",
      "learning rate for this epoch =  0.1350300853041159\n",
      "Error on this batch = 0.08781877399195694\n",
      "Error on this batch = 0.12757430893904823\n",
      "Cost on val dataset after 189 epochs is = 0.1254212144646705\n",
      "Initial Cost on Val dataset for this epoch 189 = 0.1254212144646705\n",
      "learning rate for this epoch =  0.13485111859503687\n",
      "Error on this batch = 0.08759020663649014\n",
      "Error on this batch = 0.1272980235489508\n",
      "Cost on val dataset after 190 epochs is = 0.12523981199658915\n",
      "Initial Cost on Val dataset for this epoch 190 = 0.12523981199658915\n",
      "learning rate for this epoch =  0.13467333163158285\n",
      "Error on this batch = 0.08736350870776988\n",
      "Error on this batch = 0.12702358184966245\n",
      "Cost on val dataset after 191 epochs is = 0.1250598382295932\n",
      "Initial Cost on Val dataset for this epoch 191 = 0.1250598382295932\n",
      "learning rate for this epoch =  0.13449671048880912\n",
      "Error on this batch = 0.08713865078574731\n",
      "Error on this batch = 0.12675096192947993\n",
      "Cost on val dataset after 192 epochs is = 0.12488127275641034\n",
      "Initial Cost on Val dataset for this epoch 192 = 0.12488127275641034\n",
      "learning rate for this epoch =  0.13432124147794275\n",
      "Error on this batch = 0.08691560410481029\n",
      "Error on this batch = 0.12648014230393562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on val dataset after 193 epochs is = 0.12470409559271328\n",
      "Initial Cost on Val dataset for this epoch 193 = 0.12470409559271328\n",
      "learning rate for this epoch =  0.1341469111411715\n",
      "Error on this batch = 0.08669434053530578\n",
      "Error on this batch = 0.12621110190557708\n",
      "Cost on val dataset after 194 epochs is = 0.1245282871660557\n",
      "Initial Cost on Val dataset for this epoch 194 = 0.1245282871660557\n",
      "learning rate for this epoch =  0.13397370624657456\n",
      "Error on this batch = 0.08647483256560286\n",
      "Error on this batch = 0.12594382007403188\n",
      "Cost on val dataset after 195 epochs is = 0.12435382830514706\n",
      "Initial Cost on Val dataset for this epoch 195 = 0.12435382830514706\n",
      "learning rate for this epoch =  0.13380161378318955\n",
      "Error on this batch = 0.08625705328468598\n",
      "Error on this batch = 0.12567827654634287\n",
      "Cost on val dataset after 196 epochs is = 0.12418070022945499\n",
      "Initial Cost on Val dataset for this epoch 196 = 0.12418070022945499\n",
      "learning rate for this epoch =  0.1336306209562122\n",
      "Error on this batch = 0.0860409763652678\n",
      "Error on this batch = 0.1254144514475604\n",
      "Cost on val dataset after 197 epochs is = 0.1240088845391234\n",
      "Initial Cost on Val dataset for this epoch 197 = 0.1240088845391234\n",
      "learning rate for this epoch =  0.13346071518232402\n",
      "Error on this batch = 0.08582657604741087\n",
      "Error on this batch = 0.1251523252815796\n",
      "Cost on val dataset after 198 epochs is = 0.12383836320519546\n",
      "Initial Cost on Val dataset for this epoch 198 = 0.12383836320519546\n",
      "learning rate for this epoch =  0.13329188408514428\n",
      "Error on this batch = 0.08561382712264708\n",
      "Error on this batch = 0.12489187892220985\n",
      "Cost on val dataset after 199 epochs is = 0.1236691185601309\n",
      "Initial Cost on Val dataset for this epoch 199 = 0.1236691185601309\n",
      "learning rate for this epoch =  0.13312411549080203\n",
      "Error on this batch = 0.08540270491858351\n",
      "Error on this batch = 0.12463309360446508\n",
      "Cost on val dataset after 200 epochs is = 0.12350113328860753\n",
      "Initial Cost on Val dataset for this epoch 200 = 0.12350113328860753\n",
      "learning rate for this epoch =  0.13295739742362472\n",
      "Error on this batch = 0.08519318528398355\n",
      "Error on this batch = 0.1243759509160647\n",
      "Cost on val dataset after 201 epochs is = 0.12333439041859741\n",
      "Initial Cost on Val dataset for this epoch 201 = 0.12333439041859741\n",
      "learning rate for this epoch =  0.13279171810193946\n",
      "Error on this batch = 0.08498524457431202\n",
      "Error on this batch = 0.12412043278913495\n",
      "Cost on val dataset after 202 epochs is = 0.12316887331270805\n",
      "Initial Cost on Val dataset for this epoch 202 = 0.12316887331270805\n",
      "learning rate for this epoch =  0.13262706593398382\n",
      "Error on this batch = 0.08477885963773257\n",
      "Error on this batch = 0.12386652149210071\n",
      "Cost on val dataset after 203 epochs is = 0.12300456565977984\n",
      "Initial Cost on Val dataset for this epoch 203 = 0.12300456565977984\n",
      "learning rate for this epoch =  0.13246342951392248\n",
      "Error on this batch = 0.08457400780154588\n",
      "Error on this batch = 0.12361419962175965\n",
      "Cost on val dataset after 204 epochs is = 0.12284145146673124\n",
      "Initial Cost on Val dataset for this epoch 204 = 0.12284145146673124\n",
      "learning rate for this epoch =  0.13230079761796648\n",
      "Error on this batch = 0.08437066685905763\n",
      "Error on this batch = 0.12336345009552989\n",
      "Cost on val dataset after 205 epochs is = 0.12267951505064313\n",
      "Initial Cost on Val dataset for this epoch 205 = 0.12267951505064313\n",
      "learning rate for this epoch =  0.13213915920059222\n",
      "Error on this batch = 0.08416881505686431\n",
      "Error on this batch = 0.12311425614386397\n",
      "Cost on val dataset after 206 epochs is = 0.12251874103107457\n",
      "Initial Cost on Val dataset for this epoch 206 = 0.12251874103107457\n",
      "learning rate for this epoch =  0.13197850339085695\n",
      "Error on this batch = 0.08396843108254594\n",
      "Error on this batch = 0.12286660130282136\n",
      "Cost on val dataset after 207 epochs is = 0.12235911432260242\n",
      "Initial Cost on Val dataset for this epoch 207 = 0.12235911432260242\n",
      "learning rate for this epoch =  0.1318188194888078\n",
      "Error on this batch = 0.08376949405275369\n",
      "Error on this batch = 0.12262046940679376\n",
      "Cost on val dataset after 208 epochs is = 0.12220062012757676\n",
      "Initial Cost on Val dataset for this epoch 208 = 0.12220062012757676\n",
      "learning rate for this epoch =  0.13166009696198164\n",
      "Error on this batch = 0.08357198350168227\n",
      "Error on this batch = 0.12237584458137643\n",
      "Cost on val dataset after 209 epochs is = 0.12204324392908598\n",
      "Initial Cost on Val dataset for this epoch 209 = 0.12204324392908598\n",
      "learning rate for this epoch =  0.1315023254419931\n",
      "Error on this batch = 0.08337587936991504\n",
      "Error on this batch = 0.12213271123638018\n",
      "Cost on val dataset after 210 epochs is = 0.1218869714841239\n",
      "Initial Cost on Val dataset for this epoch 210 = 0.1218869714841239\n",
      "learning rate for this epoch =  0.1313454947212079\n",
      "Error on this batch = 0.08318116199363114\n",
      "Error on this batch = 0.12189105405897906\n",
      "Cost on val dataset after 211 epochs is = 0.12173178881695275\n",
      "Initial Cost on Val dataset for this epoch 211 = 0.12173178881695275\n",
      "learning rate for this epoch =  0.13118959474949932\n",
      "Error on this batch = 0.08298781209416418\n",
      "Error on this batch = 0.1216508580069889\n",
      "Cost on val dataset after 212 epochs is = 0.12157768221265533\n",
      "Initial Cost on Val dataset for this epoch 212 = 0.12157768221265533\n",
      "learning rate for this epoch =  0.1310346156310848\n",
      "Error on this batch = 0.08279581076790116\n",
      "Error on this batch = 0.12141210830227298\n",
      "Cost on val dataset after 213 epochs is = 0.12142463821087045\n",
      "Initial Cost on Val dataset for this epoch 213 = 0.12142463821087045\n",
      "learning rate for this epoch =  0.13088054762144102\n",
      "Error on this batch = 0.08260513947651152\n",
      "Error on this batch = 0.12117479042427015\n",
      "Cost on val dataset after 214 epochs is = 0.1212726435997058\n",
      "Initial Cost on Val dataset for this epoch 214 = 0.1212726435997058\n",
      "learning rate for this epoch =  0.13072738112429463\n",
      "Error on this batch = 0.08241578003749613\n",
      "Error on this batch = 0.12093889010364314\n",
      "Cost on val dataset after 215 epochs is = 0.12112168540982214\n",
      "Initial Cost on Val dataset for this epoch 215 = 0.12112168540982214\n",
      "learning rate for this epoch =  0.1305751066886864\n",
      "Error on this batch = 0.0822277146150453\n",
      "Error on this batch = 0.12070439331604334\n",
      "Cost on val dataset after 216 epochs is = 0.1209717509086838\n",
      "Initial Cost on Val dataset for this epoch 216 = 0.1209717509086838\n",
      "learning rate for this epoch =  0.13042371500610728\n",
      "Error on this batch = 0.08204092571119677\n",
      "Error on this batch = 0.12047128627598962\n",
      "Cost on val dataset after 217 epochs is = 0.12082282759496984\n",
      "Initial Cost on Val dataset for this epoch 217 = 0.12082282759496984\n",
      "learning rate for this epoch =  0.13027319690770342\n",
      "Error on this batch = 0.08185539615728317\n",
      "Error on this batch = 0.12023955543085892\n",
      "Cost on val dataset after 218 epochs is = 0.12067490319314104\n",
      "Initial Cost on Val dataset for this epoch 218 = 0.12067490319314104\n",
      "learning rate for this epoch =  0.13012354336154894\n",
      "Error on this batch = 0.08167110910565972\n",
      "Error on this batch = 0.12000918745498687\n",
      "Cost on val dataset after 219 epochs is = 0.1205279656481574\n",
      "Initial Cost on Val dataset for this epoch 219 = 0.1205279656481574\n",
      "learning rate for this epoch =  0.12997474546998408\n",
      "Error on this batch = 0.08148804802170281\n",
      "Error on this batch = 0.11978016924387618\n",
      "Cost on val dataset after 220 epochs is = 0.12038200312034167\n",
      "Initial Cost on Val dataset for this epoch 220 = 0.12038200312034167\n",
      "learning rate for this epoch =  0.12982679446701692\n",
      "Error on this batch = 0.08130619667607009\n",
      "Error on this batch = 0.11955248790851225\n",
      "Cost on val dataset after 221 epochs is = 0.1202370039803844\n",
      "Initial Cost on Val dataset for this epoch 221 = 0.1202370039803844\n",
      "learning rate for this epoch =  0.12967968171578698\n",
      "Error on this batch = 0.08112553913721325\n",
      "Error on this batch = 0.11932613076978403\n",
      "Cost on val dataset after 222 epochs is = 0.12009295680448595\n",
      "Initial Cost on Val dataset for this epoch 222 = 0.12009295680448595\n",
      "learning rate for this epoch =  0.12953339870608896\n",
      "Error on this batch = 0.08094605976413487\n",
      "Error on this batch = 0.1191010853530098\n",
      "Cost on val dataset after 223 epochs is = 0.11994985036963061\n",
      "Initial Cost on Val dataset for this epoch 223 = 0.11994985036963061\n",
      "learning rate for this epoch =  0.12938793705195484\n",
      "Error on this batch = 0.0807677431993807\n",
      "Error on this batch = 0.11887733938256698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on val dataset after 224 epochs is = 0.11980767364899003\n",
      "Initial Cost on Val dataset for this epoch 224 = 0.11980767364899003\n",
      "learning rate for this epoch =  0.12924328848929265\n",
      "Error on this batch = 0.08059057436225889\n",
      "Error on this batch = 0.11865488077662534\n",
      "Cost on val dataset after 225 epochs is = 0.11966641580745065\n",
      "Initial Cost on Val dataset for this epoch 225 = 0.11966641580745065\n",
      "learning rate for this epoch =  0.12909944487358055\n",
      "Error on this batch = 0.08041453844227882\n",
      "Error on this batch = 0.1184336976419835\n",
      "Cost on val dataset after 226 epochs is = 0.11952606619726189\n",
      "Initial Cost on Val dataset for this epoch 226 = 0.11952606619726189\n",
      "learning rate for this epoch =  0.1289563981776146\n",
      "Error on this batch = 0.08023962089280079\n",
      "Error on this batch = 0.11821377826900857\n",
      "Cost on val dataset after 227 epochs is = 0.11938661435380152\n",
      "Initial Cost on Val dataset for this epoch 227 = 0.11938661435380152\n",
      "learning rate for this epoch =  0.12881414048930848\n",
      "Error on this batch = 0.08006580742488957\n",
      "Error on this batch = 0.11799511112667833\n",
      "Cost on val dataset after 228 epochs is = 0.11924804999145415\n",
      "Initial Cost on Val dataset for this epoch 228 = 0.11924804999145415\n",
      "learning rate for this epoch =  0.1286726640095442\n",
      "Error on this batch = 0.07989308400136431\n",
      "Error on this batch = 0.11777768485772688\n",
      "Cost on val dataset after 229 epochs is = 0.11911036299959973\n",
      "Initial Cost on Val dataset for this epoch 229 = 0.11911036299959973\n",
      "learning rate for this epoch =  0.12853196105007209\n",
      "Error on this batch = 0.07972143683103747\n",
      "Error on this batch = 0.11756148827389311\n",
      "Cost on val dataset after 230 epochs is = 0.11897354343870861\n",
      "Initial Cost on Val dataset for this epoch 230 = 0.11897354343870861\n",
      "learning rate for this epoch =  0.12839202403145872\n",
      "Error on this batch = 0.07955085236313582\n",
      "Error on this batch = 0.11734651035127262\n",
      "Cost on val dataset after 231 epochs is = 0.11883758153654005\n",
      "Initial Cost on Val dataset for this epoch 231 = 0.11883758153654005\n",
      "learning rate for this epoch =  0.12825284548108173\n",
      "Error on this batch = 0.0793813172818974\n",
      "Error on this batch = 0.11713274022577348\n",
      "Cost on val dataset after 232 epochs is = 0.11870246768444057\n",
      "Initial Cost on Val dataset for this epoch 232 = 0.11870246768444057\n",
      "learning rate for this epoch =  0.1281144180311698\n",
      "Error on this batch = 0.0792128185013369\n",
      "Error on this batch = 0.11692016718867562\n",
      "Cost on val dataset after 233 epochs is = 0.1185681924337399\n",
      "Initial Cost on Val dataset for this epoch 233 = 0.1185681924337399\n",
      "learning rate for this epoch =  0.12797673441688712\n",
      "Error on this batch = 0.07904534316017393\n",
      "Error on this batch = 0.1167087806822945\n",
      "Cost on val dataset after 234 epochs is = 0.11843474649224124\n",
      "Initial Cost on Val dataset for this epoch 234 = 0.11843474649224124\n",
      "learning rate for this epoch =  0.12783978747446093\n",
      "Error on this batch = 0.07887887861691803\n",
      "Error on this batch = 0.1164985702957499\n",
      "Cost on val dataset after 235 epochs is = 0.11830212072080253\n",
      "Initial Cost on Val dataset for this epoch 235 = 0.11830212072080253\n",
      "learning rate for this epoch =  0.12770357013935066\n",
      "Error on this batch = 0.07871341244510405\n",
      "Error on this batch = 0.11628952576083883\n",
      "Cost on val dataset after 236 epochs is = 0.11817030613000688\n",
      "Initial Cost on Val dataset for this epoch 236 = 0.11817030613000688\n",
      "learning rate for this epoch =  0.12756807544445822\n",
      "Error on this batch = 0.07854893242867289\n",
      "Error on this batch = 0.11608163694801447\n",
      "Cost on val dataset after 237 epochs is = 0.11803929387691932\n",
      "Initial Cost on Val dataset for this epoch 237 = 0.11803929387691932\n",
      "learning rate for this epoch =  0.1274332965183777\n",
      "Error on this batch = 0.0783854265574917\n",
      "Error on this batch = 0.11587489386247019\n",
      "Cost on val dataset after 238 epochs is = 0.11790907526192657\n",
      "Initial Cost on Val dataset for this epoch 238 = 0.11790907526192657\n",
      "learning rate for this epoch =  0.12729922658368395\n",
      "Error on this batch = 0.07822288302300831\n",
      "Error on this batch = 0.11566928664032929\n",
      "Cost on val dataset after 239 epochs is = 0.11777964172565855\n",
      "Initial Cost on Val dataset for this epoch 239 = 0.11777964172565855\n",
      "learning rate for this epoch =  0.12716585895525878\n",
      "Error on this batch = 0.07806129021403495\n",
      "Error on this batch = 0.11546480554494112\n",
      "Cost on val dataset after 240 epochs is = 0.11765098484598838\n",
      "Initial Cost on Val dataset for this epoch 240 = 0.11765098484598838\n",
      "learning rate for this epoch =  0.12703318703865368\n",
      "Error on this batch = 0.0779006367126564\n",
      "Error on this batch = 0.1152614409632826\n",
      "Cost on val dataset after 241 epochs is = 0.11752309633510914\n",
      "Initial Cost on Val dataset for this epoch 241 = 0.11752309633510914\n",
      "learning rate for this epoch =  0.12690120432848842\n",
      "Error on this batch = 0.07774091129025759\n",
      "Error on this batch = 0.11505918340246624\n",
      "Cost on val dataset after 242 epochs is = 0.11739596803668469\n",
      "Initial Cost on Val dataset for this epoch 242 = 0.11739596803668469\n",
      "learning rate for this epoch =  0.12676990440688446\n",
      "Error on this batch = 0.07758210290366649\n",
      "Error on this batch = 0.11485802348635399\n",
      "Cost on val dataset after 243 epochs is = 0.1172695919230729\n",
      "Initial Cost on Val dataset for this epoch 243 = 0.1172695919230729\n",
      "learning rate for this epoch =  0.12663928094193208\n",
      "Error on this batch = 0.07742420069140753\n",
      "Error on this batch = 0.11465795195227702\n",
      "Cost on val dataset after 244 epochs is = 0.11714396009261892\n",
      "Initial Cost on Val dataset for this epoch 244 = 0.11714396009261892\n",
      "learning rate for this epoch =  0.12650932768619078\n",
      "Error on this batch = 0.07726719397006188\n",
      "Error on this batch = 0.11445895964786088\n",
      "Cost on val dataset after 245 epochs is = 0.1170190647670167\n",
      "Initial Cost on Val dataset for this epoch 245 = 0.1170190647670167\n",
      "learning rate for this epoch =  0.12638003847522164\n",
      "Error on this batch = 0.0771110722307301\n",
      "Error on this batch = 0.11426103752795624\n",
      "Cost on val dataset after 246 epochs is = 0.11689489828873662\n",
      "Initial Cost on Val dataset for this epoch 246 = 0.11689489828873662\n",
      "learning rate for this epoch =  0.1262514072261512\n",
      "Error on this batch = 0.0769558251355936\n",
      "Error on this batch = 0.114064176651674\n",
      "Cost on val dataset after 247 epochs is = 0.11677145311851728\n",
      "Initial Cost on Val dataset for this epoch 247 = 0.11677145311851728\n",
      "learning rate for this epoch =  0.12612342793626585\n",
      "Error on this batch = 0.07680144251457104\n",
      "Error on this batch = 0.11386836817952511\n",
      "Cost on val dataset after 248 epochs is = 0.11664872183292047\n",
      "Initial Cost on Val dataset for this epoch 248 = 0.11664872183292047\n",
      "learning rate for this epoch =  0.1259960946816361\n",
      "Error on this batch = 0.0766479143620664\n",
      "Error on this batch = 0.11367360337066339\n",
      "Cost on val dataset after 249 epochs is = 0.11652669712194631\n",
      "Initial Cost on Val dataset for this epoch 249 = 0.11652669712194631\n",
      "learning rate for this epoch =  0.12586940161576976\n",
      "Error on this batch = 0.07649523083380488\n",
      "Error on this batch = 0.11347987358023154\n",
      "Cost on val dataset after 250 epochs is = 0.11640537178670783\n",
      "Initial Cost on Val dataset for this epoch 250 = 0.11640537178670783\n",
      "learning rate for this epoch =  0.12574334296829354\n",
      "Error on this batch = 0.07634338224375395\n",
      "Error on this batch = 0.11328717025680864\n",
      "Cost on val dataset after 251 epochs is = 0.11628473873716297\n",
      "Initial Cost on Val dataset for this epoch 251 = 0.11628473873716297\n",
      "learning rate for this epoch =  0.1256179130436622\n",
      "Error on this batch = 0.07619235906112598\n",
      "Error on this batch = 0.11309548493995852\n",
      "Cost on val dataset after 252 epochs is = 0.11616479098990261\n",
      "Initial Cost on Val dataset for this epoch 252 = 0.11616479098990261\n",
      "learning rate for this epoch =  0.12549310621989482\n",
      "Error on this batch = 0.07604215190745955\n",
      "Error on this batch = 0.11290480925787803\n",
      "Cost on val dataset after 253 epochs is = 0.11604552166599286\n",
      "Initial Cost on Val dataset for this epoch 253 = 0.11604552166599286\n",
      "learning rate for this epoch =  0.125368916947337\n",
      "Error on this batch = 0.07589275155377685\n",
      "Error on this batch = 0.11271513492514319\n",
      "Cost on val dataset after 254 epochs is = 0.11592692398887043\n",
      "Initial Cost on Val dataset for this epoch 254 = 0.11592692398887043\n",
      "learning rate for this epoch =  0.12524533974744914\n",
      "Error on this batch = 0.0757441489178141\n",
      "Error on this batch = 0.11252645374055266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on val dataset after 255 epochs is = 0.11580899128228916\n",
      "Initial Cost on Val dataset for this epoch 255 = 0.11580899128228916\n",
      "learning rate for this epoch =  0.12512236921161915\n",
      "Error on this batch = 0.0755963350613228\n",
      "Error on this batch = 0.1123387575850667\n",
      "Cost on val dataset after 256 epochs is = 0.11569171696831704\n",
      "Initial Cost on Val dataset for this epoch 256 = 0.11569171696831704\n",
      "learning rate for this epoch =  0.125\n",
      "Error on this batch = 0.07544930118743873\n",
      "Error on this batch = 0.11215203841983996\n",
      "Cost on val dataset after 257 epochs is = 0.11557509456538176\n",
      "Initial Cost on Val dataset for this epoch 257 = 0.11557509456538176\n",
      "learning rate for this epoch =  0.12487822684037092\n",
      "Error on this batch = 0.07530303863811691\n",
      "Error on this batch = 0.11196628828434638\n",
      "Cost on val dataset after 258 epochs is = 0.11545911768636377\n",
      "Initial Cost on Val dataset for this epoch 258 = 0.11545911768636377\n",
      "learning rate for this epoch =  0.12475704452702165\n",
      "Error on this batch = 0.07515753889162995\n",
      "Error on this batch = 0.11178149929459476\n",
      "Cost on val dataset after 259 epochs is = 0.11534378003673534\n",
      "Initial Cost on Val dataset for this epoch 259 = 0.11534378003673534\n",
      "learning rate for this epoch =  0.12463644791965951\n",
      "Error on this batch = 0.07501279356012754\n",
      "Error on this batch = 0.11159766364143271\n",
      "Cost on val dataset after 260 epochs is = 0.11522907541274482\n",
      "Initial Cost on Val dataset for this epoch 260 = 0.11522907541274482\n",
      "learning rate for this epoch =  0.12451643194233865\n",
      "Error on this batch = 0.07486879438725529\n",
      "Error on this batch = 0.1114147735889371\n",
      "Cost on val dataset after 261 epochs is = 0.11511499769964464\n",
      "Initial Cost on Val dataset for this epoch 261 = 0.11511499769964464\n",
      "learning rate for this epoch =  0.12439699158241055\n",
      "Error on this batch = 0.0747255332458306\n",
      "Error on this batch = 0.11123282147288899\n",
      "Cost on val dataset after 262 epochs is = 0.1150015408699614\n",
      "Initial Cost on Val dataset for this epoch 262 = 0.1150015408699614\n",
      "learning rate for this epoch =  0.12427812188949586\n",
      "Error on this batch = 0.07458300213557414\n",
      "Error on this batch = 0.11105179969933109\n",
      "Cost on val dataset after 263 epochs is = 0.11488869898180833\n",
      "Initial Cost on Val dataset for this epoch 263 = 0.11488869898180833\n",
      "learning rate for this epoch =  0.1241598179744767\n",
      "Error on this batch = 0.0744411931808943\n",
      "Error on this batch = 0.11087170074320507\n",
      "Cost on val dataset after 264 epochs is = 0.11477646617723729\n",
      "Initial Cost on Val dataset for this epoch 264 = 0.11477646617723729\n",
      "learning rate for this epoch =  0.12404207500850907\n",
      "Error on this batch = 0.07430009862872394\n",
      "Error on this batch = 0.11069251714706697\n",
      "Cost on val dataset after 265 epochs is = 0.11466483668063056\n",
      "Initial Cost on Val dataset for this epoch 265 = 0.11466483668063056\n",
      "learning rate for this epoch =  0.12392488822205483\n",
      "Error on this batch = 0.07415971084640699\n",
      "Error on this batch = 0.11051424151987782\n",
      "Cost on val dataset after 266 epochs is = 0.1145538047971307\n",
      "Initial Cost on Val dataset for this epoch 266 = 0.1145538047971307\n",
      "learning rate for this epoch =  0.12380825290393263\n",
      "Error on this batch = 0.07402002231963366\n",
      "Error on this batch = 0.1103368665358677\n",
      "Cost on val dataset after 267 epochs is = 0.11444336491110786\n",
      "Initial Cost on Val dataset for this epoch 267 = 0.11444336491110786\n",
      "learning rate for this epoch =  0.12369216440038802\n",
      "Error on this batch = 0.07388102565042264\n",
      "Error on this batch = 0.11016038493347022\n",
      "Cost on val dataset after 268 epochs is = 0.11433351148466313\n",
      "Initial Cost on Val dataset for this epoch 268 = 0.11433351148466313\n",
      "learning rate for this epoch =  0.1235766181141811\n",
      "Error on this batch = 0.07374271355514898\n",
      "Error on this batch = 0.10998478951432525\n",
      "Cost on val dataset after 269 epochs is = 0.1142242390561677\n",
      "Initial Cost on Val dataset for this epoch 269 = 0.1142242390561677\n",
      "learning rate for this epoch =  0.12346160950369273\n",
      "Error on this batch = 0.07360507886261612\n",
      "Error on this batch = 0.10981007314234756\n",
      "Cost on val dataset after 270 epochs is = 0.1141155422388359\n",
      "Initial Cost on Val dataset for this epoch 270 = 0.1141155422388359\n",
      "learning rate for this epoch =  0.12334713408204756\n",
      "Error on this batch = 0.07346811451217092\n",
      "Error on this batch = 0.10963622874285817\n",
      "Cost on val dataset after 271 epochs is = 0.11400741571933222\n",
      "Initial Cost on Val dataset for this epoch 271 = 0.11400741571933222\n",
      "learning rate for this epoch =  0.12323318741625437\n",
      "Error on this batch = 0.07333181355186008\n",
      "Error on this batch = 0.10946324930177678\n",
      "Cost on val dataset after 272 epochs is = 0.1138998542564106\n",
      "Initial Cost on Val dataset for this epoch 272 = 0.1138998542564106\n",
      "learning rate for this epoch =  0.12311976512636309\n",
      "Error on this batch = 0.07319616913662749\n",
      "Error on this batch = 0.10929112786487187\n",
      "Cost on val dataset after 273 epochs is = 0.113792852679586\n",
      "Initial Cost on Val dataset for this epoch 273 = 0.113792852679586\n",
      "learning rate for this epoch =  0.12300686288463769\n",
      "Error on this batch = 0.0730611745265504\n",
      "Error on this batch = 0.10911985753706638\n",
      "Cost on val dataset after 274 epochs is = 0.11368640588783659\n",
      "Initial Cost on Val dataset for this epoch 274 = 0.11368640588783659\n",
      "learning rate for this epoch =  0.12289447641474543\n",
      "Error on this batch = 0.0729268230851139\n",
      "Error on this batch = 0.10894943148179635\n",
      "Cost on val dataset after 275 epochs is = 0.11358050884833619\n",
      "Initial Cost on Val dataset for this epoch 275 = 0.11358050884833619\n",
      "learning rate for this epoch =  0.12278260149096118\n",
      "Error on this batch = 0.07279310827752287\n",
      "Error on this batch = 0.1087798429204198\n",
      "Cost on val dataset after 276 epochs is = 0.11347515659521593\n",
      "Initial Cost on Val dataset for this epoch 276 = 0.11347515659521593\n",
      "learning rate for this epoch =  0.12267123393738709\n",
      "Error on this batch = 0.07266002366904946\n",
      "Error on this batch = 0.10861108513167328\n",
      "Cost on val dataset after 277 epochs is = 0.11337034422835504\n",
      "Initial Cost on Val dataset for this epoch 277 = 0.11337034422835504\n",
      "learning rate for this epoch =  0.12256036962718717\n",
      "Error on this batch = 0.0725275629234163\n",
      "Error on this batch = 0.1084431514511738\n",
      "Cost on val dataset after 278 epochs is = 0.11326606691219902\n",
      "Initial Cost on Val dataset for this epoch 278 = 0.11326606691219902\n",
      "learning rate for this epoch =  0.12245000448183609\n",
      "Error on this batch = 0.07239571980121338\n",
      "Error on this batch = 0.10827603527096351\n",
      "Cost on val dataset after 279 epochs is = 0.11316231987460487\n",
      "Initial Cost on Val dataset for this epoch 279 = 0.11316231987460487\n",
      "learning rate for this epoch =  0.12234013447038239\n",
      "Error on this batch = 0.07226448815834872\n",
      "Error on this batch = 0.10810973003909423\n",
      "Cost on val dataset after 280 epochs is = 0.11305909840571332\n",
      "Initial Cost on Val dataset for this epoch 280 = 0.11305909840571332\n",
      "learning rate for this epoch =  0.12223075560872526\n",
      "Error on this batch = 0.07213386194453111\n",
      "Error on this batch = 0.1079442292592504\n",
      "Cost on val dataset after 281 epochs is = 0.11295639785684626\n",
      "Initial Cost on Val dataset for this epoch 281 = 0.11295639785684626\n",
      "learning rate for this epoch =  0.12212186395890517\n",
      "Error on this batch = 0.0720038352017847\n",
      "Error on this batch = 0.10777952649040672\n",
      "Cost on val dataset after 282 epochs is = 0.1128542136394294\n",
      "Initial Cost on Val dataset for this epoch 282 = 0.1128542136394294\n",
      "learning rate for this epoch =  0.12201345562840739\n",
      "Error on this batch = 0.07187440206299404\n",
      "Error on this batch = 0.10761561534651921\n",
      "Cost on val dataset after 283 epochs is = 0.11275254122393938\n",
      "Initial Cost on Val dataset for this epoch 283 = 0.11275254122393938\n",
      "learning rate for this epoch =  0.12190552676947876\n",
      "Error on this batch = 0.07174555675047986\n",
      "Error on this batch = 0.10745248949624685\n",
      "Cost on val dataset after 284 epochs is = 0.11265137613887473\n",
      "Initial Cost on Val dataset for this epoch 284 = 0.11265137613887473\n",
      "learning rate for this epoch =  0.12179807357845675\n",
      "Error on this batch = 0.07161729357460296\n",
      "Error on this batch = 0.10729014266270148\n",
      "Cost on val dataset after 285 epochs is = 0.11255071396974964\n",
      "Initial Cost on Val dataset for this epoch 285 = 0.11255071396974964\n",
      "learning rate for this epoch =  0.12169109229511134\n",
      "Error on this batch = 0.07148960693239798\n",
      "Error on this batch = 0.10712856862322406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on val dataset after 286 epochs is = 0.11245055035811069\n",
      "Initial Cost on Val dataset for this epoch 286 = 0.11245055035811069\n",
      "learning rate for this epoch =  0.12158457920199858\n",
      "Error on this batch = 0.07136249130623389\n",
      "Error on this batch = 0.10696776120918486\n",
      "Cost on val dataset after 287 epochs is = 0.11235088100057516\n",
      "Initial Cost on Val dataset for this epoch 287 = 0.11235088100057516\n",
      "learning rate for this epoch =  0.1214785306238262\n",
      "Error on this batch = 0.07123594126250225\n",
      "Error on this batch = 0.10680771430580538\n",
      "Cost on val dataset after 288 epochs is = 0.11225170164789094\n",
      "Initial Cost on Val dataset for this epoch 288 = 0.11225170164789094\n",
      "learning rate for this epoch =  0.12137294292683086\n",
      "Error on this batch = 0.07110995145033171\n",
      "Error on this batch = 0.10664842185200073\n",
      "Cost on val dataset after 289 epochs is = 0.11215300810401718\n",
      "Initial Cost on Val dataset for this epoch 289 = 0.11215300810401718\n",
      "learning rate for this epoch =  0.12126781251816648\n",
      "Error on this batch = 0.0709845166003285\n",
      "Error on this batch = 0.10648987784023894\n",
      "Cost on val dataset after 290 epochs is = 0.11205479622522511\n",
      "Initial Cost on Val dataset for this epoch 290 = 0.11205479622522511\n",
      "learning rate for this epoch =  0.121163135845304\n",
      "Error on this batch = 0.07085963152334188\n",
      "Error on this batch = 0.10633207631641733\n",
      "Cost on val dataset after 291 epochs is = 0.11195706191921871\n",
      "Initial Cost on Val dataset for this epoch 291 = 0.11195706191921871\n",
      "learning rate for this epoch =  0.12105890939544156\n",
      "Error on this batch = 0.07073529110925436\n",
      "Error on this batch = 0.10617501137975208\n",
      "Cost on val dataset after 292 epochs is = 0.1118598011442746\n",
      "Initial Cost on Val dataset for this epoch 292 = 0.1118598011442746\n",
      "learning rate for this epoch =  0.1209551296949258\n",
      "Error on this batch = 0.07061149032579593\n",
      "Error on this batch = 0.10601867718268082\n",
      "Cost on val dataset after 293 epochs is = 0.11176300990840043\n",
      "Initial Cost on Val dataset for this epoch 293 = 0.11176300990840043\n",
      "learning rate for this epoch =  0.12085179330868305\n",
      "Error on this batch = 0.07048822421738146\n",
      "Error on this batch = 0.10586306793077563\n",
      "Cost on val dataset after 294 epochs is = 0.11166668426851178\n",
      "Initial Cost on Val dataset for this epoch 294 = 0.11166668426851178\n",
      "learning rate for this epoch =  0.12074889683966107\n",
      "Error on this batch = 0.07036548790397118\n",
      "Error on this batch = 0.10570817788266482\n",
      "Cost on val dataset after 295 epochs is = 0.11157082032962659\n",
      "Initial Cost on Val dataset for this epoch 295 = 0.11157082032962659\n",
      "learning rate for this epoch =  0.12064643692828043\n",
      "Error on this batch = 0.07024327657995313\n",
      "Error on this batch = 0.10555400134996276\n",
      "Cost on val dataset after 296 epochs is = 0.11147541424407693\n",
      "Initial Cost on Val dataset for this epoch 296 = 0.11147541424407693\n",
      "learning rate for this epoch =  0.120544410251896\n",
      "Error on this batch = 0.07012158551304747\n",
      "Error on this batch = 0.10540053269720517\n",
      "Cost on val dataset after 297 epochs is = 0.11138046221073757\n",
      "Initial Cost on Val dataset for this epoch 297 = 0.11138046221073757\n",
      "learning rate for this epoch =  0.12044281352426756\n",
      "Error on this batch = 0.07000041004323183\n",
      "Error on this batch = 0.10524776634178906\n",
      "Cost on val dataset after 298 epochs is = 0.11128596047427106\n",
      "Initial Cost on Val dataset for this epoch 298 = 0.11128596047427106\n",
      "learning rate for this epoch =  0.12034164349504001\n",
      "Error on this batch = 0.06987974558168733\n",
      "Error on this batch = 0.10509569675391617\n",
      "Cost on val dataset after 299 epochs is = 0.11119190532438865\n",
      "Initial Cost on Val dataset for this epoch 299 = 0.11119190532438865\n",
      "learning rate for this epoch =  0.12024089694923273\n",
      "Error on this batch = 0.06975958760976445\n",
      "Error on this batch = 0.10494431845653784\n",
      "Cost on val dataset after 300 epochs is = 0.11109829309512674\n",
      "Initial Cost on Val dataset for this epoch 300 = 0.11109829309512674\n",
      "learning rate for this epoch =  0.12014057070673773\n",
      "Error on this batch = 0.06963993167796868\n",
      "Error on this batch = 0.10479362602530123\n",
      "Cost on val dataset after 301 epochs is = 0.11100512016413855\n",
      "Initial Cost on Val dataset for this epoch 301 = 0.11100512016413855\n",
      "learning rate for this epoch =  0.1200406616218266\n",
      "Error on this batch = 0.06952077340496492\n",
      "Error on this batch = 0.10464361408849435\n",
      "Cost on val dataset after 302 epochs is = 0.11091238295200057\n",
      "Initial Cost on Val dataset for this epoch 302 = 0.11091238295200057\n",
      "learning rate for this epoch =  0.11994116658266626\n",
      "Error on this batch = 0.06940210847660044\n",
      "Error on this batch = 0.10449427732699024\n",
      "Cost on val dataset after 303 epochs is = 0.11082007792153327\n",
      "Initial Cost on Val dataset for this epoch 303 = 0.11082007792153327\n",
      "learning rate for this epoch =  0.1198420825108428\n",
      "Error on this batch = 0.06928393264494581\n",
      "Error on this batch = 0.10434561047418833\n",
      "Cost on val dataset after 304 epochs is = 0.1107282015771361\n",
      "Initial Cost on Val dataset for this epoch 304 = 0.1107282015771361\n",
      "learning rate for this epoch =  0.11974340636089367\n",
      "Error on this batch = 0.06916624172735328\n",
      "Error on this batch = 0.10419760831595241\n",
      "Cost on val dataset after 305 epochs is = 0.11063675046413562\n",
      "Initial Cost on Val dataset for this epoch 305 = 0.11063675046413562\n",
      "learning rate for this epoch =  0.11964513511984808\n",
      "Error on this batch = 0.06904903160553183\n",
      "Error on this batch = 0.10405026569054421\n",
      "Cost on val dataset after 306 epochs is = 0.11054572116814755\n",
      "Initial Cost on Val dataset for this epoch 306 = 0.11054572116814755\n",
      "learning rate for this epoch =  0.11954726580677509\n",
      "Error on this batch = 0.06893229822463916\n",
      "Error on this batch = 0.10390357748855206\n",
      "Cost on val dataset after 307 epochs is = 0.11045511031445135\n",
      "Initial Cost on Val dataset for this epoch 307 = 0.11045511031445135\n",
      "learning rate for this epoch =  0.11944979547233951\n",
      "Error on this batch = 0.06881603759238906\n",
      "Error on this batch = 0.10375753865281336\n",
      "Cost on val dataset after 308 epochs is = 0.11036491456737771\n",
      "Initial Cost on Val dataset for this epoch 308 = 0.11036491456737771\n",
      "learning rate for this epoch =  0.1193527211983654\n",
      "Error on this batch = 0.06870024577817456\n",
      "Error on this batch = 0.10361214417833058\n",
      "Cost on val dataset after 309 epochs is = 0.11027513062970808\n",
      "Initial Cost on Val dataset for this epoch 309 = 0.11027513062970808\n",
      "learning rate for this epoch =  0.11925604009740705\n",
      "Error on this batch = 0.06858491891220574\n",
      "Error on this batch = 0.10346738911218023\n",
      "Cost on val dataset after 310 epochs is = 0.11018575524208662\n",
      "Initial Cost on Val dataset for this epoch 310 = 0.11018575524208662\n",
      "learning rate for this epoch =  0.11915974931232703\n",
      "Error on this batch = 0.06847005318466214\n",
      "Error on this batch = 0.1033232685534136\n",
      "Cost on val dataset after 311 epochs is = 0.11009678518244342\n",
      "Initial Cost on Val dataset for this epoch 311 = 0.11009678518244342\n",
      "learning rate for this epoch =  0.1190638460158816\n",
      "Error on this batch = 0.068355644844859\n",
      "Error on this batch = 0.10317977765294951\n",
      "Cost on val dataset after 312 epochs is = 0.11000821726542943\n",
      "Initial Cost on Val dataset for this epoch 312 = 0.11000821726542943\n",
      "learning rate for this epoch =  0.11896832741031306\n",
      "Error on this batch = 0.0682416902004271\n",
      "Error on this batch = 0.10303691161345802\n",
      "Cost on val dataset after 313 epochs is = 0.10992004834186257\n",
      "Initial Cost on Val dataset for this epoch 313 = 0.10992004834186257\n",
      "learning rate for this epoch =  0.11887319072694875\n",
      "Error on this batch = 0.06812818561650523\n",
      "Error on this batch = 0.10289466568923512\n",
      "Cost on val dataset after 314 epochs is = 0.10983227529818444\n",
      "Initial Cost on Val dataset for this epoch 314 = 0.10983227529818444\n",
      "learning rate for this epoch =  0.11877843322580707\n",
      "Error on this batch = 0.06801512751494554\n",
      "Error on this batch = 0.10275303518606745\n",
      "Cost on val dataset after 315 epochs is = 0.10974489505592784\n",
      "Initial Cost on Val dataset for this epoch 315 = 0.10974489505592784\n",
      "learning rate for this epoch =  0.11868405219520976\n",
      "Error on this batch = 0.06790251237353041\n",
      "Error on this batch = 0.1026120154610872\n",
      "Cost on val dataset after 316 epochs is = 0.10965790457119465\n",
      "Initial Cost on Val dataset for this epoch 316 = 0.10965790457119465\n",
      "learning rate for this epoch =  0.11859004495140096\n",
      "Error on this batch = 0.0677903367252013\n",
      "Error on this batch = 0.10247160192261694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on val dataset after 317 epochs is = 0.10957130083414365\n",
      "Initial Cost on Val dataset for this epoch 317 = 0.10957130083414365\n",
      "learning rate for this epoch =  0.11849640883817222\n",
      "Error on this batch = 0.06767859715729818\n",
      "Error on this batch = 0.10233179003000346\n",
      "Cost on val dataset after 318 epochs is = 0.10948508086848825\n",
      "Initial Cost on Val dataset for this epoch 318 = 0.10948508086848825\n",
      "learning rate for this epoch =  0.11840314122649412\n",
      "Error on this batch = 0.06756729031080998\n",
      "Error on this batch = 0.1021925752934413\n",
      "Cost on val dataset after 319 epochs is = 0.10939924173100386\n",
      "Initial Cost on Val dataset for this epoch 319 = 0.10939924173100386\n",
      "learning rate for this epoch =  0.11831023951415345\n",
      "Error on this batch = 0.06745641287963457\n",
      "Error on this batch = 0.10205395327378515\n",
      "Cost on val dataset after 320 epochs is = 0.10931378051104451\n",
      "Initial Cost on Val dataset for this epoch 320 = 0.10931378051104451\n",
      "learning rate for this epoch =  0.11821770112539698\n",
      "Error on this batch = 0.06734596160984899\n",
      "Error on this batch = 0.10191591958235108\n",
      "Cost on val dataset after 321 epochs is = 0.10922869433006883\n",
      "Initial Cost on Val dataset for this epoch 321 = 0.10922869433006883\n",
      "learning rate for this epoch =  0.11812552351058042\n",
      "Error on this batch = 0.06723593329898822\n",
      "Error on this batch = 0.10177846988070695\n",
      "Cost on val dataset after 322 epochs is = 0.10914398034117462\n",
      "Initial Cost on Val dataset for this epoch 322 = 0.10914398034117462\n",
      "learning rate for this epoch =  0.11803370414582362\n",
      "Error on this batch = 0.06712632479533323\n",
      "Error on this batch = 0.10164159988045106\n",
      "Cost on val dataset after 323 epochs is = 0.10905963572864241\n",
      "Initial Cost on Val dataset for this epoch 323 = 0.10905963572864241\n",
      "learning rate for this epoch =  0.11794224053267104\n",
      "Error on this batch = 0.06701713299720698\n",
      "Error on this batch = 0.10150530534298004\n",
      "Cost on val dataset after 324 epochs is = 0.10897565770748745\n",
      "Initial Cost on Val dataset for this epoch 324 = 0.10897565770748745\n",
      "learning rate for this epoch =  0.11785113019775793\n",
      "Error on this batch = 0.06690835485227822\n",
      "Error on this batch = 0.10136958207924504\n",
      "Cost on val dataset after 325 epochs is = 0.10889204352301983\n",
      "Initial Cost on Val dataset for this epoch 325 = 0.10889204352301983\n",
      "learning rate for this epoch =  0.11776037069248181\n",
      "Error on this batch = 0.06679998735687302\n",
      "Error on this batch = 0.10123442594949665\n",
      "Cost on val dataset after 326 epochs is = 0.10880879045041274\n",
      "Initial Cost on Val dataset for this epoch 326 = 0.10880879045041274\n",
      "learning rate for this epoch =  0.11766995959267931\n",
      "Error on this batch = 0.0666920275552928\n",
      "Error on this batch = 0.1010998328630189\n",
      "Cost on val dataset after 327 epochs is = 0.10872589579427884\n",
      "Initial Cost on Val dataset for this epoch 327 = 0.10872589579427884\n",
      "learning rate for this epoch =  0.11757989449830816\n",
      "Error on this batch = 0.06658447253913956\n",
      "Error on this batch = 0.10096579877785145\n",
      "Cost on val dataset after 328 epochs is = 0.10864335688825404\n",
      "Initial Cost on Val dataset for this epoch 328 = 0.10864335688825404\n",
      "learning rate for this epoch =  0.11749017303313421\n",
      "Error on this batch = 0.06647731944664657\n",
      "Error on this batch = 0.1008323197005011\n",
      "Cost on val dataset after 329 epochs is = 0.1085611710945888\n",
      "Initial Cost on Val dataset for this epoch 329 = 0.1085611710945888\n",
      "learning rate for this epoch =  0.11740079284442367\n",
      "Error on this batch = 0.06637056546201553\n",
      "Error on this batch = 0.10069939168564229\n",
      "Cost on val dataset after 330 epochs is = 0.10847933580374702\n",
      "Initial Cost on Val dataset for this epoch 330 = 0.10847933580374702\n",
      "learning rate for this epoch =  0.11731175160264\n",
      "Error on this batch = 0.06626420781475856\n",
      "Error on this batch = 0.10056701083580614\n",
      "Cost on val dataset after 331 epochs is = 0.10839784843401185\n",
      "Initial Cost on Val dataset for this epoch 331 = 0.10839784843401185\n",
      "learning rate for this epoch =  0.11722304700114572\n",
      "Error on this batch = 0.06615824377904564\n",
      "Error on this batch = 0.1004351733010596\n",
      "Cost on val dataset after 332 epochs is = 0.10831670643109839\n",
      "Initial Cost on Val dataset for this epoch 332 = 0.10831670643109839\n",
      "learning rate for this epoch =  0.11713467675590904\n",
      "Error on this batch = 0.06605267067305651\n",
      "Error on this batch = 0.10030387527867303\n",
      "Cost on val dataset after 333 epochs is = 0.10823590726777355\n",
      "Initial Cost on Val dataset for this epoch 333 = 0.10823590726777355\n",
      "learning rate for this epoch =  0.11704663860521486\n",
      "Error on this batch = 0.06594748585833701\n",
      "Error on this batch = 0.10017311301277804\n",
      "Cost on val dataset after 334 epochs is = 0.10815544844348211\n",
      "Initial Cost on Val dataset for this epoch 334 = 0.10815544844348211\n",
      "learning rate for this epoch =  0.1169589303093807\n",
      "Error on this batch = 0.06584268673915937\n",
      "Error on this batch = 0.10004288279401476\n",
      "Cost on val dataset after 335 epochs is = 0.10807532748397963\n",
      "Initial Cost on Val dataset for this epoch 335 = 0.10807532748397963\n",
      "learning rate for this epoch =  0.11687154965047665\n",
      "Error on this batch = 0.06573827076188629\n",
      "Error on this batch = 0.09991318095916903\n",
      "Cost on val dataset after 336 epochs is = 0.10799554194097129\n",
      "Initial Cost on Val dataset for this epoch 336 = 0.10799554194097129\n",
      "learning rate for this epoch =  0.11678449443205002\n",
      "Error on this batch = 0.0656342354143383\n",
      "Error on this batch = 0.09978400389080004\n",
      "Cost on val dataset after 337 epochs is = 0.10791608939175737\n",
      "Initial Cost on Val dataset for this epoch 337 = 0.10791608939175737\n",
      "learning rate for this epoch =  0.11669776247885426\n",
      "Error on this batch = 0.06553057822516445\n",
      "Error on this batch = 0.09965534801685795\n",
      "Cost on val dataset after 338 epochs is = 0.10783696743888423\n",
      "Initial Cost on Val dataset for this epoch 338 = 0.10783696743888423\n",
      "learning rate for this epoch =  0.1166113516365818\n",
      "Error on this batch = 0.0654272967632157\n",
      "Error on this batch = 0.09952720981029249\n",
      "Cost on val dataset after 339 epochs is = 0.10775817370980154\n",
      "Initial Cost on Val dataset for this epoch 339 = 0.10775817370980154\n",
      "learning rate for this epoch =  0.1165252597716015\n",
      "Error on this batch = 0.06532438863692092\n",
      "Error on this batch = 0.09939958578865216\n",
      "Cost on val dataset after 340 epochs is = 0.10767970585652493\n",
      "Initial Cost on Val dataset for this epoch 340 = 0.10767970585652493\n",
      "learning rate for this epoch =  0.11643948477069971\n",
      "Error on this batch = 0.06522185149366554\n",
      "Error on this batch = 0.0992724725136749\n",
      "Cost on val dataset after 341 epochs is = 0.10760156155530444\n",
      "Initial Cost on Val dataset for this epoch 341 = 0.10760156155530444\n",
      "learning rate for this epoch =  0.11635402454082565\n",
      "Error on this batch = 0.06511968301917213\n",
      "Error on this batch = 0.09914586659086976\n",
      "Cost on val dataset after 342 epochs is = 0.10752373850629837\n",
      "Initial Cost on Val dataset for this epoch 342 = 0.10752373850629837\n",
      "learning rate for this epoch =  0.1162688770088405\n",
      "Error on this batch = 0.06501788093688296\n",
      "Error on this batch = 0.09901976466909058\n",
      "Cost on val dataset after 343 epochs is = 0.10744623443325217\n",
      "Initial Cost on Val dataset for this epoch 343 = 0.10744623443325217\n",
      "learning rate for this epoch =  0.11618404012127041\n",
      "Error on this batch = 0.06491644300734474\n",
      "Error on this batch = 0.09889416344010163\n",
      "Cost on val dataset after 344 epochs is = 0.10736904708318301\n",
      "Initial Cost on Val dataset for this epoch 344 = 0.10736904708318301\n",
      "learning rate for this epoch =  0.11609951184406334\n",
      "Error on this batch = 0.06481536702759458\n",
      "Error on this batch = 0.0987690596381352\n",
      "Cost on val dataset after 345 epochs is = 0.10729217422606883\n",
      "Initial Cost on Val dataset for this epoch 345 = 0.10729217422606883\n",
      "learning rate for this epoch =  0.11601529016234945\n",
      "Error on this batch = 0.06471465083054782\n",
      "Error on this batch = 0.09864445003944215\n",
      "Cost on val dataset after 346 epochs is = 0.10721561365454271\n",
      "Initial Cost on Val dataset for this epoch 346 = 0.10721561365454271\n",
      "learning rate for this epoch =  0.11593137308020533\n",
      "Error on this batch = 0.06461429228438728\n",
      "Error on this batch = 0.09852033146183505\n",
      "Cost on val dataset after 347 epochs is = 0.10713936318359198\n",
      "Initial Cost on Val dataset for this epoch 347 = 0.10713936318359198\n",
      "learning rate for this epoch =  0.11584775862042163\n",
      "Error on this batch = 0.06451428929195373\n",
      "Error on this batch = 0.09839670076422471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on val dataset after 348 epochs is = 0.10706342065026185\n",
      "Initial Cost on Val dataset for this epoch 348 = 0.10706342065026185\n",
      "learning rate for this epoch =  0.11576444482427425\n",
      "Error on this batch = 0.06441463979013755\n",
      "Error on this batch = 0.09827355484614987\n",
      "Cost on val dataset after 349 epochs is = 0.10698778391336378\n",
      "Initial Cost on Val dataset for this epoch 349 = 0.10698778391336378\n",
      "learning rate for this epoch =  0.11568142975129903\n",
      "Error on this batch = 0.06431534174927196\n",
      "Error on this batch = 0.0981508906473012\n",
      "Cost on val dataset after 350 epochs is = 0.1069124508531883\n",
      "Initial Cost on Val dataset for this epoch 350 = 0.1069124508531883\n",
      "learning rate for this epoch =  0.11559871147906978\n",
      "Error on this batch = 0.06421639317252716\n",
      "Error on this batch = 0.098028705147039\n",
      "Cost on val dataset after 351 epochs is = 0.10683741937122232\n",
      "Initial Cost on Val dataset for this epoch 351 = 0.10683741937122232\n",
      "learning rate for this epoch =  0.11551628810297963\n",
      "Error on this batch = 0.06411779209530559\n",
      "Error on this batch = 0.09790699536390558\n",
      "Cost on val dataset after 352 epochs is = 0.10676268738987026\n",
      "Initial Cost on Val dataset for this epoch 352 = 0.10676268738987026\n",
      "learning rate for this epoch =  0.11543415773602565\n",
      "Error on this batch = 0.06401953658463874\n",
      "Error on this batch = 0.09778575835513229\n",
      "Cost on val dataset after 353 epochs is = 0.10668825285217992\n",
      "Initial Cost on Val dataset for this epoch 353 = 0.10668825285217992\n",
      "learning rate for this epoch =  0.11535231850859669\n",
      "Error on this batch = 0.06392162473858462\n",
      "Error on this batch = 0.09766499121614189\n",
      "Cost on val dataset after 354 epochs is = 0.10661411372157213\n",
      "Initial Cost on Val dataset for this epoch 354 = 0.10661411372157213\n",
      "learning rate for this epoch =  0.11527076856826429\n",
      "Error on this batch = 0.06382405468562698\n",
      "Error on this batch = 0.09754469108004621\n",
      "Cost on val dataset after 355 epochs is = 0.10654026798157444\n",
      "Initial Cost on Val dataset for this epoch 355 = 0.10654026798157444\n",
      "learning rate for this epoch =  0.1151895060795769\n",
      "Error on this batch = 0.06372682458407551\n",
      "Error on this batch = 0.09742485511713948\n",
      "Cost on val dataset after 356 epochs is = 0.10646671363555862\n",
      "Initial Cost on Val dataset for this epoch 356 = 0.10646671363555862\n",
      "learning rate for this epoch =  0.11510852922385682\n",
      "Error on this batch = 0.06362993262146763\n",
      "Error on this batch = 0.0973054805343882\n",
      "Cost on val dataset after 357 epochs is = 0.10639344870648233\n",
      "Initial Cost on Val dataset for this epoch 357 = 0.10639344870648233\n",
      "learning rate for this epoch =  0.11502783619900045\n",
      "Error on this batch = 0.06353337701397142\n",
      "Error on this batch = 0.09718656457491705\n",
      "Cost on val dataset after 358 epochs is = 0.10632047123663403\n",
      "Initial Cost on Val dataset for this epoch 358 = 0.10632047123663403\n",
      "learning rate for this epoch =  0.11494742521928122\n",
      "Error on this batch = 0.06343715600579068\n",
      "Error on this batch = 0.0970681045174917\n",
      "Cost on val dataset after 359 epochs is = 0.10624777928738194\n",
      "Initial Cost on Val dataset for this epoch 359 = 0.10624777928738194\n",
      "learning rate for this epoch =  0.11486729451515557\n",
      "Error on this batch = 0.06334126786857101\n",
      "Error on this batch = 0.09695009767599899\n",
      "Cost on val dataset after 360 epochs is = 0.10617537093892655\n",
      "Initial Cost on Val dataset for this epoch 360 = 0.10617537093892655\n",
      "learning rate for this epoch =  0.11478744233307164\n",
      "Error on this batch = 0.06324571090080842\n",
      "Error on this batch = 0.09683254139892405\n",
      "Cost on val dataset after 361 epochs is = 0.10610324429005648\n",
      "Initial Cost on Val dataset for this epoch 361 = 0.10610324429005648\n",
      "learning rate for this epoch =  0.11470786693528087\n",
      "Error on this batch = 0.06315048342725922\n",
      "Error on this batch = 0.0967154330688259\n",
      "Cost on val dataset after 362 epochs is = 0.10603139745790803\n",
      "Initial Cost on Val dataset for this epoch 362 = 0.10603139745790803\n",
      "learning rate for this epoch =  0.11462856659965227\n",
      "Error on this batch = 0.06305558379835283\n",
      "Error on this batch = 0.0965987701018106\n",
      "Cost on val dataset after 363 epochs is = 0.10595982857772798\n",
      "Initial Cost on Val dataset for this epoch 363 = 0.10595982857772798\n",
      "learning rate for this epoch =  0.11454953961948931\n",
      "Error on this batch = 0.06296101038960616\n",
      "Error on this batch = 0.09648254994700306\n",
      "Cost on val dataset after 364 epochs is = 0.10588853580263978\n",
      "Initial Cost on Val dataset for this epoch 364 = 0.10588853580263978\n",
      "learning rate for this epoch =  0.11447078430334955\n",
      "Error on this batch = 0.06286676160104111\n",
      "Error on this batch = 0.09636677008601761\n",
      "Cost on val dataset after 365 epochs is = 0.10581751730341307\n",
      "Initial Cost on Val dataset for this epoch 365 = 0.10581751730341307\n",
      "learning rate for this epoch =  0.11439229897486693\n",
      "Error on this batch = 0.0627728358566044\n",
      "Error on this batch = 0.09625142803242763\n",
      "Cost on val dataset after 366 epochs is = 0.10574677126823648\n",
      "Initial Cost on Val dataset for this epoch 366 = 0.10574677126823648\n",
      "learning rate for this epoch =  0.11431408197257639\n",
      "Error on this batch = 0.0626792316035905\n",
      "Error on this batch = 0.0961365213312347\n",
      "Cost on val dataset after 367 epochs is = 0.10567629590249339\n",
      "Initial Cost on Val dataset for this epoch 367 = 0.10567629590249339\n",
      "learning rate for this epoch =  0.11423613164974125\n",
      "Error on this batch = 0.06258594731206749\n",
      "Error on this batch = 0.09602204755833718\n",
      "Cost on val dataset after 368 epochs is = 0.10560608942854115\n",
      "Initial Cost on Val dataset for this epoch 368 = 0.10560608942854115\n",
      "learning rate for this epoch =  0.11415844637418282\n",
      "Error on this batch = 0.06249298147430658\n",
      "Error on this batch = 0.0959080043199991\n",
      "Cost on val dataset after 369 epochs is = 0.10553615008549326\n",
      "Initial Cost on Val dataset for this epoch 369 = 0.10553615008549326\n",
      "learning rate for this epoch =  0.11408102452811265\n",
      "Error on this batch = 0.06240033260421502\n",
      "Error on this batch = 0.09579438925231927\n",
      "Cost on val dataset after 370 epochs is = 0.10546647612900432\n",
      "Initial Cost on Val dataset for this epoch 370 = 0.10546647612900432\n",
      "learning rate for this epoch =  0.11400386450796704\n",
      "Error on this batch = 0.062307999236772584\n",
      "Error on this batch = 0.09568120002070077\n",
      "Cost on val dataset after 371 epochs is = 0.10539706583105878\n",
      "Initial Cost on Val dataset for this epoch 371 = 0.10539706583105878\n",
      "learning rate for this epoch =  0.11392696472424395\n",
      "Error on this batch = 0.06221597992747256\n",
      "Error on this batch = 0.09556843431932185\n",
      "Cost on val dataset after 372 epochs is = 0.1053279174797616\n",
      "Initial Cost on Val dataset for this epoch 372 = 0.1053279174797616\n",
      "learning rate for this epoch =  0.11385032360134212\n",
      "Error on this batch = 0.06212427325176664\n",
      "Error on this batch = 0.09545608987060757\n",
      "Cost on val dataset after 373 epochs is = 0.10525902937913278\n",
      "Initial Cost on Val dataset for this epoch 373 = 0.10525902937913278\n",
      "learning rate for this epoch =  0.11377393957740255\n",
      "Error on this batch = 0.06203287780451446\n",
      "Error on this batch = 0.09534416442470298\n",
      "Cost on val dataset after 374 epochs is = 0.10519039984890435\n",
      "Initial Cost on Val dataset for this epoch 374 = 0.10519039984890435\n",
      "learning rate for this epoch =  0.11369781110415222\n",
      "Error on this batch = 0.06194179219943798\n",
      "Error on this batch = 0.09523265575894818\n",
      "Cost on val dataset after 375 epochs is = 0.10512202722432022\n",
      "Initial Cost on Val dataset for this epoch 375 = 0.10512202722432022\n",
      "learning rate for this epoch =  0.11362193664674994\n",
      "Error on this batch = 0.06185101506858055\n",
      "Error on this batch = 0.09512156167735533\n",
      "Cost on val dataset after 376 epochs is = 0.10505390985593914\n",
      "Initial Cost on Val dataset for this epoch 376 = 0.10505390985593914\n",
      "learning rate for this epoch =  0.11354631468363437\n",
      "Error on this batch = 0.061760545061771685\n",
      "Error on this batch = 0.09501088001008767\n",
      "Cost on val dataset after 377 epochs is = 0.1049860461094402\n",
      "Initial Cost on Val dataset for this epoch 377 = 0.1049860461094402\n",
      "learning rate for this epoch =  0.1134709437063741\n",
      "Error on this batch = 0.06167038084609687\n",
      "Error on this batch = 0.09490060861294154\n",
      "Cost on val dataset after 378 epochs is = 0.10491843436543129\n",
      "Initial Cost on Val dataset for this epoch 378 = 0.10491843436543129\n",
      "learning rate for this epoch =  0.11339582221952002\n",
      "Error on this batch = 0.06158052110537333\n",
      "Error on this batch = 0.09479074536683045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on val dataset after 379 epochs is = 0.10485107301926028\n",
      "Initial Cost on Val dataset for this epoch 379 = 0.10485107301926028\n",
      "learning rate for this epoch =  0.11332094874045952\n",
      "Error on this batch = 0.061490964539631605\n",
      "Error on this batch = 0.09468128817727282\n",
      "Cost on val dataset after 380 epochs is = 0.10478396048082896\n",
      "Initial Cost on Val dataset for this epoch 380 = 0.10478396048082896\n",
      "learning rate for this epoch =  0.1132463217992727\n",
      "Error on this batch = 0.06140170986460351\n",
      "Error on this batch = 0.09457223497388231\n",
      "Cost on val dataset after 381 epochs is = 0.10471709517440966\n",
      "Initial Cost on Val dataset for this epoch 381 = 0.10471709517440966\n",
      "learning rate for this epoch =  0.11317193993859079\n",
      "Error on this batch = 0.0613127558112163\n",
      "Error on this batch = 0.09446358370986206\n",
      "Cost on val dataset after 382 epochs is = 0.10465047553846464\n",
      "Initial Cost on Val dataset for this epoch 382 = 0.10465047553846464\n",
      "learning rate for this epoch =  0.11309780171345626\n",
      "Error on this batch = 0.06122410112509334\n",
      "Error on this batch = 0.09435533236150206\n",
      "Cost on val dataset after 383 epochs is = 0.10458410002546799\n",
      "Initial Cost on Val dataset for this epoch 383 = 0.10458410002546799\n",
      "learning rate for this epoch =  0.11302390569118509\n",
      "Error on this batch = 0.06113574456606213\n",
      "Error on this batch = 0.09424747892768068\n",
      "Cost on val dataset after 384 epochs is = 0.10451796710173043\n",
      "Initial Cost on Val dataset for this epoch 384 = 0.10451796710173043\n",
      "learning rate for this epoch =  0.11295025045123061\n",
      "Error on this batch = 0.06104768490766881\n",
      "Error on this batch = 0.09414002142936996\n",
      "Cost on val dataset after 385 epochs is = 0.10445207524722655\n",
      "Initial Cost on Val dataset for this epoch 385 = 0.10445207524722655\n",
      "learning rate for this epoch =  0.11287683458504956\n",
      "Error on this batch = 0.06095992093670018\n",
      "Error on this batch = 0.09403295790914537\n",
      "Cost on val dataset after 386 epochs is = 0.10438642295542468\n",
      "Initial Cost on Val dataset for this epoch 386 = 0.10438642295542468\n",
      "learning rate for this epoch =  0.11280365669596971\n",
      "Error on this batch = 0.060872451452713125\n",
      "Error on this batch = 0.09392628643069958\n",
      "Cost on val dataset after 387 epochs is = 0.10432100873311954\n",
      "Initial Cost on Val dataset for this epoch 387 = 0.10432100873311954\n",
      "learning rate for this epoch =  0.11273071539905938\n",
      "Error on this batch = 0.0607852752675718\n",
      "Error on this batch = 0.09382000507836079\n",
      "Cost on val dataset after 388 epochs is = 0.10425583110026708\n",
      "Initial Cost on Val dataset for this epoch 388 = 0.10425583110026708\n",
      "learning rate for this epoch =  0.11265800932099873\n",
      "Error on this batch = 0.06069839120499223\n",
      "Error on this batch = 0.09371411195661607\n",
      "Cost on val dataset after 389 epochs is = 0.1041908885898223\n",
      "Initial Cost on Val dataset for this epoch 389 = 0.1041908885898223\n",
      "learning rate for this epoch =  0.11258553709995278\n",
      "Error on this batch = 0.060611798100095406\n",
      "Error on this batch = 0.09360860518963897\n",
      "Cost on val dataset after 390 epochs is = 0.1041261797475794\n",
      "Initial Cost on Val dataset for this epoch 390 = 0.1041261797475794\n",
      "learning rate for this epoch =  0.11251329738544609\n",
      "Error on this batch = 0.06052549479896815\n",
      "Error on this batch = 0.09350348292082253\n",
      "Cost on val dataset after 391 epochs is = 0.10406170313201425\n",
      "Initial Cost on Val dataset for this epoch 391 = 0.10406170313201425\n",
      "learning rate for this epoch =  0.11244128883823923\n",
      "Error on this batch = 0.06043948015823235\n",
      "Error on this batch = 0.09339874331231701\n",
      "Cost on val dataset after 392 epochs is = 0.1039974573141298\n",
      "Initial Cost on Val dataset for this epoch 392 = 0.1039974573141298\n",
      "learning rate for this epoch =  0.11236951013020673\n",
      "Error on this batch = 0.06035375304462276\n",
      "Error on this batch = 0.09329438454457323\n",
      "Cost on val dataset after 393 epochs is = 0.10393344087730354\n",
      "Initial Cost on Val dataset for this epoch 393 = 0.10393344087730354\n",
      "learning rate for this epoch =  0.11229795994421696\n",
      "Error on this batch = 0.06026831233457312\n",
      "Error on this batch = 0.0931904048158906\n",
      "Cost on val dataset after 394 epochs is = 0.10386965241713768\n",
      "Initial Cost on Val dataset for this epoch 394 = 0.10386965241713768\n",
      "learning rate for this epoch =  0.11222663697401325\n",
      "Error on this batch = 0.06018315691381121\n",
      "Error on this batch = 0.09308680234197121\n",
      "Cost on val dataset after 395 epochs is = 0.10380609054131158\n",
      "Initial Cost on Val dataset for this epoch 395 = 0.10380609054131158\n",
      "learning rate for this epoch =  0.11215553992409688\n",
      "Error on this batch = 0.0600982856769625\n",
      "Error on this batch = 0.0929835753554789\n",
      "Cost on val dataset after 396 epochs is = 0.10374275386943672\n",
      "Initial Cost on Val dataset for this epoch 396 = 0.10374275386943672\n",
      "learning rate for this epoch =  0.11208466750961145\n",
      "Error on this batch = 0.06001369752716278\n",
      "Error on this batch = 0.09288072210560433\n",
      "Cost on val dataset after 397 epochs is = 0.10367964103291405\n",
      "Initial Cost on Val dataset for this epoch 397 = 0.10367964103291405\n",
      "learning rate for this epoch =  0.11201401845622891\n",
      "Error on this batch = 0.05992939137567985\n",
      "Error on this batch = 0.09277824085763532\n",
      "Cost on val dataset after 398 epochs is = 0.10361675067479341\n",
      "Initial Cost on Val dataset for this epoch 398 = 0.10361675067479341\n",
      "learning rate for this epoch =  0.11194359150003692\n",
      "Error on this batch = 0.05984536614154411\n",
      "Error on this batch = 0.0926761298925333\n",
      "Cost on val dataset after 399 epochs is = 0.10355408144963597\n",
      "Initial Cost on Val dataset for this epoch 399 = 0.10355408144963597\n",
      "learning rate for this epoch =  0.11187338538742793\n",
      "Error on this batch = 0.05976162075118834\n",
      "Error on this batch = 0.09257438750651524\n",
      "Cost on val dataset after 400 epochs is = 0.10349163202337813\n",
      "Initial Cost on Val dataset for this epoch 400 = 0.10349163202337813\n",
      "learning rate for this epoch =  0.11180339887498948\n",
      "Error on this batch = 0.05967815413809688\n",
      "Error on this batch = 0.09247301201064176\n",
      "Cost on val dataset after 401 epochs is = 0.10342940107319835\n",
      "Initial Cost on Val dataset for this epoch 401 = 0.10342940107319835\n",
      "learning rate for this epoch =  0.11173363072939616\n",
      "Error on this batch = 0.05959496524246369\n",
      "Error on this batch = 0.09237200173041087\n",
      "Cost on val dataset after 402 epochs is = 0.10336738728738604\n",
      "Initial Cost on Val dataset for this epoch 402 = 0.10336738728738604\n",
      "learning rate for this epoch =  0.11166407972730269\n",
      "Error on this batch = 0.059512053010859996\n",
      "Error on this batch = 0.09227135500535802\n",
      "Cost on val dataset after 403 epochs is = 0.10330558936521253\n",
      "Initial Cost on Val dataset for this epoch 403 = 0.10330558936521253\n",
      "learning rate for this epoch =  0.1115947446552388\n",
      "Error on this batch = 0.059429416395911136\n",
      "Error on this batch = 0.09217107018866165\n",
      "Cost on val dataset after 404 epochs is = 0.1032440060168046\n",
      "Initial Cost on Val dataset for this epoch 404 = 0.1032440060168046\n",
      "learning rate for this epoch =  0.11152562430950505\n",
      "Error on this batch = 0.059347054355982753\n",
      "Error on this batch = 0.09207114564675538\n",
      "Cost on val dataset after 405 epochs is = 0.10318263596301988\n",
      "Initial Cost on Val dataset for this epoch 405 = 0.10318263596301988\n",
      "learning rate for this epoch =  0.11145671749607033\n",
      "Error on this batch = 0.05926496585487627\n",
      "Error on this batch = 0.09197157975894582\n",
      "Cost on val dataset after 406 epochs is = 0.10312147793532456\n",
      "Initial Cost on Val dataset for this epoch 406 = 0.10312147793532456\n",
      "learning rate for this epoch =  0.1113880230304705\n",
      "Error on this batch = 0.05918314986153393\n",
      "Error on this batch = 0.09187237091703672\n",
      "Cost on val dataset after 407 epochs is = 0.10306053067567322\n",
      "Initial Cost on Val dataset for this epoch 407 = 0.10306053067567322\n",
      "learning rate for this epoch =  0.11131953973770842\n",
      "Error on this batch = 0.05910160534975289\n",
      "Error on this batch = 0.09177351752495914\n",
      "Cost on val dataset after 408 epochs is = 0.1029997929363908\n",
      "Initial Cost on Val dataset for this epoch 408 = 0.1029997929363908\n",
      "learning rate for this epoch =  0.11125126645215519\n",
      "Error on this batch = 0.05902033129790873\n",
      "Error on this batch = 0.09167501799840778\n",
      "Cost on val dataset after 409 epochs is = 0.10293926348005632\n",
      "Initial Cost on Val dataset for this epoch 409 = 0.10293926348005632\n",
      "learning rate for this epoch =  0.11118320201745278\n",
      "Error on this batch = 0.058939326688688516\n",
      "Error on this batch = 0.09157687076448344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on val dataset after 410 epochs is = 0.10287894107938934\n",
      "Initial Cost on Val dataset for this epoch 410 = 0.10287894107938934\n",
      "learning rate for this epoch =  0.11111534528641788\n",
      "Error on this batch = 0.05885859050883257\n",
      "Error on this batch = 0.09147907426134182\n",
      "Cost on val dataset after 411 epochs is = 0.10281882451713764\n",
      "Initial Cost on Val dataset for this epoch 411 = 0.10281882451713764\n",
      "learning rate for this epoch =  0.11104769512094681\n",
      "Error on this batch = 0.05877812174888618\n",
      "Error on this batch = 0.091381626937848\n",
      "Cost on val dataset after 412 epochs is = 0.10275891258596727\n",
      "Initial Cost on Val dataset for this epoch 412 = 0.10275891258596727\n",
      "learning rate for this epoch =  0.11098025039192183\n",
      "Error on this batch = 0.05869791940295971\n",
      "Error on this batch = 0.09128452725323782\n",
      "Cost on val dataset after 413 epochs is = 0.10269920408835466\n",
      "Initial Cost on Val dataset for this epoch 413 = 0.10269920408835466\n",
      "learning rate for this epoch =  0.11091300997911864\n",
      "Error on this batch = 0.05861798246849847\n",
      "Error on this batch = 0.09118777367678466\n",
      "Cost on val dataset after 414 epochs is = 0.10263969783648039\n",
      "Initial Cost on Val dataset for this epoch 414 = 0.10263969783648039\n",
      "learning rate for this epoch =  0.11084597277111498\n",
      "Error on this batch = 0.05853830994606114\n",
      "Error on this batch = 0.0910913646874729\n",
      "Cost on val dataset after 415 epochs is = 0.10258039265212485\n",
      "Initial Cost on Val dataset for this epoch 415 = 0.10258039265212485\n",
      "learning rate for this epoch =  0.11077913766520031\n",
      "Error on this batch = 0.05845890083910744\n",
      "Error on this batch = 0.09099529877367735\n",
      "Cost on val dataset after 416 epochs is = 0.10252128736656588\n",
      "Initial Cost on Val dataset for this epoch 416 = 0.10252128736656588\n",
      "learning rate for this epoch =  0.11071250356728685\n",
      "Error on this batch = 0.05837975415379465\n",
      "Error on this batch = 0.09089957443284878\n",
      "Cost on val dataset after 417 epochs is = 0.10246238082047812\n",
      "Initial Cost on Val dataset for this epoch 417 = 0.10246238082047812\n",
      "learning rate for this epoch =  0.11064606939182157\n",
      "Error on this batch = 0.058300868898782804\n",
      "Error on this batch = 0.09080419017120558\n",
      "Cost on val dataset after 418 epochs is = 0.10240367186383409\n",
      "Initial Cost on Val dataset for this epoch 418 = 0.10240367186383409\n",
      "learning rate for this epoch =  0.11057983406169934\n",
      "Error on this batch = 0.058222244085048704\n",
      "Error on this batch = 0.09070914450343179\n",
      "Cost on val dataset after 419 epochs is = 0.10234515935580724\n",
      "Initial Cost on Val dataset for this epoch 419 = 0.10234515935580724\n",
      "learning rate for this epoch =  0.11051379650817714\n",
      "Error on this batch = 0.05814387872570841\n",
      "Error on this batch = 0.09061443595238078\n",
      "Cost on val dataset after 420 epochs is = 0.10228684216467612\n",
      "Initial Cost on Val dataset for this epoch 420 = 0.10228684216467612\n",
      "learning rate for this epoch =  0.11044795567078942\n",
      "Error on this batch = 0.05806577183584845\n",
      "Error on this batch = 0.09052006304878538\n",
      "Cost on val dataset after 421 epochs is = 0.1022287191677311\n",
      "Initial Cost on Val dataset for this epoch 421 = 0.1022287191677311\n",
      "learning rate for this epoch =  0.1103823104972644\n",
      "Error on this batch = 0.057987922432365106\n",
      "Error on this batch = 0.09042602433097412\n",
      "Cost on val dataset after 422 epochs is = 0.10217078925118164\n",
      "Initial Cost on Val dataset for this epoch 422 = 0.10217078925118164\n",
      "learning rate for this epoch =  0.11031685994344151\n",
      "Error on this batch = 0.057910329533812255\n",
      "Error on this batch = 0.09033231834459306\n",
      "Cost on val dataset after 423 epochs is = 0.10211305131006616\n",
      "Initial Cost on Val dataset for this epoch 423 = 0.10211305131006616\n",
      "learning rate for this epoch =  0.11025160297318981\n",
      "Error on this batch = 0.057832992160257175\n",
      "Error on this batch = 0.0902389436423343\n",
      "Cost on val dataset after 424 epochs is = 0.10205550424816257\n",
      "Initial Cost on Val dataset for this epoch 424 = 0.10205550424816257\n",
      "learning rate for this epoch =  0.11018653855832754\n",
      "Error on this batch = 0.05775590933314453\n",
      "Error on this batch = 0.09014589878366998\n",
      "Cost on val dataset after 425 epochs is = 0.10199814697790105\n",
      "Initial Cost on Val dataset for this epoch 425 = 0.10199814697790105\n",
      "learning rate for this epoch =  0.11012166567854233\n",
      "Error on this batch = 0.057679080075167914\n",
      "Error on this batch = 0.09005318233459257\n",
      "Cost on val dataset after 426 epochs is = 0.10194097842027763\n",
      "Initial Cost on Val dataset for this epoch 426 = 0.10194097842027763\n",
      "learning rate for this epoch =  0.11005698332131283\n",
      "Error on this batch = 0.05760250341014972\n",
      "Error on this batch = 0.08996079286736107\n",
      "Cost on val dataset after 427 epochs is = 0.10188399750476967\n",
      "Initial Cost on Val dataset for this epoch 427 = 0.10188399750476967\n",
      "learning rate for this epoch =  0.10999249048183099\n",
      "Error on this batch = 0.05752617836292807\n",
      "Error on this batch = 0.08986872896025318\n",
      "Cost on val dataset after 428 epochs is = 0.10182720316925255\n",
      "Initial Cost on Val dataset for this epoch 428 = 0.10182720316925255\n",
      "learning rate for this epoch =  0.10992818616292545\n",
      "Error on this batch = 0.05745010395925146\n",
      "Error on this batch = 0.08977698919732341\n",
      "Cost on val dataset after 429 epochs is = 0.10177059435991748\n",
      "Initial Cost on Val dataset for this epoch 429 = 0.10177059435991748\n",
      "learning rate for this epoch =  0.10986406937498579\n",
      "Error on this batch = 0.05737427922568086\n",
      "Error on this batch = 0.0896855721681671\n",
      "Cost on val dataset after 430 epochs is = 0.10171417003119108\n",
      "Initial Cost on Val dataset for this epoch 430 = 0.10171417003119108\n",
      "learning rate for this epoch =  0.10980013913588772\n",
      "Error on this batch = 0.05729870318949889\n",
      "Error on this batch = 0.08959447646769046\n",
      "Cost on val dataset after 431 epochs is = 0.10165792914565575\n",
      "Initial Cost on Val dataset for this epoch 431 = 0.10165792914565575\n",
      "learning rate for this epoch =  0.10973639447091926\n",
      "Error on this batch = 0.05722337487862584\n",
      "Error on this batch = 0.08950370069588644\n",
      "Cost on val dataset after 432 epochs is = 0.10160187067397163\n",
      "Initial Cost on Val dataset for this epoch 432 = 0.10160187067397163\n",
      "learning rate for this epoch =  0.10967283441270771\n",
      "Error on this batch = 0.05714829332154308\n",
      "Error on this batch = 0.08941324345761632\n",
      "Cost on val dataset after 433 epochs is = 0.10154599359479932\n",
      "Initial Cost on Val dataset for this epoch 433 = 0.10154599359479932\n",
      "learning rate for this epoch =  0.10960945800114749\n",
      "Error on this batch = 0.0570734575472226\n",
      "Error on this batch = 0.08932310336239771\n",
      "Cost on val dataset after 434 epochs is = 0.10149029689472429\n",
      "Initial Cost on Val dataset for this epoch 434 = 0.10149029689472429\n",
      "learning rate for this epoch =  0.10954626428332909\n",
      "Error on this batch = 0.05699886658506364\n",
      "Error on this batch = 0.08923327902419756\n",
      "Cost on val dataset after 435 epochs is = 0.10143477956818181\n",
      "Initial Cost on Val dataset for this epoch 435 = 0.10143477956818181\n",
      "learning rate for this epoch =  0.10948325231346849\n",
      "Error on this batch = 0.056924519464835455\n",
      "Error on this batch = 0.08914376906123188\n",
      "Cost on val dataset after 436 epochs is = 0.10137944061738338\n",
      "Initial Cost on Val dataset for this epoch 436 = 0.10137944061738338\n",
      "learning rate for this epoch =  0.1094204211528378\n",
      "Error on this batch = 0.05685041521662654\n",
      "Error on this batch = 0.08905457209577085\n",
      "Cost on val dataset after 437 epochs is = 0.10132427905224405\n",
      "Initial Cost on Val dataset for this epoch 437 = 0.10132427905224405\n",
      "learning rate for this epoch =  0.1093577698696965\n",
      "Error on this batch = 0.05677655287079971\n",
      "Error on this batch = 0.08896568675394964\n",
      "Cost on val dataset after 438 epochs is = 0.10126929389031056\n",
      "Initial Cost on Val dataset for this epoch 438 = 0.10126929389031056\n",
      "learning rate for this epoch =  0.1092952975392236\n",
      "Error on this batch = 0.05670293145795345\n",
      "Error on this batch = 0.08887711166558541\n",
      "Cost on val dataset after 439 epochs is = 0.10121448415669058\n",
      "Initial Cost on Val dataset for this epoch 439 = 0.10121448415669058\n",
      "learning rate for this epoch =  0.10923300324345062\n",
      "Error on this batch = 0.056629550008888786\n",
      "Error on this batch = 0.08878884546399989\n",
      "Cost on val dataset after 440 epochs is = 0.10115984888398301\n",
      "Initial Cost on Val dataset for this epoch 440 = 0.10115984888398301\n",
      "learning rate for this epoch =  0.10917088607119531\n",
      "Error on this batch = 0.056556407554581946\n",
      "Error on this batch = 0.08870088678584777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on val dataset after 441 epochs is = 0.10110538711220882\n",
      "Initial Cost on Val dataset for this epoch 441 = 0.10110538711220882\n",
      "learning rate for this epoch =  0.1091089451179962\n",
      "Error on this batch = 0.056483503126162446\n",
      "Error on this batch = 0.08861323427095105\n",
      "Cost on val dataset after 442 epochs is = 0.10105109788874292\n",
      "Initial Cost on Val dataset for this epoch 442 = 0.10105109788874292\n",
      "learning rate for this epoch =  0.10904717948604793\n",
      "Error on this batch = 0.056410835754896346\n",
      "Error on this batch = 0.08852588656213893\n",
      "Cost on val dataset after 443 epochs is = 0.10099698026824695\n",
      "Initial Cost on Val dataset for this epoch 443 = 0.10099698026824695\n",
      "learning rate for this epoch =  0.10898558828413735\n",
      "Error on this batch = 0.056338404472175085\n",
      "Error on this batch = 0.08843884230509376\n",
      "Cost on val dataset after 444 epochs is = 0.10094303331260256\n",
      "Initial Cost on Val dataset for this epoch 444 = 0.10094303331260256\n",
      "learning rate for this epoch =  0.10892417062758035\n",
      "Error on this batch = 0.056266208309508674\n",
      "Error on this batch = 0.08835210014820245\n",
      "Cost on val dataset after 445 epochs is = 0.1008892560908457\n",
      "Initial Cost on Val dataset for this epoch 445 = 0.1008892560908457\n",
      "learning rate for this epoch =  0.10886292563815944\n",
      "Error on this batch = 0.05619424629852426\n",
      "Error on this batch = 0.0882656587424141\n",
      "Cost on val dataset after 446 epochs is = 0.10083564767910137\n",
      "Initial Cost on Val dataset for this epoch 446 = 0.10083564767910137\n",
      "learning rate for this epoch =  0.10880185244406208\n",
      "Error on this batch = 0.056122517470969024\n",
      "Error on this batch = 0.08817951674110301\n",
      "Cost on val dataset after 447 epochs is = 0.10078220716051936\n",
      "Initial Cost on Val dataset for this epoch 447 = 0.10078220716051936\n",
      "learning rate for this epoch =  0.10874095017981981\n",
      "Error on this batch = 0.05605102085871783\n",
      "Error on this batch = 0.08809367279993775\n",
      "Cost on val dataset after 448 epochs is = 0.10072893362521027\n",
      "Initial Cost on Val dataset for this epoch 448 = 0.10072893362521027\n",
      "learning rate for this epoch =  0.10868021798624786\n",
      "Error on this batch = 0.055979755493784956\n",
      "Error on this batch = 0.08800812557675577\n",
      "Cost on val dataset after 449 epochs is = 0.10067582617018243\n",
      "Initial Cost on Val dataset for this epoch 449 = 0.10067582617018243\n",
      "learning rate for this epoch =  0.10861965501038574\n",
      "Error on this batch = 0.05590872040834029\n",
      "Error on this batch = 0.08792287373144418\n",
      "Cost on val dataset after 450 epochs is = 0.10062288389927943\n",
      "Initial Cost on Val dataset for this epoch 450 = 0.10062288389927943\n",
      "learning rate for this epoch =  0.10855926040543842\n",
      "Error on this batch = 0.05583791463472938\n",
      "Error on this batch = 0.08783791592582581\n",
      "Cost on val dataset after 451 epochs is = 0.10057010592311783\n",
      "Initial Cost on Val dataset for this epoch 451 = 0.10057010592311783\n",
      "learning rate for this epoch =  0.1084990333307181\n",
      "Error on this batch = 0.05576733720549738\n",
      "Error on this batch = 0.08775325082355155\n",
      "Cost on val dataset after 452 epochs is = 0.10051749135902603\n",
      "Initial Cost on Val dataset for this epoch 452 = 0.10051749135902603\n",
      "learning rate for this epoch =  0.10843897295158678\n",
      "Error on this batch = 0.05569698715341694\n",
      "Error on this batch = 0.0876688770899982\n",
      "Cost on val dataset after 453 epochs is = 0.1004650393309831\n",
      "Initial Cost on Val dataset for this epoch 453 = 0.1004650393309831\n",
      "learning rate for this epoch =  0.10837907843939941\n",
      "Error on this batch = 0.05562686351151926\n",
      "Error on this batch = 0.0875847933921722\n",
      "Cost on val dataset after 454 epochs is = 0.10041274896955833\n",
      "Initial Cost on Val dataset for this epoch 454 = 0.10041274896955833\n",
      "learning rate for this epoch =  0.10831934897144786\n",
      "Error on this batch = 0.05555696531312911\n",
      "Error on this batch = 0.08750099839861929\n",
      "Cost on val dataset after 455 epochs is = 0.10036061941185155\n",
      "Initial Cost on Val dataset for this epoch 455 = 0.10036061941185155\n",
      "learning rate for this epoch =  0.10825978373090529\n",
      "Error on this batch = 0.05548729159190252\n",
      "Error on this batch = 0.08741749077933962\n",
      "Cost on val dataset after 456 epochs is = 0.1003086498014332\n",
      "Initial Cost on Val dataset for this epoch 456 = 0.1003086498014332\n",
      "learning rate for this epoch =  0.10820038190677136\n",
      "Error on this batch = 0.05541784138186815\n",
      "Error on this batch = 0.0873342692057092\n",
      "Cost on val dataset after 457 epochs is = 0.10025683928828556\n",
      "Initial Cost on Val dataset for this epoch 457 = 0.10025683928828556\n",
      "learning rate for this epoch =  0.10814114269381797\n",
      "Error on this batch = 0.055348613717471365\n",
      "Error on this batch = 0.08725133235040668\n",
      "Cost on val dataset after 458 epochs is = 0.10020518702874412\n",
      "Initial Cost on Val dataset for this epoch 458 = 0.10020518702874412\n",
      "learning rate for this epoch =  0.10808206529253567\n",
      "Error on this batch = 0.05527960763362123\n",
      "Error on this batch = 0.08716867888734627\n",
      "Cost on val dataset after 459 epochs is = 0.10015369218543917\n",
      "Initial Cost on Val dataset for this epoch 459 = 0.10015369218543917\n",
      "learning rate for this epoch =  0.10802314890908067\n",
      "Error on this batch = 0.055210822165740046\n",
      "Error on this batch = 0.08708630749161636\n",
      "Cost on val dataset after 460 epochs is = 0.10010235392723812\n",
      "Initial Cost on Val dataset for this epoch 460 = 0.10010235392723812\n",
      "learning rate for this epoch =  0.10796439275522242\n",
      "Error on this batch = 0.055142256349815816\n",
      "Error on this batch = 0.08700421683942387\n",
      "Cost on val dataset after 461 epochs is = 0.10005117142918814\n",
      "Initial Cost on Val dataset for this epoch 461 = 0.10005117142918814\n",
      "learning rate for this epoch =  0.10790579604829184\n",
      "Error on this batch = 0.05507390922245674\n",
      "Error on this batch = 0.08692240560804473\n",
      "Cost on val dataset after 462 epochs is = 0.10000014387245887\n",
      "Initial Cost on Val dataset for this epoch 462 = 0.10000014387245887\n",
      "learning rate for this epoch =  0.10784735801113018\n",
      "Error on this batch = 0.055005779820948314\n",
      "Error on this batch = 0.08684087247577976\n",
      "Cost on val dataset after 463 epochs is = 0.099949270444286\n",
      "Initial Cost on Val dataset for this epoch 463 = 0.099949270444286\n",
      "learning rate for this epoch =  0.10778907787203826\n",
      "Error on this batch = 0.054937867183312304\n",
      "Error on this batch = 0.08675961612191672\n",
      "Cost on val dataset after 464 epochs is = 0.09989855033791487\n",
      "Initial Cost on Val dataset for this epoch 464 = 0.09989855033791487\n",
      "learning rate for this epoch =  0.1077309548647265\n",
      "Error on this batch = 0.054870170348368025\n",
      "Error on this batch = 0.08667863522669801\n",
      "Cost on val dataset after 465 epochs is = 0.09984798275254445\n",
      "Initial Cost on Val dataset for this epoch 465 = 0.09984798275254445\n",
      "learning rate for this epoch =  0.10767298822826553\n",
      "Error on this batch = 0.05480268835579543\n",
      "Error on this batch = 0.08659792847129424\n",
      "Cost on val dataset after 466 epochs is = 0.09979756689327193\n",
      "Initial Cost on Val dataset for this epoch 466 = 0.09979756689327193\n",
      "learning rate for this epoch =  0.10761517720703706\n",
      "Error on this batch = 0.054735420246199824\n",
      "Error on this batch = 0.08651749453778351\n",
      "Cost on val dataset after 467 epochs is = 0.09974730197103734\n",
      "Initial Cost on Val dataset for this epoch 467 = 0.09974730197103734\n",
      "learning rate for this epoch =  0.10755752105068565\n",
      "Error on this batch = 0.05466836506117846\n",
      "Error on this batch = 0.0864373321091365\n",
      "Cost on val dataset after 468 epochs is = 0.0996971872025688\n",
      "Initial Cost on Val dataset for this epoch 468 = 0.0996971872025688\n",
      "learning rate for this epoch =  0.1075000190140709\n",
      "Error on this batch = 0.054601521843388526\n",
      "Error on this batch = 0.0863574398692075\n",
      "Cost on val dataset after 469 epochs is = 0.09964722181032791\n",
      "Initial Cost on Val dataset for this epoch 469 = 0.09964722181032791\n",
      "learning rate for this epoch =  0.10744267035722005\n",
      "Error on this batch = 0.054534889636616575\n",
      "Error on this batch = 0.086277816502731\n",
      "Cost on val dataset after 470 epochs is = 0.09959740502245583\n",
      "Initial Cost on Val dataset for this epoch 470 = 0.09959740502245583\n",
      "learning rate for this epoch =  0.10738547434528128\n",
      "Error on this batch = 0.05446846748584933\n",
      "Error on this batch = 0.08619846069532397\n",
      "Cost on val dataset after 471 epochs is = 0.09954773607271931\n",
      "Initial Cost on Val dataset for this epoch 471 = 0.09954773607271931\n",
      "learning rate for this epoch =  0.10732843024847744\n",
      "Error on this batch = 0.05440225443734553\n",
      "Error on this batch = 0.08611937113349404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on val dataset after 472 epochs is = 0.09949821420045747\n",
      "Initial Cost on Val dataset for this epoch 472 = 0.09949821420045747\n",
      "learning rate for this epoch =  0.10727153734206031\n",
      "Error on this batch = 0.05433624953870902\n",
      "Error on this batch = 0.0860405465046533\n",
      "Cost on val dataset after 473 epochs is = 0.09944883865052873\n",
      "Initial Cost on Val dataset for this epoch 473 = 0.09944883865052873\n",
      "learning rate for this epoch =  0.1072147949062655\n",
      "Error on this batch = 0.05427045183896258\n",
      "Error on this batch = 0.08596198549713761\n",
      "Cost on val dataset after 474 epochs is = 0.09939960867325832\n",
      "Initial Cost on Val dataset for this epoch 474 = 0.09939960867325832\n",
      "learning rate for this epoch =  0.10715820222626746\n",
      "Error on this batch = 0.05420486038862296\n",
      "Error on this batch = 0.08588368680023177\n",
      "Cost on val dataset after 475 epochs is = 0.099350523524386\n",
      "Initial Cost on Val dataset for this epoch 475 = 0.099350523524386\n",
      "learning rate for this epoch =  0.1071017585921356\n",
      "Error on this batch = 0.05413947423977631\n",
      "Error on this batch = 0.08580564910419995\n",
      "Cost on val dataset after 476 epochs is = 0.0993015824650145\n",
      "Initial Cost on Val dataset for this epoch 476 = 0.0993015824650145\n",
      "learning rate for this epoch =  0.1070454632987902\n",
      "Error on this batch = 0.05407429244615445\n",
      "Error on this batch = 0.08572787110032191\n",
      "Cost on val dataset after 477 epochs is = 0.09925278476155798\n",
      "Initial Cost on Val dataset for this epoch 477 = 0.09925278476155798\n",
      "learning rate for this epoch =  0.10698931564595948\n",
      "Error on this batch = 0.05400931406321172\n",
      "Error on this batch = 0.08565035148093467\n",
      "Cost on val dataset after 478 epochs is = 0.09920412968569144\n",
      "Initial Cost on Val dataset for this epoch 478 = 0.09920412968569144\n",
      "learning rate for this epoch =  0.1069333149381366\n",
      "Error on this batch = 0.05394453814820214\n",
      "Error on this batch = 0.08557308893947922\n",
      "Cost on val dataset after 479 epochs is = 0.09915561651430015\n",
      "Initial Cost on Val dataset for this epoch 479 = 0.09915561651430015\n",
      "learning rate for this epoch =  0.10687746048453738\n",
      "Error on this batch = 0.05387996376025713\n",
      "Error on this batch = 0.08549608217055323\n",
      "Cost on val dataset after 480 epochs is = 0.09910724452942989\n",
      "Initial Cost on Val dataset for this epoch 480 = 0.09910724452942989\n",
      "learning rate for this epoch =  0.10682175159905852\n",
      "Error on this batch = 0.05381558996046337\n",
      "Error on this batch = 0.08541932986996843\n",
      "Cost on val dataset after 481 epochs is = 0.09905901301823754\n",
      "Initial Cost on Val dataset for this epoch 481 = 0.09905901301823754\n",
      "learning rate for this epoch =  0.1067661876002362\n",
      "Error on this batch = 0.05375141581194089\n",
      "Error on this batch = 0.08534283073481348\n",
      "Cost on val dataset after 482 epochs is = 0.09901092127294227\n",
      "Initial Cost on Val dataset for this epoch 482 = 0.09901092127294227\n",
      "learning rate for this epoch =  0.1067107678112051\n",
      "Error on this batch = 0.05368744037992123\n",
      "Error on this batch = 0.08526658346352207\n",
      "Cost on val dataset after 483 epochs is = 0.09896296859077713\n",
      "Initial Cost on Val dataset for this epoch 483 = 0.09896296859077713\n",
      "learning rate for this epoch =  0.10665549155965787\n",
      "Error on this batch = 0.053623662731825644\n",
      "Error on this batch = 0.08519058675594574\n",
      "Cost on val dataset after 484 epochs is = 0.09891515427394167\n",
      "Initial Cost on Val dataset for this epoch 484 = 0.09891515427394167\n",
      "learning rate for this epoch =  0.10660035817780521\n",
      "Error on this batch = 0.05356008193734324\n",
      "Error on this batch = 0.08511483931343189\n",
      "Cost on val dataset after 485 epochs is = 0.09886747762955439\n",
      "Initial Cost on Val dataset for this epoch 485 = 0.09886747762955439\n",
      "learning rate for this epoch =  0.10654536700233612\n",
      "Error on this batch = 0.05349669706850893\n",
      "Error on this batch = 0.08503933983890667\n",
      "Cost on val dataset after 486 epochs is = 0.09881993796960659\n",
      "Initial Cost on Val dataset for this epoch 486 = 0.09881993796960659\n",
      "learning rate for this epoch =  0.10649051737437876\n",
      "Error on this batch = 0.053433507199781245\n",
      "Error on this batch = 0.08496408703696215\n",
      "Cost on val dataset after 487 epochs is = 0.09877253461091628\n",
      "Initial Cost on Val dataset for this epoch 487 = 0.09877253461091628\n",
      "learning rate for this epoch =  0.10643580863946162\n",
      "Error on this batch = 0.05337051140811973\n",
      "Error on this batch = 0.08488907961394869\n",
      "Cost on val dataset after 488 epochs is = 0.09872526687508336\n",
      "Initial Cost on Val dataset for this epoch 488 = 0.09872526687508336\n",
      "learning rate for this epoch =  0.10638124014747533\n",
      "Error on this batch = 0.05330770877306206\n",
      "Error on this batch = 0.08481431627807146\n",
      "Cost on val dataset after 489 epochs is = 0.09867813408844466\n",
      "Initial Cost on Val dataset for this epoch 489 = 0.09867813408844466\n",
      "learning rate for this epoch =  0.1063268112526345\n",
      "Error on this batch = 0.0532450983768006\n",
      "Error on this batch = 0.0847397957394913\n",
      "Cost on val dataset after 490 epochs is = 0.09863113558203059\n",
      "Initial Cost on Val dataset for this epoch 490 = 0.09863113558203059\n",
      "learning rate for this epoch =  0.10627252131344038\n",
      "Error on this batch = 0.05318267930425869\n",
      "Error on this batch = 0.08466551671042986\n",
      "Cost on val dataset after 491 epochs is = 0.09858427069152183\n",
      "Initial Cost on Val dataset for this epoch 491 = 0.09858427069152183\n",
      "learning rate for this epoch =  0.10621836969264359\n",
      "Error on this batch = 0.053120450643166034\n",
      "Error on this batch = 0.08459147790527888\n",
      "Cost on val dataset after 492 epochs is = 0.09853753875720728\n",
      "Initial Cost on Val dataset for this epoch 492 = 0.09853753875720728\n",
      "learning rate for this epoch =  0.10616435575720744\n",
      "Error on this batch = 0.05305841148413361\n",
      "Error on this batch = 0.0845176780407132\n",
      "Cost on val dataset after 493 epochs is = 0.09849093912394255\n",
      "Initial Cost on Val dataset for this epoch 493 = 0.09849093912394255\n",
      "learning rate for this epoch =  0.1061104788782716\n",
      "Error on this batch = 0.05299656092072788\n",
      "Error on this batch = 0.08444411583580749\n",
      "Cost on val dataset after 494 epochs is = 0.09844447114110925\n",
      "Initial Cost on Val dataset for this epoch 494 = 0.09844447114110925\n",
      "learning rate for this epoch =  0.10605673843111615\n",
      "Error on this batch = 0.05293489804954437\n",
      "Error on this batch = 0.0843707900121568\n",
      "Cost on val dataset after 495 epochs is = 0.0983981341625753\n",
      "Initial Cost on Val dataset for this epoch 495 = 0.0983981341625753\n",
      "learning rate for this epoch =  0.10600313379512592\n",
      "Error on this batch = 0.05287342197027991\n",
      "Error on this batch = 0.0842976992940003\n",
      "Cost on val dataset after 496 epochs is = 0.09835192754665596\n",
      "Initial Cost on Val dataset for this epoch 496 = 0.09835192754665596\n",
      "learning rate for this epoch =  0.10594966435375541\n",
      "Error on this batch = 0.0528121317858047\n",
      "Error on this batch = 0.08422484240834799\n",
      "Cost on val dataset after 497 epochs is = 0.09830585065607582\n",
      "Initial Cost on Val dataset for this epoch 497 = 0.09830585065607582\n",
      "learning rate for this epoch =  0.10589632949449389\n",
      "Error on this batch = 0.05275102660223288\n",
      "Error on this batch = 0.08415221808511093\n",
      "Cost on val dataset after 498 epochs is = 0.09825990285793168\n",
      "Initial Cost on Val dataset for this epoch 498 = 0.09825990285793168\n",
      "learning rate for this epoch =  0.10584312860883092\n",
      "Error on this batch = 0.05269010552899232\n",
      "Error on this batch = 0.08407982505723391\n",
      "Cost on val dataset after 499 epochs is = 0.09821408352365632\n",
      "Initial Cost on Val dataset for this epoch 499 = 0.09821408352365632\n",
      "learning rate for this epoch =  0.10579006109222232\n",
      "Error on this batch = 0.052629367678893486\n",
      "Error on this batch = 0.08400766206083045\n",
      "Cost on val dataset after 500 epochs is = 0.09816839202898338\n",
      "Initial Cost on Val dataset for this epoch 500 = 0.09816839202898338\n",
      "learning rate for this epoch =  0.10573712634405641\n",
      "Error on this batch = 0.05256881216819686\n",
      "Error on this batch = 0.08393572783532084\n",
      "Cost on val dataset after 501 epochs is = 0.09812282775391311\n",
      "Initial Cost on Val dataset for this epoch 501 = 0.09812282775391311\n",
      "learning rate for this epoch =  0.1056843237676206\n",
      "Error on this batch = 0.05250843811667967\n",
      "Error on this batch = 0.08386402112357175\n",
      "Cost on val dataset after 502 epochs is = 0.09807739008267904\n",
      "Initial Cost on Val dataset for this epoch 502 = 0.09807739008267904\n",
      "learning rate for this epoch =  0.10563165277006838\n",
      "Error on this batch = 0.052448244647701085\n",
      "Error on this batch = 0.08379254067203797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on val dataset after 503 epochs is = 0.09803207840371585\n",
      "Initial Cost on Val dataset for this epoch 503 = 0.09803207840371585\n",
      "learning rate for this epoch =  0.10557911276238661\n",
      "Error on this batch = 0.05238823088826632\n",
      "Error on this batch = 0.08372128523090547\n",
      "Cost on val dataset after 504 epochs is = 0.09798689210962809\n",
      "Initial Cost on Val dataset for this epoch 504 = 0.09798689210962809\n",
      "learning rate for this epoch =  0.10552670315936317\n",
      "Error on this batch = 0.0523283959690895\n",
      "Error on this batch = 0.08365025355423658\n",
      "Cost on val dataset after 505 epochs is = 0.09794183059716018\n",
      "Initial Cost on Val dataset for this epoch 505 = 0.09794183059716018\n",
      "learning rate for this epoch =  0.105474423379555\n",
      "Error on this batch = 0.05226873902465508\n",
      "Error on this batch = 0.08357944440011565\n",
      "Cost on val dataset after 506 epochs is = 0.0978968932671671\n",
      "Initial Cost on Val dataset for this epoch 506 = 0.0978968932671671\n",
      "learning rate for this epoch =  0.10542227284525636\n",
      "Error on this batch = 0.052209259193278096\n",
      "Error on this batch = 0.0835088565307958\n",
      "Cost on val dataset after 507 epochs is = 0.09785207952458669\n",
      "Initial Cost on Val dataset for this epoch 507 = 0.09785207952458669\n",
      "learning rate for this epoch =  0.10537025098246748\n",
      "Error on this batch = 0.052149955617162846\n",
      "Error on this batch = 0.08343848871284658\n",
      "Cost on val dataset after 508 epochs is = 0.09780738877841244\n",
      "Initial Cost on Val dataset for this epoch 508 = 0.09780738877841244\n",
      "learning rate for this epoch =  0.10531835722086355\n",
      "Error on this batch = 0.05209082744246038\n",
      "Error on this batch = 0.08336833971730172\n",
      "Cost on val dataset after 509 epochs is = 0.09776282044166786\n",
      "Initial Cost on Val dataset for this epoch 509 = 0.09776282044166786\n",
      "learning rate for this epoch =  0.10526659099376405\n",
      "Error on this batch = 0.05203187381932416\n",
      "Error on this batch = 0.08329840831980713\n",
      "Cost on val dataset after 510 epochs is = 0.09771837393138158\n",
      "Initial Cost on Val dataset for this epoch 510 = 0.09771837393138158\n",
      "learning rate for this epoch =  0.10521495173810227\n",
      "Error on this batch = 0.051973093901964716\n",
      "Error on this batch = 0.08322869330076861\n",
      "Cost on val dataset after 511 epochs is = 0.09767404866856393\n",
      "Initial Cost on Val dataset for this epoch 511 = 0.09767404866856393\n",
      "learning rate for this epoch =  0.10516343889439533\n",
      "Error on this batch = 0.05191448684870232\n",
      "Error on this batch = 0.08315919344549944\n",
      "Cost on val dataset after 512 epochs is = 0.0976298440781843\n",
      "Initial Cost on Val dataset for this epoch 512 = 0.0976298440781843\n",
      "learning rate for this epoch =  0.10511205190671433\n",
      "Error on this batch = 0.051856051822018454\n",
      "Error on this batch = 0.08308990754436697\n",
      "Cost on val dataset after 513 epochs is = 0.0975857595891497\n",
      "Initial Cost on Val dataset for this epoch 513 = 0.0975857595891497\n",
      "learning rate for this epoch =  0.10506079022265488\n",
      "Error on this batch = 0.0517977879886055\n",
      "Error on this batch = 0.08302083439293842\n",
      "Cost on val dataset after 514 epochs is = 0.0975417946342846\n",
      "Initial Cost on Val dataset for this epoch 514 = 0.0975417946342846\n",
      "learning rate for this epoch =  0.10500965329330811\n",
      "Error on this batch = 0.05173969451941504\n",
      "Error on this batch = 0.08295197279212527\n",
      "Cost on val dataset after 515 epochs is = 0.09749794865031165\n",
      "Initial Cost on Val dataset for this epoch 515 = 0.09749794865031165\n",
      "learning rate for this epoch =  0.10495864057323148\n",
      "Error on this batch = 0.05168177058970417\n",
      "Error on this batch = 0.08288332154832645\n",
      "Cost on val dataset after 516 epochs is = 0.09745422107783365\n",
      "Initial Cost on Val dataset for this epoch 516 = 0.09745422107783365\n",
      "learning rate for this epoch =  0.10490775152042053\n",
      "Error on this batch = 0.0516240153790806\n",
      "Error on this batch = 0.0828148794735694\n",
      "Cost on val dataset after 517 epochs is = 0.09741061136131642\n",
      "Initial Cost on Val dataset for this epoch 517 = 0.09741061136131642\n",
      "learning rate for this epoch =  0.10485698559628044\n",
      "Error on this batch = 0.051566428071545836\n",
      "Error on this batch = 0.08274664538564952\n",
      "Cost on val dataset after 518 epochs is = 0.09736711894907317\n",
      "Initial Cost on Val dataset for this epoch 518 = 0.09736711894907317\n",
      "learning rate for this epoch =  0.10480634226559798\n",
      "Error on this batch = 0.05150900785553669\n",
      "Error on this batch = 0.08267861810826668\n",
      "Cost on val dataset after 519 epochs is = 0.09732374329324939\n",
      "Initial Cost on Val dataset for this epoch 519 = 0.09732374329324939\n",
      "learning rate for this epoch =  0.10475582099651397\n",
      "Error on this batch = 0.05145175392396519\n",
      "Error on this batch = 0.08261079647115988\n",
      "Cost on val dataset after 520 epochs is = 0.09728048384980918\n",
      "Initial Cost on Val dataset for this epoch 520 = 0.09728048384980918\n",
      "learning rate for this epoch =  0.10470542126049572\n",
      "Error on this batch = 0.05139466547425669\n",
      "Error on this batch = 0.08254317931023857\n",
      "Cost on val dataset after 521 epochs is = 0.09723734007852258\n",
      "Initial Cost on Val dataset for this epoch 521 = 0.09723734007852258\n",
      "learning rate for this epoch =  0.10465514253230984\n",
      "Error on this batch = 0.051337741708386356\n",
      "Error on this batch = 0.08247576546771117\n",
      "Cost on val dataset after 522 epochs is = 0.09719431144295386\n",
      "Initial Cost on Val dataset for this epoch 522 = 0.09719431144295386\n",
      "learning rate for this epoch =  0.10460498428999554\n",
      "Error on this batch = 0.05128098183291373\n",
      "Error on this batch = 0.08240855379221017\n",
      "Cost on val dataset after 523 epochs is = 0.09715139741045062\n",
      "Initial Cost on Val dataset for this epoch 523 = 0.09715139741045062\n",
      "learning rate for this epoch =  0.10455494601483782\n",
      "Error on this batch = 0.05122438505901584\n",
      "Error on this batch = 0.08234154313891383\n",
      "Cost on val dataset after 524 epochs is = 0.09710859745213439\n",
      "Initial Cost on Val dataset for this epoch 524 = 0.09710859745213439\n",
      "learning rate for this epoch =  0.10450502719134125\n",
      "Error on this batch = 0.051167950602518206\n",
      "Error on this batch = 0.0822747323696639\n",
      "Cost on val dataset after 525 epochs is = 0.09706591104289168\n",
      "Initial Cost on Val dataset for this epoch 525 = 0.09706591104289168\n",
      "learning rate for this epoch =  0.10445522730720382\n",
      "Error on this batch = 0.05111167768392437\n",
      "Error on this batch = 0.08220812035307958\n",
      "Cost on val dataset after 526 epochs is = 0.0970233376613661\n",
      "Initial Cost on Val dataset for this epoch 526 = 0.0970233376613661\n",
      "learning rate for this epoch =  0.10440554585329116\n",
      "Error on this batch = 0.05105556552844359\n",
      "Error on this batch = 0.08214170596466716\n",
      "Cost on val dataset after 527 epochs is = 0.09698087678995168\n",
      "Initial Cost on Val dataset for this epoch 527 = 0.09698087678995168\n",
      "learning rate for this epoch =  0.10435598232361096\n",
      "Error on this batch = 0.05099961336601657\n",
      "Error on this batch = 0.08207548808692522\n",
      "Cost on val dataset after 528 epochs is = 0.09693852791478659\n",
      "Initial Cost on Val dataset for this epoch 528 = 0.09693852791478659\n",
      "learning rate for this epoch =  0.10430653621528765\n",
      "Error on this batch = 0.050943820431339826\n",
      "Error on this batch = 0.08200946560944558\n",
      "Cost on val dataset after 529 epochs is = 0.09689629052574802\n",
      "Initial Cost on Val dataset for this epoch 529 = 0.09689629052574802\n",
      "learning rate for this epoch =  0.10425720702853739\n",
      "Error on this batch = 0.050888185963887785\n",
      "Error on this batch = 0.08194363742900912\n",
      "Cost on val dataset after 530 epochs is = 0.09685416411644786\n",
      "Initial Cost on Val dataset for this epoch 530 = 0.09685416411644786\n",
      "learning rate for this epoch =  0.10420799426664315\n",
      "Error on this batch = 0.05083270920793364\n",
      "Error on this batch = 0.08187800244967679\n",
      "Cost on val dataset after 531 epochs is = 0.09681214818422892\n",
      "Initial Cost on Val dataset for this epoch 531 = 0.09681214818422892\n",
      "learning rate for this epoch =  0.10415889743593033\n",
      "Error on this batch = 0.05077738941256795\n",
      "Error on this batch = 0.08181255958287591\n",
      "Cost on val dataset after 532 epochs is = 0.09677024223016219\n",
      "Initial Cost on Val dataset for this epoch 532 = 0.09677024223016219\n",
      "learning rate for this epoch =  0.10410991604574225\n",
      "Error on this batch = 0.05072222583171592\n",
      "Error on this batch = 0.08174730774748074\n",
      "Cost on val dataset after 533 epochs is = 0.09672844575904453\n",
      "Initial Cost on Val dataset for this epoch 533 = 0.09672844575904453\n",
      "learning rate for this epoch =  0.10406104960841617\n",
      "Error on this batch = 0.050667217724152634\n",
      "Error on this batch = 0.08168224586988825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on val dataset after 534 epochs is = 0.09668675827939725\n",
      "Initial Cost on Val dataset for this epoch 534 = 0.09668675827939725\n",
      "learning rate for this epoch =  0.1040122976392594\n",
      "Error on this batch = 0.05061236435351666\n",
      "Error on this batch = 0.08161737288408795\n",
      "Cost on val dataset after 535 epochs is = 0.09664517930346496\n",
      "Initial Cost on Val dataset for this epoch 535 = 0.09664517930346496\n",
      "learning rate for this epoch =  0.10396365965652576\n",
      "Error on this batch = 0.050557664988322044\n",
      "Error on this batch = 0.08155268773172686\n",
      "Cost on val dataset after 536 epochs is = 0.09660370834721553\n",
      "Initial Cost on Val dataset for this epoch 536 = 0.09660370834721553\n",
      "learning rate for this epoch =  0.10391513518139214\n",
      "Error on this batch = 0.0505031189019683\n",
      "Error on this batch = 0.08148818936216821\n",
      "Cost on val dataset after 537 epochs is = 0.09656234493033995\n",
      "Initial Cost on Val dataset for this epoch 537 = 0.09656234493033995\n",
      "learning rate for this epoch =  0.10386672373793533\n",
      "Error on this batch = 0.050448725372748984\n",
      "Error on this batch = 0.08142387673254495\n",
      "Cost on val dataset after 538 epochs is = 0.09652108857625319\n",
      "Initial Cost on Val dataset for this epoch 538 = 0.09652108857625319\n",
      "learning rate for this epoch =  0.10381842485310916\n",
      "Error on this batch = 0.050394483683858536\n",
      "Error on this batch = 0.08135974880780741\n",
      "Cost on val dataset after 539 epochs is = 0.09647993881209514\n",
      "Initial Cost on Val dataset for this epoch 539 = 0.09647993881209514\n",
      "learning rate for this epoch =  0.10377023805672174\n",
      "Error on this batch = 0.0503403931233973\n",
      "Error on this batch = 0.0812958045607652\n",
      "Cost on val dataset after 540 epochs is = 0.09643889516873211\n",
      "Initial Cost on Val dataset for this epoch 540 = 0.09643889516873211\n",
      "learning rate for this epoch =  0.10372216288141306\n",
      "Error on this batch = 0.050286452984375014\n",
      "Error on this batch = 0.08123204297212326\n",
      "Cost on val dataset after 541 epochs is = 0.09639795718075861\n",
      "Initial Cost on Val dataset for this epoch 541 = 0.09639795718075861\n",
      "learning rate for this epoch =  0.10367419886263263\n",
      "Error on this batch = 0.05023266256471271\n",
      "Error on this batch = 0.08116846303051248\n",
      "Cost on val dataset after 542 epochs is = 0.09635712438649953\n",
      "Initial Cost on Val dataset for this epoch 542 = 0.09635712438649953\n",
      "learning rate for this epoch =  0.10362634553861746\n",
      "Error on this batch = 0.05017902116724292\n",
      "Error on this batch = 0.08110506373251405\n",
      "Cost on val dataset after 543 epochs is = 0.09631639632801221\n",
      "Initial Cost on Val dataset for this epoch 543 = 0.09631639632801221\n",
      "learning rate for this epoch =  0.10357860245037034\n",
      "Error on this batch = 0.05012552809970833\n",
      "Error on this batch = 0.08104184408267863\n",
      "Cost on val dataset after 544 epochs is = 0.09627577255108925\n",
      "Initial Cost on Val dataset for this epoch 544 = 0.09627577255108925\n",
      "learning rate for this epoch =  0.10353096914163801\n",
      "Error on this batch = 0.05007218267475889\n",
      "Error on this batch = 0.0809788030935394\n",
      "Cost on val dataset after 545 epochs is = 0.09623525260526089\n",
      "Initial Cost on Val dataset for this epoch 545 = 0.09623525260526089\n",
      "learning rate for this epoch =  0.10348344515888994\n",
      "Error on this batch = 0.05001898420994742\n",
      "Error on this batch = 0.08091593978561971\n",
      "Cost on val dataset after 546 epochs is = 0.09619483604379783\n",
      "Initial Cost on Val dataset for this epoch 546 = 0.09619483604379783\n",
      "learning rate for this epoch =  0.10343603005129703\n",
      "Error on this batch = 0.049965932027723564\n",
      "Error on this batch = 0.0808532531874349\n",
      "Cost on val dataset after 547 epochs is = 0.09615452242371393\n",
      "Initial Cost on Val dataset for this epoch 547 = 0.09615452242371393\n",
      "learning rate for this epoch =  0.1033887233707106\n",
      "Error on this batch = 0.04991302545542643\n",
      "Error on this batch = 0.08079074233548912\n",
      "Cost on val dataset after 548 epochs is = 0.096114311305769\n",
      "Initial Cost on Val dataset for this epoch 548 = 0.096114311305769\n",
      "learning rate for this epoch =  0.10334152467164161\n",
      "Error on this batch = 0.04986026382527569\n",
      "Error on this batch = 0.0807284062742661\n",
      "Cost on val dataset after 549 epochs is = 0.09607420225447134\n",
      "Initial Cost on Val dataset for this epoch 549 = 0.09607420225447134\n",
      "learning rate for this epoch =  0.10329443351124007\n",
      "Error on this batch = 0.04980764647436127\n",
      "Error on this batch = 0.08066624405621507\n",
      "Cost on val dataset after 550 epochs is = 0.09603419483808039\n",
      "Initial Cost on Val dataset for this epoch 550 = 0.09603419483808039\n",
      "learning rate for this epoch =  0.10324744944927464\n",
      "Error on this batch = 0.049755172744631614\n",
      "Error on this batch = 0.08060425474173144\n",
      "Cost on val dataset after 551 epochs is = 0.09599428862860887\n",
      "Initial Cost on Val dataset for this epoch 551 = 0.09599428862860887\n",
      "learning rate for this epoch =  0.10320057204811232\n",
      "Error on this batch = 0.04970284198288065\n",
      "Error on this batch = 0.08054243739913224\n",
      "Cost on val dataset after 552 epochs is = 0.09595448320182512\n",
      "Initial Cost on Val dataset for this epoch 552 = 0.09595448320182512\n",
      "learning rate for this epoch =  0.10315380087269863\n",
      "Error on this batch = 0.049650653540733336\n",
      "Error on this batch = 0.08048079110462669\n",
      "Cost on val dataset after 553 epochs is = 0.09591477813725477\n",
      "Initial Cost on Val dataset for this epoch 553 = 0.09591477813725477\n",
      "learning rate for this epoch =  0.10310713549053749\n",
      "Error on this batch = 0.04959860677462991\n",
      "Error on this batch = 0.0804193149422823\n",
      "Cost on val dataset after 554 epochs is = 0.09587517301818226\n",
      "Initial Cost on Val dataset for this epoch 554 = 0.09587517301818226\n",
      "learning rate for this epoch =  0.10306057547167191\n",
      "Error on this batch = 0.049546701045809056\n",
      "Error on this batch = 0.08035800800398576\n",
      "Cost on val dataset after 555 epochs is = 0.09583566743165205\n",
      "Initial Cost on Val dataset for this epoch 555 = 0.09583566743165205\n",
      "learning rate for this epoch =  0.1030141203886643\n",
      "Error on this batch = 0.04949493572028945\n",
      "Error on this batch = 0.08029686938940002\n",
      "Cost on val dataset after 556 epochs is = 0.09579626096846913\n",
      "Initial Cost on Val dataset for this epoch 556 = 0.09579626096846913\n",
      "learning rate for this epoch =  0.10296776981657725\n",
      "Error on this batch = 0.0494433101688504\n",
      "Error on this batch = 0.08023589820591658\n",
      "Cost on val dataset after 557 epochs is = 0.09575695322319941\n",
      "Initial Cost on Val dataset for this epoch 557 = 0.09575695322319941\n",
      "learning rate for this epoch =  0.10292152333295448\n",
      "Error on this batch = 0.049391823767011386\n",
      "Error on this batch = 0.08017509356860397\n",
      "Cost on val dataset after 558 epochs is = 0.09571774379416921\n",
      "Initial Cost on Val dataset for this epoch 558 = 0.09571774379416921\n",
      "learning rate for this epoch =  0.10287538051780193\n",
      "Error on this batch = 0.04934047589500998\n",
      "Error on this batch = 0.08011445460015221\n",
      "Cost on val dataset after 559 epochs is = 0.09567863228346447\n",
      "Initial Cost on Val dataset for this epoch 559 = 0.09567863228346447\n",
      "learning rate for this epoch =  0.10282934095356899\n",
      "Error on this batch = 0.049289265937779304\n",
      "Error on this batch = 0.08005398043081352\n",
      "Cost on val dataset after 560 epochs is = 0.09563961829692919\n",
      "Initial Cost on Val dataset for this epoch 560 = 0.09563961829692919\n",
      "learning rate for this epoch =  0.10278340422512992\n",
      "Error on this batch = 0.04923819328492388\n",
      "Error on this batch = 0.07999367019833957\n",
      "Cost on val dataset after 561 epochs is = 0.09560070144416316\n",
      "Initial Cost on Val dataset for this epoch 561 = 0.09560070144416316\n",
      "learning rate for this epoch =  0.10273756991976561\n",
      "Error on this batch = 0.04918725733069474\n",
      "Error on this batch = 0.07993352304791534\n",
      "Cost on val dataset after 562 epochs is = 0.0955618813385191\n",
      "Initial Cost on Val dataset for this epoch 562 = 0.0955618813385191\n",
      "learning rate for this epoch =  0.10269183762714515\n",
      "Error on this batch = 0.04913645747396338\n",
      "Error on this batch = 0.0798735381320898\n",
      "Cost on val dataset after 563 epochs is = 0.09552315759709906\n",
      "Initial Cost on Val dataset for this epoch 563 = 0.09552315759709906\n",
      "learning rate for this epoch =  0.10264620693930798\n",
      "Error on this batch = 0.04908579311819491\n",
      "Error on this batch = 0.0798137146107038\n",
      "Cost on val dataset after 564 epochs is = 0.09548452984074975\n",
      "Initial Cost on Val dataset for this epoch 564 = 0.09548452984074975\n",
      "learning rate for this epoch =  0.10260067745064594\n",
      "Error on this batch = 0.04903526367142003\n",
      "Error on this batch = 0.0797540516508151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on val dataset after 565 epochs is = 0.09544599769405729\n",
      "Initial Cost on Val dataset for this epoch 565 = 0.09544599769405729\n",
      "learning rate for this epoch =  0.10255524875788552\n",
      "Error on this batch = 0.0489848685462063\n",
      "Error on this batch = 0.079694548426621\n",
      "Cost on val dataset after 566 epochs is = 0.09540756078534124\n",
      "Initial Cost on Val dataset for this epoch 566 = 0.09540756078534124\n",
      "learning rate for this epoch =  0.10250992046007043\n",
      "Error on this batch = 0.04893460715962851\n",
      "Error on this batch = 0.07963520411937848\n",
      "Cost on val dataset after 567 epochs is = 0.09536921874664708\n",
      "Initial Cost on Val dataset for this epoch 567 = 0.09536921874664708\n",
      "learning rate for this epoch =  0.10246469215854406\n",
      "Error on this batch = 0.04888447893323813\n",
      "Error on this batch = 0.07957601791732241\n",
      "Cost on val dataset after 568 epochs is = 0.09533097121373846\n",
      "Initial Cost on Val dataset for this epoch 568 = 0.09533097121373846\n",
      "learning rate for this epoch =  0.10241956345693246\n",
      "Error on this batch = 0.04883448329303193\n",
      "Error on this batch = 0.07951698901558187\n",
      "Cost on val dataset after 569 epochs is = 0.09529281782608795\n",
      "Initial Cost on Val dataset for this epoch 569 = 0.09529281782608795\n",
      "learning rate for this epoch =  0.1023745339611271\n",
      "Error on this batch = 0.04878461966942016\n",
      "Error on this batch = 0.07945811661609452\n",
      "Cost on val dataset after 570 epochs is = 0.09525475822686712\n",
      "Initial Cost on Val dataset for this epoch 570 = 0.09525475822686712\n",
      "learning rate for this epoch =  0.10232960327926806\n",
      "Error on this batch = 0.04873488749719351\n",
      "Error on this batch = 0.07939939992751978\n",
      "Cost on val dataset after 571 epochs is = 0.09521679206293521\n",
      "Initial Cost on Val dataset for this epoch 571 = 0.09521679206293521\n",
      "learning rate for this epoch =  0.10228477102172728\n",
      "Error on this batch = 0.048685286215489704\n",
      "Error on this batch = 0.07934083816515057\n",
      "Cost on val dataset after 572 epochs is = 0.09517891898482707\n",
      "Initial Cost on Val dataset for this epoch 572 = 0.09517891898482707\n",
      "learning rate for this epoch =  0.10224003680109194\n",
      "Error on this batch = 0.048635815267759314\n",
      "Error on this batch = 0.07928243055082379\n",
      "Cost on val dataset after 573 epochs is = 0.09514113864673968\n",
      "Initial Cost on Val dataset for this epoch 573 = 0.09514113864673968\n",
      "learning rate for this epoch =  0.10219540023214801\n",
      "Error on this batch = 0.04858647410173099\n",
      "Error on this batch = 0.07922417631283027\n",
      "Cost on val dataset after 574 epochs is = 0.09510345070651784\n",
      "Initial Cost on Val dataset for this epoch 574 = 0.09510345070651784\n",
      "learning rate for this epoch =  0.10215086093186404\n",
      "Error on this batch = 0.04853726216937606\n",
      "Error on this batch = 0.07916607468582347\n",
      "Cost on val dataset after 575 epochs is = 0.09506585482563831\n",
      "Initial Cost on Val dataset for this epoch 575 = 0.09506585482563831\n",
      "learning rate for this epoch =  0.10210641851937487\n",
      "Error on this batch = 0.04848817892687246\n",
      "Error on this batch = 0.0791081249107281\n",
      "Cost on val dataset after 576 epochs is = 0.09502835066919328\n",
      "Initial Cost on Val dataset for this epoch 576 = 0.09502835066919328\n",
      "learning rate for this epoch =  0.10206207261596577\n",
      "Error on this batch = 0.048439223834568355\n",
      "Error on this batch = 0.07905032623464811\n",
      "Cost on val dataset after 577 epochs is = 0.09499093790587206\n",
      "Initial Cost on Val dataset for this epoch 577 = 0.09499093790587206\n",
      "learning rate for this epoch =  0.10201782284505649\n",
      "Error on this batch = 0.048390396356945045\n",
      "Error on this batch = 0.07899267791077437\n",
      "Cost on val dataset after 578 epochs is = 0.09495361620794196\n",
      "Initial Cost on Val dataset for this epoch 578 = 0.09495361620794196\n",
      "learning rate for this epoch =  0.10197366883218573\n",
      "Error on this batch = 0.04834169596257949\n",
      "Error on this batch = 0.07893517919829261\n",
      "Cost on val dataset after 579 epochs is = 0.09491638525122789\n",
      "Initial Cost on Val dataset for this epoch 579 = 0.09491638525122789\n",
      "learning rate for this epoch =  0.1019296102049953\n",
      "Error on this batch = 0.04829312212410634\n",
      "Error on this batch = 0.07887782936229108\n",
      "Cost on val dataset after 580 epochs is = 0.09487924471509043\n",
      "Initial Cost on Val dataset for this epoch 580 = 0.09487924471509043\n",
      "learning rate for this epoch =  0.10188564659321496\n",
      "Error on this batch = 0.048244674318179616\n",
      "Error on this batch = 0.07882062767366886\n",
      "Cost on val dataset after 581 epochs is = 0.09484219428240297\n",
      "Initial Cost on Val dataset for this epoch 581 = 0.09484219428240297\n",
      "learning rate for this epoch =  0.10184177762864698\n",
      "Error on this batch = 0.048196352025433986\n",
      "Error on this batch = 0.0787635734090442\n",
      "Cost on val dataset after 582 epochs is = 0.0948052336395274\n",
      "Initial Cost on Val dataset for this epoch 582 = 0.0948052336395274\n",
      "learning rate for this epoch =  0.10179800294515103\n",
      "Error on this batch = 0.04814815473044567\n",
      "Error on this batch = 0.07870666585066355\n",
      "Cost on val dataset after 583 epochs is = 0.09476836247628857\n",
      "Initial Cost on Val dataset for this epoch 583 = 0.09476836247628857\n",
      "learning rate for this epoch =  0.10175432217862923\n",
      "Error on this batch = 0.04810008192169292\n",
      "Error on this batch = 0.07864990428631131\n",
      "Cost on val dataset after 584 epochs is = 0.09473158048594742\n",
      "Initial Cost on Val dataset for this epoch 584 = 0.09473158048594742\n",
      "learning rate for this epoch =  0.10171073496701123\n",
      "Error on this batch = 0.048052133091516444\n",
      "Error on this batch = 0.07859328800922018\n",
      "Cost on val dataset after 585 epochs is = 0.0946948873651729\n",
      "Initial Cost on Val dataset for this epoch 585 = 0.0946948873651729\n",
      "learning rate for this epoch =  0.10166724095023941\n",
      "Error on this batch = 0.04800430773607939\n",
      "Error on this batch = 0.07853681631798251\n",
      "Cost on val dataset after 586 epochs is = 0.0946582828140126\n",
      "Initial Cost on Val dataset for this epoch 586 = 0.0946582828140126\n",
      "learning rate for this epoch =  0.10162383977025442\n",
      "Error on this batch = 0.047956605355327094\n",
      "Error on this batch = 0.07848048851646262\n",
      "Cost on val dataset after 587 epochs is = 0.09462176653586193\n",
      "Initial Cost on Val dataset for this epoch 587 = 0.09462176653586193\n",
      "learning rate for this epoch =  0.10158053107098057\n",
      "Error on this batch = 0.047909025452946584\n",
      "Error on this batch = 0.07842430391371016\n",
      "Cost on val dataset after 588 epochs is = 0.09458533823743237\n",
      "Initial Cost on Val dataset for this epoch 588 = 0.09458533823743237\n",
      "learning rate for this epoch =  0.10153731449831156\n",
      "Error on this batch = 0.04786156753632617\n",
      "Error on this batch = 0.07836826182387464\n",
      "Cost on val dataset after 589 epochs is = 0.09454899762871809\n",
      "Initial Cost on Val dataset for this epoch 589 = 0.09454899762871809\n",
      "learning rate for this epoch =  0.1014941897000962\n",
      "Error on this batch = 0.047814231116514366\n",
      "Error on this batch = 0.0783123615661213\n",
      "Cost on val dataset after 590 epochs is = 0.09451274442296152\n",
      "Initial Cost on Val dataset for this epoch 590 = 0.09451274442296152\n",
      "learning rate for this epoch =  0.10145115632612439\n",
      "Error on this batch = 0.04776701570817929\n",
      "Error on this batch = 0.0782566024645482\n",
      "Cost on val dataset after 591 epochs is = 0.09447657833661753\n",
      "Initial Cost on Val dataset for this epoch 591 = 0.09447657833661753\n",
      "learning rate for this epoch =  0.10140821402811308\n",
      "Error on this batch = 0.04771992082956745\n",
      "Error on this batch = 0.07820098384810488\n",
      "Cost on val dataset after 592 epochs is = 0.0944404990893167\n",
      "Initial Cost on Val dataset for this epoch 592 = 0.0944404990893167\n",
      "learning rate for this epoch =  0.10136536245969245\n",
      "Error on this batch = 0.04767294600246271\n",
      "Error on this batch = 0.07814550505051208\n",
      "Cost on val dataset after 593 epochs is = 0.09440450640382687\n",
      "Initial Cost on Val dataset for this epoch 593 = 0.09440450640382687\n",
      "learning rate for this epoch =  0.10132260127639228\n",
      "Error on this batch = 0.04762609075214519\n",
      "Error on this batch = 0.07809016541018364\n",
      "Cost on val dataset after 594 epochs is = 0.09436860000601398\n",
      "Initial Cost on Val dataset for this epoch 594 = 0.09436860000601398\n",
      "learning rate for this epoch =  0.10127993013562818\n",
      "Error on this batch = 0.047579354607350036\n",
      "Error on this batch = 0.07803496427014922\n",
      "Cost on val dataset after 595 epochs is = 0.09433277962480148\n",
      "Initial Cost on Val dataset for this epoch 595 = 0.09433277962480148\n",
      "learning rate for this epoch =  0.10123734869668825\n",
      "Error on this batch = 0.04753273710022621\n",
      "Error on this batch = 0.07797990097797912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on val dataset after 596 epochs is = 0.09429704499212856\n",
      "Initial Cost on Val dataset for this epoch 596 = 0.09429704499212856\n",
      "learning rate for this epoch =  0.10119485662071964\n",
      "Error on this batch = 0.04748623776629552\n",
      "Error on this batch = 0.07792497488571058\n",
      "Cost on val dataset after 597 epochs is = 0.09426139584290745\n",
      "Initial Cost on Val dataset for this epoch 597 = 0.09426139584290745\n",
      "learning rate for this epoch =  0.10115245357071535\n",
      "Error on this batch = 0.04743985614441126\n",
      "Error on this batch = 0.07787018534977555\n",
      "Cost on val dataset after 598 epochs is = 0.0942258319149795\n",
      "Initial Cost on Val dataset for this epoch 598 = 0.0942258319149795\n",
      "learning rate for this epoch =  0.10111013921150111\n",
      "Error on this batch = 0.04739359177671737\n",
      "Error on this batch = 0.07781553173093057\n",
      "Cost on val dataset after 599 epochs is = 0.09419035294907036\n",
      "Initial Cost on Val dataset for this epoch 599 = 0.09419035294907036\n",
      "learning rate for this epoch =  0.1010679132097223\n",
      "Error on this batch = 0.04734744420860738\n",
      "Error on this batch = 0.0777610133941879\n",
      "Cost on val dataset after 600 epochs is = 0.0941549586887437\n",
      "Initial Cost on Val dataset for this epoch 600 = 0.0941549586887437\n",
      "learning rate for this epoch =  0.10102577523383117\n",
      "Error on this batch = 0.047301412988683594\n",
      "Error on this batch = 0.0777066297087486\n",
      "Cost on val dataset after 601 epochs is = 0.09411964888035454\n",
      "Initial Cost on Val dataset for this epoch 601 = 0.09411964888035454\n",
      "learning rate for this epoch =  0.10098372495407393\n",
      "Error on this batch = 0.04725549766871638\n",
      "Error on this batch = 0.07765238004793722\n",
      "Cost on val dataset after 602 epochs is = 0.09408442327300132\n",
      "Initial Cost on Val dataset for this epoch 602 = 0.09408442327300132\n",
      "learning rate for this epoch =  0.10094176204247814\n",
      "Error on this batch = 0.04720969780360363\n",
      "Error on this batch = 0.07759826378913819\n",
      "Cost on val dataset after 603 epochs is = 0.09404928161847691\n",
      "Initial Cost on Val dataset for this epoch 603 = 0.09404928161847691\n",
      "learning rate for this epoch =  0.10089988617284017\n",
      "Error on this batch = 0.047164012951330464\n",
      "Error on this batch = 0.07754428031373402\n",
      "Cost on val dataset after 604 epochs is = 0.09401422367121909\n",
      "Initial Cost on Val dataset for this epoch 604 = 0.09401422367121909\n",
      "learning rate for this epoch =  0.10085809702071269\n",
      "Error on this batch = 0.04711844267292893\n",
      "Error on this batch = 0.07749042900704516\n",
      "Cost on val dataset after 605 epochs is = 0.09397924918826002\n",
      "Initial Cost on Val dataset for this epoch 605 = 0.09397924918826002\n",
      "learning rate for this epoch =  0.10081639426339235\n",
      "Error on this batch = 0.04707298653243811\n",
      "Error on this batch = 0.07743670925827116\n",
      "Cost on val dataset after 606 epochs is = 0.09394435792917487\n",
      "Initial Cost on Val dataset for this epoch 606 = 0.09394435792917487\n",
      "learning rate for this epoch =  0.10077477757990758\n",
      "Error on this batch = 0.04702764409686444\n",
      "Error on this batch = 0.07738312046043433\n",
      "Cost on val dataset after 607 epochs is = 0.0939095496560299\n",
      "Initial Cost on Val dataset for this epoch 607 = 0.0939095496560299\n",
      "learning rate for this epoch =  0.10073324665100637\n",
      "Error on this batch = 0.04698241493614228\n",
      "Error on this batch = 0.07732966201032401\n",
      "Cost on val dataset after 608 epochs is = 0.09387482413332976\n",
      "Initial Cost on Val dataset for this epoch 608 = 0.09387482413332976\n",
      "learning rate for this epoch =  0.10069180115914433\n",
      "Error on this batch = 0.04693729862309463\n",
      "Error on this batch = 0.07727633330844327\n",
      "Cost on val dataset after 609 epochs is = 0.09384018112796405\n",
      "Initial Cost on Val dataset for this epoch 609 = 0.09384018112796405\n",
      "learning rate for this epoch =  0.1006504407884727\n",
      "Error on this batch = 0.046892294733394346\n",
      "Error on this batch = 0.07722313375895669\n",
      "Cost on val dataset after 610 epochs is = 0.09380562040915363\n",
      "Initial Cost on Val dataset for this epoch 610 = 0.09380562040915363\n",
      "learning rate for this epoch =  0.10060916522482655\n",
      "Error on this batch = 0.046847402845525486\n",
      "Error on this batch = 0.07717006276964002\n",
      "Cost on val dataset after 611 epochs is = 0.09377114174839597\n",
      "Initial Cost on Val dataset for this epoch 611 = 0.09377114174839597\n",
      "learning rate for this epoch =  0.10056797415571314\n",
      "Error on this batch = 0.04680262254074521\n",
      "Error on this batch = 0.07711711975183104\n",
      "Cost on val dataset after 612 epochs is = 0.09373674491941032\n",
      "Initial Cost on Val dataset for this epoch 612 = 0.09373674491941032\n",
      "learning rate for this epoch =  0.10052686727030014\n",
      "Error on this batch = 0.04675795340304574\n",
      "Error on this batch = 0.07706430412038216\n",
      "Cost on val dataset after 613 epochs is = 0.09370242969808251\n",
      "Initial Cost on Val dataset for this epoch 613 = 0.09370242969808251\n",
      "learning rate for this epoch =  0.10048584425940424\n",
      "Error on this batch = 0.04671339501911685\n",
      "Error on this batch = 0.07701161529361437\n",
      "Cost on val dataset after 614 epochs is = 0.09366819586240911\n",
      "Initial Cost on Val dataset for this epoch 614 = 0.09366819586240911\n",
      "learning rate for this epoch =  0.10044490481547967\n",
      "Error on this batch = 0.046668946978308784\n",
      "Error on this batch = 0.07695905269327247\n",
      "Cost on val dataset after 615 epochs is = 0.09363404319244148\n",
      "Initial Cost on Val dataset for this epoch 615 = 0.09363404319244148\n",
      "learning rate for this epoch =  0.10040404863260693\n",
      "Error on this batch = 0.04662460887259544\n",
      "Error on this batch = 0.07690661574448182\n",
      "Cost on val dataset after 616 epochs is = 0.09359997147022954\n",
      "Initial Cost on Val dataset for this epoch 616 = 0.09359997147022954\n",
      "learning rate for this epoch =  0.10036327540648149\n",
      "Error on this batch = 0.046580380296538\n",
      "Error on this batch = 0.07685430387570634\n",
      "Cost on val dataset after 617 epochs is = 0.09356598047976546\n",
      "Initial Cost on Val dataset for this epoch 617 = 0.09356598047976546\n",
      "learning rate for this epoch =  0.10032258483440272\n",
      "Error on this batch = 0.04653626084724902\n",
      "Error on this batch = 0.07680211651870775\n",
      "Cost on val dataset after 618 epochs is = 0.09353207000692687\n",
      "Initial Cost on Val dataset for this epoch 618 = 0.09353207000692687\n",
      "learning rate for this epoch =  0.10028197661526282\n",
      "Error on this batch = 0.046492250124356926\n",
      "Error on this batch = 0.0767500531085062\n",
      "Cost on val dataset after 619 epochs is = 0.09349823983942053\n",
      "Initial Cost on Val dataset for this epoch 619 = 0.09349823983942053\n",
      "learning rate for this epoch =  0.10024145044953589\n",
      "Error on this batch = 0.04644834772997088\n",
      "Error on this batch = 0.07669811308334165\n",
      "Cost on val dataset after 620 epochs is = 0.09346448976672539\n",
      "Initial Cost on Val dataset for this epoch 620 = 0.09346448976672539\n",
      "learning rate for this epoch =  0.10020100603926707\n",
      "Error on this batch = 0.04640455326864622\n",
      "Error on this batch = 0.07664629588463696\n",
      "Cost on val dataset after 621 epochs is = 0.09343081958003618\n",
      "Initial Cost on Val dataset for this epoch 621 = 0.09343081958003618\n",
      "learning rate for this epoch =  0.1001606430880618\n",
      "Error on this batch = 0.04636086634735035\n",
      "Error on this batch = 0.07659460095696156\n",
      "Cost on val dataset after 622 epochs is = 0.093397229072207\n",
      "Initial Cost on Val dataset for this epoch 622 = 0.093397229072207\n",
      "learning rate for this epoch =  0.10012036130107511\n",
      "Error on this batch = 0.046317286575428974\n",
      "Error on this batch = 0.07654302774799641\n",
      "Cost on val dataset after 623 epochs is = 0.09336371803769458\n",
      "Initial Cost on Val dataset for this epoch 623 = 0.09336371803769458\n",
      "learning rate for this epoch =  0.10008016038500113\n",
      "Error on this batch = 0.04627381356457301\n",
      "Error on this batch = 0.07649157570850004\n",
      "Cost on val dataset after 624 epochs is = 0.09333028627250267\n",
      "Initial Cost on Val dataset for this epoch 624 = 0.09333028627250267\n",
      "learning rate for this epoch =  0.10004004004806248\n",
      "Error on this batch = 0.04623044692878582\n",
      "Error on this batch = 0.07644024429227511\n",
      "Cost on val dataset after 625 epochs is = 0.09329693357412591\n",
      "Initial Cost on Val dataset for this epoch 625 = 0.09329693357412591\n",
      "learning rate for this epoch =  0.1\n",
      "Error on this batch = 0.04618718628435102\n",
      "Error on this batch = 0.0763890329561365\n",
      "Cost on val dataset after 626 epochs is = 0.09326365974149431\n",
      "Initial Cost on Val dataset for this epoch 626 = 0.09326365974149431\n",
      "learning rate for this epoch =  0.09996003995206232\n",
      "Error on this batch = 0.046144031249800876\n",
      "Error on this batch = 0.07633794115987948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on val dataset after 627 epochs is = 0.09323046457491825\n",
      "Initial Cost on Val dataset for this epoch 627 = 0.09323046457491825\n",
      "learning rate for this epoch =  0.0999201596169957\n",
      "Error on this batch = 0.04610098144588509\n",
      "Error on this batch = 0.07628696836624949\n",
      "Cost on val dataset after 628 epochs is = 0.09319734787603373\n",
      "Initial Cost on Val dataset for this epoch 628 = 0.09319734787603373\n",
      "learning rate for this epoch =  0.09988035870903386\n",
      "Error on this batch = 0.04605803649554013\n",
      "Error on this batch = 0.07623611404091199\n",
      "Cost on val dataset after 629 epochs is = 0.09316430944774816\n",
      "Initial Cost on Val dataset for this epoch 629 = 0.09316430944774816\n",
      "learning rate for this epoch =  0.09984063694388798\n",
      "Error on this batch = 0.04601519602385912\n",
      "Error on this batch = 0.07618537765242339\n",
      "Cost on val dataset after 630 epochs is = 0.09313134909418677\n",
      "Initial Cost on Val dataset for this epoch 630 = 0.09313134909418677\n",
      "learning rate for this epoch =  0.09980099403873664\n",
      "Error on this batch = 0.045972459658062285\n",
      "Error on this batch = 0.0761347586722027\n",
      "Cost on val dataset after 631 epochs is = 0.09309846662063952\n",
      "Initial Cost on Val dataset for this epoch 631 = 0.09309846662063952\n",
      "learning rate for this epoch =  0.099761429712216\n",
      "Error on this batch = 0.04592982702746789\n",
      "Error on this batch = 0.07608425657450346\n",
      "Cost on val dataset after 632 epochs is = 0.09306566183350866\n",
      "Initial Cost on Val dataset for this epoch 632 = 0.09306566183350866\n",
      "learning rate for this epoch =  0.09972194368440992\n",
      "Error on this batch = 0.04588729776346366\n",
      "Error on this batch = 0.07603387083638667\n",
      "Cost on val dataset after 633 epochs is = 0.09303293454025713\n",
      "Initial Cost on Val dataset for this epoch 633 = 0.09303293454025713\n",
      "learning rate for this epoch =  0.09968253567684038\n",
      "Error on this batch = 0.0458448714994789\n",
      "Error on this batch = 0.07598360093769399\n",
      "Cost on val dataset after 634 epochs is = 0.09300028454935748\n",
      "Initial Cost on Val dataset for this epoch 634 = 0.09300028454935748\n",
      "learning rate for this epoch =  0.09964320541245761\n",
      "Error on this batch = 0.04580254787095703\n",
      "Error on this batch = 0.07593344636102152\n",
      "Cost on val dataset after 635 epochs is = 0.09296771167024179\n",
      "Initial Cost on Val dataset for this epoch 635 = 0.09296771167024179\n",
      "learning rate for this epoch =  0.09960395261563074\n",
      "Error on this batch = 0.04576032651532861\n",
      "Error on this batch = 0.07588340659169408\n",
      "Cost on val dataset after 636 epochs is = 0.09293521571325211\n",
      "Initial Cost on Val dataset for this epoch 636 = 0.09293521571325211\n",
      "learning rate for this epoch =  0.0995647770121382\n",
      "Error on this batch = 0.045718207071985144\n",
      "Error on this batch = 0.0758334811177399\n",
      "Cost on val dataset after 637 epochs is = 0.09290279648959225\n",
      "Initial Cost on Val dataset for this epoch 637 = 0.09290279648959225\n",
      "learning rate for this epoch =  0.0995256783291583\n",
      "Error on this batch = 0.04567618918225309\n",
      "Error on this batch = 0.07578366942986584\n",
      "Cost on val dataset after 638 epochs is = 0.09287045381128\n",
      "Initial Cost on Val dataset for this epoch 638 = 0.09287045381128\n",
      "learning rate for this epoch =  0.09948665629526\n",
      "Error on this batch = 0.045634272489368834\n",
      "Error on this batch = 0.07573397102143266\n",
      "Cost on val dataset after 639 epochs is = 0.09283818749110043\n",
      "Initial Cost on Val dataset for this epoch 639 = 0.09283818749110043\n",
      "learning rate for this epoch =  0.09944771064039355\n",
      "Error on this batch = 0.04559245663845369\n",
      "Error on this batch = 0.075684385388431\n",
      "Cost on val dataset after 640 epochs is = 0.09280599734256048\n",
      "Initial Cost on Val dataset for this epoch 640 = 0.09280599734256048\n",
      "learning rate for this epoch =  0.09940884109588133\n",
      "Error on this batch = 0.045550741276489934\n",
      "Error on this batch = 0.07563491202945744\n",
      "Cost on val dataset after 641 epochs is = 0.09277388317984407\n",
      "Initial Cost on Val dataset for this epoch 641 = 0.09277388317984407\n",
      "learning rate for this epoch =  0.09937004739440881\n",
      "Error on this batch = 0.04550912605229703\n",
      "Error on this batch = 0.0755855504456909\n",
      "Cost on val dataset after 642 epochs is = 0.09274184481776855\n",
      "Initial Cost on Val dataset for this epoch 642 = 0.09274184481776855\n",
      "learning rate for this epoch =  0.09933132927001546\n",
      "Error on this batch = 0.04546761061650862\n",
      "Error on this batch = 0.07553630014086936\n",
      "Cost on val dataset after 643 epochs is = 0.09270988207174218\n",
      "Initial Cost on Val dataset for this epoch 643 = 0.09270988207174218\n",
      "learning rate for this epoch =  0.09929268645808582\n",
      "Error on this batch = 0.045426194621549815\n",
      "Error on this batch = 0.07548716062126663\n",
      "Cost on val dataset after 644 epochs is = 0.09267799475772268\n",
      "Initial Cost on Val dataset for this epoch 644 = 0.09267799475772268\n",
      "learning rate for this epoch =  0.09925411869534059\n",
      "Error on this batch = 0.045384877721615195\n",
      "Error on this batch = 0.07543813139566957\n",
      "Cost on val dataset after 645 epochs is = 0.09264618269217689\n",
      "Initial Cost on Val dataset for this epoch 645 = 0.09264618269217689\n",
      "learning rate for this epoch =  0.0992156257198279\n",
      "Error on this batch = 0.04534365957264727\n",
      "Error on this batch = 0.07538921197535542\n",
      "Cost on val dataset after 646 epochs is = 0.0926144456920416\n",
      "Initial Cost on Val dataset for this epoch 646 = 0.0926144456920416\n",
      "learning rate for this epoch =  0.09917720727091442\n",
      "Error on this batch = 0.04530253983231544\n",
      "Error on this batch = 0.07534040187406893\n",
      "Cost on val dataset after 647 epochs is = 0.09258278357468545\n",
      "Initial Cost on Val dataset for this epoch 647 = 0.09258278357468545\n",
      "learning rate for this epoch =  0.09913886308927693\n",
      "Error on this batch = 0.045261518159995455\n",
      "Error on this batch = 0.07529170060800033\n",
      "Cost on val dataset after 648 epochs is = 0.09255119615787216\n",
      "Initial Cost on Val dataset for this epoch 648 = 0.09255119615787216\n",
      "learning rate for this epoch =  0.09910059291689342\n",
      "Error on this batch = 0.04522059421674933\n",
      "Error on this batch = 0.07524310769576292\n",
      "Cost on val dataset after 649 epochs is = 0.09251968325972484\n",
      "Initial Cost on Val dataset for this epoch 649 = 0.09251968325972484\n",
      "learning rate for this epoch =  0.09906239649703485\n",
      "Error on this batch = 0.04517976766530603\n",
      "Error on this batch = 0.07519462265837078\n",
      "Cost on val dataset after 650 epochs is = 0.0924882446986916\n",
      "Initial Cost on Val dataset for this epoch 650 = 0.0924882446986916\n",
      "learning rate for this epoch =  0.09902427357425653\n",
      "Error on this batch = 0.04513903817004211\n",
      "Error on this batch = 0.07514624501921685\n",
      "Cost on val dataset after 651 epochs is = 0.09245688029351219\n",
      "Initial Cost on Val dataset for this epoch 651 = 0.09245688029351219\n",
      "learning rate for this epoch =  0.09898622389438974\n",
      "Error on this batch = 0.04509840539696346\n",
      "Error on this batch = 0.07509797430405092\n",
      "Cost on val dataset after 652 epochs is = 0.09242558986318611\n",
      "Initial Cost on Val dataset for this epoch 652 = 0.09242558986318611\n",
      "learning rate for this epoch =  0.09894824720453346\n",
      "Error on this batch = 0.04505786901368703\n",
      "Error on this batch = 0.07504981004095777\n",
      "Cost on val dataset after 653 epochs is = 0.09239437322694176\n",
      "Initial Cost on Val dataset for this epoch 653 = 0.09239437322694176\n",
      "learning rate for this epoch =  0.09891034325304611\n",
      "Error on this batch = 0.04501742868942324\n",
      "Error on this batch = 0.07500175176033523\n",
      "Cost on val dataset after 654 epochs is = 0.09236323020420698\n",
      "Initial Cost on Val dataset for this epoch 654 = 0.09236323020420698\n",
      "learning rate for this epoch =  0.09887251178953727\n",
      "Error on this batch = 0.04497708409495884\n",
      "Error on this batch = 0.07495379899487242\n",
      "Cost on val dataset after 655 epochs is = 0.09233216061458065\n",
      "Initial Cost on Val dataset for this epoch 655 = 0.09233216061458065\n",
      "learning rate for this epoch =  0.09883475256485971\n",
      "Error on this batch = 0.04493683490264013\n",
      "Error on this batch = 0.07490595127952816\n",
      "Cost on val dataset after 656 epochs is = 0.09230116427780566\n",
      "Initial Cost on Val dataset for this epoch 656 = 0.09230116427780566\n",
      "learning rate for this epoch =  0.09879706533110119\n",
      "Error on this batch = 0.04489668078635675\n",
      "Error on this batch = 0.07485820815150912\n",
      "Cost on val dataset after 657 epochs is = 0.09227024101374315\n",
      "Initial Cost on Val dataset for this epoch 657 = 0.09227024101374315\n",
      "learning rate for this epoch =  0.09875944984157659\n",
      "Error on this batch = 0.04485662142152576\n",
      "Error on this batch = 0.07481056915024821\n",
      "Cost on val dataset after 658 epochs is = 0.09223939064234782\n",
      "Initial Cost on Val dataset for this epoch 658 = 0.09223939064234782\n",
      "learning rate for this epoch =  0.09872190585081982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on this batch = 0.04481665648507617\n",
      "Error on this batch = 0.07476303381738308\n",
      "Cost on val dataset after 659 epochs is = 0.09220861298364459\n",
      "Initial Cost on Val dataset for this epoch 659 = 0.09220861298364459\n",
      "learning rate for this epoch =  0.09868443311457611\n",
      "Error on this batch = 0.04477678565543397\n",
      "Error on this batch = 0.07471560169673441\n",
      "Cost on val dataset after 660 epochs is = 0.09217790785770649\n",
      "Initial Cost on Val dataset for this epoch 660 = 0.09217790785770649\n",
      "learning rate for this epoch =  0.09864703138979418\n",
      "Error on this batch = 0.044737008612507526\n",
      "Error on this batch = 0.07466827233428447\n",
      "Cost on val dataset after 661 epochs is = 0.09214727508463366\n",
      "Initial Cost on Val dataset for this epoch 661 = 0.09214727508463366\n",
      "learning rate for this epoch =  0.09860970043461836\n",
      "Error on this batch = 0.044697325037673145\n",
      "Error on this batch = 0.07462104527815552\n",
      "Cost on val dataset after 662 epochs is = 0.09211671448453367\n",
      "Initial Cost on Val dataset for this epoch 662 = 0.09211671448453367\n",
      "learning rate for this epoch =  0.09857244000838114\n",
      "Error on this batch = 0.04465773461376144\n",
      "Error on this batch = 0.07457392007858839\n",
      "Cost on val dataset after 663 epochs is = 0.09208622587750284\n",
      "Initial Cost on Val dataset for this epoch 663 = 0.09208622587750284\n",
      "learning rate for this epoch =  0.09853524987159529\n",
      "Error on this batch = 0.04461823702504367\n",
      "Error on this batch = 0.074526896287921\n",
      "Cost on val dataset after 664 epochs is = 0.09205580908360915\n",
      "Initial Cost on Val dataset for this epoch 664 = 0.09205580908360915\n",
      "learning rate for this epoch =  0.0984981297859465\n",
      "Error on this batch = 0.04457883195721864\n",
      "Error on this batch = 0.07447997346056696\n",
      "Cost on val dataset after 665 epochs is = 0.09202546392287561\n",
      "Initial Cost on Val dataset for this epoch 665 = 0.09202546392287561\n",
      "learning rate for this epoch =  0.09846107951428583\n",
      "Error on this batch = 0.044539519097399886\n",
      "Error on this batch = 0.07443315115299426\n",
      "Cost on val dataset after 666 epochs is = 0.0919951902152656\n",
      "Initial Cost on Val dataset for this epoch 666 = 0.0919951902152656\n",
      "learning rate for this epoch =  0.09842409882062221\n",
      "Error on this batch = 0.044500298134103264\n",
      "Error on this batch = 0.07438642892370383\n",
      "Cost on val dataset after 667 epochs is = 0.0919649877806686\n",
      "Initial Cost on Val dataset for this epoch 667 = 0.0919649877806686\n",
      "learning rate for this epoch =  0.09838718747011513\n",
      "Error on this batch = 0.04446116875723472\n",
      "Error on this batch = 0.07433980633320834\n",
      "Cost on val dataset after 668 epochs is = 0.09193485643888748\n",
      "Initial Cost on Val dataset for this epoch 668 = 0.09193485643888748\n",
      "learning rate for this epoch =  0.09835034522906724\n",
      "Error on this batch = 0.044422130658078575\n",
      "Error on this batch = 0.07429328294401107\n",
      "Cost on val dataset after 669 epochs is = 0.09190479600962678\n",
      "Initial Cost on Val dataset for this epoch 669 = 0.09190479600962678\n",
      "learning rate for this epoch =  0.09831357186491718\n",
      "Error on this batch = 0.04438318352928589\n",
      "Error on this batch = 0.07424685832058446\n",
      "Cost on val dataset after 670 epochs is = 0.09187480631248192\n",
      "Initial Cost on Val dataset for this epoch 670 = 0.09187480631248192\n",
      "learning rate for this epoch =  0.09827686714623232\n",
      "Error on this batch = 0.04434432706486339\n",
      "Error on this batch = 0.07420053202934934\n",
      "Cost on val dataset after 671 epochs is = 0.09184488716692953\n",
      "Initial Cost on Val dataset for this epoch 671 = 0.09184488716692953\n",
      "learning rate for this epoch =  0.09824023084270153\n",
      "Error on this batch = 0.04430556096016229\n",
      "Error on this batch = 0.07415430363865375\n",
      "Cost on val dataset after 672 epochs is = 0.09181503839231897\n",
      "Initial Cost on Val dataset for this epoch 672 = 0.09181503839231897\n",
      "learning rate for this epoch =  0.09820366272512825\n",
      "Error on this batch = 0.044266884911867914\n",
      "Error on this batch = 0.07410817271875192\n",
      "Cost on val dataset after 673 epochs is = 0.0917852598078645\n",
      "Initial Cost on Val dataset for this epoch 673 = 0.0917852598078645\n",
      "learning rate for this epoch =  0.0981671625654233\n",
      "Error on this batch = 0.0442282986179891\n",
      "Error on this batch = 0.07406213884178371\n",
      "Cost on val dataset after 674 epochs is = 0.0917555512326387\n",
      "Initial Cost on Val dataset for this epoch 674 = 0.0917555512326387\n",
      "learning rate for this epoch =  0.09813073013659797\n",
      "Error on this batch = 0.04418980177784811\n",
      "Error on this batch = 0.0740162015817535\n",
      "Cost on val dataset after 675 epochs is = 0.09172591248556676\n",
      "Initial Cost on Val dataset for this epoch 675 = 0.09172591248556676\n",
      "learning rate for this epoch =  0.09809436521275706\n",
      "Error on this batch = 0.04415139409207067\n",
      "Error on this batch = 0.07397036051450985\n",
      "Cost on val dataset after 676 epochs is = 0.09169634338542164\n",
      "Initial Cost on Val dataset for this epoch 676 = 0.09169634338542164\n",
      "learning rate for this epoch =  0.09805806756909202\n",
      "Error on this batch = 0.0441130752625764\n",
      "Error on this batch = 0.07392461521772485\n",
      "Cost on val dataset after 677 epochs is = 0.0916668437508202\n",
      "Initial Cost on Val dataset for this epoch 677 = 0.0916668437508202\n",
      "learning rate for this epoch =  0.09802183698187408\n",
      "Error on this batch = 0.04407484499256922\n",
      "Error on this batch = 0.07387896527087368\n",
      "Cost on val dataset after 678 epochs is = 0.09163741340022004\n",
      "Initial Cost on Val dataset for this epoch 678 = 0.09163741340022004\n",
      "learning rate for this epoch =  0.09798567322844758\n",
      "Error on this batch = 0.044036702986528294\n",
      "Error on this batch = 0.07383341025521448\n",
      "Cost on val dataset after 679 epochs is = 0.0916080521519175\n",
      "Initial Cost on Val dataset for this epoch 679 = 0.0916080521519175\n",
      "learning rate for this epoch =  0.09794957608722307\n",
      "Error on this batch = 0.043998648950198865\n",
      "Error on this batch = 0.07378794975376816\n",
      "Cost on val dataset after 680 epochs is = 0.0915787598240461\n",
      "Initial Cost on Val dataset for this epoch 680 = 0.0915787598240461\n",
      "learning rate for this epoch =  0.09791354533767088\n",
      "Error on this batch = 0.043960682590583375\n",
      "Error on this batch = 0.07374258335129856\n",
      "Cost on val dataset after 681 epochs is = 0.09154953623457598\n",
      "Initial Cost on Val dataset for this epoch 681 = 0.09154953623457598\n",
      "learning rate for this epoch =  0.09787758076031428\n",
      "Error on this batch = 0.043922803615933044\n",
      "Error on this batch = 0.07369731063429244\n",
      "Cost on val dataset after 682 epochs is = 0.09152038120131409\n",
      "Initial Cost on Val dataset for this epoch 682 = 0.09152038120131409\n",
      "learning rate for this epoch =  0.09784168213672302\n",
      "Error on this batch = 0.04388501173573909\n",
      "Error on this batch = 0.07365213119094016\n",
      "Cost on val dataset after 683 epochs is = 0.09149129454190509\n",
      "Initial Cost on Val dataset for this epoch 683 = 0.09149129454190509\n",
      "learning rate for this epoch =  0.09780584924950686\n",
      "Error on this batch = 0.04384730666072472\n",
      "Error on this batch = 0.07360704461111615\n",
      "Cost on val dataset after 684 epochs is = 0.09146227607383284\n",
      "Initial Cost on Val dataset for this epoch 684 = 0.09146227607383284\n",
      "learning rate for this epoch =  0.09777008188230901\n",
      "Error on this batch = 0.043809688102836704\n",
      "Error on this batch = 0.07356205048635961\n",
      "Cost on val dataset after 685 epochs is = 0.09143332561442277\n",
      "Initial Cost on Val dataset for this epoch 685 = 0.09143332561442277\n",
      "learning rate for this epoch =  0.09773437981979974\n",
      "Error on this batch = 0.0437721557752376\n",
      "Error on this batch = 0.07351714840985575\n",
      "Cost on val dataset after 686 epochs is = 0.09140444298084477\n",
      "Initial Cost on Val dataset for this epoch 686 = 0.09140444298084477\n",
      "learning rate for this epoch =  0.09769874284767002\n",
      "Error on this batch = 0.0437347093922978\n",
      "Error on this batch = 0.07347233797641682\n",
      "Cost on val dataset after 687 epochs is = 0.09137562799011673\n",
      "Initial Cost on Val dataset for this epoch 687 = 0.09137562799011673\n",
      "learning rate for this epoch =  0.0976631707526253\n",
      "Error on this batch = 0.04369734866958784\n",
      "Error on this batch = 0.07342761878246361\n",
      "Cost on val dataset after 688 epochs is = 0.09134688045910855\n",
      "Initial Cost on Val dataset for this epoch 688 = 0.09134688045910855\n",
      "learning rate for this epoch =  0.09762766332237903\n",
      "Error on this batch = 0.04366007332387084\n",
      "Error on this batch = 0.0733829904260073\n",
      "Cost on val dataset after 689 epochs is = 0.09131820020454703\n",
      "Initial Cost on Val dataset for this epoch 689 = 0.09131820020454703\n",
      "learning rate for this epoch =  0.09759222034564662\n",
      "Error on this batch = 0.04362288307309495\n",
      "Error on this batch = 0.07333845250663128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on val dataset after 690 epochs is = 0.091289587043021\n",
      "Initial Cost on Val dataset for this epoch 690 = 0.091289587043021\n",
      "learning rate for this epoch =  0.09755684161213918\n",
      "Error on this batch = 0.04358577763638607\n",
      "Error on this batch = 0.07329400462547342\n",
      "Cost on val dataset after 691 epochs is = 0.09126104079098704\n",
      "Initial Cost on Val dataset for this epoch 691 = 0.09126104079098704\n",
      "learning rate for this epoch =  0.09752152691255746\n",
      "Error on this batch = 0.04354875673404064\n",
      "Error on this batch = 0.0732496463852086\n",
      "Cost on val dataset after 692 epochs is = 0.09123256126477577\n",
      "Initial Cost on Val dataset for this epoch 692 = 0.09123256126477577\n",
      "learning rate for this epoch =  0.09748627603858566\n",
      "Error on this batch = 0.043511820087518346\n",
      "Error on this batch = 0.07320537739003158\n",
      "Cost on val dataset after 693 epochs is = 0.09120414828059853\n",
      "Initial Cost on Val dataset for this epoch 693 = 0.09120414828059853\n",
      "learning rate for this epoch =  0.09745108878288547\n",
      "Error on this batch = 0.043474967419435226\n",
      "Error on this batch = 0.07316119724563987\n",
      "Cost on val dataset after 694 epochs is = 0.09117580165455456\n",
      "Initial Cost on Val dataset for this epoch 694 = 0.09117580165455456\n",
      "learning rate for this epoch =  0.09741596493909016\n",
      "Error on this batch = 0.04343819845355659\n",
      "Error on this batch = 0.07311710555921741\n",
      "Cost on val dataset after 695 epochs is = 0.09114752120263842\n",
      "Initial Cost on Val dataset for this epoch 695 = 0.09114752120263842\n",
      "learning rate for this epoch =  0.09738090430179841\n",
      "Error on this batch = 0.04340151291479018\n",
      "Error on this batch = 0.0730731019394181\n",
      "Cost on val dataset after 696 epochs is = 0.09111930674074804\n",
      "Initial Cost on Val dataset for this epoch 696 = 0.09111930674074804\n",
      "learning rate for this epoch =  0.09734590666656863\n",
      "Error on this batch = 0.04336491052917928\n",
      "Error on this batch = 0.07302918599634999\n",
      "Cost on val dataset after 697 epochs is = 0.09109115808469301\n",
      "Initial Cost on Val dataset for this epoch 697 = 0.09109115808469301\n",
      "learning rate for this epoch =  0.09731097182991302\n",
      "Error on this batch = 0.04332839102389613\n",
      "Error on this batch = 0.07298535734155942\n",
      "Cost on val dataset after 698 epochs is = 0.09106307505020311\n",
      "Initial Cost on Val dataset for this epoch 698 = 0.09106307505020311\n",
      "learning rate for this epoch =  0.09727609958929176\n",
      "Error on this batch = 0.043291954127235084\n",
      "Error on this batch = 0.07294161558801603\n",
      "Cost on val dataset after 699 epochs is = 0.09103505745293743\n",
      "Initial Cost on Val dataset for this epoch 699 = 0.09103505745293743\n",
      "learning rate for this epoch =  0.09724128974310718\n",
      "Error on this batch = 0.04325559956860598\n",
      "Error on this batch = 0.07289796035009773\n",
      "Cost on val dataset after 700 epochs is = 0.09100710510849351\n",
      "Initial Cost on Val dataset for this epoch 700 = 0.09100710510849351\n",
      "learning rate for this epoch =  0.09720654209069819\n",
      "Error on this batch = 0.04321932707852771\n",
      "Error on this batch = 0.07285439124357597\n",
      "Cost on val dataset after 701 epochs is = 0.0909792178324168\n",
      "Initial Cost on Val dataset for this epoch 701 = 0.0909792178324168\n",
      "learning rate for this epoch =  0.09717185643233449\n",
      "Error on this batch = 0.04318313638862153\n",
      "Error on this batch = 0.07281090788560182\n",
      "Cost on val dataset after 702 epochs is = 0.09095139544021065\n",
      "Initial Cost on Val dataset for this epoch 702 = 0.09095139544021065\n",
      "learning rate for this epoch =  0.09713723256921088\n",
      "Error on this batch = 0.043147027231604594\n",
      "Error on this batch = 0.072767509894692\n",
      "Cost on val dataset after 703 epochs is = 0.09092363774734605\n",
      "Initial Cost on Val dataset for this epoch 703 = 0.09092363774734605\n",
      "learning rate for this epoch =  0.09710267030344186\n",
      "Error on this batch = 0.043110999341283665\n",
      "Error on this batch = 0.07272419689071546\n",
      "Cost on val dataset after 704 epochs is = 0.09089594456927212\n",
      "Initial Cost on Val dataset for this epoch 704 = 0.09089594456927212\n",
      "learning rate for this epoch =  0.09706816943805581\n",
      "Error on this batch = 0.0430750524525485\n",
      "Error on this batch = 0.07268096849488036\n",
      "Cost on val dataset after 705 epochs is = 0.09086831572142626\n",
      "Initial Cost on Val dataset for this epoch 705 = 0.09086831572142626\n",
      "learning rate for this epoch =  0.09703372977698975\n",
      "Error on this batch = 0.04303918630136552\n",
      "Error on this batch = 0.07263782432972142\n",
      "Cost on val dataset after 706 epochs is = 0.09084075101924494\n",
      "Initial Cost on Val dataset for this epoch 706 = 0.09084075101924494\n",
      "learning rate for this epoch =  0.09699935112508366\n",
      "Error on this batch = 0.043003400624771565\n",
      "Error on this batch = 0.0725947640190877\n",
      "Cost on val dataset after 707 epochs is = 0.0908132502781743\n",
      "Initial Cost on Val dataset for this epoch 707 = 0.0908132502781743\n",
      "learning rate for this epoch =  0.09696503328807513\n",
      "Error on this batch = 0.04296769516086733\n",
      "Error on this batch = 0.07255178718813063\n",
      "Cost on val dataset after 708 epochs is = 0.09078581331368112\n",
      "Initial Cost on Val dataset for this epoch 708 = 0.09078581331368112\n",
      "learning rate for this epoch =  0.09693077607259398\n",
      "Error on this batch = 0.04293206964881128\n",
      "Error on this batch = 0.07250889346329291\n",
      "Cost on val dataset after 709 epochs is = 0.09075843994126379\n",
      "Initial Cost on Val dataset for this epoch 709 = 0.09075843994126379\n",
      "learning rate for this epoch =  0.09689657928615694\n",
      "Error on this batch = 0.042896523828813184\n",
      "Error on this batch = 0.07246608247229709\n",
      "Cost on val dataset after 710 epochs is = 0.0907311299764634\n",
      "Initial Cost on Val dataset for this epoch 710 = 0.0907311299764634\n",
      "learning rate for this epoch =  0.09686244273716216\n",
      "Error on this batch = 0.04286105744212785\n",
      "Error on this batch = 0.0724233538441354\n",
      "Cost on val dataset after 711 epochs is = 0.09070388323487492\n",
      "Initial Cost on Val dataset for this epoch 711 = 0.09070388323487492\n",
      "learning rate for this epoch =  0.09682836623488422\n",
      "Error on this batch = 0.042825670231048926\n",
      "Error on this batch = 0.07238070720905926\n",
      "Cost on val dataset after 712 epochs is = 0.09067669953215854\n",
      "Initial Cost on Val dataset for this epoch 712 = 0.09067669953215854\n",
      "learning rate for this epoch =  0.09679434958946864\n",
      "Error on this batch = 0.04279036193890242\n",
      "Error on this batch = 0.07233814219856981\n",
      "Cost on val dataset after 713 epochs is = 0.09064957868405087\n",
      "Initial Cost on Val dataset for this epoch 713 = 0.09064957868405087\n",
      "learning rate for this epoch =  0.09676039261192684\n",
      "Error on this batch = 0.04275513231004066\n",
      "Error on this batch = 0.0722956584454086\n",
      "Cost on val dataset after 714 epochs is = 0.09062252050637642\n",
      "Initial Cost on Val dataset for this epoch 714 = 0.09062252050637642\n",
      "learning rate for this epoch =  0.09672649511413095\n",
      "Error on this batch = 0.04271998108983583\n",
      "Error on this batch = 0.07225325558354855\n",
      "Cost on val dataset after 715 epochs is = 0.09059552481505903\n",
      "Initial Cost on Val dataset for this epoch 715 = 0.09059552481505903\n",
      "learning rate for this epoch =  0.0966926569088086\n",
      "Error on this batch = 0.04268490802467385\n",
      "Error on this batch = 0.07221093324818577\n",
      "Cost on val dataset after 716 epochs is = 0.09056859142613309\n",
      "Initial Cost on Val dataset for this epoch 716 = 0.09056859142613309\n",
      "learning rate for this epoch =  0.09665887780953801\n",
      "Error on this batch = 0.04264991286194784\n",
      "Error on this batch = 0.07216869107573151\n",
      "Cost on val dataset after 717 epochs is = 0.09054172015575519\n",
      "Initial Cost on Val dataset for this epoch 717 = 0.09054172015575519\n",
      "learning rate for this epoch =  0.09662515763074277\n",
      "Error on this batch = 0.04261499535005212\n",
      "Error on this batch = 0.07212652870380469\n",
      "Cost on val dataset after 718 epochs is = 0.0905149108202155\n",
      "Initial Cost on Val dataset for this epoch 718 = 0.0905149108202155\n",
      "learning rate for this epoch =  0.09659149618768699\n",
      "Error on this batch = 0.04258015523837573\n",
      "Error on this batch = 0.07208444577122473\n",
      "Cost on val dataset after 719 epochs is = 0.09048816323594905\n",
      "Initial Cost on Val dataset for this epoch 719 = 0.09048816323594905\n",
      "learning rate for this epoch =  0.09655789329647017\n",
      "Error on this batch = 0.042545392277296125\n",
      "Error on this batch = 0.07204244191800507\n",
      "Cost on val dataset after 720 epochs is = 0.09046147721954721\n",
      "Initial Cost on Val dataset for this epoch 720 = 0.09046147721954721\n",
      "learning rate for this epoch =  0.09652434877402243\n",
      "Error on this batch = 0.04251070621817291\n",
      "Error on this batch = 0.07200051678534687\n",
      "Cost on val dataset after 721 epochs is = 0.09043485258776904\n",
      "Initial Cost on Val dataset for this epoch 721 = 0.09043485258776904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate for this epoch =  0.09649086243809947\n",
      "Error on this batch = 0.04247609681334148\n",
      "Error on this batch = 0.07195867001563326\n",
      "Cost on val dataset after 722 epochs is = 0.0904082891575526\n",
      "Initial Cost on Val dataset for this epoch 722 = 0.0904082891575526\n",
      "learning rate for this epoch =  0.09645743410727778\n",
      "Error on this batch = 0.04244156381610668\n",
      "Error on this batch = 0.07191690125242424\n",
      "Cost on val dataset after 723 epochs is = 0.09038178674602597\n",
      "Initial Cost on Val dataset for this epoch 723 = 0.09038178674602597\n",
      "learning rate for this epoch =  0.09642406360094986\n",
      "Error on this batch = 0.04240710698073641\n",
      "Error on this batch = 0.07187521014045156\n",
      "Cost on val dataset after 724 epochs is = 0.09035534517051884\n",
      "Initial Cost on Val dataset for this epoch 724 = 0.09035534517051884\n",
      "learning rate for this epoch =  0.09639075073931928\n",
      "Error on this batch = 0.04237272606245521\n",
      "Error on this batch = 0.07183359632561455\n",
      "Cost on val dataset after 725 epochs is = 0.09032896424857319\n",
      "Initial Cost on Val dataset for this epoch 725 = 0.09032896424857319\n",
      "learning rate for this epoch =  0.09635749534339606\n",
      "Error on this batch = 0.042338420817437966\n",
      "Error on this batch = 0.07179205945497606\n",
      "Cost on val dataset after 726 epochs is = 0.09030264379795451\n",
      "Initial Cost on Val dataset for this epoch 726 = 0.09030264379795451\n",
      "learning rate for this epoch =  0.0963242972349919\n",
      "Error on this batch = 0.04230419100280338\n",
      "Error on this batch = 0.07175059917675884\n",
      "Cost on val dataset after 727 epochs is = 0.09027638363666272\n",
      "Initial Cost on Val dataset for this epoch 727 = 0.09027638363666272\n",
      "learning rate for this epoch =  0.09629115623671548\n",
      "Error on this batch = 0.04227003637660758\n",
      "Error on this batch = 0.07170921514034262\n",
      "Cost on val dataset after 728 epochs is = 0.09025018358294302\n",
      "Initial Cost on Val dataset for this epoch 728 = 0.09025018358294302\n",
      "learning rate for this epoch =  0.09625807217196783\n",
      "Error on this batch = 0.042235956697837626\n",
      "Error on this batch = 0.07166790699626134\n",
      "Cost on val dataset after 729 epochs is = 0.09022404345529647\n",
      "Initial Cost on Val dataset for this epoch 729 = 0.09022404345529647\n",
      "learning rate for this epoch =  0.09622504486493763\n",
      "Error on this batch = 0.04220195172640507\n",
      "Error on this batch = 0.0716266743962009\n",
      "Cost on val dataset after 730 epochs is = 0.09019796307249077\n",
      "Initial Cost on Val dataset for this epoch 730 = 0.09019796307249077\n",
      "learning rate for this epoch =  0.09619207414059677\n",
      "Error on this batch = 0.04216802122313944\n",
      "Error on this batch = 0.0715855169929974\n",
      "Cost on val dataset after 731 epochs is = 0.09017194225357072\n",
      "Initial Cost on Val dataset for this epoch 731 = 0.09017194225357072\n",
      "learning rate for this epoch =  0.09615915982469565\n",
      "Error on this batch = 0.04213416494978167\n",
      "Error on this batch = 0.07154443444063567\n",
      "Cost on val dataset after 732 epochs is = 0.09014598081786855\n",
      "Initial Cost on Val dataset for this epoch 732 = 0.09014598081786855\n",
      "learning rate for this epoch =  0.09612630174375877\n",
      "Error on this batch = 0.04210038266897764\n",
      "Error on this batch = 0.07150342639424839\n",
      "Cost on val dataset after 733 epochs is = 0.09012007858501415\n",
      "Initial Cost on Val dataset for this epoch 733 = 0.09012007858501415\n",
      "learning rate for this epoch =  0.09609349972508012\n",
      "Error on this batch = 0.04206667414427151\n",
      "Error on this batch = 0.07146249251011537\n",
      "Cost on val dataset after 734 epochs is = 0.09009423537494533\n",
      "Initial Cost on Val dataset for this epoch 734 = 0.09009423537494533\n",
      "learning rate for this epoch =  0.09606075359671883\n",
      "Error on this batch = 0.04203303914009921\n",
      "Error on this batch = 0.07142163244566344\n",
      "Cost on val dataset after 735 epochs is = 0.09006845100791763\n",
      "Initial Cost on Val dataset for this epoch 735 = 0.09006845100791763\n",
      "learning rate for this epoch =  0.09602806318749467\n",
      "Error on this batch = 0.04199947742178178\n",
      "Error on this batch = 0.07138084585946676\n",
      "Cost on val dataset after 736 epochs is = 0.09004272530451435\n",
      "Initial Cost on Val dataset for this epoch 736 = 0.09004272530451435\n",
      "learning rate for this epoch =  0.09599542832698374\n",
      "Error on this batch = 0.04196598875551876\n",
      "Error on this batch = 0.07134013241124722\n",
      "Cost on val dataset after 737 epochs is = 0.09001705808565602\n",
      "Initial Cost on Val dataset for this epoch 737 = 0.09001705808565602\n",
      "learning rate for this epoch =  0.095962848845514\n",
      "Error on this batch = 0.041932572908381587\n",
      "Error on this batch = 0.07129949176187551\n",
      "Cost on val dataset after 738 epochs is = 0.08999144917261012\n",
      "Initial Cost on Val dataset for this epoch 738 = 0.08999144917261012\n",
      "learning rate for this epoch =  0.09593032457416101\n",
      "Error on this batch = 0.04189922964830684\n",
      "Error on this batch = 0.07125892357337253\n",
      "Cost on val dataset after 739 epochs is = 0.08996589838700048\n",
      "Initial Cost on Val dataset for this epoch 739 = 0.08996589838700048\n",
      "learning rate for this epoch =  0.09589785534474361\n",
      "Error on this batch = 0.04186595874408962\n",
      "Error on this batch = 0.07121842750891105\n",
      "Cost on val dataset after 740 epochs is = 0.08994040555081628\n",
      "Initial Cost on Val dataset for this epoch 740 = 0.08994040555081628\n",
      "learning rate for this epoch =  0.09586544098981967\n",
      "Error on this batch = 0.0418327599653768\n",
      "Error on this batch = 0.07117800323281777\n",
      "Cost on val dataset after 741 epochs is = 0.08991497048642155\n",
      "Initial Cost on Val dataset for this epoch 741 = 0.08991497048642155\n",
      "learning rate for this epoch =  0.09583308134268179\n",
      "Error on this batch = 0.041799633082660374\n",
      "Error on this batch = 0.07113765041057578\n",
      "Cost on val dataset after 742 epochs is = 0.08988959301656363\n",
      "Initial Cost on Val dataset for this epoch 742 = 0.08988959301656363\n",
      "learning rate for this epoch =  0.09580077623735313\n",
      "Error on this batch = 0.04176657786727061\n",
      "Error on this batch = 0.0710973687088272\n",
      "Cost on val dataset after 743 epochs is = 0.08986427296438222\n",
      "Initial Cost on Val dataset for this epoch 743 = 0.08986427296438222\n",
      "learning rate for this epoch =  0.09576852550858327\n",
      "Error on this batch = 0.041733594091369376\n",
      "Error on this batch = 0.07105715779537644\n",
      "Cost on val dataset after 744 epochs is = 0.08983901015341782\n",
      "Initial Cost on Val dataset for this epoch 744 = 0.08983901015341782\n",
      "learning rate for this epoch =  0.09573632899184395\n",
      "Error on this batch = 0.04170068152794336\n",
      "Error on this batch = 0.07101701733919351\n",
      "Cost on val dataset after 745 epochs is = 0.08981380440762023\n",
      "Initial Cost on Val dataset for this epoch 745 = 0.08981380440762023\n",
      "learning rate for this epoch =  0.09570418652332507\n",
      "Error on this batch = 0.04166783995079736\n",
      "Error on this batch = 0.07097694701041773\n",
      "Cost on val dataset after 746 epochs is = 0.08978865555135672\n",
      "Initial Cost on Val dataset for this epoch 746 = 0.08978865555135672\n",
      "learning rate for this epoch =  0.0956720979399305\n",
      "Error on this batch = 0.04163506913454732\n",
      "Error on this batch = 0.07093694648036179\n",
      "Cost on val dataset after 747 epochs is = 0.08976356340942003\n",
      "Initial Cost on Val dataset for this epoch 747 = 0.08976356340942003\n",
      "learning rate for this epoch =  0.0956400630792741\n",
      "Error on this batch = 0.04160236885461368\n",
      "Error on this batch = 0.07089701542151597\n",
      "Cost on val dataset after 748 epochs is = 0.08973852780703644\n",
      "Initial Cost on Val dataset for this epoch 748 = 0.08973852780703644\n",
      "learning rate for this epoch =  0.09560808177967559\n",
      "Error on this batch = 0.041569738887214526\n",
      "Error on this batch = 0.07085715350755285\n",
      "Cost on val dataset after 749 epochs is = 0.0897135485698733\n",
      "Initial Cost on Val dataset for this epoch 749 = 0.0897135485698733\n",
      "learning rate for this epoch =  0.09557615388015658\n",
      "Error on this batch = 0.04153717900935879\n",
      "Error on this batch = 0.07081736041333196\n",
      "Cost on val dataset after 750 epochs is = 0.08968862552404665\n",
      "Initial Cost on Val dataset for this epoch 750 = 0.08968862552404665\n",
      "learning rate for this epoch =  0.09554427922043668\n",
      "Error on this batch = 0.04150468899883949\n",
      "Error on this batch = 0.07077763581490512\n",
      "Cost on val dataset after 751 epochs is = 0.08966375849612848\n",
      "Initial Cost on Val dataset for this epoch 751 = 0.08966375849612848\n",
      "learning rate for this epoch =  0.0955124576409294\n",
      "Error on this batch = 0.041472268634226685\n",
      "Error on this batch = 0.07073797938952167\n",
      "Cost on val dataset after 752 epochs is = 0.08963894731315392\n",
      "Initial Cost on Val dataset for this epoch 752 = 0.08963894731315392\n",
      "learning rate for this epoch =  0.09548068898273834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on this batch = 0.04143991769486098\n",
      "Error on this batch = 0.07069839081563395\n",
      "Cost on val dataset after 753 epochs is = 0.08961419180262829\n",
      "Initial Cost on Val dataset for this epoch 753 = 0.08961419180262829\n",
      "learning rate for this epoch =  0.09544897308765321\n",
      "Error on this batch = 0.04140763596084643\n",
      "Error on this batch = 0.07065886977290335\n",
      "Cost on val dataset after 754 epochs is = 0.08958949179253375\n",
      "Initial Cost on Val dataset for this epoch 754 = 0.08958949179253375\n",
      "learning rate for this epoch =  0.09541730979814601\n",
      "Error on this batch = 0.04137542321304399\n",
      "Error on this batch = 0.07061941594220607\n",
      "Cost on val dataset after 755 epochs is = 0.08956484711133601\n",
      "Initial Cost on Val dataset for this epoch 755 = 0.08956484711133601\n",
      "learning rate for this epoch =  0.09538569895736723\n",
      "Error on this batch = 0.04134327923306444\n",
      "Error on this batch = 0.07058002900563952\n",
      "Cost on val dataset after 756 epochs is = 0.08954025758799071\n",
      "Initial Cost on Val dataset for this epoch 756 = 0.08954025758799071\n",
      "learning rate for this epoch =  0.0953541404091419\n",
      "Error on this batch = 0.0413112038032618\n",
      "Error on this batch = 0.07054070864652863\n",
      "Cost on val dataset after 757 epochs is = 0.08951572305194955\n",
      "Initial Cost on Val dataset for this epoch 757 = 0.08951572305194955\n",
      "learning rate for this epoch =  0.09532263399796595\n",
      "Error on this batch = 0.0412791967067264\n",
      "Error on this batch = 0.07050145454943238\n",
      "Cost on val dataset after 758 epochs is = 0.08949124333316641\n",
      "Initial Cost on Val dataset for this epoch 758 = 0.08949124333316641\n",
      "learning rate for this epoch =  0.09529117956900235\n",
      "Error on this batch = 0.041247257727278205\n",
      "Error on this batch = 0.07046226640015069\n",
      "Cost on val dataset after 759 epochs is = 0.08946681826210326\n",
      "Initial Cost on Val dataset for this epoch 759 = 0.08946681826210326\n",
      "learning rate for this epoch =  0.09525977696807737\n",
      "Error on this batch = 0.04121538664946004\n",
      "Error on this batch = 0.07042314388573108\n",
      "Cost on val dataset after 760 epochs is = 0.08944244766973566\n",
      "Initial Cost on Val dataset for this epoch 760 = 0.08944244766973566\n",
      "learning rate for this epoch =  0.095228426041677\n",
      "Error on this batch = 0.04118358325853081\n",
      "Error on this batch = 0.07038408669447581\n",
      "Cost on val dataset after 761 epochs is = 0.08941813138755832\n",
      "Initial Cost on Val dataset for this epoch 761 = 0.08941813138755832\n",
      "learning rate for this epoch =  0.09519712663694305\n",
      "Error on this batch = 0.04115184734045879\n",
      "Error on this batch = 0.07034509451594911\n",
      "Cost on val dataset after 762 epochs is = 0.08939386924759028\n",
      "Initial Cost on Val dataset for this epoch 762 = 0.08939386924759028\n",
      "learning rate for this epoch =  0.09516587860166968\n",
      "Error on this batch = 0.04112017868191499\n",
      "Error on this batch = 0.07030616704098423\n",
      "Cost on val dataset after 763 epochs is = 0.08936966108238015\n",
      "Initial Cost on Val dataset for this epoch 763 = 0.08936966108238015\n",
      "learning rate for this epoch =  0.09513468178429965\n",
      "Error on this batch = 0.041088577070266405\n",
      "Error on this batch = 0.07026730396169088\n",
      "Cost on val dataset after 764 epochs is = 0.08934550672501076\n",
      "Initial Cost on Val dataset for this epoch 764 = 0.08934550672501076\n",
      "learning rate for this epoch =  0.0951035360339208\n",
      "Error on this batch = 0.041057042293569455\n",
      "Error on this batch = 0.07022850497146271\n",
      "Cost on val dataset after 765 epochs is = 0.08932140600910411\n",
      "Initial Cost on Val dataset for this epoch 765 = 0.08932140600910411\n",
      "learning rate for this epoch =  0.09507244120026234\n",
      "Error on this batch = 0.04102557414056335\n",
      "Error on this batch = 0.07018976976498484\n",
      "Cost on val dataset after 766 epochs is = 0.08929735876882562\n",
      "Initial Cost on Val dataset for this epoch 766 = 0.08929735876882562\n",
      "learning rate for this epoch =  0.09504139713369145\n",
      "Error on this batch = 0.04099417240066347\n",
      "Error on this batch = 0.07015109803824122\n",
      "Cost on val dataset after 767 epochs is = 0.08927336483888863\n",
      "Initial Cost on Val dataset for this epoch 767 = 0.08927336483888863\n",
      "learning rate for this epoch =  0.09501040368520958\n",
      "Error on this batch = 0.04096283686395495\n",
      "Error on this batch = 0.07011248948852256\n",
      "Cost on val dataset after 768 epochs is = 0.08924942405455848\n",
      "Initial Cost on Val dataset for this epoch 768 = 0.08924942405455848\n",
      "learning rate for this epoch =  0.09497946070644907\n",
      "Error on this batch = 0.04093156732118608\n",
      "Error on this batch = 0.07007394381443369\n",
      "Cost on val dataset after 769 epochs is = 0.08922553625165629\n",
      "Initial Cost on Val dataset for this epoch 769 = 0.08922553625165629\n",
      "learning rate for this epoch =  0.09494856804966957\n",
      "Error on this batch = 0.04090036356376194\n",
      "Error on this batch = 0.07003546071590154\n",
      "Cost on val dataset after 770 epochs is = 0.08920170126656288\n",
      "Initial Cost on Val dataset for this epoch 770 = 0.08920170126656288\n",
      "learning rate for this epoch =  0.09491772556775467\n",
      "Error on this batch = 0.040869225383737925\n",
      "Error on this batch = 0.06999703989418257\n",
      "Cost on val dataset after 771 epochs is = 0.08917791893622228\n",
      "Initial Cost on Val dataset for this epoch 771 = 0.08917791893622228\n",
      "learning rate for this epoch =  0.09488693311420834\n",
      "Error on this batch = 0.04083815257381346\n",
      "Error on this batch = 0.06995868105187059\n",
      "Cost on val dataset after 772 epochs is = 0.08915418909814497\n",
      "Initial Cost on Val dataset for this epoch 772 = 0.08915418909814497\n",
      "learning rate for this epoch =  0.0948561905431516\n",
      "Error on this batch = 0.040807144927325734\n",
      "Error on this batch = 0.06992038389290439\n",
      "Cost on val dataset after 773 epochs is = 0.0891305115904112\n",
      "Initial Cost on Val dataset for this epoch 773 = 0.0891305115904112\n",
      "learning rate for this epoch =  0.09482549770931908\n",
      "Error on this batch = 0.040776202238243346\n",
      "Error on this batch = 0.06988214812257565\n",
      "Cost on val dataset after 774 epochs is = 0.08910688625167382\n",
      "Initial Cost on Val dataset for this epoch 774 = 0.08910688625167382\n",
      "learning rate for this epoch =  0.09479485446805574\n",
      "Error on this batch = 0.040745324301160304\n",
      "Error on this batch = 0.06984397344753612\n",
      "Cost on val dataset after 775 epochs is = 0.0890833129211612\n",
      "Initial Cost on Val dataset for this epoch 775 = 0.0890833129211612\n",
      "learning rate for this epoch =  0.09476426067531338\n",
      "Error on this batch = 0.04071451091128976\n",
      "Error on this batch = 0.06980585957580564\n",
      "Cost on val dataset after 776 epochs is = 0.08905979143867983\n",
      "Initial Cost on Val dataset for this epoch 776 = 0.08905979143867983\n",
      "learning rate for this epoch =  0.0947337161876474\n",
      "Error on this batch = 0.04068376186445804\n",
      "Error on this batch = 0.06976780621677948\n",
      "Cost on val dataset after 777 epochs is = 0.08903632164461654\n",
      "Initial Cost on Val dataset for this epoch 777 = 0.08903632164461654\n",
      "learning rate for this epoch =  0.09470322086221351\n",
      "Error on this batch = 0.04065307695709871\n",
      "Error on this batch = 0.06972981308123577\n",
      "Cost on val dataset after 778 epochs is = 0.08901290337994106\n",
      "Initial Cost on Val dataset for this epoch 778 = 0.08901290337994106\n",
      "learning rate for this epoch =  0.09467277455676439\n",
      "Error on this batch = 0.040622455986246545\n",
      "Error on this batch = 0.06969187988134304\n",
      "Cost on val dataset after 779 epochs is = 0.08898953648620772\n",
      "Initial Cost on Val dataset for this epoch 779 = 0.08898953648620772\n",
      "learning rate for this epoch =  0.0946423771296465\n",
      "Error on this batch = 0.040591898749531834\n",
      "Error on this batch = 0.06965400633066741\n",
      "Cost on val dataset after 780 epochs is = 0.0889662208055577\n",
      "Initial Cost on Val dataset for this epoch 780 = 0.0889662208055577\n",
      "learning rate for this epoch =  0.09461202843979676\n",
      "Error on this batch = 0.040561405045174616\n",
      "Error on this batch = 0.06961619214417986\n",
      "Cost on val dataset after 781 epochs is = 0.08894295618072033\n",
      "Initial Cost on Val dataset for this epoch 781 = 0.08894295618072033\n",
      "learning rate for this epoch =  0.09458172834673945\n",
      "Error on this batch = 0.040530974671979025\n",
      "Error on this batch = 0.06957843703826334\n",
      "Cost on val dataset after 782 epochs is = 0.08891974245501494\n",
      "Initial Cost on Val dataset for this epoch 782 = 0.08891974245501494\n",
      "learning rate for this epoch =  0.09455147671058287\n",
      "Error on this batch = 0.040500607429327616\n",
      "Error on this batch = 0.06954074073071977\n",
      "Cost on val dataset after 783 epochs is = 0.088896579472352\n",
      "Initial Cost on Val dataset for this epoch 783 = 0.088896579472352\n",
      "learning rate for this epoch =  0.09452127339201631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on this batch = 0.04047030311717604\n",
      "Error on this batch = 0.06950310294077687\n",
      "Cost on val dataset after 784 epochs is = 0.08887346707723427\n",
      "Initial Cost on Val dataset for this epoch 784 = 0.08887346707723427\n",
      "learning rate for this epoch =  0.0944911182523068\n",
      "Error on this batch = 0.04044006153604761\n",
      "Error on this batch = 0.06946552338909504\n",
      "Cost on val dataset after 785 epochs is = 0.08885040511475797\n",
      "Initial Cost on Val dataset for this epoch 785 = 0.08885040511475797\n",
      "learning rate for this epoch =  0.09446101115329605\n",
      "Error on this batch = 0.040409882487027884\n",
      "Error on this batch = 0.06942800179777377\n",
      "Cost on val dataset after 786 epochs is = 0.08882739343061337\n",
      "Initial Cost on Val dataset for this epoch 786 = 0.08882739343061337\n",
      "learning rate for this epoch =  0.09443095195739727\n",
      "Error on this batch = 0.04037976577175965\n",
      "Error on this batch = 0.06939053789035826\n",
      "Cost on val dataset after 787 epochs is = 0.08880443187108558\n",
      "Initial Cost on Val dataset for this epoch 787 = 0.08880443187108558\n",
      "learning rate for this epoch =  0.09440094052759214\n",
      "Error on this batch = 0.04034971119243771\n",
      "Error on this batch = 0.06935313139184567\n",
      "Cost on val dataset after 788 epochs is = 0.08878152028305504\n",
      "Initial Cost on Val dataset for this epoch 788 = 0.08878152028305504\n",
      "learning rate for this epoch =  0.09437097672742772\n",
      "Error on this batch = 0.040319718551804044\n",
      "Error on this batch = 0.06931578202869129\n",
      "Cost on val dataset after 789 epochs is = 0.08875865851399771\n",
      "Initial Cost on Val dataset for this epoch 789 = 0.08875865851399771\n",
      "learning rate for this epoch =  0.09434106042101341\n",
      "Error on this batch = 0.04028978765314269\n",
      "Error on this batch = 0.06927848952881449\n",
      "Cost on val dataset after 790 epochs is = 0.08873584641198538\n",
      "Initial Cost on Val dataset for this epoch 790 = 0.08873584641198538\n",
      "learning rate for this epoch =  0.09431119147301793\n",
      "Error on this batch = 0.040259918300275345\n",
      "Error on this batch = 0.06924125362160446\n",
      "Cost on val dataset after 791 epochs is = 0.08871308382568557\n",
      "Initial Cost on Val dataset for this epoch 791 = 0.08871308382568557\n",
      "learning rate for this epoch =  0.09428136974866627\n",
      "Error on this batch = 0.04023011029755624\n",
      "Error on this batch = 0.0692040740379261\n",
      "Cost on val dataset after 792 epochs is = 0.08869037060436133\n",
      "Initial Cost on Val dataset for this epoch 792 = 0.08869037060436133\n",
      "learning rate for this epoch =  0.09425159511373676\n",
      "Error on this batch = 0.04020036344986811\n",
      "Error on this batch = 0.06916695051012506\n",
      "Cost on val dataset after 793 epochs is = 0.08866770659787111\n",
      "Initial Cost on Val dataset for this epoch 793 = 0.08866770659787111\n",
      "learning rate for this epoch =  0.09422186743455808\n",
      "Error on this batch = 0.040170677562617385\n",
      "Error on this batch = 0.06912988277203332\n",
      "Cost on val dataset after 794 epochs is = 0.08864509165666802\n",
      "Initial Cost on Val dataset for this epoch 794 = 0.08864509165666802\n",
      "learning rate for this epoch =  0.0941921865780063\n",
      "Error on this batch = 0.04014105244173012\n",
      "Error on this batch = 0.06909287055897409\n",
      "Cost on val dataset after 795 epochs is = 0.08862252563179947\n",
      "Initial Cost on Val dataset for this epoch 795 = 0.08862252563179947\n",
      "learning rate for this epoch =  0.09416255241150198\n",
      "Error on this batch = 0.04011148789364784\n",
      "Error on this batch = 0.0690559136077667\n",
      "Cost on val dataset after 796 epochs is = 0.08860000837490625\n",
      "Initial Cost on Val dataset for this epoch 796 = 0.08860000837490625\n",
      "learning rate for this epoch =  0.09413296480300724\n",
      "Error on this batch = 0.04008198372532334\n",
      "Error on this batch = 0.06901901165673127\n",
      "Cost on val dataset after 797 epochs is = 0.08857753973822169\n",
      "Initial Cost on Val dataset for this epoch 797 = 0.08857753973822169\n",
      "learning rate for this epoch =  0.09410342362102285\n",
      "Error on this batch = 0.040052539744216915\n",
      "Error on this batch = 0.06898216444569306\n",
      "Cost on val dataset after 798 epochs is = 0.08855511957457056\n",
      "Initial Cost on Val dataset for this epoch 798 = 0.08855511957457056\n",
      "learning rate for this epoch =  0.09407392873458542\n",
      "Error on this batch = 0.04002315575829262\n",
      "Error on this batch = 0.06894537171598683\n",
      "Cost on val dataset after 799 epochs is = 0.088532747737368\n",
      "Initial Cost on Val dataset for this epoch 799 = 0.088532747737368\n",
      "learning rate for this epoch =  0.09404448001326453\n",
      "Error on this batch = 0.039993831576014466\n",
      "Error on this batch = 0.06890863321046073\n",
      "Cost on val dataset after 800 epochs is = 0.08851042408061807\n",
      "Initial Cost on Val dataset for this epoch 800 = 0.08851042408061807\n",
      "learning rate for this epoch =  0.09401507732715984\n",
      "Error on this batch = 0.03996456700634311\n",
      "Error on this batch = 0.06887194867348022\n",
      "Cost on val dataset after 801 epochs is = 0.08848814845891247\n",
      "Initial Cost on Val dataset for this epoch 801 = 0.08848814845891247\n",
      "learning rate for this epoch =  0.09398572054689833\n",
      "Error on this batch = 0.039935361858732386\n",
      "Error on this batch = 0.06883531785093147\n",
      "Cost on val dataset after 802 epochs is = 0.08846592072742879\n",
      "Initial Cost on Val dataset for this epoch 802 = 0.08846592072742879\n",
      "learning rate for this epoch =  0.09395640954363149\n",
      "Error on this batch = 0.039906215943126114\n",
      "Error on this batch = 0.06879874049022497\n",
      "Cost on val dataset after 803 epochs is = 0.08844374074192896\n",
      "Initial Cost on Val dataset for this epoch 803 = 0.08844374074192896\n",
      "learning rate for this epoch =  0.09392714418903259\n",
      "Error on this batch = 0.03987712906995502\n",
      "Error on this batch = 0.06876221634029843\n",
      "Cost on val dataset after 804 epochs is = 0.08842160835875745\n",
      "Initial Cost on Val dataset for this epoch 804 = 0.08842160835875745\n",
      "learning rate for this epoch =  0.0938979243552938\n",
      "Error on this batch = 0.03984810105013385\n",
      "Error on this batch = 0.06872574515161972\n",
      "Cost on val dataset after 805 epochs is = 0.08839952343483923\n",
      "Initial Cost on Val dataset for this epoch 805 = 0.08839952343483923\n",
      "learning rate for this epoch =  0.0938687499151236\n",
      "Error on this batch = 0.0398191316950585\n",
      "Error on this batch = 0.06868932667618984\n",
      "Cost on val dataset after 806 epochs is = 0.08837748582767785\n",
      "Initial Cost on Val dataset for this epoch 806 = 0.08837748582767785\n",
      "learning rate for this epoch =  0.09383962074174393\n",
      "Error on this batch = 0.039790220816603404\n",
      "Error on this batch = 0.06865296066754495\n",
      "Cost on val dataset after 807 epochs is = 0.0883554953953533\n",
      "Initial Cost on Val dataset for this epoch 807 = 0.0883554953953533\n",
      "learning rate for this epoch =  0.09381053670888753\n",
      "Error on this batch = 0.039761368227119176\n",
      "Error on this batch = 0.06861664688075894\n",
      "Cost on val dataset after 808 epochs is = 0.08833355199651968\n",
      "Initial Cost on Val dataset for this epoch 808 = 0.08833355199651968\n",
      "learning rate for this epoch =  0.09378149769079532\n",
      "Error on this batch = 0.039732573739430066\n",
      "Error on this batch = 0.06858038507244528\n",
      "Cost on val dataset after 809 epochs is = 0.08831165549040296\n",
      "Initial Cost on Val dataset for this epoch 809 = 0.08831165549040296\n",
      "learning rate for this epoch =  0.09375250356221361\n",
      "Error on this batch = 0.0397038371668319\n",
      "Error on this batch = 0.06854417500075885\n",
      "Cost on val dataset after 810 epochs is = 0.0882898057367985\n",
      "Initial Cost on Val dataset for this epoch 810 = 0.0882898057367985\n",
      "learning rate for this epoch =  0.09372355419839151\n",
      "Error on this batch = 0.039675158323090105\n",
      "Error on this batch = 0.06850801642539744\n",
      "Cost on val dataset after 811 epochs is = 0.08826800259606855\n",
      "Initial Cost on Val dataset for this epoch 811 = 0.08826800259606855\n",
      "learning rate for this epoch =  0.09369464947507833\n",
      "Error on this batch = 0.03964653702243766\n",
      "Error on this batch = 0.06847190910760333\n",
      "Cost on val dataset after 812 epochs is = 0.0882462459291397\n",
      "Initial Cost on Val dataset for this epoch 812 = 0.0882462459291397\n",
      "learning rate for this epoch =  0.09366578926852086\n",
      "Error on this batch = 0.039617973079573494\n",
      "Error on this batch = 0.06843585281016402\n",
      "Cost on val dataset after 813 epochs is = 0.08822453559750003\n",
      "Initial Cost on Val dataset for this epoch 813 = 0.08822453559750003\n",
      "learning rate for this epoch =  0.09363697345546085\n",
      "Error on this batch = 0.039589466309660884\n",
      "Error on this batch = 0.06839984729741354\n",
      "Cost on val dataset after 814 epochs is = 0.08820287146319652\n",
      "Initial Cost on Val dataset for this epoch 814 = 0.08820287146319652\n",
      "learning rate for this epoch =  0.09360820191313243\n",
      "Error on this batch = 0.03956101652832604\n",
      "Error on this batch = 0.06836389233523275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on val dataset after 815 epochs is = 0.08818125338883208\n",
      "Initial Cost on Val dataset for this epoch 815 = 0.08818125338883208\n",
      "learning rate for this epoch =  0.09357947451925948\n",
      "Error on this batch = 0.03953262355165684\n",
      "Error on this batch = 0.06832798769105017\n",
      "Cost on val dataset after 816 epochs is = 0.08815968123756283\n",
      "Initial Cost on Val dataset for this epoch 816 = 0.08815968123756283\n",
      "learning rate for this epoch =  0.09355079115205313\n",
      "Error on this batch = 0.03950428719620174\n",
      "Error on this batch = 0.06829213313384193\n",
      "Cost on val dataset after 817 epochs is = 0.08813815487309483\n",
      "Initial Cost on Val dataset for this epoch 817 = 0.08813815487309483\n",
      "learning rate for this epoch =  0.09352215169020917\n",
      "Error on this batch = 0.039476007278968786\n",
      "Error on this batch = 0.068256328434132\n",
      "Cost on val dataset after 818 epochs is = 0.08811667415968136\n",
      "Initial Cost on Val dataset for this epoch 818 = 0.08811667415968136\n",
      "learning rate for this epoch =  0.09349355601290561\n",
      "Error on this batch = 0.03944778361742489\n",
      "Error on this batch = 0.068220573363992\n",
      "Cost on val dataset after 819 epochs is = 0.08809523896211963\n",
      "Initial Cost on Val dataset for this epoch 819 = 0.08809523896211963\n",
      "learning rate for this epoch =  0.09346500399980012\n",
      "Error on this batch = 0.03941961602949511\n",
      "Error on this batch = 0.06818486769704075\n",
      "Cost on val dataset after 820 epochs is = 0.08807384914574774\n",
      "Initial Cost on Val dataset for this epoch 820 = 0.08807384914574774\n",
      "learning rate for this epoch =  0.09343649553102754\n",
      "Error on this batch = 0.03939150433356216\n",
      "Error on this batch = 0.06814921120844382\n",
      "Cost on val dataset after 821 epochs is = 0.08805250457644148\n",
      "Initial Cost on Val dataset for this epoch 821 = 0.08805250457644148\n",
      "learning rate for this epoch =  0.0934080304871974\n",
      "Error on this batch = 0.0393634483484662\n",
      "Error on this batch = 0.06811360367491269\n",
      "Cost on val dataset after 822 epochs is = 0.08803120512061102\n",
      "Initial Cost on Val dataset for this epoch 822 = 0.08803120512061102\n",
      "learning rate for this epoch =  0.09337960874939158\n",
      "Error on this batch = 0.0393354478935045\n",
      "Error on this batch = 0.06807804487470377\n",
      "Cost on val dataset after 823 epochs is = 0.08800995064519783\n",
      "Initial Cost on Val dataset for this epoch 823 = 0.08800995064519783\n",
      "learning rate for this epoch =  0.09335123019916165\n",
      "Error on this batch = 0.03930750278843154\n",
      "Error on this batch = 0.06804253458761741\n",
      "Cost on val dataset after 824 epochs is = 0.0879887410176712\n",
      "Initial Cost on Val dataset for this epoch 824 = 0.0879887410176712\n",
      "learning rate for this epoch =  0.09332289471852671\n",
      "Error on this batch = 0.039279612853458976\n",
      "Error on this batch = 0.06800707259499632\n",
      "Cost on val dataset after 825 epochs is = 0.08796757610602497\n",
      "Initial Cost on Val dataset for this epoch 825 = 0.08796757610602497\n",
      "learning rate for this epoch =  0.09329460218997072\n",
      "Error on this batch = 0.039251777909256054\n",
      "Error on this batch = 0.06797165867972421\n",
      "Cost on val dataset after 826 epochs is = 0.08794645577877423\n",
      "Initial Cost on Val dataset for this epoch 826 = 0.08794645577877423\n",
      "learning rate for this epoch =  0.09326635249644033\n",
      "Error on this batch = 0.03922399777695001\n",
      "Error on this batch = 0.06793629262622412\n",
      "Cost on val dataset after 827 epochs is = 0.08792537990495194\n",
      "Initial Cost on Val dataset for this epoch 827 = 0.08792537990495194\n",
      "learning rate for this epoch =  0.09323814552134235\n",
      "Error on this batch = 0.039196272278126526\n",
      "Error on this batch = 0.06790097422045649\n",
      "Cost on val dataset after 828 epochs is = 0.0879043483541054\n",
      "Initial Cost on Val dataset for this epoch 828 = 0.0879043483541054\n",
      "learning rate for this epoch =  0.09320998114854143\n",
      "Error on this batch = 0.03916860123483055\n",
      "Error on this batch = 0.0678657032499171\n",
      "Cost on val dataset after 829 epochs is = 0.08788336099629304\n",
      "Initial Cost on Val dataset for this epoch 829 = 0.08788336099629304\n",
      "learning rate for this epoch =  0.09318185926235777\n",
      "Error on this batch = 0.03914098446956709\n",
      "Error on this batch = 0.06783047950363494\n",
      "Cost on val dataset after 830 epochs is = 0.08786241770208086\n",
      "Initial Cost on Val dataset for this epoch 830 = 0.08786241770208086\n",
      "learning rate for this epoch =  0.09315377974756468\n",
      "Error on this batch = 0.03911342180530222\n",
      "Error on this batch = 0.06779530277216983\n",
      "Cost on val dataset after 831 epochs is = 0.08784151834253917\n",
      "Initial Cost on Val dataset for this epoch 831 = 0.08784151834253917\n",
      "learning rate for this epoch =  0.09312574248938638\n",
      "Error on this batch = 0.03908591306546421\n",
      "Error on this batch = 0.06776017284760981\n",
      "Cost on val dataset after 832 epochs is = 0.08782066278923899\n",
      "Initial Cost on Val dataset for this epoch 832 = 0.08782066278923899\n",
      "learning rate for this epoch =  0.0930977473734956\n",
      "Error on this batch = 0.03905845807394478\n",
      "Error on this batch = 0.06772508952356868\n",
      "Cost on val dataset after 833 epochs is = 0.08779985091424869\n",
      "Initial Cost on Val dataset for this epoch 833 = 0.08779985091424869\n",
      "learning rate for this epoch =  0.09306979428601131\n",
      "Error on this batch = 0.03903105665510048\n",
      "Error on this batch = 0.067690052595183\n",
      "Cost on val dataset after 834 epochs is = 0.0877790825901307\n",
      "Initial Cost on Val dataset for this epoch 834 = 0.0877790825901307\n",
      "learning rate for this epoch =  0.0930418831134965\n",
      "Error on this batch = 0.039003708633754235\n",
      "Error on this batch = 0.06765506185910918\n",
      "Cost on val dataset after 835 epochs is = 0.087758357689938\n",
      "Initial Cost on Val dataset for this epoch 835 = 0.087758357689938\n",
      "learning rate for this epoch =  0.09301401374295587\n",
      "Error on this batch = 0.03897641383519706\n",
      "Error on this batch = 0.06762011711352042\n",
      "Cost on val dataset after 836 epochs is = 0.08773767608721071\n",
      "Initial Cost on Val dataset for this epoch 836 = 0.08773767608721071\n",
      "learning rate for this epoch =  0.09298618606183358\n",
      "Error on this batch = 0.0389491720851897\n",
      "Error on this batch = 0.06758521815810353\n",
      "Cost on val dataset after 837 epochs is = 0.08771703765597284\n",
      "Initial Cost on Val dataset for this epoch 837 = 0.08771703765597284\n",
      "learning rate for this epoch =  0.09295839995801103\n",
      "Error on this batch = 0.03892198320996464\n",
      "Error on this batch = 0.06755036479405549\n",
      "Cost on val dataset after 838 epochs is = 0.08769644227072881\n",
      "Initial Cost on Val dataset for this epoch 838 = 0.08769644227072881\n",
      "learning rate for this epoch =  0.09293065531980463\n",
      "Error on this batch = 0.03889484703622809\n",
      "Error on this batch = 0.06751555682408002\n",
      "Cost on val dataset after 839 epochs is = 0.0876758898064602\n",
      "Initial Cost on Val dataset for this epoch 839 = 0.0876758898064602\n",
      "learning rate for this epoch =  0.09290295203596367\n",
      "Error on this batch = 0.038867763391162116\n",
      "Error on this batch = 0.06748079405238402\n",
      "Cost on val dataset after 840 epochs is = 0.08765538013862251\n",
      "Initial Cost on Val dataset for this epoch 840 = 0.08765538013862251\n",
      "learning rate for this epoch =  0.09287528999566799\n",
      "Error on this batch = 0.038840732102426896\n",
      "Error on this batch = 0.06744607628467378\n",
      "Cost on val dataset after 841 epochs is = 0.0876349131431418\n",
      "Initial Cost on Val dataset for this epoch 841 = 0.0876349131431418\n",
      "learning rate for this epoch =  0.09284766908852593\n",
      "Error on this batch = 0.03881375299816305\n",
      "Error on this batch = 0.06741140332815135\n",
      "Cost on val dataset after 842 epochs is = 0.08761448869641149\n",
      "Initial Cost on Val dataset for this epoch 842 = 0.08761448869641149\n",
      "learning rate for this epoch =  0.09282008920457209\n",
      "Error on this batch = 0.038786825906994116\n",
      "Error on this batch = 0.0673767749915104\n",
      "Cost on val dataset after 843 epochs is = 0.08759410667528916\n",
      "Initial Cost on Val dataset for this epoch 843 = 0.08759410667528916\n",
      "learning rate for this epoch =  0.09279255023426522\n",
      "Error on this batch = 0.03875995065802916\n",
      "Error on this batch = 0.06734219108493235\n",
      "Cost on val dataset after 844 epochs is = 0.08757376695709346\n",
      "Initial Cost on Val dataset for this epoch 844 = 0.08757376695709346\n",
      "learning rate for this epoch =  0.09276505206848605\n",
      "Error on this batch = 0.03873312708086528\n",
      "Error on this batch = 0.06730765142008233\n",
      "Cost on val dataset after 845 epochs is = 0.08755346941960086\n",
      "Initial Cost on Val dataset for this epoch 845 = 0.08755346941960086\n",
      "learning rate for this epoch =  0.09273759459853521\n",
      "Error on this batch = 0.03870635500559049\n",
      "Error on this batch = 0.06727315581010476\n",
      "Cost on val dataset after 846 epochs is = 0.08753321394104278\n",
      "Initial Cost on Val dataset for this epoch 846 = 0.08753321394104278\n",
      "learning rate for this epoch =  0.0927101777161311\n",
      "Error on this batch = 0.038679634262786575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on this batch = 0.06723870406961922\n",
      "Cost on val dataset after 847 epochs is = 0.08751300040010236\n",
      "Initial Cost on Val dataset for this epoch 847 = 0.08751300040010236\n",
      "learning rate for this epoch =  0.09268280131340775\n",
      "Error on this batch = 0.03865296468353184\n",
      "Error on this batch = 0.06720429601471622\n",
      "Cost on val dataset after 848 epochs is = 0.08749282867591164\n",
      "Initial Cost on Val dataset for this epoch 848 = 0.08749282867591164\n",
      "learning rate for this epoch =  0.09265546528291284\n",
      "Error on this batch = 0.03862634609940432\n",
      "Error on this batch = 0.0671699314629524\n",
      "Cost on val dataset after 849 epochs is = 0.0874726986480486\n",
      "Initial Cost on Val dataset for this epoch 849 = 0.0874726986480486\n",
      "learning rate for this epoch =  0.09262816951760545\n",
      "Error on this batch = 0.03859977834248471\n",
      "Error on this batch = 0.06713561023334641\n",
      "Cost on val dataset after 850 epochs is = 0.08745261019653422\n",
      "Initial Cost on Val dataset for this epoch 850 = 0.08745261019653422\n",
      "learning rate for this epoch =  0.09260091391085426\n",
      "Error on this batch = 0.03857326124535952\n",
      "Error on this batch = 0.06710133214637401\n",
      "Cost on val dataset after 851 epochs is = 0.08743256320182978\n",
      "Initial Cost on Val dataset for this epoch 851 = 0.08743256320182978\n",
      "learning rate for this epoch =  0.09257369835643524\n",
      "Error on this batch = 0.038546794641124336\n",
      "Error on this batch = 0.06706709702396363\n",
      "Cost on val dataset after 852 epochs is = 0.08741255754483403\n",
      "Initial Cost on Val dataset for this epoch 852 = 0.08741255754483403\n",
      "learning rate for this epoch =  0.09254652274852981\n",
      "Error on this batch = 0.03852037836338717\n",
      "Error on this batch = 0.06703290468949157\n",
      "Cost on val dataset after 853 epochs is = 0.08739259310688055\n",
      "Initial Cost on Val dataset for this epoch 853 = 0.08739259310688055\n",
      "learning rate for this epoch =  0.0925193869817227\n",
      "Error on this batch = 0.038494012246271574\n",
      "Error on this batch = 0.06699875496777727\n",
      "Cost on val dataset after 854 epochs is = 0.08737266976973504\n",
      "Initial Cost on Val dataset for this epoch 854 = 0.08737266976973504\n",
      "learning rate for this epoch =  0.092492290951\n",
      "Error on this batch = 0.038467696124420254\n",
      "Error on this batch = 0.06696464768507852\n",
      "Cost on val dataset after 855 epochs is = 0.08735278741559288\n",
      "Initial Cost on Val dataset for this epoch 855 = 0.08735278741559288\n",
      "learning rate for this epoch =  0.09246523455174717\n",
      "Error on this batch = 0.03844142983299828\n",
      "Error on this batch = 0.06693058266908648\n",
      "Cost on val dataset after 856 epochs is = 0.08733294592707642\n",
      "Initial Cost on Val dataset for this epoch 856 = 0.08733294592707642\n",
      "learning rate for this epoch =  0.092438217679747\n",
      "Error on this batch = 0.038415213207696794\n",
      "Error on this batch = 0.06689655974892088\n",
      "Cost on val dataset after 857 epochs is = 0.08731314518723275\n",
      "Initial Cost on Val dataset for this epoch 857 = 0.08731314518723275\n",
      "learning rate for this epoch =  0.09241124023117771\n",
      "Error on this batch = 0.03838904608473629\n",
      "Error on this batch = 0.06686257875512495\n",
      "Cost on val dataset after 858 epochs is = 0.08729338507953122\n",
      "Initial Cost on Val dataset for this epoch 858 = 0.08729338507953122\n",
      "learning rate for this epoch =  0.09238430210261095\n",
      "Error on this batch = 0.038362928300870285\n",
      "Error on this batch = 0.06682863951966056\n",
      "Cost on val dataset after 859 epochs is = 0.08727366548786109\n",
      "Initial Cost on Val dataset for this epoch 859 = 0.08727366548786109\n",
      "learning rate for this epoch =  0.09235740319100984\n",
      "Error on this batch = 0.03833685969338899\n",
      "Error on this batch = 0.06679474187590308\n",
      "Cost on val dataset after 860 epochs is = 0.08725398629652933\n",
      "Initial Cost on Val dataset for this epoch 860 = 0.08725398629652933\n",
      "learning rate for this epoch =  0.09233054339372707\n",
      "Error on this batch = 0.0383108401001226\n",
      "Error on this batch = 0.06676088565863626\n",
      "Cost on val dataset after 861 epochs is = 0.08723434739025847\n",
      "Initial Cost on Val dataset for this epoch 861 = 0.08723434739025847\n",
      "learning rate for this epoch =  0.09230372260850297\n",
      "Error on this batch = 0.03828486935944519\n",
      "Error on this batch = 0.06672707070404732\n",
      "Cost on val dataset after 862 epochs is = 0.08721474865418449\n",
      "Initial Cost on Val dataset for this epoch 862 = 0.08721474865418449\n",
      "learning rate for this epoch =  0.09227694073346356\n",
      "Error on this batch = 0.03825894731027817\n",
      "Error on this batch = 0.06669329684972156\n",
      "Cost on val dataset after 863 epochs is = 0.08719518997385474\n",
      "Initial Cost on Val dataset for this epoch 863 = 0.08719518997385474\n",
      "learning rate for this epoch =  0.09225019766711869\n",
      "Error on this batch = 0.03823307379209405\n",
      "Error on this batch = 0.06665956393463736\n",
      "Cost on val dataset after 864 epochs is = 0.08717567123522593\n",
      "Initial Cost on Val dataset for this epoch 864 = 0.08717567123522593\n",
      "learning rate for this epoch =  0.09222349330836013\n",
      "Error on this batch = 0.038207248644919874\n",
      "Error on this batch = 0.066625871799161\n",
      "Cost on val dataset after 865 epochs is = 0.08715619232466244\n",
      "Initial Cost on Val dataset for this epoch 865 = 0.08715619232466244\n",
      "learning rate for this epoch =  0.09219682755645973\n",
      "Error on this batch = 0.03818147170934104\n",
      "Error on this batch = 0.06659222028504135\n",
      "Cost on val dataset after 866 epochs is = 0.08713675312893422\n",
      "Initial Cost on Val dataset for this epoch 866 = 0.08713675312893422\n",
      "learning rate for this epoch =  0.09217020031106751\n",
      "Error on this batch = 0.03815574282650482\n",
      "Error on this batch = 0.06655860923540476\n",
      "Cost on val dataset after 867 epochs is = 0.08711735353521519\n",
      "Initial Cost on Val dataset for this epoch 867 = 0.08711735353521519\n",
      "learning rate for this epoch =  0.09214361147220981\n",
      "Error on this batch = 0.03813006183812395\n",
      "Error on this batch = 0.06652503849474962\n",
      "Cost on val dataset after 868 epochs is = 0.08709799343108154\n",
      "Initial Cost on Val dataset for this epoch 868 = 0.08709799343108154\n",
      "learning rate for this epoch =  0.09211706094028746\n",
      "Error on this batch = 0.03810442858648034\n",
      "Error on this batch = 0.06649150790894137\n",
      "Cost on val dataset after 869 epochs is = 0.08707867270451011\n",
      "Initial Cost on Val dataset for this epoch 869 = 0.08707867270451011\n",
      "learning rate for this epoch =  0.09209054861607395\n",
      "Error on this batch = 0.03807884291442841\n",
      "Error on this batch = 0.06645801732520697\n",
      "Cost on val dataset after 870 epochs is = 0.0870593912438768\n",
      "Initial Cost on Val dataset for this epoch 870 = 0.0870593912438768\n",
      "learning rate for this epoch =  0.0920640744007136\n",
      "Error on this batch = 0.03805330466539886\n",
      "Error on this batch = 0.06642456659212978\n",
      "Cost on val dataset after 871 epochs is = 0.08704014893795506\n",
      "Initial Cost on Val dataset for this epoch 871 = 0.08704014893795506\n",
      "learning rate for this epoch =  0.09203763819571976\n",
      "Error on this batch = 0.03802781368340197\n",
      "Error on this batch = 0.06639115555964412\n",
      "Cost on val dataset after 872 epochs is = 0.08702094567591466\n",
      "Initial Cost on Val dataset for this epoch 872 = 0.08702094567591466\n",
      "learning rate for this epoch =  0.09201123990297302\n",
      "Error on this batch = 0.03800236981303125\n",
      "Error on this batch = 0.06635778407903006\n",
      "Cost on val dataset after 873 epochs is = 0.0870017813473201\n",
      "Initial Cost on Val dataset for this epoch 873 = 0.0870017813473201\n",
      "learning rate for this epoch =  0.09198487942471935\n",
      "Error on this batch = 0.03797697289946667\n",
      "Error on this batch = 0.06632445200290807\n",
      "Cost on val dataset after 874 epochs is = 0.0869826558421295\n",
      "Initial Cost on Val dataset for this epoch 874 = 0.0869826558421295\n",
      "learning rate for this epoch =  0.09195855666356846\n",
      "Error on this batch = 0.03795162278847816\n",
      "Error on this batch = 0.06629115918523372\n",
      "Cost on val dataset after 875 epochs is = 0.08696356905069343\n",
      "Initial Cost on Val dataset for this epoch 875 = 0.08696356905069343\n",
      "learning rate for this epoch =  0.09193227152249185\n",
      "Error on this batch = 0.037926319326428856\n",
      "Error on this batch = 0.06625790548129225\n",
      "Cost on val dataset after 876 epochs is = 0.0869445208637535\n",
      "Initial Cost on Val dataset for this epoch 876 = 0.0869445208637535\n",
      "learning rate for this epoch =  0.09190602390482124\n",
      "Error on this batch = 0.037901062360278355\n",
      "Error on this batch = 0.0662246907476933\n",
      "Cost on val dataset after 877 epochs is = 0.08692551117244164\n",
      "Initial Cost on Val dataset for this epoch 877 = 0.08692551117244164\n",
      "learning rate for this epoch =  0.09187981371424671\n",
      "Error on this batch = 0.03787585173758593\n",
      "Error on this batch = 0.06619151484236561\n",
      "Cost on val dataset after 878 epochs is = 0.08690653986827883\n",
      "Initial Cost on Val dataset for this epoch 878 = 0.08690653986827883\n",
      "learning rate for this epoch =  0.091853640854815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on this batch = 0.03785068730651366\n",
      "Error on this batch = 0.06615837762455161\n",
      "Cost on val dataset after 879 epochs is = 0.08688760684317419\n",
      "Initial Cost on Val dataset for this epoch 879 = 0.08688760684317419\n",
      "learning rate for this epoch =  0.09182750523092773\n",
      "Error on this batch = 0.03782556891582947\n",
      "Error on this batch = 0.06612527895480211\n",
      "Cost on val dataset after 880 epochs is = 0.08686871198942425\n",
      "Initial Cost on Val dataset for this epoch 880 = 0.08686871198942425\n",
      "learning rate for this epoch =  0.0918014067473398\n",
      "Error on this batch = 0.037800496414910026\n",
      "Error on this batch = 0.06609221869497085\n",
      "Cost on val dataset after 881 epochs is = 0.08684985519971189\n",
      "Initial Cost on Val dataset for this epoch 881 = 0.08684985519971189\n",
      "learning rate for this epoch =  0.09177534530915758\n",
      "Error on this batch = 0.037775469653743746\n",
      "Error on this batch = 0.06605919670820934\n",
      "Cost on val dataset after 882 epochs is = 0.08683103636710578\n",
      "Initial Cost on Val dataset for this epoch 882 = 0.08683103636710578\n",
      "learning rate for this epoch =  0.09174932082183727\n",
      "Error on this batch = 0.0377504884829335\n",
      "Error on this batch = 0.0660262128589613\n",
      "Cost on val dataset after 883 epochs is = 0.08681225538505946\n",
      "Initial Cost on Val dataset for this epoch 883 = 0.08681225538505946\n",
      "learning rate for this epoch =  0.09172333319118318\n",
      "Error on this batch = 0.037725552753699425\n",
      "Error on this batch = 0.06599326701295746\n",
      "Cost on val dataset after 884 epochs is = 0.08679351214741086\n",
      "Initial Cost on Val dataset for this epoch 884 = 0.08679351214741086\n",
      "learning rate for this epoch =  0.0916973823233461\n",
      "Error on this batch = 0.03770066231788144\n",
      "Error on this batch = 0.06596035903721019\n",
      "Cost on val dataset after 885 epochs is = 0.08677480654838164\n",
      "Initial Cost on Val dataset for this epoch 885 = 0.08677480654838164\n",
      "learning rate for this epoch =  0.09167146812482159\n",
      "Error on this batch = 0.037675817027941816\n",
      "Error on this batch = 0.06592748880000804\n",
      "Cost on val dataset after 886 epochs is = 0.08675613848257663\n",
      "Initial Cost on Val dataset for this epoch 886 = 0.08675613848257663\n",
      "learning rate for this epoch =  0.09164559050244833\n",
      "Error on this batch = 0.037651016736967614\n",
      "Error on this batch = 0.06589465617091046\n",
      "Cost on val dataset after 887 epochs is = 0.08673750784498334\n",
      "Initial Cost on Val dataset for this epoch 887 = 0.08673750784498334\n",
      "learning rate for this epoch =  0.09161974936340654\n",
      "Error on this batch = 0.0376262612986729\n",
      "Error on this batch = 0.06586186102074257\n",
      "Cost on val dataset after 888 epochs is = 0.08671891453097151\n",
      "Initial Cost on Val dataset for this epoch 888 = 0.08671891453097151\n",
      "learning rate for this epoch =  0.09159394461521626\n",
      "Error on this batch = 0.03760155056740105\n",
      "Error on this batch = 0.06582910322158965\n",
      "Cost on val dataset after 889 epochs is = 0.08670035843629277\n",
      "Initial Cost on Val dataset for this epoch 889 = 0.08670035843629277\n",
      "learning rate for this epoch =  0.09156817616573577\n",
      "Error on this batch = 0.037576884398126786\n",
      "Error on this batch = 0.06579638264679176\n",
      "Cost on val dataset after 890 epochs is = 0.0866818394570802\n",
      "Initial Cost on Val dataset for this epoch 890 = 0.0866818394570802\n",
      "learning rate for this epoch =  0.09154244392315995\n",
      "Error on this batch = 0.037552262646458154\n",
      "Error on this batch = 0.06576369917093862\n",
      "Cost on val dataset after 891 epochs is = 0.08666335748984816\n",
      "Initial Cost on Val dataset for this epoch 891 = 0.08666335748984816\n",
      "learning rate for this epoch =  0.09151674779601875\n",
      "Error on this batch = 0.03752768516863832\n",
      "Error on this batch = 0.06573105266986401\n",
      "Cost on val dataset after 892 epochs is = 0.08664491243149183\n",
      "Initial Cost on Val dataset for this epoch 892 = 0.08664491243149183\n",
      "learning rate for this epoch =  0.0914910876931754\n",
      "Error on this batch = 0.03750315182154735\n",
      "Error on this batch = 0.0656984430206406\n",
      "Cost on val dataset after 893 epochs is = 0.08662650417928729\n",
      "Initial Cost on Val dataset for this epoch 893 = 0.08662650417928729\n",
      "learning rate for this epoch =  0.09146546352382512\n",
      "Error on this batch = 0.03747866246270374\n",
      "Error on this batch = 0.0656658701015745\n",
      "Cost on val dataset after 894 epochs is = 0.08660813263089102\n",
      "Initial Cost on Val dataset for this epoch 894 = 0.08660813263089102\n",
      "learning rate for this epoch =  0.09143987519749323\n",
      "Error on this batch = 0.037454216950265976\n",
      "Error on this batch = 0.06563333379219999\n",
      "Cost on val dataset after 895 epochs is = 0.08658979768434004\n",
      "Initial Cost on Val dataset for this epoch 895 = 0.08658979768434004\n",
      "learning rate for this epoch =  0.09141432262403383\n",
      "Error on this batch = 0.03742981514303376\n",
      "Error on this batch = 0.06560083397327414\n",
      "Cost on val dataset after 896 epochs is = 0.08657149923805167\n",
      "Initial Cost on Val dataset for this epoch 896 = 0.08657149923805167\n",
      "learning rate for this epoch =  0.09138880571362809\n",
      "Error on this batch = 0.0374054569004492\n",
      "Error on this batch = 0.06556837052677147\n",
      "Cost on val dataset after 897 epochs is = 0.08655323719082347\n",
      "Initial Cost on Val dataset for this epoch 897 = 0.08655323719082347\n",
      "learning rate for this epoch =  0.09136332437678274\n",
      "Error on this batch = 0.03738114208259794\n",
      "Error on this batch = 0.06553594333587866\n",
      "Cost on val dataset after 898 epochs is = 0.08653501144183329\n",
      "Initial Cost on Val dataset for this epoch 898 = 0.08653501144183329\n",
      "learning rate for this epoch =  0.09133787852432855\n",
      "Error on this batch = 0.03735687055020997\n",
      "Error on this batch = 0.06550355228498903\n",
      "Cost on val dataset after 899 epochs is = 0.08651682189063921\n",
      "Initial Cost on Val dataset for this epoch 899 = 0.08651682189063921\n",
      "learning rate for this epoch =  0.09131246806741879\n",
      "Error on this batch = 0.03733264216466047\n",
      "Error on this batch = 0.06547119725969738\n",
      "Cost on val dataset after 900 epochs is = 0.08649866843717956\n",
      "Initial Cost on Val dataset for this epoch 900 = 0.08649866843717956\n",
      "learning rate for this epoch =  0.09128709291752768\n",
      "Error on this batch = 0.03730845678797042\n",
      "Error on this batch = 0.06543887814679458\n",
      "Cost on val dataset after 901 epochs is = 0.08648055098177311\n",
      "Initial Cost on Val dataset for this epoch 901 = 0.08648055098177311\n",
      "learning rate for this epoch =  0.09126175298644894\n",
      "Error on this batch = 0.03728431428280696\n",
      "Error on this batch = 0.06540659483426219\n",
      "Cost on val dataset after 902 epochs is = 0.08646246942511901\n",
      "Initial Cost on Val dataset for this epoch 902 = 0.08646246942511901\n",
      "learning rate for this epoch =  0.09123644818629414\n",
      "Error on this batch = 0.037260214512483854\n",
      "Error on this batch = 0.06537434721126703\n",
      "Cost on val dataset after 903 epochs is = 0.086444423668297\n",
      "Initial Cost on Val dataset for this epoch 903 = 0.086444423668297\n",
      "learning rate for this epoch =  0.09121117842949143\n",
      "Error on this batch = 0.03723615734096155\n",
      "Error on this batch = 0.06534213516815605\n",
      "Cost on val dataset after 904 epochs is = 0.08642641361276747\n",
      "Initial Cost on Val dataset for this epoch 904 = 0.08642641361276747\n",
      "learning rate for this epoch =  0.09118594362878382\n",
      "Error on this batch = 0.03721214263284728\n",
      "Error on this batch = 0.06530995859645071\n",
      "Cost on val dataset after 905 epochs is = 0.08640843916037175\n",
      "Initial Cost on Val dataset for this epoch 905 = 0.08640843916037175\n",
      "learning rate for this epoch =  0.09116074369722786\n",
      "Error on this batch = 0.0371881702533948\n",
      "Error on this batch = 0.06527781738884171\n",
      "Cost on val dataset after 906 epochs is = 0.08639050021333222\n",
      "Initial Cost on Val dataset for this epoch 906 = 0.08639050021333222\n",
      "learning rate for this epoch =  0.09113557854819211\n",
      "Error on this batch = 0.03716424006850417\n",
      "Error on this batch = 0.06524571143918365\n",
      "Cost on val dataset after 907 epochs is = 0.0863725966742524\n",
      "Initial Cost on Val dataset for this epoch 907 = 0.0863725966742524\n",
      "learning rate for this epoch =  0.09111044809535562\n",
      "Error on this batch = 0.03714035194472132\n",
      "Error on this batch = 0.06521364064248956\n",
      "Cost on val dataset after 908 epochs is = 0.08635472844611744\n",
      "Initial Cost on Val dataset for this epoch 908 = 0.08635472844611744\n",
      "learning rate for this epoch =  0.09108535225270664\n",
      "Error on this batch = 0.037116505749237326\n",
      "Error on this batch = 0.06518160489492558\n",
      "Cost on val dataset after 909 epochs is = 0.08633689543229402\n",
      "Initial Cost on Val dataset for this epoch 909 = 0.08633689543229402\n",
      "learning rate for this epoch =  0.09106029093454096\n",
      "Error on this batch = 0.037092701349887694\n",
      "Error on this batch = 0.06514960409380552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on val dataset after 910 epochs is = 0.08631909753653096\n",
      "Initial Cost on Val dataset for this epoch 910 = 0.08631909753653096\n",
      "learning rate for this epoch =  0.09103526405546067\n",
      "Error on this batch = 0.03706893861515147\n",
      "Error on this batch = 0.0651176381375854\n",
      "Cost on val dataset after 911 epochs is = 0.0863013346629591\n",
      "Initial Cost on Val dataset for this epoch 911 = 0.0863013346629591\n",
      "learning rate for this epoch =  0.09101027153037257\n",
      "Error on this batch = 0.03704521741415007\n",
      "Error on this batch = 0.06508570692585816\n",
      "Cost on val dataset after 912 epochs is = 0.08628360671609187\n",
      "Initial Cost on Val dataset for this epoch 912 = 0.08628360671609187\n",
      "learning rate for this epoch =  0.09098531327448692\n",
      "Error on this batch = 0.03702153761664605\n",
      "Error on this batch = 0.06505381035934807\n",
      "Cost on val dataset after 913 epochs is = 0.08626591360082544\n",
      "Initial Cost on Val dataset for this epoch 913 = 0.08626591360082544\n",
      "learning rate for this epoch =  0.09096038920331581\n",
      "Error on this batch = 0.03699789909304165\n",
      "Error on this batch = 0.06502194833990534\n",
      "Cost on val dataset after 914 epochs is = 0.08624825522243905\n",
      "Initial Cost on Val dataset for this epoch 914 = 0.08624825522243905\n",
      "learning rate for this epoch =  0.09093549923267195\n",
      "Error on this batch = 0.03697430171437728\n",
      "Error on this batch = 0.06499012077050077\n",
      "Cost on val dataset after 915 epochs is = 0.08623063148659514\n",
      "Initial Cost on Val dataset for this epoch 915 = 0.08623063148659514\n",
      "learning rate for this epoch =  0.0909106432786672\n",
      "Error on this batch = 0.03695074535232971\n",
      "Error on this batch = 0.06495832755522012\n",
      "Cost on val dataset after 916 epochs is = 0.08621304229933996\n",
      "Initial Cost on Val dataset for this epoch 916 = 0.08621304229933996\n",
      "learning rate for this epoch =  0.09088582125771116\n",
      "Error on this batch = 0.03692722987921024\n",
      "Error on this batch = 0.06492656859925862\n",
      "Cost on val dataset after 917 epochs is = 0.08619548756710359\n",
      "Initial Cost on Val dataset for this epoch 917 = 0.08619548756710359\n",
      "learning rate for this epoch =  0.09086103308650982\n",
      "Error on this batch = 0.03690375516796255\n",
      "Error on this batch = 0.06489484380891565\n",
      "Cost on val dataset after 918 epochs is = 0.08617796719670036\n",
      "Initial Cost on Val dataset for this epoch 918 = 0.08617796719670036\n",
      "learning rate for this epoch =  0.09083627868206413\n",
      "Error on this batch = 0.03688032109216054\n",
      "Error on this batch = 0.06486315309158894\n",
      "Cost on val dataset after 919 epochs is = 0.08616048109532916\n",
      "Initial Cost on Val dataset for this epoch 919 = 0.08616048109532916\n",
      "learning rate for this epoch =  0.09081155796166877\n",
      "Error on this batch = 0.0368569275260059\n",
      "Error on this batch = 0.0648314963557693\n",
      "Cost on val dataset after 920 epochs is = 0.08614302917057366\n",
      "Initial Cost on Val dataset for this epoch 920 = 0.08614302917057366\n",
      "learning rate for this epoch =  0.09078687084291064\n",
      "Error on this batch = 0.036833574344325694\n",
      "Error on this batch = 0.06479987351103489\n",
      "Cost on val dataset after 921 epochs is = 0.0861256113304027\n",
      "Initial Cost on Val dataset for this epoch 921 = 0.0861256113304027\n",
      "learning rate for this epoch =  0.09076221724366758\n",
      "Error on this batch = 0.03681026142256948\n",
      "Error on this batch = 0.06476828446804567\n",
      "Cost on val dataset after 922 epochs is = 0.0861082274831706\n",
      "Initial Cost on Val dataset for this epoch 922 = 0.0861082274831706\n",
      "learning rate for this epoch =  0.09073759708210706\n",
      "Error on this batch = 0.03678698863680664\n",
      "Error on this batch = 0.06473672913853783\n",
      "Cost on val dataset after 923 epochs is = 0.0860908775376173\n",
      "Initial Cost on Val dataset for this epoch 923 = 0.0860908775376173\n",
      "learning rate for this epoch =  0.09071301027668477\n",
      "Error on this batch = 0.03676375586372327\n",
      "Error on this batch = 0.06470520743531813\n",
      "Cost on val dataset after 924 epochs is = 0.08607356140286881\n",
      "Initial Cost on Val dataset for this epoch 924 = 0.08607356140286881\n",
      "learning rate for this epoch =  0.09068845674614334\n",
      "Error on this batch = 0.0367405629806191\n",
      "Error on this batch = 0.06467371927225829\n",
      "Cost on val dataset after 925 epochs is = 0.08605627898843744\n",
      "Initial Cost on Val dataset for this epoch 925 = 0.08605627898843744\n",
      "learning rate for this epoch =  0.09066393640951106\n",
      "Error on this batch = 0.036717409865404095\n",
      "Error on this batch = 0.06464226456428925\n",
      "Cost on val dataset after 926 epochs is = 0.08603903020422202\n",
      "Initial Cost on Val dataset for this epoch 926 = 0.08603903020422202\n",
      "learning rate for this epoch =  0.09063944918610045\n",
      "Error on this batch = 0.03669429639659513\n",
      "Error on this batch = 0.06461084322739549\n",
      "Cost on val dataset after 927 epochs is = 0.08602181496050817\n",
      "Initial Cost on Val dataset for this epoch 927 = 0.08602181496050817\n",
      "learning rate for this epoch =  0.0906149949955071\n",
      "Error on this batch = 0.036671222453312356\n",
      "Error on this batch = 0.06457945517860933\n",
      "Cost on val dataset after 928 epochs is = 0.08600463316796873\n",
      "Initial Cost on Val dataset for this epoch 928 = 0.08600463316796873\n",
      "learning rate for this epoch =  0.09059057375760825\n",
      "Error on this batch = 0.0366481879152754\n",
      "Error on this batch = 0.06454810033600519\n",
      "Cost on val dataset after 929 epochs is = 0.08598748473766368\n",
      "Initial Cost on Val dataset for this epoch 929 = 0.08598748473766368\n",
      "learning rate for this epoch =  0.09056618539256157\n",
      "Error on this batch = 0.03662519266279962\n",
      "Error on this batch = 0.06451677861869363\n",
      "Cost on val dataset after 930 epochs is = 0.08597036958104065\n",
      "Initial Cost on Val dataset for this epoch 930 = 0.08597036958104065\n",
      "learning rate for this epoch =  0.09054182982080389\n",
      "Error on this batch = 0.036602236576791944\n",
      "Error on this batch = 0.06448548994681576\n",
      "Cost on val dataset after 931 epochs is = 0.085953287609935\n",
      "Initial Cost on Val dataset for this epoch 931 = 0.085953287609935\n",
      "learning rate for this epoch =  0.09051750696304985\n",
      "Error on this batch = 0.03657931953874693\n",
      "Error on this batch = 0.0644542342415372\n",
      "Cost on val dataset after 932 epochs is = 0.08593623873657003\n",
      "Initial Cost on Val dataset for this epoch 932 = 0.08593623873657003\n",
      "learning rate for this epoch =  0.0904932167402907\n",
      "Error on this batch = 0.03655644143074222\n",
      "Error on this batch = 0.06442301142504228\n",
      "Cost on val dataset after 933 epochs is = 0.08591922287355722\n",
      "Initial Cost on Val dataset for this epoch 933 = 0.08591922287355722\n",
      "learning rate for this epoch =  0.09046895907379304\n",
      "Error on this batch = 0.036533602135434495\n",
      "Error on this batch = 0.06439182142052802\n",
      "Cost on val dataset after 934 epochs is = 0.08590223993389638\n",
      "Initial Cost on Val dataset for this epoch 934 = 0.08590223993389638\n",
      "learning rate for this epoch =  0.09044473388509751\n",
      "Error on this batch = 0.03651080153605456\n",
      "Error on this batch = 0.0643606641521982\n",
      "Cost on val dataset after 935 epochs is = 0.08588528983097579\n",
      "Initial Cost on Val dataset for this epoch 935 = 0.08588528983097579\n",
      "learning rate for this epoch =  0.0904205410960176\n",
      "Error on this batch = 0.03648803951640305\n",
      "Error on this batch = 0.06432953954525743\n",
      "Cost on val dataset after 936 epochs is = 0.08586837247857237\n",
      "Initial Cost on Val dataset for this epoch 936 = 0.08586837247857237\n",
      "learning rate for this epoch =  0.09039638062863838\n",
      "Error on this batch = 0.036465315960845436\n",
      "Error on this batch = 0.06429844752590494\n",
      "Cost on val dataset after 937 epochs is = 0.08585148779085178\n",
      "Initial Cost on Val dataset for this epoch 937 = 0.08585148779085178\n",
      "learning rate for this epoch =  0.09037225240531528\n",
      "Error on this batch = 0.03644263075430727\n",
      "Error on this batch = 0.06426738802132856\n",
      "Cost on val dataset after 938 epochs is = 0.08583463568236856\n",
      "Initial Cost on Val dataset for this epoch 938 = 0.08583463568236856\n",
      "learning rate for this epoch =  0.09034815634867288\n",
      "Error on this batch = 0.03641998378226922\n",
      "Error on this batch = 0.06423636095969852\n",
      "Cost on val dataset after 939 epochs is = 0.08581781606806613\n",
      "Initial Cost on Val dataset for this epoch 939 = 0.08581781606806613\n",
      "learning rate for this epoch =  0.09032409238160365\n",
      "Error on this batch = 0.03639737493076189\n",
      "Error on this batch = 0.06420536627016137\n",
      "Cost on val dataset after 940 epochs is = 0.08580102886327699\n",
      "Initial Cost on Val dataset for this epoch 940 = 0.08580102886327699\n",
      "learning rate for this epoch =  0.09030006042726675\n",
      "Error on this batch = 0.03637480408636073\n",
      "Error on this batch = 0.06417440388283355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on val dataset after 941 epochs is = 0.08578427398372264\n",
      "Initial Cost on Val dataset for this epoch 941 = 0.08578427398372264\n",
      "learning rate for this epoch =  0.0902760604090869\n",
      "Error on this batch = 0.03635227113618075\n",
      "Error on this batch = 0.06414347372879518\n",
      "Cost on val dataset after 942 epochs is = 0.08576755134551367\n",
      "Initial Cost on Val dataset for this epoch 942 = 0.08576755134551367\n",
      "learning rate for this epoch =  0.09025209225075301\n",
      "Error on this batch = 0.0363297759678711\n",
      "Error on this batch = 0.06411257574008372\n",
      "Cost on val dataset after 943 epochs is = 0.08575086086514978\n",
      "Initial Cost on Val dataset for this epoch 943 = 0.08575086086514978\n",
      "learning rate for this epoch =  0.09022815587621721\n",
      "Error on this batch = 0.03630731846960971\n",
      "Error on this batch = 0.06408170984968753\n",
      "Cost on val dataset after 944 epochs is = 0.08573420245951975\n",
      "Initial Cost on Val dataset for this epoch 944 = 0.08573420245951975\n",
      "learning rate for this epoch =  0.0902042512096935\n",
      "Error on this batch = 0.03628489853009769\n",
      "Error on this batch = 0.0640508759915394\n",
      "Cost on val dataset after 945 epochs is = 0.08571757604590133\n",
      "Initial Cost on Val dataset for this epoch 945 = 0.08571757604590133\n",
      "learning rate for this epoch =  0.09018037817565662\n",
      "Error on this batch = 0.0362625160385538\n",
      "Error on this batch = 0.06402007410050994\n",
      "Cost on val dataset after 946 epochs is = 0.08570098154196129\n",
      "Initial Cost on Val dataset for this epoch 946 = 0.08570098154196129\n",
      "learning rate for this epoch =  0.09015653669884087\n",
      "Error on this batch = 0.03624017088470866\n",
      "Error on this batch = 0.06398930411240109\n",
      "Cost on val dataset after 947 epochs is = 0.08568441886575526\n",
      "Initial Cost on Val dataset for this epoch 947 = 0.08568441886575526\n",
      "learning rate for this epoch =  0.09013272670423898\n",
      "Error on this batch = 0.03621786295879921\n",
      "Error on this batch = 0.0639585659639393\n",
      "Cost on val dataset after 948 epochs is = 0.08566788793572762\n",
      "Initial Cost on Val dataset for this epoch 948 = 0.08566788793572762\n",
      "learning rate for this epoch =  0.09010894811710093\n",
      "Error on this batch = 0.03619559215156273\n",
      "Error on this batch = 0.06392785959276898\n",
      "Cost on val dataset after 949 epochs is = 0.08565138867071144\n",
      "Initial Cost on Val dataset for this epoch 949 = 0.08565138867071144\n",
      "learning rate for this epoch =  0.09008520086293277\n",
      "Error on this batch = 0.03617335835423109\n",
      "Error on this batch = 0.06389718493744552\n",
      "Cost on val dataset after 950 epochs is = 0.0856349209899282\n",
      "Initial Cost on Val dataset for this epoch 950 = 0.0856349209899282\n",
      "learning rate for this epoch =  0.09006148486749553\n",
      "Error on this batch = 0.036151161458524844\n",
      "Error on this batch = 0.06386654193742843\n",
      "Cost on val dataset after 951 epochs is = 0.0856184848129877\n",
      "Initial Cost on Val dataset for this epoch 951 = 0.0856184848129877\n",
      "learning rate for this epoch =  0.09003780005680402\n",
      "Error on this batch = 0.03612900135664734\n",
      "Error on this batch = 0.06383593053307436\n",
      "Cost on val dataset after 952 epochs is = 0.08560208005988783\n",
      "Initial Cost on Val dataset for this epoch 952 = 0.08560208005988783\n",
      "learning rate for this epoch =  0.09001414635712576\n",
      "Error on this batch = 0.03610687794127874\n",
      "Error on this batch = 0.06380535066563017\n",
      "Cost on val dataset after 953 epochs is = 0.08558570665101431\n",
      "Initial Cost on Val dataset for this epoch 953 = 0.08558570665101431\n",
      "learning rate for this epoch =  0.08999052369497976\n",
      "Error on this batch = 0.03608479110556994\n",
      "Error on this batch = 0.0637748022772256\n",
      "Cost on val dataset after 954 epochs is = 0.08556936450714044\n",
      "Initial Cost on Val dataset for this epoch 954 = 0.08556936450714044\n",
      "learning rate for this epoch =  0.08996693199713549\n",
      "Error on this batch = 0.036062740743136784\n",
      "Error on this batch = 0.0637442853108662\n",
      "Cost on val dataset after 955 epochs is = 0.0855530535494268\n",
      "Initial Cost on Val dataset for this epoch 955 = 0.0855530535494268\n",
      "learning rate for this epoch =  0.08994337119061177\n",
      "Error on this batch = 0.0360407267480538\n",
      "Error on this batch = 0.06371379971042596\n",
      "Cost on val dataset after 956 epochs is = 0.0855367736994209\n",
      "Initial Cost on Val dataset for this epoch 956 = 0.0855367736994209\n",
      "learning rate for this epoch =  0.08991984120267553\n",
      "Error on this batch = 0.036018749014848306\n",
      "Error on this batch = 0.06368334542063978\n",
      "Cost on val dataset after 957 epochs is = 0.08552052487905693\n",
      "Initial Cost on Val dataset for this epoch 957 = 0.08552052487905693\n",
      "learning rate for this epoch =  0.08989634196084093\n",
      "Error on this batch = 0.03599680743849441\n",
      "Error on this batch = 0.06365292238709619\n",
      "Cost on val dataset after 958 epochs is = 0.08550430701065527\n",
      "Initial Cost on Val dataset for this epoch 958 = 0.08550430701065527\n",
      "learning rate for this epoch =  0.089872873392868\n",
      "Error on this batch = 0.03597490191440686\n",
      "Error on this batch = 0.06362253055622959\n",
      "Cost on val dataset after 959 epochs is = 0.08548812001692212\n",
      "Initial Cost on Val dataset for this epoch 959 = 0.08548812001692212\n",
      "learning rate for this epoch =  0.08984943542676176\n",
      "Error on this batch = 0.035953032338435185\n",
      "Error on this batch = 0.06359216987531259\n",
      "Cost on val dataset after 960 epochs is = 0.08547196382094908\n",
      "Initial Cost on Val dataset for this epoch 960 = 0.08547196382094908\n",
      "learning rate for this epoch =  0.08982602799077107\n",
      "Error on this batch = 0.03593119860685752\n",
      "Error on this batch = 0.06356184029244809\n",
      "Cost on val dataset after 961 epochs is = 0.08545583834621268\n",
      "Initial Cost on Val dataset for this epoch 961 = 0.08545583834621268\n",
      "learning rate for this epoch =  0.08980265101338746\n",
      "Error on this batch = 0.035909400616374876\n",
      "Error on this batch = 0.06353154175656153\n",
      "Cost on val dataset after 962 epochs is = 0.08543974351657385\n",
      "Initial Cost on Val dataset for this epoch 962 = 0.08543974351657385\n",
      "learning rate for this epoch =  0.0897793044233442\n",
      "Error on this batch = 0.03588763826410506\n",
      "Error on this batch = 0.06350127421739264\n",
      "Cost on val dataset after 963 epochs is = 0.0854236792562774\n",
      "Initial Cost on Val dataset for this epoch 963 = 0.0854236792562774\n",
      "learning rate for this epoch =  0.0897559881496152\n",
      "Error on this batch = 0.0358659114475768\n",
      "Error on this batch = 0.06347103762548752\n",
      "Cost on val dataset after 964 epochs is = 0.0854076454899515\n",
      "Initial Cost on Val dataset for this epoch 964 = 0.0854076454899515\n",
      "learning rate for this epoch =  0.08973270212141383\n",
      "Error on this batch = 0.03584422006472395\n",
      "Error on this batch = 0.06344083193219006\n",
      "Cost on val dataset after 965 epochs is = 0.08539164214260699\n",
      "Initial Cost on Val dataset for this epoch 965 = 0.08539164214260699\n",
      "learning rate for this epoch =  0.08970944626819201\n",
      "Error on this batch = 0.035822564013879746\n",
      "Error on this batch = 0.06341065708963384\n",
      "Cost on val dataset after 966 epochs is = 0.0853756691396368\n",
      "Initial Cost on Val dataset for this epoch 966 = 0.0853756691396368\n",
      "learning rate for this epoch =  0.0896862205196391\n",
      "Error on this batch = 0.03580094319377103\n",
      "Error on this batch = 0.06338051305073328\n",
      "Cost on val dataset after 967 epochs is = 0.08535972640681533\n",
      "Initial Cost on Val dataset for this epoch 967 = 0.08535972640681533\n",
      "learning rate for this epoch =  0.08966302480568086\n",
      "Error on this batch = 0.035779357503512584\n",
      "Error on this batch = 0.06335039976917521\n",
      "Cost on val dataset after 968 epochs is = 0.08534381387029764\n",
      "Initial Cost on Val dataset for this epoch 968 = 0.08534381387029764\n",
      "learning rate for this epoch =  0.08963985905647841\n",
      "Error on this batch = 0.035757806842601714\n",
      "Error on this batch = 0.06332031719941\n",
      "Cost on val dataset after 969 epochs is = 0.08532793145661882\n",
      "Initial Cost on Val dataset for this epoch 969 = 0.08532793145661882\n",
      "learning rate for this epoch =  0.08961672320242715\n",
      "Error on this batch = 0.03573629111091261\n",
      "Error on this batch = 0.06329026529664253\n",
      "Cost on val dataset after 970 epochs is = 0.0853120790926931\n",
      "Initial Cost on Val dataset for this epoch 970 = 0.0853120790926931\n",
      "learning rate for this epoch =  0.08959361717415586\n",
      "Error on this batch = 0.035714810208691085\n",
      "Error on this batch = 0.06326024401682317\n",
      "Cost on val dataset after 971 epochs is = 0.08529625670581314\n",
      "Initial Cost on Val dataset for this epoch 971 = 0.08529625670581314\n",
      "learning rate for this epoch =  0.08957054090252553\n",
      "Error on this batch = 0.03569336403654923\n",
      "Error on this batch = 0.06323025331663856\n",
      "Cost on val dataset after 972 epochs is = 0.08528046422364917\n",
      "Initial Cost on Val dataset for this epoch 972 = 0.08528046422364917\n",
      "learning rate for this epoch =  0.08954749431862849\n",
      "Error on this batch = 0.0356719524954603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on this batch = 0.0632002931535022\n",
      "Cost on val dataset after 973 epochs is = 0.08526470157424802\n",
      "Initial Cost on Val dataset for this epoch 973 = 0.08526470157424802\n",
      "learning rate for this epoch =  0.08952447735378725\n",
      "Error on this batch = 0.03565057548675363\n",
      "Error on this batch = 0.06317036348554485\n",
      "Cost on val dataset after 974 epochs is = 0.08524896868603217\n",
      "Initial Cost on Val dataset for this epoch 974 = 0.08524896868603217\n",
      "learning rate for this epoch =  0.08950148993955362\n",
      "Error on this batch = 0.035629232912109725\n",
      "Error on this batch = 0.06314046427160486\n",
      "Cost on val dataset after 975 epochs is = 0.08523326548779898\n",
      "Initial Cost on Val dataset for this epoch 975 = 0.08523326548779898\n",
      "learning rate for this epoch =  0.08947853200770761\n",
      "Error on this batch = 0.03560792467355544\n",
      "Error on this batch = 0.06311059547121835\n",
      "Cost on val dataset after 976 epochs is = 0.08521759190871946\n",
      "Initial Cost on Val dataset for this epoch 976 = 0.08521759190871946\n",
      "learning rate for this epoch =  0.08945560349025655\n",
      "Error on this batch = 0.03558665067345942\n",
      "Error on this batch = 0.06308075704460922\n",
      "Cost on val dataset after 977 epochs is = 0.08520194787833724\n",
      "Initial Cost on Val dataset for this epoch 977 = 0.08520194787833724\n",
      "learning rate for this epoch =  0.08943270431943395\n",
      "Error on this batch = 0.03556541081452752\n",
      "Error on this batch = 0.06305094895267875\n",
      "Cost on val dataset after 978 epochs is = 0.08518633332656761\n",
      "Initial Cost on Val dataset for this epoch 978 = 0.08518633332656761\n",
      "learning rate for this epoch =  0.08940983442769869\n",
      "Error on this batch = 0.03554420499979845\n",
      "Error on this batch = 0.06302117115699545\n",
      "Cost on val dataset after 979 epochs is = 0.0851707481836962\n",
      "Initial Cost on Val dataset for this epoch 979 = 0.0851707481836962\n",
      "learning rate for this epoch =  0.08938699374773387\n",
      "Error on this batch = 0.03552303313263964\n",
      "Error on this batch = 0.06299142361978442\n",
      "Cost on val dataset after 980 epochs is = 0.08515519238037789\n",
      "Initial Cost on Val dataset for this epoch 980 = 0.08515519238037789\n",
      "learning rate for this epoch =  0.089364182212446\n",
      "Error on this batch = 0.03550189511674318\n",
      "Error on this batch = 0.0629617063039166\n",
      "Cost on val dataset after 981 epochs is = 0.08513966584763545\n",
      "Initial Cost on Val dataset for this epoch 981 = 0.08513966584763545\n",
      "learning rate for this epoch =  0.08934139975496388\n",
      "Error on this batch = 0.03548079085612207\n",
      "Error on this batch = 0.06293201917289795\n",
      "Cost on val dataset after 982 epochs is = 0.0851241685168585\n",
      "Initial Cost on Val dataset for this epoch 982 = 0.0851241685168585\n",
      "learning rate for this epoch =  0.08931864630863778\n",
      "Error on this batch = 0.035459720255106436\n",
      "Error on this batch = 0.06290236219085828\n",
      "Cost on val dataset after 983 epochs is = 0.08510870031980189\n",
      "Initial Cost on Val dataset for this epoch 983 = 0.08510870031980189\n",
      "learning rate for this epoch =  0.08929592180703835\n",
      "Error on this batch = 0.0354386832183401\n",
      "Error on this batch = 0.06287273532254002\n",
      "Cost on val dataset after 984 epochs is = 0.08509326118858442\n",
      "Initial Cost on Val dataset for this epoch 984 = 0.08509326118858442\n",
      "learning rate for this epoch =  0.08927322618395579\n",
      "Error on this batch = 0.035417679650777446\n",
      "Error on this batch = 0.0628431385332868\n",
      "Cost on val dataset after 985 epochs is = 0.0850778510556874\n",
      "Initial Cost on Val dataset for this epoch 985 = 0.0850778510556874\n",
      "learning rate for this epoch =  0.08925055937339876\n",
      "Error on this batch = 0.035396709457680225\n",
      "Error on this batch = 0.0628135717890316\n",
      "Cost on val dataset after 986 epochs is = 0.08506246985395323\n",
      "Initial Cost on Val dataset for this epoch 986 = 0.08506246985395323\n",
      "learning rate for this epoch =  0.08922792130959359\n",
      "Error on this batch = 0.035375772544614666\n",
      "Error on this batch = 0.06278403505628523\n",
      "Cost on val dataset after 987 epochs is = 0.08504711751658363\n",
      "Initial Cost on Val dataset for this epoch 987 = 0.08504711751658363\n",
      "learning rate for this epoch =  0.08920531192698324\n",
      "Error on this batch = 0.035354868817449\n",
      "Error on this batch = 0.06275452830212394\n",
      "Cost on val dataset after 988 epochs is = 0.08503179397713825\n",
      "Initial Cost on Val dataset for this epoch 988 = 0.08503179397713825\n",
      "learning rate for this epoch =  0.08918273116022643\n",
      "Error on this batch = 0.03533399818235086\n",
      "Error on this batch = 0.0627250514941774\n",
      "Cost on val dataset after 989 epochs is = 0.0850164991695329\n",
      "Initial Cost on Val dataset for this epoch 989 = 0.0850164991695329\n",
      "learning rate for this epoch =  0.08916017894419664\n",
      "Error on this batch = 0.03531316054578527\n",
      "Error on this batch = 0.06269560460061617\n",
      "Cost on val dataset after 990 epochs is = 0.0850012330280378\n",
      "Initial Cost on Val dataset for this epoch 990 = 0.0850012330280378\n",
      "learning rate for this epoch =  0.08913765521398127\n",
      "Error on this batch = 0.03529235581451261\n",
      "Error on this batch = 0.06266618759013907\n",
      "Cost on val dataset after 991 epochs is = 0.08498599548727594\n",
      "Initial Cost on Val dataset for this epoch 991 = 0.08498599548727594\n",
      "learning rate for this epoch =  0.08911515990488066\n",
      "Error on this batch = 0.03527158389558676\n",
      "Error on this batch = 0.06263680043196045\n",
      "Cost on val dataset after 992 epochs is = 0.08497078648222102\n",
      "Initial Cost on Val dataset for this epoch 992 = 0.08497078648222102\n",
      "learning rate for this epoch =  0.0890926929524072\n",
      "Error on this batch = 0.035250844696353964\n",
      "Error on this batch = 0.06260744309579681\n",
      "Cost on val dataset after 993 epochs is = 0.08495560594819565\n",
      "Initial Cost on Val dataset for this epoch 993 = 0.08495560594819565\n",
      "learning rate for this epoch =  0.08907025429228442\n",
      "Error on this batch = 0.03523013812445131\n",
      "Error on this batch = 0.06257811555185397\n",
      "Cost on val dataset after 994 epochs is = 0.08494045382086941\n",
      "Initial Cost on Val dataset for this epoch 994 = 0.08494045382086941\n",
      "learning rate for this epoch =  0.08904784386044612\n",
      "Error on this batch = 0.03520946408780594\n",
      "Error on this batch = 0.06254881777081332\n",
      "Cost on val dataset after 995 epochs is = 0.08492533003625681\n",
      "Initial Cost on Val dataset for this epoch 995 = 0.08492533003625681\n",
      "learning rate for this epoch =  0.08902546159303537\n",
      "Error on this batch = 0.035188822494634196\n",
      "Error on this batch = 0.06251954972381833\n",
      "Cost on val dataset after 996 epochs is = 0.084910234530715\n",
      "Initial Cost on Val dataset for this epoch 996 = 0.084910234530715\n",
      "learning rate for this epoch =  0.0890031074264038\n",
      "Error on this batch = 0.0351682132534413\n",
      "Error on this batch = 0.06249031138246059\n",
      "Cost on val dataset after 997 epochs is = 0.08489516724094189\n",
      "Initial Cost on Val dataset for this epoch 997 = 0.08489516724094189\n",
      "learning rate for this epoch =  0.08898078129711047\n",
      "Error on this batch = 0.03514763627302109\n",
      "Error on this batch = 0.06246110271876577\n",
      "Cost on val dataset after 998 epochs is = 0.08488012810397359\n",
      "Initial Cost on Val dataset for this epoch 998 = 0.08488012810397359\n",
      "learning rate for this epoch =  0.08895848314192122\n",
      "Error on this batch = 0.03512709146245614\n",
      "Error on this batch = 0.06243192370517941\n",
      "Cost on val dataset after 999 epochs is = 0.08486511705718243\n",
      "Initial Cost on Val dataset for this epoch 999 = 0.08486511705718243\n",
      "learning rate for this epoch =  0.08893621289780759\n",
      "Error on this batch = 0.035106578731118145\n",
      "Error on this batch = 0.062402774314552455\n",
      "Cost on val dataset after 1000 epochs is = 0.08485013403827413\n",
      "Initial Cost on Val dataset for this epoch 1000 = 0.08485013403827413\n",
      "learning rate for this epoch =  0.08891397050194613\n",
      "Error on this batch = 0.03508609798866845\n",
      "Error on this batch = 0.06237365452012654\n",
      "Cost on val dataset after 1001 epochs is = 0.08483517898528582\n",
      "Initial Cost on Val dataset for this epoch 1001 = 0.08483517898528582\n",
      "learning rate for this epoch =  0.0888917558917174\n",
      "Error on this batch = 0.03506564914505904\n",
      "Error on this batch = 0.06234456429551918\n",
      "Cost on val dataset after 1002 epochs is = 0.08482025183658318\n",
      "Initial Cost on Val dataset for this epoch 1002 = 0.08482025183658318\n",
      "learning rate for this epoch =  0.0888695690047051\n",
      "Error on this batch = 0.03504523211053371\n",
      "Error on this batch = 0.062315503614708674\n",
      "Cost on val dataset after 1003 epochs is = 0.08480535253085786\n",
      "Initial Cost on Val dataset for this epoch 1003 = 0.08480535253085786\n",
      "learning rate for this epoch =  0.08884740977869533\n",
      "Error on this batch = 0.03502484679562946\n",
      "Error on this batch = 0.06228647245201908\n",
      "Cost on val dataset after 1004 epochs is = 0.08479048100712488\n",
      "Initial Cost on Val dataset for this epoch 1004 = 0.08479048100712488\n",
      "learning rate for this epoch =  0.0888252781516756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on this batch = 0.03500449311117833\n",
      "Error on this batch = 0.06225747078210447\n",
      "Cost on val dataset after 1005 epochs is = 0.08477563720471981\n",
      "Initial Cost on Val dataset for this epoch 1005 = 0.08477563720471981\n",
      "learning rate for this epoch =  0.08880317406183406\n",
      "Error on this batch = 0.03498417096830931\n",
      "Error on this batch = 0.062228498579933685\n",
      "Cost on val dataset after 1006 epochs is = 0.08476082106329588\n",
      "Initial Cost on Val dataset for this epoch 1006 = 0.08476082106329588\n",
      "learning rate for this epoch =  0.08878109744755859\n",
      "Error on this batch = 0.03496388027845069\n",
      "Error on this batch = 0.062199555820774285\n",
      "Cost on val dataset after 1007 epochs is = 0.08474603252282109\n",
      "Initial Cost on Val dataset for this epoch 1007 = 0.08474603252282109\n",
      "learning rate for this epoch =  0.08875904824743605\n",
      "Error on this batch = 0.03494362095333266\n",
      "Error on this batch = 0.062170642480176806\n",
      "Cost on val dataset after 1008 epochs is = 0.08473127152357518\n",
      "Initial Cost on Val dataset for this epoch 1008 = 0.08473127152357518\n",
      "learning rate for this epoch =  0.08873702640025133\n",
      "Error on this batch = 0.03492339290499008\n",
      "Error on this batch = 0.06214175853395847\n",
      "Cost on val dataset after 1009 epochs is = 0.0847165380061465\n",
      "Initial Cost on Val dataset for this epoch 1009 = 0.0847165380061465\n",
      "learning rate for this epoch =  0.08871503184498658\n",
      "Error on this batch = 0.03490319604576566\n",
      "Error on this batch = 0.0621129039581872\n",
      "Cost on val dataset after 1010 epochs is = 0.08470183191142883\n",
      "Initial Cost on Val dataset for this epoch 1010 = 0.08470183191142883\n",
      "learning rate for this epoch =  0.08869306452082039\n",
      "Error on this batch = 0.03488303028831336\n",
      "Error on this batch = 0.06208407872916487\n",
      "Cost on val dataset after 1011 epochs is = 0.08468715318061812\n",
      "Initial Cost on Val dataset for this epoch 1011 = 0.08468715318061812\n",
      "learning rate for this epoch =  0.0886711243671269\n",
      "Error on this batch = 0.03486289554560202\n",
      "Error on this batch = 0.06205528282341101\n",
      "Cost on val dataset after 1012 epochs is = 0.08467250175520921\n",
      "Initial Cost on Val dataset for this epoch 1012 = 0.08467250175520921\n",
      "learning rate for this epoch =  0.08864921132347509\n",
      "Error on this batch = 0.03484279173091928\n",
      "Error on this batch = 0.06202651621764599\n",
      "Cost on val dataset after 1013 epochs is = 0.08465787757699217\n",
      "Initial Cost on Val dataset for this epoch 1013 = 0.08465787757699217\n",
      "learning rate for this epoch =  0.0886273253296278\n",
      "Error on this batch = 0.03482271875787587\n",
      "Error on this batch = 0.06199777888877418\n",
      "Cost on val dataset after 1014 epochs is = 0.08464328058804896\n",
      "Initial Cost on Val dataset for this epoch 1014 = 0.08464328058804896\n",
      "learning rate for this epoch =  0.08860546632554109\n",
      "Error on this batch = 0.03480267654040995\n",
      "Error on this batch = 0.06196907081386711\n",
      "Cost on val dataset after 1015 epochs is = 0.08462871073074972\n",
      "Initial Cost on Val dataset for this epoch 1015 = 0.08462871073074972\n",
      "learning rate for this epoch =  0.0885836342513633\n",
      "Error on this batch = 0.03478266499279188\n",
      "Error on this batch = 0.06194039197014622\n",
      "Cost on val dataset after 1016 epochs is = 0.08461416794774916\n",
      "Initial Cost on Val dataset for this epoch 1016 = 0.08461416794774916\n",
      "learning rate for this epoch =  0.08856182904743433\n",
      "Error on this batch = 0.034762684029629246\n",
      "Error on this batch = 0.0619117423349659\n",
      "Cost on val dataset after 1017 epochs is = 0.08459965218198252\n",
      "Initial Cost on Val dataset for this epoch 1017 = 0.08459965218198252\n",
      "learning rate for this epoch =  0.08854005065428476\n",
      "Error on this batch = 0.03474273356587188\n",
      "Error on this batch = 0.061883121885796144\n",
      "Cost on val dataset after 1018 epochs is = 0.084585163376662\n",
      "Initial Cost on Val dataset for this epoch 1018 = 0.084585163376662\n",
      "learning rate for this epoch =  0.08851829901263515\n",
      "Error on this batch = 0.03472281351681757\n",
      "Error on this batch = 0.06185453060020517\n",
      "Cost on val dataset after 1019 epochs is = 0.08457070147527254\n",
      "Initial Cost on Val dataset for this epoch 1019 = 0.08457070147527254\n",
      "learning rate for this epoch =  0.08849657406339513\n",
      "Error on this batch = 0.0347029237981174\n",
      "Error on this batch = 0.06182596845584204\n",
      "Cost on val dataset after 1020 epochs is = 0.08455626642156785\n",
      "Initial Cost on Val dataset for this epoch 1020 = 0.08455626642156785\n",
      "learning rate for this epoch =  0.08847487574766279\n",
      "Error on this batch = 0.03468306432578193\n",
      "Error on this batch = 0.06179743543041931\n",
      "Cost on val dataset after 1021 epochs is = 0.08454185815956623\n",
      "Initial Cost on Val dataset for this epoch 1021 = 0.08454185815956623\n",
      "learning rate for this epoch =  0.08845320400672366\n",
      "Error on this batch = 0.034663235016187184\n",
      "Error on this batch = 0.061768931501695275\n",
      "Cost on val dataset after 1022 epochs is = 0.08452747663354632\n",
      "Initial Cost on Val dataset for this epoch 1022 = 0.08452747663354632\n",
      "learning rate for this epoch =  0.0884315587820501\n",
      "Error on this batch = 0.03464343578608103\n",
      "Error on this batch = 0.06174045664745667\n",
      "Cost on val dataset after 1023 epochs is = 0.08451312178804277\n",
      "Initial Cost on Val dataset for this epoch 1023 = 0.08451312178804277\n",
      "learning rate for this epoch =  0.08840994001530049\n",
      "Error on this batch = 0.03462366655258947\n",
      "Error on this batch = 0.06171201084550086\n",
      "Cost on val dataset after 1024 epochs is = 0.08449879356784183\n",
      "Initial Cost on Val dataset for this epoch 1024 = 0.08449879356784183\n",
      "learning rate for this epoch =  0.08838834764831843\n",
      "Error on this batch = 0.034603927233223725\n",
      "Error on this batch = 0.06168359407361848\n",
      "Cost on val dataset after 1025 epochs is = 0.08448449191797686\n",
      "Initial Cost on Val dataset for this epoch 1025 = 0.08448449191797686\n",
      "learning rate for this epoch =  0.08836678162313201\n",
      "Error on this batch = 0.0345842177458868\n",
      "Error on this batch = 0.06165520630957573\n",
      "Cost on val dataset after 1026 epochs is = 0.08447021678372357\n",
      "Initial Cost on Val dataset for this epoch 1026 = 0.08447021678372357\n",
      "learning rate for this epoch =  0.088345241881953\n",
      "Error on this batch = 0.03456453800888062\n",
      "Error on this batch = 0.061626847531096804\n",
      "Cost on val dataset after 1027 epochs is = 0.08445596811059562\n",
      "Initial Cost on Val dataset for this epoch 1027 = 0.08445596811059562\n",
      "learning rate for this epoch =  0.0883237283671761\n",
      "Error on this batch = 0.03454488794091331\n",
      "Error on this batch = 0.061598517715846415\n",
      "Cost on val dataset after 1028 epochs is = 0.0844417458443396\n",
      "Initial Cost on Val dataset for this epoch 1028 = 0.0844417458443396\n",
      "learning rate for this epoch =  0.0883022410213782\n",
      "Error on this batch = 0.03452526746110642\n",
      "Error on this batch = 0.06157021684141229\n",
      "Cost on val dataset after 1029 epochs is = 0.08442754993093025\n",
      "Initial Cost on Val dataset for this epoch 1029 = 0.08442754993093025\n",
      "learning rate for this epoch =  0.08828077978731765\n",
      "Error on this batch = 0.034505676489002574\n",
      "Error on this batch = 0.06154194488528777\n",
      "Cost on val dataset after 1030 epochs is = 0.0844133803165656\n",
      "Initial Cost on Val dataset for this epoch 1030 = 0.0844133803165656\n",
      "learning rate for this epoch =  0.08825934460793343\n",
      "Error on this batch = 0.03448611494457285\n",
      "Error on this batch = 0.061513701824854404\n",
      "Cost on val dataset after 1031 epochs is = 0.08439923694766185\n",
      "Initial Cost on Val dataset for this epoch 1031 = 0.08439923694766185\n",
      "learning rate for this epoch =  0.08823793542634452\n",
      "Error on this batch = 0.03446658274822469\n",
      "Error on this batch = 0.06148548763736485\n",
      "Cost on val dataset after 1032 epochs is = 0.08438511977084837\n",
      "Initial Cost on Val dataset for this epoch 1032 = 0.08438511977084837\n",
      "learning rate for this epoch =  0.08821655218584905\n",
      "Error on this batch = 0.03444707982080965\n",
      "Error on this batch = 0.06145730229992559\n",
      "Cost on val dataset after 1033 epochs is = 0.08437102873296239\n",
      "Initial Cost on Val dataset for this epoch 1033 = 0.08437102873296239\n",
      "learning rate for this epoch =  0.08819519482992363\n",
      "Error on this batch = 0.034427606083631206\n",
      "Error on this batch = 0.06142914578947995\n",
      "Cost on val dataset after 1034 epochs is = 0.0843569637810439\n",
      "Initial Cost on Val dataset for this epoch 1034 = 0.0843569637810439\n",
      "learning rate for this epoch =  0.08817386330222259\n",
      "Error on this batch = 0.03440816145845286\n",
      "Error on this batch = 0.06140101808279131\n",
      "Cost on val dataset after 1035 epochs is = 0.08434292486233029\n",
      "Initial Cost on Val dataset for this epoch 1035 = 0.08434292486233029\n",
      "learning rate for this epoch =  0.08815255754657725\n",
      "Error on this batch = 0.03438874586750605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on this batch = 0.061372919156426314\n",
      "Cost on val dataset after 1036 epochs is = 0.08432891192425086\n",
      "Initial Cost on Val dataset for this epoch 1036 = 0.08432891192425086\n",
      "learning rate for this epoch =  0.08813127750699522\n",
      "Error on this batch = 0.03436935923349822\n",
      "Error on this batch = 0.06134484898673836\n",
      "Cost on val dataset after 1037 epochs is = 0.08431492491442148\n",
      "Initial Cost on Val dataset for this epoch 1037 = 0.08431492491442148\n",
      "learning rate for this epoch =  0.08811002312765961\n",
      "Error on this batch = 0.03435000147962089\n",
      "Error on this batch = 0.06131680754985125\n",
      "Cost on val dataset after 1038 epochs is = 0.08430096378063896\n",
      "Initial Cost on Val dataset for this epoch 1038 = 0.08430096378063896\n",
      "learning rate for this epoch =  0.08808879435292839\n",
      "Error on this batch = 0.03433067252955789\n",
      "Error on this batch = 0.061288794821643074\n",
      "Cost on val dataset after 1039 epochs is = 0.08428702847087562\n",
      "Initial Cost on Val dataset for this epoch 1039 = 0.08428702847087562\n",
      "learning rate for this epoch =  0.08806759112733367\n",
      "Error on this batch = 0.03431137230749317\n",
      "Error on this batch = 0.061260810777730254\n",
      "Cost on val dataset after 1040 epochs is = 0.08427311893327345\n",
      "Initial Cost on Val dataset for this epoch 1040 = 0.08427311893327345\n",
      "learning rate for this epoch =  0.0880464133955809\n",
      "Error on this batch = 0.034292100738119106\n",
      "Error on this batch = 0.06123285539345192\n",
      "Cost on val dataset after 1041 epochs is = 0.08425923511613849\n",
      "Initial Cost on Val dataset for this epoch 1041 = 0.08425923511613849\n",
      "learning rate for this epoch =  0.08802526110254827\n",
      "Error on this batch = 0.034272857746644465\n",
      "Error on this batch = 0.06120492864385449\n",
      "Cost on val dataset after 1042 epochs is = 0.0842453769679351\n",
      "Initial Cost on Val dataset for this epoch 1042 = 0.0842453769679351\n",
      "learning rate for this epoch =  0.0880041341932859\n",
      "Error on this batch = 0.03425364325880235\n",
      "Error on this batch = 0.06117703050367648\n",
      "Cost on val dataset after 1043 epochs is = 0.08423154443728016\n",
      "Initial Cost on Val dataset for this epoch 1043 = 0.08423154443728016\n",
      "learning rate for this epoch =  0.08798303261301525\n",
      "Error on this batch = 0.03423445720085813\n",
      "Error on this batch = 0.06114916094733377\n",
      "Cost on val dataset after 1044 epochs is = 0.08421773747293708\n",
      "Initial Cost on Val dataset for this epoch 1044 = 0.08421773747293708\n",
      "learning rate for this epoch =  0.08796195630712837\n",
      "Error on this batch = 0.034215299499617303\n",
      "Error on this batch = 0.0611213199489049\n",
      "Cost on val dataset after 1045 epochs is = 0.08420395602381009\n",
      "Initial Cost on Val dataset for this epoch 1045 = 0.08420395602381009\n",
      "learning rate for this epoch =  0.08794090522118717\n",
      "Error on this batch = 0.03419617008243326\n",
      "Error on this batch = 0.06109350748211703\n",
      "Cost on val dataset after 1046 epochs is = 0.08419020003893814\n",
      "Initial Cost on Val dataset for this epoch 1046 = 0.08419020003893814\n",
      "learning rate for this epoch =  0.08791987930092278\n",
      "Error on this batch = 0.034177068877214827\n",
      "Error on this batch = 0.06106572352033195\n",
      "Cost on val dataset after 1047 epochs is = 0.08417646946748907\n",
      "Initial Cost on Val dataset for this epoch 1047 = 0.08417646946748907\n",
      "learning rate for this epoch =  0.08789887849223484\n",
      "Error on this batch = 0.03415799581243384\n",
      "Error on this batch = 0.06103796803653262\n",
      "Cost on val dataset after 1048 epochs is = 0.0841627642587535\n",
      "Initial Cost on Val dataset for this epoch 1048 = 0.0841627642587535\n",
      "learning rate for this epoch =  0.08787790274119082\n",
      "Error on this batch = 0.03413895081713253\n",
      "Error on this batch = 0.06101024100330994\n",
      "Cost on val dataset after 1049 epochs is = 0.08414908436213883\n",
      "Initial Cost on Val dataset for this epoch 1049 = 0.08414908436213883\n",
      "learning rate for this epoch =  0.08785695199402538\n",
      "Error on this batch = 0.03411993382093058\n",
      "Error on this batch = 0.06098254239285008\n",
      "Cost on val dataset after 1050 epochs is = 0.08413542972716322\n",
      "Initial Cost on Val dataset for this epoch 1050 = 0.08413542972716322\n",
      "learning rate for this epoch =  0.08783602619713961\n",
      "Error on this batch = 0.03410094475403235\n",
      "Error on this batch = 0.06095487217692197\n",
      "Cost on val dataset after 1051 epochs is = 0.0841218003034495\n",
      "Initial Cost on Val dataset for this epoch 1051 = 0.0841218003034495\n",
      "learning rate for this epoch =  0.08781512529710042\n",
      "Error on this batch = 0.03408198354723361\n",
      "Error on this batch = 0.0609272303268656\n",
      "Cost on val dataset after 1052 epochs is = 0.08410819604071912\n",
      "Initial Cost on Val dataset for this epoch 1052 = 0.08410819604071912\n",
      "learning rate for this epoch =  0.08779424924063986\n",
      "Error on this batch = 0.03406305013192815\n",
      "Error on this batch = 0.060899616813580186\n",
      "Cost on val dataset after 1053 epochs is = 0.084094616888786\n",
      "Initial Cost on Val dataset for this epoch 1053 = 0.084094616888786\n",
      "learning rate for this epoch =  0.08777339797465443\n",
      "Error on this batch = 0.034044144440114325\n",
      "Error on this batch = 0.0608720316075133\n",
      "Cost on val dataset after 1054 epochs is = 0.0840810627975505\n",
      "Initial Cost on Val dataset for this epoch 1054 = 0.0840810627975505\n",
      "learning rate for this epoch =  0.08775257144620446\n",
      "Error on this batch = 0.034025266404401115\n",
      "Error on this batch = 0.06084447467865011\n",
      "Cost on val dataset after 1055 epochs is = 0.08406753371699328\n",
      "Initial Cost on Val dataset for this epoch 1055 = 0.08406753371699328\n",
      "learning rate for this epoch =  0.08773176960251337\n",
      "Error on this batch = 0.03400641595801418\n",
      "Error on this batch = 0.0608169459965033\n",
      "Cost on val dataset after 1056 epochs is = 0.0840540295971693\n",
      "Initial Cost on Val dataset for this epoch 1056 = 0.0840540295971693\n",
      "learning rate for this epoch =  0.08771099239096714\n",
      "Error on this batch = 0.03398759303480155\n",
      "Error on this batch = 0.06078944553010338\n",
      "Cost on val dataset after 1057 epochs is = 0.08404055038820173\n",
      "Initial Cost on Val dataset for this epoch 1057 = 0.08404055038820173\n",
      "learning rate for this epoch =  0.08769023975911351\n",
      "Error on this batch = 0.03396879756923899\n",
      "Error on this batch = 0.060761973247989286\n",
      "Cost on val dataset after 1058 epochs is = 0.08402709604027571\n",
      "Initial Cost on Val dataset for this epoch 1058 = 0.08402709604027571\n",
      "learning rate for this epoch =  0.08766951165466147\n",
      "Error on this batch = 0.033950029496435205\n",
      "Error on this batch = 0.06073452911819992\n",
      "Cost on val dataset after 1059 epochs is = 0.0840136665036327\n",
      "Initial Cost on Val dataset for this epoch 1059 = 0.0840136665036327\n",
      "learning rate for this epoch =  0.08764880802548045\n",
      "Error on this batch = 0.033931288752136676\n",
      "Error on this batch = 0.06070711310826582\n",
      "Cost on val dataset after 1060 epochs is = 0.08400026172856408\n",
      "Initial Cost on Val dataset for this epoch 1060 = 0.08400026172856408\n",
      "learning rate for this epoch =  0.08762812881959987\n",
      "Error on this batch = 0.03391257527273224\n",
      "Error on this batch = 0.060679725185201434\n",
      "Cost on val dataset after 1061 epochs is = 0.0839868816654055\n",
      "Initial Cost on Val dataset for this epoch 1061 = 0.0839868816654055\n",
      "learning rate for this epoch =  0.08760747398520836\n",
      "Error on this batch = 0.03389388899525727\n",
      "Error on this batch = 0.06065236531549801\n",
      "Cost on val dataset after 1062 epochs is = 0.08397352626453072\n",
      "Initial Cost on Val dataset for this epoch 1062 = 0.08397352626453072\n",
      "learning rate for this epoch =  0.08758684347065314\n",
      "Error on this batch = 0.03387522985739763\n",
      "Error on this batch = 0.06062503346511698\n",
      "Cost on val dataset after 1063 epochs is = 0.08396019547634591\n",
      "Initial Cost on Val dataset for this epoch 1063 = 0.08396019547634591\n",
      "learning rate for this epoch =  0.08756623722443944\n",
      "Error on this batch = 0.03385659779749326\n",
      "Error on this batch = 0.060597729599483736\n",
      "Cost on val dataset after 1064 epochs is = 0.08394688925128366\n",
      "Initial Cost on Val dataset for this epoch 1064 = 0.08394688925128366\n",
      "learning rate for this epoch =  0.08754565519522983\n",
      "Error on this batch = 0.03383799275454139\n",
      "Error on this batch = 0.06057045368348219\n",
      "Cost on val dataset after 1065 epochs is = 0.08393360753979735\n",
      "Initial Cost on Val dataset for this epoch 1065 = 0.08393360753979735\n",
      "learning rate for this epoch =  0.08752509733184359\n",
      "Error on this batch = 0.03381941466819945\n",
      "Error on this batch = 0.060543205681449656\n",
      "Cost on val dataset after 1066 epochs is = 0.08392035029235528\n",
      "Initial Cost on Val dataset for this epoch 1066 = 0.08392035029235528\n",
      "learning rate for this epoch =  0.0875045635832561\n",
      "Error on this batch = 0.033800863478787586\n",
      "Error on this batch = 0.06051598555717248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on val dataset after 1067 epochs is = 0.0839071174594351\n",
      "Initial Cost on Val dataset for this epoch 1067 = 0.0839071174594351\n",
      "learning rate for this epoch =  0.08748405389859822\n",
      "Error on this batch = 0.03378233912729079\n",
      "Error on this batch = 0.060488793273882076\n",
      "Cost on val dataset after 1068 epochs is = 0.08389390899151827\n",
      "Initial Cost on Val dataset for this epoch 1068 = 0.08389390899151827\n",
      "learning rate for this epoch =  0.08746356822715562\n",
      "Error on this batch = 0.03376384155536074\n",
      "Error on this batch = 0.06046162879425156\n",
      "Cost on val dataset after 1069 epochs is = 0.08388072483908446\n",
      "Initial Cost on Val dataset for this epoch 1069 = 0.08388072483908446\n",
      "learning rate for this epoch =  0.08744310651836827\n",
      "Error on this batch = 0.033745370705317156\n",
      "Error on this batch = 0.06043449208039295\n",
      "Cost on val dataset after 1070 epochs is = 0.08386756495260618\n",
      "Initial Cost on Val dataset for this epoch 1070 = 0.08386756495260618\n",
      "learning rate for this epoch =  0.08742266872182974\n",
      "Error on this batch = 0.0337269265201489\n",
      "Error on this batch = 0.06040738309385496\n",
      "Cost on val dataset after 1071 epochs is = 0.08385442928254341\n",
      "Initial Cost on Val dataset for this epoch 1071 = 0.08385442928254341\n",
      "learning rate for this epoch =  0.08740225478728658\n",
      "Error on this batch = 0.03370850894351455\n",
      "Error on this batch = 0.06038030179562133\n",
      "Cost on val dataset after 1072 epochs is = 0.08384131777933838\n",
      "Initial Cost on Val dataset for this epoch 1072 = 0.08384131777933838\n",
      "learning rate for this epoch =  0.0873818646646378\n",
      "Error on this batch = 0.03369011791974273\n",
      "Error on this batch = 0.06035324814610954\n",
      "Cost on val dataset after 1073 epochs is = 0.08382823039341049\n",
      "Initial Cost on Val dataset for this epoch 1073 = 0.08382823039341049\n",
      "learning rate for this epoch =  0.08736149830393418\n",
      "Error on this batch = 0.03367175339383187\n",
      "Error on this batch = 0.06032622210517055\n",
      "Cost on val dataset after 1074 epochs is = 0.08381516707515106\n",
      "Initial Cost on Val dataset for this epoch 1074 = 0.08381516707515106\n",
      "learning rate for this epoch =  0.0873411556553777\n",
      "Error on this batch = 0.03365341531144977\n",
      "Error on this batch = 0.06029922363208848\n",
      "Cost on val dataset after 1075 epochs is = 0.08380212777491865\n",
      "Initial Cost on Val dataset for this epoch 1075 = 0.08380212777491865\n",
      "learning rate for this epoch =  0.087320836669321\n",
      "Error on this batch = 0.03363510361893256\n",
      "Error on this batch = 0.06027225268558128\n",
      "Cost on val dataset after 1076 epochs is = 0.08378911244303397\n",
      "Initial Cost on Val dataset for this epoch 1076 = 0.08378911244303397\n",
      "learning rate for this epoch =  0.08730054129626662\n",
      "Error on this batch = 0.0336168182632835\n",
      "Error on this batch = 0.06024530922380185\n",
      "Cost on val dataset after 1077 epochs is = 0.08377612102977539\n",
      "Initial Cost on Val dataset for this epoch 1077 = 0.08377612102977539\n",
      "learning rate for this epoch =  0.08728026948686665\n",
      "Error on this batch = 0.03359855919217122\n",
      "Error on this batch = 0.060218393204339596\n",
      "Cost on val dataset after 1078 epochs is = 0.08376315348537428\n",
      "Initial Cost on Val dataset for this epoch 1078 = 0.08376315348537428\n",
      "learning rate for this epoch =  0.0872600211919219\n",
      "Error on this batch = 0.03358032635392753\n",
      "Error on this batch = 0.06019150458422253\n",
      "Cost on val dataset after 1079 epochs is = 0.08375020976001042\n",
      "Initial Cost on Val dataset for this epoch 1079 = 0.08375020976001042\n",
      "learning rate for this epoch =  0.08723979636238147\n",
      "Error on this batch = 0.033562119697544975\n",
      "Error on this batch = 0.0601646433199201\n",
      "Cost on val dataset after 1080 epochs is = 0.08373728980380787\n",
      "Initial Cost on Val dataset for this epoch 1080 = 0.08373728980380787\n",
      "learning rate for this epoch =  0.08721959494934213\n",
      "Error on this batch = 0.033543939172673974\n",
      "Error on this batch = 0.06013780936734625\n",
      "Cost on val dataset after 1081 epochs is = 0.08372439356683059\n",
      "Initial Cost on Val dataset for this epoch 1081 = 0.08372439356683059\n",
      "learning rate for this epoch =  0.0871994169040477\n",
      "Error on this batch = 0.03352578472961949\n",
      "Error on this batch = 0.06011100268186331\n",
      "Cost on val dataset after 1082 epochs is = 0.08371152099907848\n",
      "Initial Cost on Val dataset for this epoch 1082 = 0.08371152099907848\n",
      "learning rate for this epoch =  0.08717926217788849\n",
      "Error on this batch = 0.033507656319337324\n",
      "Error on this batch = 0.06008422321828594\n",
      "Cost on val dataset after 1083 epochs is = 0.08369867205048348\n",
      "Initial Cost on Val dataset for this epoch 1083 = 0.08369867205048348\n",
      "learning rate for this epoch =  0.0871591307224008\n",
      "Error on this batch = 0.033489553893430106\n",
      "Error on this batch = 0.06005747093088616\n",
      "Cost on val dataset after 1084 epochs is = 0.08368584667090559\n",
      "Initial Cost on Val dataset for this epoch 1084 = 0.08368584667090559\n",
      "learning rate for this epoch =  0.08713902248926618\n",
      "Error on this batch = 0.03347147740414296\n",
      "Error on this batch = 0.0600307457733984\n",
      "Cost on val dataset after 1085 epochs is = 0.08367304481012958\n",
      "Initial Cost on Val dataset for this epoch 1085 = 0.08367304481012958\n",
      "learning rate for this epoch =  0.08711893743031107\n",
      "Error on this batch = 0.03345342680435852\n",
      "Error on this batch = 0.06000404769902506\n",
      "Cost on val dataset after 1086 epochs is = 0.08366026641786116\n",
      "Initial Cost on Val dataset for this epoch 1086 = 0.08366026641786116\n",
      "learning rate for this epoch =  0.08709887549750603\n",
      "Error on this batch = 0.03343540204759202\n",
      "Error on this batch = 0.05997737666044291\n",
      "Cost on val dataset after 1087 epochs is = 0.0836475114437239\n",
      "Initial Cost on Val dataset for this epoch 1087 = 0.0836475114437239\n",
      "learning rate for this epoch =  0.08707883664296534\n",
      "Error on this batch = 0.03341740308798557\n",
      "Error on this batch = 0.05995073260980943\n",
      "Cost on val dataset after 1088 epochs is = 0.08363477983725595\n",
      "Initial Cost on Val dataset for this epoch 1088 = 0.08363477983725595\n",
      "learning rate for this epoch =  0.08705882081894635\n",
      "Error on this batch = 0.033399429880302506\n",
      "Error on this batch = 0.05992411549877004\n",
      "Cost on val dataset after 1089 epochs is = 0.08362207154790709\n",
      "Initial Cost on Val dataset for this epoch 1089 = 0.08362207154790709\n",
      "learning rate for this epoch =  0.08703882797784893\n",
      "Error on this batch = 0.03338148237992097\n",
      "Error on this batch = 0.05989752527846541\n",
      "Cost on val dataset after 1090 epochs is = 0.08360938652503584\n",
      "Initial Cost on Val dataset for this epoch 1090 = 0.08360938652503584\n",
      "learning rate for this epoch =  0.08701885807221492\n",
      "Error on this batch = 0.03336356054282768\n",
      "Error on this batch = 0.059870961899539346\n",
      "Cost on val dataset after 1091 epochs is = 0.0835967247179069\n",
      "Initial Cost on Val dataset for this epoch 1091 = 0.0835967247179069\n",
      "learning rate for this epoch =  0.08699891105472762\n",
      "Error on this batch = 0.03334566432561087\n",
      "Error on this batch = 0.059844425312147155\n",
      "Cost on val dataset after 1092 epochs is = 0.08358408607568839\n",
      "Initial Cost on Val dataset for this epoch 1092 = 0.08358408607568839\n",
      "learning rate for this epoch =  0.08697898687821116\n",
      "Error on this batch = 0.03332779368545324\n",
      "Error on this batch = 0.059817915465964176\n",
      "Cost on val dataset after 1093 epochs is = 0.08357147054744983\n",
      "Initial Cost on Val dataset for this epoch 1093 = 0.08357147054744983\n",
      "learning rate for this epoch =  0.08695908549563003\n",
      "Error on this batch = 0.03330994858012451\n",
      "Error on this batch = 0.05979143231019487\n",
      "Cost on val dataset after 1094 epochs is = 0.08355887808215975\n",
      "Initial Cost on Val dataset for this epoch 1094 = 0.08355887808215975\n",
      "learning rate for this epoch =  0.08693920686008848\n",
      "Error on this batch = 0.03329212896797364\n",
      "Error on this batch = 0.05976497579358217\n",
      "Cost on val dataset after 1095 epochs is = 0.08354630862868383\n",
      "Initial Cost on Val dataset for this epoch 1095 = 0.08354630862868383\n",
      "learning rate for this epoch =  0.08691935092482998\n",
      "Error on this batch = 0.03327433480792088\n",
      "Error on this batch = 0.05973854586441718\n",
      "Cost on val dataset after 1096 epochs is = 0.08353376213578291\n",
      "Initial Cost on Val dataset for this epoch 1096 = 0.08353376213578291\n",
      "learning rate for this epoch =  0.08689951764323674\n",
      "Error on this batch = 0.03325656605944946\n",
      "Error on this batch = 0.059712142470549234\n",
      "Cost on val dataset after 1097 epochs is = 0.08352123855211162\n",
      "Initial Cost on Val dataset for this epoch 1097 = 0.08352123855211162\n",
      "learning rate for this epoch =  0.08687970696882906\n",
      "Error on this batch = 0.033238822682597084\n",
      "Error on this batch = 0.05968576555939622\n",
      "Cost on val dataset after 1098 epochs is = 0.08350873782621661\n",
      "Initial Cost on Val dataset for this epoch 1098 = 0.08350873782621661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate for this epoch =  0.08685991885526495\n",
      "Error on this batch = 0.03322110463794709\n",
      "Error on this batch = 0.05965941507795528\n",
      "Cost on val dataset after 1099 epochs is = 0.08349625990653549\n",
      "Initial Cost on Val dataset for this epoch 1099 = 0.08349625990653549\n",
      "learning rate for this epoch =  0.08684015325633943\n",
      "Error on this batch = 0.03320341188661948\n",
      "Error on this batch = 0.05963309097281366\n",
      "Cost on val dataset after 1100 epochs is = 0.08348380474139559\n",
      "Initial Cost on Val dataset for this epoch 1100 = 0.08348380474139559\n",
      "learning rate for this epoch =  0.08682041012598414\n",
      "Error on this batch = 0.03318574439026177\n",
      "Error on this batch = 0.059606793190159844\n",
      "Cost on val dataset after 1101 epochs is = 0.08347137227901312\n",
      "Initial Cost on Val dataset for this epoch 1101 = 0.08347137227901312\n",
      "learning rate for this epoch =  0.08680068941826673\n",
      "Error on this batch = 0.033168102111039446\n",
      "Error on this batch = 0.0595805216757951\n",
      "Cost on val dataset after 1102 epochs is = 0.08345896246749232\n",
      "Initial Cost on Val dataset for this epoch 1102 = 0.08345896246749232\n",
      "learning rate for this epoch =  0.08678099108739035\n",
      "Error on this batch = 0.033150485011626325\n",
      "Error on this batch = 0.059554276375145036\n",
      "Cost on val dataset after 1103 epochs is = 0.083446575254825\n",
      "Initial Cost on Val dataset for this epoch 1103 = 0.083446575254825\n",
      "learning rate for this epoch =  0.08676131508769315\n",
      "Error on this batch = 0.033132893055194935\n",
      "Error on this batch = 0.05952805723327148\n",
      "Cost on val dataset after 1104 epochs is = 0.08343421058889\n",
      "Initial Cost on Val dataset for this epoch 1104 = 0.08343421058889\n",
      "learning rate for this epoch =  0.08674166137364775\n",
      "Error on this batch = 0.03311532620540633\n",
      "Error on this batch = 0.05950186419488448\n",
      "Cost on val dataset after 1105 epochs is = 0.08342186841745314\n",
      "Initial Cost on Val dataset for this epoch 1105 = 0.08342186841745314\n",
      "learning rate for this epoch =  0.0867220298998607\n",
      "Error on this batch = 0.03309778442640015\n",
      "Error on this batch = 0.059475697204354604\n",
      "Cost on val dataset after 1106 epochs is = 0.08340954868816698\n",
      "Initial Cost on Val dataset for this epoch 1106 = 0.08340954868816698\n",
      "learning rate for this epoch =  0.08670242062107203\n",
      "Error on this batch = 0.0330802676827842\n",
      "Error on this batch = 0.059449556205725375\n",
      "Cost on val dataset after 1107 epochs is = 0.08339725134857111\n",
      "Initial Cost on Val dataset for this epoch 1107 = 0.08339725134857111\n",
      "learning rate for this epoch =  0.08668283349215462\n",
      "Error on this batch = 0.033062775939624194\n",
      "Error on this batch = 0.059423441142725596\n",
      "Cost on val dataset after 1108 epochs is = 0.08338497634609238\n",
      "Initial Cost on Val dataset for this epoch 1108 = 0.08338497634609238\n",
      "learning rate for this epoch =  0.08666326846811381\n",
      "Error on this batch = 0.03304530916243322\n",
      "Error on this batch = 0.05939735195878221\n",
      "Cost on val dataset after 1109 epochs is = 0.0833727236280455\n",
      "Initial Cost on Val dataset for this epoch 1109 = 0.0833727236280455\n",
      "learning rate for this epoch =  0.08664372550408686\n",
      "Error on this batch = 0.03302786731716112\n",
      "Error on this batch = 0.05937128859703305\n",
      "Cost on val dataset after 1110 epochs is = 0.08336049314163346\n",
      "Initial Cost on Val dataset for this epoch 1110 = 0.08336049314163346\n",
      "learning rate for this epoch =  0.0866242045553424\n",
      "Error on this batch = 0.03301045037018376\n",
      "Error on this batch = 0.05934525100033955\n",
      "Cost on val dataset after 1111 epochs is = 0.08334828483394865\n",
      "Initial Cost on Val dataset for this epoch 1111 = 0.08334828483394865\n",
      "learning rate for this epoch =  0.08660470557727994\n",
      "Error on this batch = 0.03299305828829244\n",
      "Error on this batch = 0.05931923911129992\n",
      "Cost on val dataset after 1112 epochs is = 0.08333609865197351\n",
      "Initial Cost on Val dataset for this epoch 1112 = 0.08333609865197351\n",
      "learning rate for this epoch =  0.08658522852542944\n",
      "Error on this batch = 0.03297569103868289\n",
      "Error on this batch = 0.05929325287226206\n",
      "Cost on val dataset after 1113 epochs is = 0.08332393454258208\n",
      "Initial Cost on Val dataset for this epoch 1113 = 0.08332393454258208\n",
      "learning rate for this epoch =  0.0865657733554507\n",
      "Error on this batch = 0.03295834858894455\n",
      "Error on this batch = 0.05926729222533654\n",
      "Cost on val dataset after 1114 epochs is = 0.08331179245254097\n",
      "Initial Cost on Val dataset for this epoch 1114 = 0.08331179245254097\n",
      "learning rate for this epoch =  0.08654634002313295\n",
      "Error on this batch = 0.03294103090704963\n",
      "Error on this batch = 0.0592413571124098\n",
      "Cost on val dataset after 1115 epochs is = 0.08329967232851111\n",
      "Initial Cost on Val dataset for this epoch 1115 = 0.08329967232851111\n",
      "learning rate for this epoch =  0.08652692848439436\n",
      "Error on this batch = 0.03292373796134214\n",
      "Error on this batch = 0.059215447475157286\n",
      "Cost on val dataset after 1116 epochs is = 0.08328757411704929\n",
      "Initial Cost on Val dataset for this epoch 1116 = 0.08328757411704929\n",
      "learning rate for this epoch =  0.08650753869528147\n",
      "Error on this batch = 0.03290646972052706\n",
      "Error on this batch = 0.05918956325505648\n",
      "Cost on val dataset after 1117 epochs is = 0.08327549776460992\n",
      "Initial Cost on Val dataset for this epoch 1117 = 0.08327549776460992\n",
      "learning rate for this epoch =  0.08648817061196874\n",
      "Error on this batch = 0.032889226153659364\n",
      "Error on this batch = 0.059163704393400136\n",
      "Cost on val dataset after 1118 epochs is = 0.08326344321754708\n",
      "Initial Cost on Val dataset for this epoch 1118 = 0.08326344321754708\n",
      "learning rate for this epoch =  0.08646882419075813\n",
      "Error on this batch = 0.03287200723013318\n",
      "Error on this batch = 0.05913787083130925\n",
      "Cost on val dataset after 1119 epochs is = 0.08325141042211663\n",
      "Initial Cost on Val dataset for this epoch 1119 = 0.08325141042211663\n",
      "learning rate for this epoch =  0.08644949938807851\n",
      "Error on this batch = 0.032854812919670795\n",
      "Error on this batch = 0.059112062509746155\n",
      "Cost on val dataset after 1120 epochs is = 0.08323939932447848\n",
      "Initial Cost on Val dataset for this epoch 1120 = 0.08323939932447848\n",
      "learning rate for this epoch =  0.08643019616048525\n",
      "Error on this batch = 0.03283764319231196\n",
      "Error on this batch = 0.05908627936952758\n",
      "Cost on val dataset after 1121 epochs is = 0.08322740987069889\n",
      "Initial Cost on Val dataset for this epoch 1121 = 0.08322740987069889\n",
      "learning rate for this epoch =  0.0864109144646597\n",
      "Error on this batch = 0.03282049801840306\n",
      "Error on this batch = 0.05906052135133752\n",
      "Cost on val dataset after 1122 epochs is = 0.0832154420067532\n",
      "Initial Cost on Val dataset for this epoch 1122 = 0.0832154420067532\n",
      "learning rate for this epoch =  0.08639165425740875\n",
      "Error on this batch = 0.0328033773685863\n",
      "Error on this batch = 0.05903478839574012\n",
      "Cost on val dataset after 1123 epochs is = 0.08320349567852846\n",
      "Initial Cost on Val dataset for this epoch 1123 = 0.08320349567852846\n",
      "learning rate for this epoch =  0.08637241549566431\n",
      "Error on this batch = 0.032786281213789306\n",
      "Error on this batch = 0.059009080443192474\n",
      "Cost on val dataset after 1124 epochs is = 0.08319157083182617\n",
      "Initial Cost on Val dataset for this epoch 1124 = 0.08319157083182617\n",
      "learning rate for this epoch =  0.08635319813648287\n",
      "Error on this batch = 0.032769209525214234\n",
      "Error on this batch = 0.0589833974340572\n",
      "Cost on val dataset after 1125 epochs is = 0.0831796674123654\n",
      "Initial Cost on Val dataset for this epoch 1125 = 0.0831796674123654\n",
      "learning rate for this epoch =  0.08633400213704505\n",
      "Error on this batch = 0.03275216227432757\n",
      "Error on this batch = 0.0589577393086152\n",
      "Cost on val dataset after 1126 epochs is = 0.08316778536578585\n",
      "Initial Cost on Val dataset for this epoch 1126 = 0.08316778536578585\n",
      "learning rate for this epoch =  0.08631482745465505\n",
      "Error on this batch = 0.032735139432849614\n",
      "Error on this batch = 0.05893210600707789\n",
      "Cost on val dataset after 1127 epochs is = 0.08315592463765102\n",
      "Initial Cost on Val dataset for this epoch 1127 = 0.08315592463765102\n",
      "learning rate for this epoch =  0.08629567404674027\n",
      "Error on this batch = 0.03271814097274432\n",
      "Error on this batch = 0.05890649746959961\n",
      "Cost on val dataset after 1128 epochs is = 0.08314408517345162\n",
      "Initial Cost on Val dataset for this epoch 1128 = 0.08314408517345162\n",
      "learning rate for this epoch =  0.0862765418708508\n",
      "Error on this batch = 0.032701166866209046\n",
      "Error on this batch = 0.05888091363628981\n",
      "Cost on val dataset after 1129 epochs is = 0.08313226691860898\n",
      "Initial Cost on Val dataset for this epoch 1129 = 0.08313226691860898\n",
      "learning rate for this epoch =  0.08625743088465897\n",
      "Error on this batch = 0.03268421708566472\n",
      "Error on this batch = 0.05885535444722497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on val dataset after 1130 epochs is = 0.08312046981847872\n",
      "Initial Cost on Val dataset for this epoch 1130 = 0.08312046981847872\n",
      "learning rate for this epoch =  0.0862383410459589\n",
      "Error on this batch = 0.03266729160374576\n",
      "Error on this batch = 0.05882981984246053\n",
      "Cost on val dataset after 1131 epochs is = 0.0831086938183544\n",
      "Initial Cost on Val dataset for this epoch 1131 = 0.0831086938183544\n",
      "learning rate for this epoch =  0.08621927231266602\n",
      "Error on this batch = 0.03265039039329053\n",
      "Error on this batch = 0.058804309762042505\n",
      "Cost on val dataset after 1132 epochs is = 0.0830969388634713\n",
      "Initial Cost on Val dataset for this epoch 1132 = 0.0830969388634713\n",
      "learning rate for this epoch =  0.08620022464281663\n",
      "Error on this batch = 0.03263351342733173\n",
      "Error on this batch = 0.05877882414601903\n",
      "Cost on val dataset after 1133 epochs is = 0.08308520489901049\n",
      "Initial Cost on Val dataset for this epoch 1133 = 0.08308520489901049\n",
      "learning rate for this epoch =  0.08618119799456743\n",
      "Error on this batch = 0.03261666067908694\n",
      "Error on this batch = 0.05875336293445156\n",
      "Cost on val dataset after 1134 epochs is = 0.08307349187010257\n",
      "Initial Cost on Val dataset for this epoch 1134 = 0.08307349187010257\n",
      "learning rate for this epoch =  0.0861621923261951\n",
      "Error on this batch = 0.03259983212194943\n",
      "Error on this batch = 0.058727926067426085\n",
      "Cost on val dataset after 1135 epochs is = 0.08306179972183206\n",
      "Initial Cost on Val dataset for this epoch 1135 = 0.08306179972183206\n",
      "learning rate for this epoch =  0.08614320759609581\n",
      "Error on this batch = 0.03258302772947913\n",
      "Error on this batch = 0.05870251348506399\n",
      "Cost on val dataset after 1136 epochs is = 0.08305012839924135\n",
      "Initial Cost on Val dataset for this epoch 1136 = 0.08305012839924135\n",
      "learning rate for this epoch =  0.08612424376278484\n",
      "Error on this batch = 0.0325662474753936\n",
      "Error on this batch = 0.05867712512753272\n",
      "Cost on val dataset after 1137 epochs is = 0.08303847784733522\n",
      "Initial Cost on Val dataset for this epoch 1137 = 0.08303847784733522\n",
      "learning rate for this epoch =  0.08610530078489602\n",
      "Error on this batch = 0.03254949133355954\n",
      "Error on this batch = 0.05865176093505625\n",
      "Cost on val dataset after 1138 epochs is = 0.08302684801108508\n",
      "Initial Cost on Val dataset for this epoch 1138 = 0.08302684801108508\n",
      "learning rate for this epoch =  0.08608637862118143\n",
      "Error on this batch = 0.03253275927798408\n",
      "Error on this batch = 0.05862642084792539\n",
      "Cost on val dataset after 1139 epochs is = 0.08301523883543344\n",
      "Initial Cost on Val dataset for this epoch 1139 = 0.08301523883543344\n",
      "learning rate for this epoch =  0.08606747723051081\n",
      "Error on this batch = 0.03251605128280664\n",
      "Error on this batch = 0.05860110480650765\n",
      "Cost on val dataset after 1140 epochs is = 0.08300365026529848\n",
      "Initial Cost on Val dataset for this epoch 1140 = 0.08300365026529848\n",
      "learning rate for this epoch =  0.08604859657187126\n",
      "Error on this batch = 0.03249936732229066\n",
      "Error on this batch = 0.05857581275125714\n",
      "Cost on val dataset after 1141 epochs is = 0.08299208224557861\n",
      "Initial Cost on Val dataset for this epoch 1141 = 0.08299208224557861\n",
      "learning rate for this epoch =  0.0860297366043667\n",
      "Error on this batch = 0.03248270737081577\n",
      "Error on this batch = 0.05855054462272414\n",
      "Cost on val dataset after 1142 epochs is = 0.08298053472115716\n",
      "Initial Cost on Val dataset for this epoch 1142 = 0.08298053472115716\n",
      "learning rate for this epoch =  0.08601089728721749\n",
      "Error on this batch = 0.03246607140287011\n",
      "Error on this batch = 0.0585253003615642\n",
      "Cost on val dataset after 1143 epochs is = 0.08296900763690712\n",
      "Initial Cost on Val dataset for this epoch 1143 = 0.08296900763690712\n",
      "learning rate for this epoch =  0.08599207857975999\n",
      "Error on this batch = 0.03244945939304279\n",
      "Error on this batch = 0.05850007990854735\n",
      "Cost on val dataset after 1144 epochs is = 0.08295750093769594\n",
      "Initial Cost on Val dataset for this epoch 1144 = 0.08295750093769594\n",
      "learning rate for this epoch =  0.08597328044144606\n",
      "Error on this batch = 0.03243287131601656\n",
      "Error on this batch = 0.05847488320456675\n",
      "Cost on val dataset after 1145 epochs is = 0.0829460145683904\n",
      "Initial Cost on Val dataset for this epoch 1145 = 0.0829460145683904\n",
      "learning rate for this epoch =  0.08595450283184279\n",
      "Error on this batch = 0.03241630714656091\n",
      "Error on this batch = 0.05844971019064722\n",
      "Cost on val dataset after 1146 epochs is = 0.0829345484738614\n",
      "Initial Cost on Val dataset for this epoch 1146 = 0.0829345484738614\n",
      "learning rate for this epoch =  0.08593574571063191\n",
      "Error on this batch = 0.032399766859525\n",
      "Error on this batch = 0.05842456080795365\n",
      "Cost on val dataset after 1147 epochs is = 0.08292310259898908\n",
      "Initial Cost on Val dataset for this epoch 1147 = 0.08292310259898908\n",
      "learning rate for this epoch =  0.08591700903760942\n",
      "Error on this batch = 0.03238325042983125\n",
      "Error on this batch = 0.058399434997798726\n",
      "Cost on val dataset after 1148 epochs is = 0.08291167688866762\n",
      "Initial Cost on Val dataset for this epoch 1148 = 0.08291167688866762\n",
      "learning rate for this epoch =  0.0858982927726852\n",
      "Error on this batch = 0.03236675783246873\n",
      "Error on this batch = 0.05837433270165089\n",
      "Cost on val dataset after 1149 epochs is = 0.08290027128781048\n",
      "Initial Cost on Val dataset for this epoch 1149 = 0.08290027128781048\n",
      "learning rate for this epoch =  0.08587959687588255\n",
      "Error on this batch = 0.032350289042487136\n",
      "Error on this batch = 0.058349253861141606\n",
      "Cost on val dataset after 1150 epochs is = 0.08288888574135535\n",
      "Initial Cost on Val dataset for this epoch 1150 = 0.08288888574135535\n",
      "learning rate for this epoch =  0.08586092130733781\n",
      "Error on this batch = 0.03233384403499072\n",
      "Error on this batch = 0.058324198418072515\n",
      "Cost on val dataset after 1151 epochs is = 0.08287752019426921\n",
      "Initial Cost on Val dataset for this epoch 1151 = 0.08287752019426921\n",
      "learning rate for this epoch =  0.0858422660272999\n",
      "Error on this batch = 0.032317422785132645\n",
      "Error on this batch = 0.058299166314422515\n",
      "Cost on val dataset after 1152 epochs is = 0.0828661745915535\n",
      "Initial Cost on Val dataset for this epoch 1152 = 0.0828661745915535\n",
      "learning rate for this epoch =  0.08582363099612991\n",
      "Error on this batch = 0.03230102526810943\n",
      "Error on this batch = 0.05827415749235419\n",
      "Cost on val dataset after 1153 epochs is = 0.08285484887824934\n",
      "Initial Cost on Val dataset for this epoch 1153 = 0.08285484887824934\n",
      "learning rate for this epoch =  0.08580501617430071\n",
      "Error on this batch = 0.03228465145915556\n",
      "Error on this batch = 0.05824917189422021\n",
      "Cost on val dataset after 1154 epochs is = 0.08284354299944263\n",
      "Initial Cost on Val dataset for this epoch 1154 = 0.08284354299944263\n",
      "learning rate for this epoch =  0.08578642152239652\n",
      "Error on this batch = 0.032268301333538627\n",
      "Error on this batch = 0.0582242094625693\n",
      "Cost on val dataset after 1155 epochs is = 0.08283225690026923\n",
      "Initial Cost on Val dataset for this epoch 1155 = 0.08283225690026923\n",
      "learning rate for this epoch =  0.08576784700111252\n",
      "Error on this batch = 0.03225197486655426\n",
      "Error on this batch = 0.05819927014015219\n",
      "Cost on val dataset after 1156 epochs is = 0.08282099052592014\n",
      "Initial Cost on Val dataset for this epoch 1156 = 0.08282099052592014\n",
      "learning rate for this epoch =  0.08574929257125441\n",
      "Error on this batch = 0.0322356720335217\n",
      "Error on this batch = 0.05817435386992689\n",
      "Cost on val dataset after 1157 epochs is = 0.08280974382164677\n",
      "Initial Cost on Val dataset for this epoch 1157 = 0.08280974382164677\n",
      "learning rate for this epoch =  0.08573075819373804\n",
      "Error on this batch = 0.032219392809779306\n",
      "Error on this batch = 0.058149460595064005\n",
      "Cost on val dataset after 1158 epochs is = 0.08279851673276617\n",
      "Initial Cost on Val dataset for this epoch 1158 = 0.08279851673276617\n",
      "learning rate for this epoch =  0.085712243829589\n",
      "Error on this batch = 0.032203137170680296\n",
      "Error on this batch = 0.058124590258951586\n",
      "Cost on val dataset after 1159 epochs is = 0.08278730920466612\n",
      "Initial Cost on Val dataset for this epoch 1159 = 0.08278730920466612\n",
      "learning rate for this epoch =  0.08569374943994214\n",
      "Error on this batch = 0.03218690509158897\n",
      "Error on this batch = 0.058099742805199875\n",
      "Cost on val dataset after 1160 epochs is = 0.0827761211828104\n",
      "Initial Cost on Val dataset for this epoch 1160 = 0.0827761211828104\n",
      "learning rate for this epoch =  0.0856752749860413\n",
      "Error on this batch = 0.032170696547876784\n",
      "Error on this batch = 0.05807491817764543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on val dataset after 1161 epochs is = 0.08276495261274412\n",
      "Initial Cost on Val dataset for this epoch 1161 = 0.08276495261274412\n",
      "learning rate for this epoch =  0.08565682042923882\n",
      "Error on this batch = 0.032154511514918986\n",
      "Error on this batch = 0.05805011632035546\n",
      "Cost on val dataset after 1162 epochs is = 0.08275380344009876\n",
      "Initial Cost on Val dataset for this epoch 1162 = 0.08275380344009876\n",
      "learning rate for this epoch =  0.08563838573099518\n",
      "Error on this batch = 0.032138349968091035\n",
      "Error on this batch = 0.058025337177631364\n",
      "Cost on val dataset after 1163 epochs is = 0.08274267361059734\n",
      "Initial Cost on Val dataset for this epoch 1163 = 0.08274267361059734\n",
      "learning rate for this epoch =  0.08561997085287859\n",
      "Error on this batch = 0.032122211882765817\n",
      "Error on this batch = 0.05800058069401241\n",
      "Cost on val dataset after 1164 epochs is = 0.08273156307005963\n",
      "Initial Cost on Val dataset for this epoch 1164 = 0.08273156307005963\n",
      "learning rate for this epoch =  0.08560157575656459\n",
      "Error on this batch = 0.0321060972343105\n",
      "Error on this batch = 0.0579758468142788\n",
      "Cost on val dataset after 1165 epochs is = 0.08272047176440747\n",
      "Initial Cost on Val dataset for this epoch 1165 = 0.08272047176440747\n",
      "learning rate for this epoch =  0.08558320040383566\n",
      "Error on this batch = 0.03209000599808388\n",
      "Error on this batch = 0.05795113548345471\n",
      "Cost on val dataset after 1166 epochs is = 0.0827093996396696\n",
      "Initial Cost on Val dataset for this epoch 1166 = 0.0827093996396696\n",
      "learning rate for this epoch =  0.08556484475658087\n",
      "Error on this batch = 0.032073938149433934\n",
      "Error on this batch = 0.05792644664681088\n",
      "Cost on val dataset after 1167 epochs is = 0.08269834664198691\n",
      "Initial Cost on Val dataset for this epoch 1167 = 0.08269834664198691\n",
      "learning rate for this epoch =  0.08554650877679544\n",
      "Error on this batch = 0.032057893663695505\n",
      "Error on this batch = 0.05790178024986704\n",
      "Cost on val dataset after 1168 epochs is = 0.08268731271761758\n",
      "Initial Cost on Val dataset for this epoch 1168 = 0.08268731271761758\n",
      "learning rate for this epoch =  0.08552819242658037\n",
      "Error on this batch = 0.032041872516188105\n",
      "Error on this batch = 0.05787713623839398\n",
      "Cost on val dataset after 1169 epochs is = 0.08267629781294193\n",
      "Initial Cost on Val dataset for this epoch 1169 = 0.08267629781294193\n",
      "learning rate for this epoch =  0.08550989566814209\n",
      "Error on this batch = 0.032025874682214066\n",
      "Error on this batch = 0.05785251455841541\n",
      "Cost on val dataset after 1170 epochs is = 0.08266530187446763\n",
      "Initial Cost on Val dataset for this epoch 1170 = 0.08266530187446763\n",
      "learning rate for this epoch =  0.08549161846379197\n",
      "Error on this batch = 0.032009900137056804\n",
      "Error on this batch = 0.05782791515620943\n",
      "Cost on val dataset after 1171 epochs is = 0.08265432484883455\n",
      "Initial Cost on Val dataset for this epoch 1171 = 0.08265432484883455\n",
      "learning rate for this epoch =  0.08547336077594611\n",
      "Error on this batch = 0.03199394885597929\n",
      "Error on this batch = 0.05780333797830998\n",
      "Cost on val dataset after 1172 epochs is = 0.08264336668281964\n",
      "Initial Cost on Val dataset for this epoch 1172 = 0.08264336668281964\n",
      "learning rate for this epoch =  0.08545512256712481\n",
      "Error on this batch = 0.031978020814222595\n",
      "Error on this batch = 0.05777878297150766\n",
      "Cost on val dataset after 1173 epochs is = 0.08263242732334192\n",
      "Initial Cost on Val dataset for this epoch 1173 = 0.08263242732334192\n",
      "learning rate for this epoch =  0.08543690379995225\n",
      "Error on this batch = 0.03196211598700472\n",
      "Error on this batch = 0.057754250082850664\n",
      "Cost on val dataset after 1174 epochs is = 0.08262150671746737\n",
      "Initial Cost on Val dataset for this epoch 1174 = 0.08262150671746737\n",
      "learning rate for this epoch =  0.08541870443715613\n",
      "Error on this batch = 0.03194623434951967\n",
      "Error on this batch = 0.05772973925964514\n",
      "Cost on val dataset after 1175 epochs is = 0.08261060481241352\n",
      "Initial Cost on Val dataset for this epoch 1175 = 0.08261060481241352\n",
      "learning rate for this epoch =  0.08540052444156727\n",
      "Error on this batch = 0.0319303758769364\n",
      "Error on this batch = 0.057705250449455486\n",
      "Cost on val dataset after 1176 epochs is = 0.08259972155555435\n",
      "Initial Cost on Val dataset for this epoch 1176 = 0.08259972155555435\n",
      "learning rate for this epoch =  0.0853823637761192\n",
      "Error on this batch = 0.031914540544398344\n",
      "Error on this batch = 0.057680783600104285\n",
      "Cost on val dataset after 1177 epochs is = 0.0825888568944249\n",
      "Initial Cost on Val dataset for this epoch 1177 = 0.0825888568944249\n",
      "learning rate for this epoch =  0.08536422240384793\n",
      "Error on this batch = 0.03189872832702266\n",
      "Error on this batch = 0.057656338659672124\n",
      "Cost on val dataset after 1178 epochs is = 0.08257801077672598\n",
      "Initial Cost on Val dataset for this epoch 1178 = 0.08257801077672598\n",
      "learning rate for this epoch =  0.08534610028789137\n",
      "Error on this batch = 0.03188293919989995\n",
      "Error on this batch = 0.05763191557649689\n",
      "Cost on val dataset after 1179 epochs is = 0.0825671831503286\n",
      "Initial Cost on Val dataset for this epoch 1179 = 0.0825671831503286\n",
      "learning rate for this epoch =  0.08532799739148918\n",
      "Error on this batch = 0.03186717313809412\n",
      "Error on this batch = 0.05760751429917317\n",
      "Cost on val dataset after 1180 epochs is = 0.08255637396327874\n",
      "Initial Cost on Val dataset for this epoch 1180 = 0.08255637396327874\n",
      "learning rate for this epoch =  0.0853099136779822\n",
      "Error on this batch = 0.031851430116642036\n",
      "Error on this batch = 0.05758313477655109\n",
      "Cost on val dataset after 1181 epochs is = 0.08254558316380146\n",
      "Initial Cost on Val dataset for this epoch 1181 = 0.08254558316380146\n",
      "learning rate for this epoch =  0.08529184911081227\n",
      "Error on this batch = 0.03183571011055381\n",
      "Error on this batch = 0.057558776957735154\n",
      "Cost on val dataset after 1182 epochs is = 0.08253481070030563\n",
      "Initial Cost on Val dataset for this epoch 1182 = 0.08253481070030563\n",
      "learning rate for this epoch =  0.08527380365352172\n",
      "Error on this batch = 0.031820013094812895\n",
      "Error on this batch = 0.05753444079208271\n",
      "Cost on val dataset after 1183 epochs is = 0.08252405652138796\n",
      "Initial Cost on Val dataset for this epoch 1183 = 0.08252405652138796\n",
      "learning rate for this epoch =  0.08525577726975313\n",
      "Error on this batch = 0.03180433904437638\n",
      "Error on this batch = 0.05751012622920215\n",
      "Cost on val dataset after 1184 epochs is = 0.08251332057583748\n",
      "Initial Cost on Val dataset for this epoch 1184 = 0.08251332057583748\n",
      "learning rate for this epoch =  0.08523776992324884\n",
      "Error on this batch = 0.0317886879341754\n",
      "Error on this batch = 0.05748583321895108\n",
      "Cost on val dataset after 1185 epochs is = 0.08250260281263959\n",
      "Initial Cost on Val dataset for this epoch 1185 = 0.08250260281263959\n",
      "learning rate for this epoch =  0.08521978157785072\n",
      "Error on this batch = 0.03177305973911584\n",
      "Error on this batch = 0.05746156171143401\n",
      "Cost on val dataset after 1186 epochs is = 0.0824919031809802\n",
      "Initial Cost on Val dataset for this epoch 1186 = 0.0824919031809802\n",
      "learning rate for this epoch =  0.08520181219749971\n",
      "Error on this batch = 0.03175745443407879\n",
      "Error on this batch = 0.057437311656999966\n",
      "Cost on val dataset after 1187 epochs is = 0.08248122163024985\n",
      "Initial Cost on Val dataset for this epoch 1187 = 0.08248122163024985\n",
      "learning rate for this epoch =  0.08518386174623557\n",
      "Error on this batch = 0.03174187199392162\n",
      "Error on this batch = 0.05741308300623991\n",
      "Cost on val dataset after 1188 epochs is = 0.08247055811004751\n",
      "Initial Cost on Val dataset for this epoch 1188 = 0.08247055811004751\n",
      "learning rate for this epoch =  0.0851659301881964\n",
      "Error on this batch = 0.03172631239347864\n",
      "Error on this batch = 0.05738887570998394\n",
      "Cost on val dataset after 1189 epochs is = 0.08245991257018474\n",
      "Initial Cost on Val dataset for this epoch 1189 = 0.08245991257018474\n",
      "learning rate for this epoch =  0.0851480174876184\n",
      "Error on this batch = 0.031710775607562136\n",
      "Error on this batch = 0.05736468971929805\n",
      "Cost on val dataset after 1190 epochs is = 0.08244928496068914\n",
      "Initial Cost on Val dataset for this epoch 1190 = 0.08244928496068914\n",
      "learning rate for this epoch =  0.08513012360883546\n",
      "Error on this batch = 0.03169526161096362\n",
      "Error on this batch = 0.05734052498548108\n",
      "Cost on val dataset after 1191 epochs is = 0.08243867523180844\n",
      "Initial Cost on Val dataset for this epoch 1191 = 0.08243867523180844\n",
      "learning rate for this epoch =  0.08511224851627883\n",
      "Error on this batch = 0.03167977037845489\n",
      "Error on this batch = 0.05731638146006115\n",
      "Cost on val dataset after 1192 epochs is = 0.0824280833340139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Cost on Val dataset for this epoch 1192 = 0.0824280833340139\n",
      "learning rate for this epoch =  0.08509439217447677\n",
      "Error on this batch = 0.03166430188478931\n",
      "Error on this batch = 0.05729225909479205\n",
      "Cost on val dataset after 1193 epochs is = 0.08241750921800405\n",
      "Initial Cost on Val dataset for this epoch 1193 = 0.08241750921800405\n",
      "learning rate for this epoch =  0.08507655454805418\n",
      "Error on this batch = 0.03164885610470324\n",
      "Error on this batch = 0.05726815784164934\n",
      "Cost on val dataset after 1194 epochs is = 0.08240695283470806\n",
      "Initial Cost on Val dataset for this epoch 1194 = 0.08240695283470806\n",
      "learning rate for this epoch =  0.08505873560173234\n",
      "Error on this batch = 0.03163343301291743\n",
      "Error on this batch = 0.05724407765282646\n",
      "Cost on val dataset after 1195 epochs is = 0.08239641413528927\n",
      "Initial Cost on Val dataset for this epoch 1195 = 0.08239641413528927\n",
      "learning rate for this epoch =  0.08504093530032843\n",
      "Error on this batch = 0.031618032584138575\n",
      "Error on this batch = 0.057220018480730356\n",
      "Cost on val dataset after 1196 epochs is = 0.0823858930711484\n",
      "Initial Cost on Val dataset for this epoch 1196 = 0.0823858930711484\n",
      "learning rate for this epoch =  0.08502315360875533\n",
      "Error on this batch = 0.03160265479306088\n",
      "Error on this batch = 0.05719598027797719\n",
      "Cost on val dataset after 1197 epochs is = 0.0823753895939268\n",
      "Initial Cost on Val dataset for this epoch 1197 = 0.0823753895939268\n",
      "learning rate for this epoch =  0.08500539049202116\n",
      "Error on this batch = 0.0315872996143677\n",
      "Error on this batch = 0.05717196299738781\n",
      "Cost on val dataset after 1198 epochs is = 0.08236490365550979\n",
      "Initial Cost on Val dataset for this epoch 1198 = 0.08236490365550979\n",
      "learning rate for this epoch =  0.08498764591522903\n",
      "Error on this batch = 0.03157196702273333\n",
      "Error on this batch = 0.05714796659198296\n",
      "Cost on val dataset after 1199 epochs is = 0.08235443520802938\n",
      "Initial Cost on Val dataset for this epoch 1199 = 0.08235443520802938\n",
      "learning rate for this epoch =  0.08496991984357669\n",
      "Error on this batch = 0.03155665699282466\n",
      "Error on this batch = 0.05712399101497843\n",
      "Cost on val dataset after 1200 epochs is = 0.08234398420386768\n",
      "Initial Cost on Val dataset for this epoch 1200 = 0.08234398420386768\n",
      "learning rate for this epoch =  0.08495221224235612\n",
      "Error on this batch = 0.031541369499303126\n",
      "Error on this batch = 0.057100036219780013\n",
      "Cost on val dataset after 1201 epochs is = 0.08233355059565942\n",
      "Initial Cost on Val dataset for this epoch 1201 = 0.08233355059565942\n",
      "learning rate for this epoch =  0.08493452307695332\n",
      "Error on this batch = 0.03152610451682649\n",
      "Error on this batch = 0.0570761021599784\n",
      "Cost on val dataset after 1202 epochs is = 0.08232313433629496\n",
      "Initial Cost on Val dataset for this epoch 1202 = 0.08232313433629496\n",
      "learning rate for this epoch =  0.08491685231284785\n",
      "Error on this batch = 0.03151086202005081\n",
      "Error on this batch = 0.05705218878934368\n",
      "Cost on val dataset after 1203 epochs is = 0.08231273537892292\n",
      "Initial Cost on Val dataset for this epoch 1203 = 0.08231273537892292\n",
      "learning rate for this epoch =  0.08489919991561257\n",
      "Error on this batch = 0.03149564198363237\n",
      "Error on this batch = 0.057028296061820076\n",
      "Cost on val dataset after 1204 epochs is = 0.08230235367695293\n",
      "Initial Cost on Val dataset for this epoch 1204 = 0.08230235367695293\n",
      "learning rate for this epoch =  0.08488156585091333\n",
      "Error on this batch = 0.03148044438222963\n",
      "Error on this batch = 0.05700442393152014\n",
      "Cost on val dataset after 1205 epochs is = 0.08229198918405808\n",
      "Initial Cost on Val dataset for this epoch 1205 = 0.08229198918405808\n",
      "learning rate for this epoch =  0.0848639500845086\n",
      "Error on this batch = 0.03146526919050538\n",
      "Error on this batch = 0.0569805723527193\n",
      "Cost on val dataset after 1206 epochs is = 0.08228164185417727\n",
      "Initial Cost on Val dataset for this epoch 1206 = 0.08228164185417727\n",
      "learning rate for this epoch =  0.08484635258224914\n",
      "Error on this batch = 0.031450116383128675\n",
      "Error on this batch = 0.05695674127984967\n",
      "Cost on val dataset after 1207 epochs is = 0.08227131164151784\n",
      "Initial Cost on Val dataset for this epoch 1207 = 0.08227131164151784\n",
      "learning rate for this epoch =  0.08482877331007768\n",
      "Error on this batch = 0.03143498593477688\n",
      "Error on this batch = 0.05693293066749437\n",
      "Cost on val dataset after 1208 epochs is = 0.08226099850055758\n",
      "Initial Cost on Val dataset for this epoch 1208 = 0.08226099850055758\n",
      "learning rate for this epoch =  0.08481121223402864\n",
      "Error on this batch = 0.03141987782013793\n",
      "Error on this batch = 0.05690914047038139\n",
      "Cost on val dataset after 1209 epochs is = 0.08225070238604716\n",
      "Initial Cost on Val dataset for this epoch 1209 = 0.08225070238604716\n",
      "learning rate for this epoch =  0.08479366932022776\n",
      "Error on this batch = 0.03140479201391224\n",
      "Error on this batch = 0.05688537064337726\n",
      "Cost on val dataset after 1210 epochs is = 0.08224042325301197\n",
      "Initial Cost on Val dataset for this epoch 1210 = 0.08224042325301197\n",
      "learning rate for this epoch =  0.08477614453489178\n",
      "Error on this batch = 0.03138972849081496\n",
      "Error on this batch = 0.056861621141481\n",
      "Cost on val dataset after 1211 epochs is = 0.0822301610567543\n",
      "Initial Cost on Val dataset for this epoch 1211 = 0.0822301610567543\n",
      "learning rate for this epoch =  0.08475863784432815\n",
      "Error on this batch = 0.03137468722557801\n",
      "Error on this batch = 0.05683789191981761\n",
      "Cost on val dataset after 1212 epochs is = 0.08221991575285528\n",
      "Initial Cost on Val dataset for this epoch 1212 = 0.08221991575285528\n",
      "learning rate for this epoch =  0.08474114921493468\n",
      "Error on this batch = 0.03135966819295232\n",
      "Error on this batch = 0.056814182933631656\n",
      "Cost on val dataset after 1213 epochs is = 0.08220968729717656\n",
      "Initial Cost on Val dataset for this epoch 1213 = 0.08220968729717656\n",
      "learning rate for this epoch =  0.08472367861319927\n",
      "Error on this batch = 0.031344671367709846\n",
      "Error on this batch = 0.05679049413828079\n",
      "Cost on val dataset after 1214 epochs is = 0.08219947564586222\n",
      "Initial Cost on Val dataset for this epoch 1214 = 0.08219947564586222\n",
      "learning rate for this epoch =  0.0847062260056995\n",
      "Error on this batch = 0.031329696724645764\n",
      "Error on this batch = 0.05676682548922907\n",
      "Cost on val dataset after 1215 epochs is = 0.08218928075534028\n",
      "Initial Cost on Val dataset for this epoch 1215 = 0.08218928075534028\n",
      "learning rate for this epoch =  0.08468879135910246\n",
      "Error on this batch = 0.03131474423858059\n",
      "Error on this batch = 0.05674317694204035\n",
      "Cost on val dataset after 1216 epochs is = 0.08217910258232443\n",
      "Initial Cost on Val dataset for this epoch 1216 = 0.08217910258232443\n",
      "learning rate for this epoch =  0.08467137464016429\n",
      "Error on this batch = 0.0312998138843623\n",
      "Error on this batch = 0.056719548452371565\n",
      "Cost on val dataset after 1217 epochs is = 0.08216894108381535\n",
      "Initial Cost on Val dataset for this epoch 1217 = 0.08216894108381535\n",
      "learning rate for this epoch =  0.08465397581572995\n",
      "Error on this batch = 0.03128490563686834\n",
      "Error on this batch = 0.05669593997596588\n",
      "Cost on val dataset after 1218 epochs is = 0.08215879621710219\n",
      "Initial Cost on Val dataset for this epoch 1218 = 0.08215879621710219\n",
      "learning rate for this epoch =  0.08463659485273294\n",
      "Error on this batch = 0.03127001947100784\n",
      "Error on this batch = 0.056672351468645916\n",
      "Cost on val dataset after 1219 epochs is = 0.08214866793976386\n",
      "Initial Cost on Val dataset for this epoch 1219 = 0.08214866793976386\n",
      "learning rate for this epoch =  0.0846192317181949\n",
      "Error on this batch = 0.03125515536172365\n",
      "Error on this batch = 0.05664878288630687\n",
      "Cost on val dataset after 1220 epochs is = 0.08213855620967037\n",
      "Initial Cost on Val dataset for this epoch 1220 = 0.08213855620967037\n",
      "learning rate for this epoch =  0.08460188637922533\n",
      "Error on this batch = 0.03124031328399428\n",
      "Error on this batch = 0.05662523418490961\n",
      "Cost on val dataset after 1221 epochs is = 0.08212846098498372\n",
      "Initial Cost on Val dataset for this epoch 1221 = 0.08212846098498372\n",
      "learning rate for this epoch =  0.08458455880302138\n",
      "Error on this batch = 0.031225493212836044\n",
      "Error on this batch = 0.05660170532047377\n",
      "Cost on val dataset after 1222 epochs is = 0.08211838222415925\n",
      "Initial Cost on Val dataset for this epoch 1222 = 0.08211838222415925\n",
      "learning rate for this epoch =  0.08456724895686739\n",
      "Error on this batch = 0.03121069512330499\n",
      "Error on this batch = 0.0565781962490707\n",
      "Cost on val dataset after 1223 epochs is = 0.08210831988594637\n",
      "Initial Cost on Val dataset for this epoch 1223 = 0.08210831988594637\n",
      "learning rate for this epoch =  0.08454995680813471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on this batch = 0.031195918990498842\n",
      "Error on this batch = 0.056554706926816606\n",
      "Cost on val dataset after 1224 epochs is = 0.08209827392938956\n",
      "Initial Cost on Val dataset for this epoch 1224 = 0.08209827392938956\n",
      "learning rate for this epoch =  0.08453268232428135\n",
      "Error on this batch = 0.031181164789558992\n",
      "Error on this batch = 0.05653123730986552\n",
      "Cost on val dataset after 1225 epochs is = 0.0820882443138293\n",
      "Initial Cost on Val dataset for this epoch 1225 = 0.0820882443138293\n",
      "learning rate for this epoch =  0.08451542547285165\n",
      "Error on this batch = 0.031166432495672344\n",
      "Error on this batch = 0.05650778735440229\n",
      "Cost on val dataset after 1226 epochs is = 0.08207823099890256\n",
      "Initial Cost on Val dataset for this epoch 1226 = 0.08207823099890256\n",
      "learning rate for this epoch =  0.08449818622147606\n",
      "Error on this batch = 0.031151722084073165\n",
      "Error on this batch = 0.05648435701663568\n",
      "Cost on val dataset after 1227 epochs is = 0.08206823394454364\n",
      "Initial Cost on Val dataset for this epoch 1227 = 0.08206823394454364\n",
      "learning rate for this epoch =  0.08448096453787077\n",
      "Error on this batch = 0.03113703353004488\n",
      "Error on this batch = 0.05646094625279126\n",
      "Cost on val dataset after 1228 epochs is = 0.0820582531109847\n",
      "Initial Cost on Val dataset for this epoch 1228 = 0.0820582531109847\n",
      "learning rate for this epoch =  0.08446376038983743\n",
      "Error on this batch = 0.03112236680892192\n",
      "Error on this batch = 0.05643755501910462\n",
      "Cost on val dataset after 1229 epochs is = 0.08204828845875617\n",
      "Initial Cost on Val dataset for this epoch 1229 = 0.08204828845875617\n",
      "learning rate for this epoch =  0.08444657374526286\n",
      "Error on this batch = 0.03110772189609133\n",
      "Error on this batch = 0.05641418327181434\n",
      "Cost on val dataset after 1230 epochs is = 0.08203833994868735\n",
      "Initial Cost on Val dataset for this epoch 1230 = 0.08203833994868735\n",
      "learning rate for this epoch =  0.08442940457211878\n",
      "Error on this batch = 0.031093098766994554\n",
      "Error on this batch = 0.05639083096715506\n",
      "Cost on val dataset after 1231 epochs is = 0.08202840754190657\n",
      "Initial Cost on Val dataset for this epoch 1231 = 0.08202840754190657\n",
      "learning rate for this epoch =  0.0844122528384615\n",
      "Error on this batch = 0.031078497397128972\n",
      "Error on this batch = 0.05636749806135073\n",
      "Cost on val dataset after 1232 epochs is = 0.08201849119984153\n",
      "Initial Cost on Val dataset for this epoch 1232 = 0.08201849119984153\n",
      "learning rate for this epoch =  0.08439511851243159\n",
      "Error on this batch = 0.031063917762049514\n",
      "Error on this batch = 0.0563441845106076\n",
      "Cost on val dataset after 1233 epochs is = 0.08200859088421966\n",
      "Initial Cost on Val dataset for this epoch 1233 = 0.08200859088421966\n",
      "learning rate for this epoch =  0.08437800156225363\n",
      "Error on this batch = 0.031049359837370206\n",
      "Error on this batch = 0.056320890271107635\n",
      "Cost on val dataset after 1234 epochs is = 0.08199870655706795\n",
      "Initial Cost on Val dataset for this epoch 1234 = 0.08199870655706795\n",
      "learning rate for this epoch =  0.08436090195623593\n",
      "Error on this batch = 0.031034823598765575\n",
      "Error on this batch = 0.056297615299001666\n",
      "Cost on val dataset after 1235 epochs is = 0.08198883818071322\n",
      "Initial Cost on Val dataset for this epoch 1235 = 0.08198883818071322\n",
      "learning rate for this epoch =  0.08434381966277021\n",
      "Error on this batch = 0.03102030902197206\n",
      "Error on this batch = 0.056274359550402706\n",
      "Cost on val dataset after 1236 epochs is = 0.08197898571778207\n",
      "cost initial= 0.08198883818071322 , cost final=0.08197898571778207 , change in cost= -9.852462931142503e-06\n",
      "\n",
      "------------------------------------------------------------------------------\n",
      "The stats for number of units in the hidden layer = 100 are as below:\n",
      "------------------------------------------------------------------------------\n",
      "The number of epochs = 1236\n",
      "The training time = 249.345sec\n",
      "The training accuracy is = 95.719%\n",
      "The validation accuracy is = 90.769%\n",
      "The test accuracy is = 89.631%\n",
      "------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = []\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "valid_accuracy = []\n",
    "train_time = []\n",
    "\n",
    "lr0=0.5\n",
    "\n",
    "for i in range(len(arch_test)):\n",
    "    theta = theta_init(n, r, [arch_test[i]], 'normal')\n",
    "    #print(theta[0].shape, theta[1].shape, theta[2].shape)\n",
    "    print(\"Training the network with {} hidden layer with {} units\".format(len([arch_test[i]]), arch_test[i]))\n",
    "    print(\"The parameters of the layers are of the shape:\")\n",
    "\n",
    "    for j in range(len(theta)):\n",
    "        print(\"theta between layer {} and layer {} is {}\".format(j, j+1,theta[j].shape))\n",
    "\n",
    "    start = time.time()\n",
    "    epoch, theta = training(mini_batch, X_valid, valid_class_enc, theta, lr0, 'sigmoid', 'adaptive')\n",
    "    epochs.append(epoch)\n",
    "    train_time.append(time.time()-start)\n",
    "    train_accuracy.append(calc_accuracy(X_train, theta, train_class_enc))\n",
    "    valid_accuracy.append(calc_accuracy(X_valid, theta, valid_class_enc))\n",
    "    test_accuracy.append(calc_accuracy(X_test, theta, test_actual_class_enc))\n",
    "    print(\"\\n------------------------------------------------------------------------------\")\n",
    "    print(\"The stats for number of units in the hidden layer = {} are as below:\".format(arch_test[i]))\n",
    "    print(\"------------------------------------------------------------------------------\")\n",
    "    print(\"The number of epochs = {}\".format(epochs[-1]))\n",
    "    print(\"The training time = {:2.3f}sec\".format(train_time[-1]))\n",
    "    print(\"The training accuracy is = {:2.3f}%\".format(train_accuracy[-1]))\n",
    "    print(\"The validation accuracy is = {:2.3f}%\".format(valid_accuracy[-1]))\n",
    "    print(\"The test accuracy is = {:2.3f}%\".format(test_accuracy[-1]))\n",
    "    print(\"------------------------------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"------------------Plotting Graphs for Part C - Adaptive LR - One Hidden Layer ------------------\")\n",
    "\n",
    "plot_accuracy(arch_test, train_accuracy, test_accuracy, valid_accuracy)\n",
    "plot_epoch(arch_test, epochs, train_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part D - Implementation of ReLU activation for Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------Running Part D-----------------------------------------------------\n",
      "------------------Training a 100x100 hidden layer network with Sigmoid activation------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"----------------------------Running Part D-----------------------------------------------------\")\n",
    "print(\"------------------Training a 100x100 hidden layer network with Sigmoid activation------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = []\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "valid_accuracy = []\n",
    "train_time = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the network with 2 hidden layer with [100, 100] units\n",
      "The parameters of the layers are of the shape:\n",
      "theta between layer 0 and layer 1 is (785, 100)\n",
      "theta between layer 1 and layer 2 is (101, 100)\n",
      "theta between layer 2 and layer 3 is (101, 26)\n",
      "Initial Cost on Val dataset for this epoch 1 = 2.917916540566109\n",
      "learning rate for this epoch =  1.5\n",
      "Error on this batch = 2.8903573047255997\n",
      "Delta shape =  (5, 101) (5, 101) (5, 26)\n",
      "theta shape =  (785, 100) (101, 100) (101, 26)\n",
      "Cost on val dataset after 2 epochs is = 0.509783305593978\n",
      "\n",
      "------------------------------------------------------------------------------\n",
      "The stats for number of units in the hidden layer = [100, 100] with Sigmoid are as below:\n",
      "------------------------------------------------------------------------------\n",
      "The number of epochs with Sigmoid is = 2\n",
      "The training time with Sigmoid is = 0.028sec\n",
      "The training accuracy with Sigmoid is = 3.928%\n",
      "The validation accuracy with Sigmoid is = 3.385%\n",
      "The test accuracy with Sigmoid is = 3.846%\n",
      "------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "arch=[100, 100]\n",
    "lr=1.5\n",
    "theta = theta_init(n, r, arch, 'normal')\n",
    "print(\"Training the network with {} hidden layer with {} units\".format(len(arch), arch))\n",
    "print(\"The parameters of the layers are of the shape:\")\n",
    "\n",
    "for j in range(len(theta)):\n",
    "    print(\"theta between layer {} and layer {} is {}\".format(j, j+1,theta[j].shape))\n",
    "    \n",
    "start = time.time()\n",
    "epoch, theta = training(mini_batch, X_valid, valid_class_enc, theta, lr, 'sigmoid', 'adaptive')\n",
    "\n",
    "epochs.append(epoch)\n",
    "train_time.append(time.time()-start)\n",
    "train_accuracy.append(calc_accuracy(X_train, theta, train_class_enc))\n",
    "valid_accuracy.append(calc_accuracy(X_valid, theta, valid_class_enc))\n",
    "test_accuracy.append(calc_accuracy(X_test, theta, test_actual_class_enc))\n",
    "\n",
    "print(\"\\n------------------------------------------------------------------------------\")\n",
    "print(\"The stats for number of units in the hidden layer = {} with Sigmoid are as below:\".format(arch))\n",
    "print(\"------------------------------------------------------------------------------\")\n",
    "print(\"The number of epochs with Sigmoid is = {}\".format(epochs[-1]))\n",
    "print(\"The training time with Sigmoid is = {:2.3f}sec\".format(train_time[-1]))\n",
    "print(\"The training accuracy with Sigmoid is = {:2.3f}%\".format(train_accuracy[-1]))\n",
    "print(\"The validation accuracy with Sigmoid is = {:2.3f}%\".format(valid_accuracy[-1]))\n",
    "print(\"The test accuracy with Sigmoid is = {:2.3f}%\".format(test_accuracy[-1]))\n",
    "print(\"------------------------------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------Running Part D-----------------------------------------------------\n",
      "------------------Training a 100x100 hidden layer network with ReLU activation------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"----------------------------Running Part D-----------------------------------------------------\")\n",
    "print(\"------------------Training a 100x100 hidden layer network with ReLU activation------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the network with 2 hidden layer with [100, 100] units\n",
      "The parameters of the layers are of the shape:\n",
      "theta between layer 0 and layer 1 is (785, 100)\n",
      "theta between layer 1 and layer 2 is (101, 100)\n",
      "theta between layer 2 and layer 3 is (101, 26)\n",
      "Initial Cost on Val dataset for this epoch 1 = 3.26701228540996\n",
      "Error on this batch = 3.2685168044042814\n",
      "Error on this batch = 0.4996673061852154\n",
      "Cost on val dataset after 2 epochs is = 0.49953592365448296\n",
      "Initial Cost on Val dataset for this epoch 2 = 0.49953592365448296\n",
      "Error on this batch = 0.4995624888696448\n",
      "Error on this batch = 0.49771228058770534\n",
      "Cost on val dataset after 3 epochs is = 0.48770395180711484\n",
      "Initial Cost on Val dataset for this epoch 3 = 0.48770395180711484\n",
      "Error on this batch = 0.48590435229364864\n",
      "Error on this batch = 0.4844095017717498\n",
      "Cost on val dataset after 4 epochs is = 0.4829962758941105\n",
      "Initial Cost on Val dataset for this epoch 4 = 0.4829962758941105\n",
      "Error on this batch = 0.48198045154742886\n",
      "Error on this batch = 0.4814112339709867\n",
      "Cost on val dataset after 5 epochs is = 0.4810394833569399\n",
      "Initial Cost on Val dataset for this epoch 5 = 0.4810394833569399\n",
      "Error on this batch = 0.48029357564040737\n",
      "Error on this batch = 0.479835596767896\n",
      "Cost on val dataset after 6 epochs is = 0.4794286732477114\n",
      "Initial Cost on Val dataset for this epoch 6 = 0.4794286732477114\n",
      "Error on this batch = 0.4785326521294485\n",
      "Error on this batch = 0.47743800343794696\n",
      "Cost on val dataset after 7 epochs is = 0.47622182671657115\n",
      "Initial Cost on Val dataset for this epoch 7 = 0.47622182671657115\n",
      "Error on this batch = 0.47406241235619734\n",
      "Error on this batch = 0.4699766621549989\n",
      "Cost on val dataset after 8 epochs is = 0.46516256905543896\n",
      "Initial Cost on Val dataset for this epoch 8 = 0.46516256905543896\n",
      "Error on this batch = 0.45679539564439886\n",
      "Error on this batch = 0.44859298773298806\n",
      "Cost on val dataset after 9 epochs is = 0.4434192080474046\n",
      "Initial Cost on Val dataset for this epoch 9 = 0.4434192080474046\n",
      "Error on this batch = 0.43022405228506866\n",
      "Error on this batch = 0.42103155526587915\n",
      "Cost on val dataset after 10 epochs is = 0.4197793909623293\n",
      "Initial Cost on Val dataset for this epoch 10 = 0.4197793909623293\n",
      "Error on this batch = 0.40456329220657367\n",
      "Error on this batch = 0.3934190550380836\n",
      "Cost on val dataset after 11 epochs is = 0.3911787659676683\n",
      "Initial Cost on Val dataset for this epoch 11 = 0.3911787659676683\n",
      "Error on this batch = 0.37281453457530633\n",
      "Error on this batch = 0.36074477619004996\n",
      "Cost on val dataset after 12 epochs is = 0.35300111959140906\n",
      "Initial Cost on Val dataset for this epoch 12 = 0.35300111959140906\n",
      "Error on this batch = 0.3381941352266166\n",
      "Error on this batch = 0.32644251885789616\n",
      "Cost on val dataset after 13 epochs is = 0.3207817723276397\n",
      "Initial Cost on Val dataset for this epoch 13 = 0.3207817723276397\n",
      "Error on this batch = 0.2977979320382746\n",
      "Error on this batch = 0.30127918809602533\n",
      "Cost on val dataset after 14 epochs is = 0.29979500846097734\n",
      "Initial Cost on Val dataset for this epoch 14 = 0.29979500846097734\n",
      "Error on this batch = 0.2918204200059148\n",
      "Error on this batch = 0.2793077287043002\n",
      "Cost on val dataset after 15 epochs is = 0.2765038414297078\n",
      "Initial Cost on Val dataset for this epoch 15 = 0.2765038414297078\n",
      "Error on this batch = 0.2669611057990093\n",
      "Error on this batch = 0.25977623275765527\n",
      "Cost on val dataset after 16 epochs is = 0.25585011971784954\n",
      "Initial Cost on Val dataset for this epoch 16 = 0.25585011971784954\n",
      "Error on this batch = 0.2428550203860656\n",
      "Error on this batch = 0.2407592583059089\n",
      "Cost on val dataset after 17 epochs is = 0.25253407585649007\n",
      "Initial Cost on Val dataset for this epoch 17 = 0.25253407585649007\n",
      "Error on this batch = 0.23464722341759583\n",
      "Error on this batch = 0.2368502016371366\n",
      "Cost on val dataset after 18 epochs is = 0.23753067305268297\n",
      "Initial Cost on Val dataset for this epoch 18 = 0.23753067305268297\n",
      "Error on this batch = 0.22031652904905907\n",
      "Error on this batch = 0.22026349665371162\n",
      "Cost on val dataset after 19 epochs is = 0.22719061725121906\n",
      "Initial Cost on Val dataset for this epoch 19 = 0.22719061725121906\n",
      "Error on this batch = 0.2083635314215937\n",
      "Error on this batch = 0.20739491732942952\n",
      "Cost on val dataset after 20 epochs is = 0.21319495694131965\n",
      "Initial Cost on Val dataset for this epoch 20 = 0.21319495694131965\n",
      "Error on this batch = 0.19561747924049028\n",
      "Error on this batch = 0.20836900892618673\n",
      "Cost on val dataset after 21 epochs is = 0.21178986514372006\n",
      "Initial Cost on Val dataset for this epoch 21 = 0.21178986514372006\n",
      "Error on this batch = 0.19166096975308933\n",
      "Error on this batch = 0.19885457094910786\n",
      "Cost on val dataset after 22 epochs is = 0.20370284162606334\n",
      "Initial Cost on Val dataset for this epoch 22 = 0.20370284162606334\n",
      "Error on this batch = 0.18512040434163418\n",
      "Error on this batch = 0.1865072169051043\n",
      "Cost on val dataset after 23 epochs is = 0.19853105949620217\n",
      "Initial Cost on Val dataset for this epoch 23 = 0.19853105949620217\n",
      "Error on this batch = 0.17844211745910193\n",
      "Error on this batch = 0.2013117114542105\n",
      "Cost on val dataset after 24 epochs is = 0.20068549513783418\n",
      "Initial Cost on Val dataset for this epoch 24 = 0.20068549513783418\n",
      "Error on this batch = 0.17776322120686117\n",
      "Error on this batch = 0.1963182821972601\n",
      "Cost on val dataset after 25 epochs is = 0.18491722061226762\n",
      "Initial Cost on Val dataset for this epoch 25 = 0.18491722061226762\n",
      "Error on this batch = 0.1687922310893149\n",
      "Error on this batch = 0.19409149372875356\n",
      "Cost on val dataset after 26 epochs is = 0.18046800618241549\n",
      "Initial Cost on Val dataset for this epoch 26 = 0.18046800618241549\n",
      "Error on this batch = 0.16560805347151317\n",
      "Error on this batch = 0.18363892782533672\n",
      "Cost on val dataset after 27 epochs is = 0.18351220150347858\n",
      "Initial Cost on Val dataset for this epoch 27 = 0.18351220150347858\n",
      "Error on this batch = 0.16474552125913278\n",
      "Error on this batch = 0.17749835430172195\n",
      "Cost on val dataset after 28 epochs is = 0.187423471911141\n",
      "Initial Cost on Val dataset for this epoch 28 = 0.187423471911141\n",
      "Error on this batch = 0.16922669690439038\n",
      "Error on this batch = 0.1715328743290786\n",
      "Cost on val dataset after 29 epochs is = 0.17852115616732822\n",
      "Initial Cost on Val dataset for this epoch 29 = 0.17852115616732822\n",
      "Error on this batch = 0.15994819381063244\n",
      "Error on this batch = 0.1684808624186812\n",
      "Cost on val dataset after 30 epochs is = 0.17696324117340234\n",
      "Initial Cost on Val dataset for this epoch 30 = 0.17696324117340234\n",
      "Error on this batch = 0.15787438605116175\n",
      "Error on this batch = 0.16372058996684682\n",
      "Cost on val dataset after 31 epochs is = 0.17568418385044574\n",
      "Initial Cost on Val dataset for this epoch 31 = 0.17568418385044574\n",
      "Error on this batch = 0.1557844317948807\n",
      "Error on this batch = 0.16000596911050594\n",
      "Cost on val dataset after 32 epochs is = 0.174159788956462\n",
      "Initial Cost on Val dataset for this epoch 32 = 0.174159788956462\n",
      "Error on this batch = 0.15355817205507458\n",
      "Error on this batch = 0.15634680426882838\n",
      "Cost on val dataset after 33 epochs is = 0.1735826400319437\n",
      "Initial Cost on Val dataset for this epoch 33 = 0.1735826400319437\n",
      "Error on this batch = 0.15194879607627207\n",
      "Error on this batch = 0.1527196637459968\n",
      "Cost on val dataset after 34 epochs is = 0.17244638052035255\n",
      "Initial Cost on Val dataset for this epoch 34 = 0.17244638052035255\n",
      "Error on this batch = 0.14964992372512018\n",
      "Error on this batch = 0.15060350152871474\n",
      "Cost on val dataset after 35 epochs is = 0.1687772964229743\n",
      "Initial Cost on Val dataset for this epoch 35 = 0.1687772964229743\n",
      "Error on this batch = 0.14733344777464777\n",
      "Error on this batch = 0.14881682880211386\n",
      "Cost on val dataset after 36 epochs is = 0.17123232498527305\n",
      "Initial Cost on Val dataset for this epoch 36 = 0.17123232498527305\n",
      "Error on this batch = 0.1453945284759074\n",
      "Error on this batch = 0.14837873024636236\n",
      "Cost on val dataset after 37 epochs is = 0.17021727815911034\n",
      "Initial Cost on Val dataset for this epoch 37 = 0.17021727815911034\n",
      "Error on this batch = 0.14351018864366819\n",
      "Error on this batch = 0.14808502781041433\n",
      "Cost on val dataset after 38 epochs is = 0.16969994372308603\n",
      "Initial Cost on Val dataset for this epoch 38 = 0.16969994372308603\n",
      "Error on this batch = 0.1418036509558942\n",
      "Error on this batch = 0.14814321022838264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on val dataset after 39 epochs is = 0.16942247031865043\n",
      "Initial Cost on Val dataset for this epoch 39 = 0.16942247031865043\n",
      "Error on this batch = 0.13995539606637375\n",
      "Error on this batch = 0.14791038889566935\n",
      "Cost on val dataset after 40 epochs is = 0.16929722401971833\n",
      "Initial Cost on Val dataset for this epoch 40 = 0.16929722401971833\n",
      "Error on this batch = 0.1385138557490346\n",
      "Error on this batch = 0.1486228809326434\n",
      "Cost on val dataset after 41 epochs is = 0.1689208767653947\n",
      "Initial Cost on Val dataset for this epoch 41 = 0.1689208767653947\n",
      "Error on this batch = 0.13704762693183814\n",
      "Error on this batch = 0.14800888776817372\n",
      "Cost on val dataset after 42 epochs is = 0.16854724435679688\n",
      "Initial Cost on Val dataset for this epoch 42 = 0.16854724435679688\n",
      "Error on this batch = 0.1357615102395488\n",
      "Error on this batch = 0.14829988618202009\n",
      "Cost on val dataset after 43 epochs is = 0.16817351245498705\n",
      "Initial Cost on Val dataset for this epoch 43 = 0.16817351245498705\n",
      "Error on this batch = 0.13472708880197634\n",
      "Error on this batch = 0.1468111151166948\n",
      "Cost on val dataset after 44 epochs is = 0.1679068369980956\n",
      "Initial Cost on Val dataset for this epoch 44 = 0.1679068369980956\n",
      "Error on this batch = 0.1334070026972708\n",
      "Error on this batch = 0.14636591218714887\n",
      "Cost on val dataset after 45 epochs is = 0.16728283535243704\n",
      "Initial Cost on Val dataset for this epoch 45 = 0.16728283535243704\n",
      "Error on this batch = 0.13277511135651363\n",
      "Error on this batch = 0.14530125685972045\n",
      "Cost on val dataset after 46 epochs is = 0.1667847971799609\n",
      "Initial Cost on Val dataset for this epoch 46 = 0.1667847971799609\n",
      "Error on this batch = 0.13187767263347855\n",
      "Error on this batch = 0.14455894329903743\n",
      "Cost on val dataset after 47 epochs is = 0.16625598007096454\n",
      "Initial Cost on Val dataset for this epoch 47 = 0.16625598007096454\n",
      "Error on this batch = 0.1313279169619288\n",
      "Error on this batch = 0.14325890178117792\n",
      "Cost on val dataset after 48 epochs is = 0.1656943203942405\n",
      "Initial Cost on Val dataset for this epoch 48 = 0.1656943203942405\n",
      "Error on this batch = 0.13069124977253962\n",
      "Error on this batch = 0.14616341261963198\n",
      "Cost on val dataset after 49 epochs is = 0.16528261978835157\n",
      "Initial Cost on Val dataset for this epoch 49 = 0.16528261978835157\n",
      "Error on this batch = 0.12997570697190503\n",
      "Error on this batch = 0.14234539473265034\n",
      "Cost on val dataset after 50 epochs is = 0.1652019687828014\n",
      "Initial Cost on Val dataset for this epoch 50 = 0.1652019687828014\n",
      "Error on this batch = 0.129949194902904\n",
      "Error on this batch = 0.14206343785061948\n",
      "Cost on val dataset after 51 epochs is = 0.1650594534155557\n",
      "Initial Cost on Val dataset for this epoch 51 = 0.1650594534155557\n",
      "Error on this batch = 0.12899740109452115\n",
      "Error on this batch = 0.1404740445889825\n",
      "Cost on val dataset after 52 epochs is = 0.16494913461864755\n",
      "Initial Cost on Val dataset for this epoch 52 = 0.16494913461864755\n",
      "Error on this batch = 0.12799795468359448\n",
      "Error on this batch = 0.137888910190291\n",
      "Cost on val dataset after 53 epochs is = 0.16521739975071437\n",
      "Initial Cost on Val dataset for this epoch 53 = 0.16521739975071437\n",
      "Error on this batch = 0.12695515550418052\n",
      "Error on this batch = 0.13794356168403263\n",
      "Cost on val dataset after 54 epochs is = 0.16524832233647188\n",
      "Initial Cost on Val dataset for this epoch 54 = 0.16524832233647188\n",
      "Error on this batch = 0.12655893717221608\n",
      "Error on this batch = 0.14243763058855\n",
      "Cost on val dataset after 55 epochs is = 0.16512933390398654\n",
      "Initial Cost on Val dataset for this epoch 55 = 0.16512933390398654\n",
      "Error on this batch = 0.12570258881272142\n",
      "Error on this batch = 0.1448421523885004\n",
      "Cost on val dataset after 56 epochs is = 0.16542666662334388\n",
      "Initial Cost on Val dataset for this epoch 56 = 0.16542666662334388\n",
      "Error on this batch = 0.12518997279632824\n",
      "Error on this batch = 0.14835203706188502\n",
      "Cost on val dataset after 57 epochs is = 0.165350031344515\n",
      "Initial Cost on Val dataset for this epoch 57 = 0.165350031344515\n",
      "Error on this batch = 0.12413211616626374\n",
      "Error on this batch = 0.148286466944433\n",
      "Cost on val dataset after 58 epochs is = 0.16523486490394582\n",
      "Initial Cost on Val dataset for this epoch 58 = 0.16523486490394582\n",
      "Error on this batch = 0.1236892652583526\n",
      "Error on this batch = 0.1311661255355379\n",
      "Cost on val dataset after 59 epochs is = 0.16469178176123683\n",
      "Initial Cost on Val dataset for this epoch 59 = 0.16469178176123683\n",
      "Error on this batch = 0.12388140895937394\n",
      "Error on this batch = 0.1351163526316248\n",
      "Cost on val dataset after 60 epochs is = 0.16566368466272005\n",
      "Initial Cost on Val dataset for this epoch 60 = 0.16566368466272005\n",
      "Error on this batch = 0.12427681706506578\n",
      "Error on this batch = 0.1299877505415914\n",
      "Cost on val dataset after 61 epochs is = 0.176569771235098\n",
      "Initial Cost on Val dataset for this epoch 61 = 0.176569771235098\n",
      "Error on this batch = 0.13925762642079362\n",
      "Error on this batch = 0.14447287991765856\n",
      "Cost on val dataset after 62 epochs is = 0.1669638463539249\n",
      "Initial Cost on Val dataset for this epoch 62 = 0.1669638463539249\n",
      "Error on this batch = 0.12318455657044759\n",
      "Error on this batch = 0.13024880864595992\n",
      "Cost on val dataset after 63 epochs is = 0.16807659282530826\n",
      "Initial Cost on Val dataset for this epoch 63 = 0.16807659282530826\n",
      "Error on this batch = 0.12365446323596002\n",
      "Error on this batch = 0.1321237777549872\n",
      "Cost on val dataset after 64 epochs is = 0.16757356679817248\n",
      "Initial Cost on Val dataset for this epoch 64 = 0.16757356679817248\n",
      "Error on this batch = 0.1219908668148295\n",
      "Error on this batch = 0.14144741734727673\n",
      "Cost on val dataset after 65 epochs is = 0.16973886413200517\n",
      "Initial Cost on Val dataset for this epoch 65 = 0.16973886413200517\n",
      "Error on this batch = 0.12239352439880015\n",
      "Error on this batch = 0.14367638529372687\n",
      "Cost on val dataset after 66 epochs is = 0.16868262723178942\n",
      "Initial Cost on Val dataset for this epoch 66 = 0.16868262723178942\n",
      "Error on this batch = 0.1238927174749638\n",
      "Error on this batch = 0.13795922929753576\n",
      "Cost on val dataset after 67 epochs is = 0.17203628089350514\n",
      "Initial Cost on Val dataset for this epoch 67 = 0.17203628089350514\n",
      "Error on this batch = 0.12060507545115468\n",
      "Error on this batch = 0.13657147693414584\n",
      "Cost on val dataset after 68 epochs is = 0.17211467507166997\n",
      "Initial Cost on Val dataset for this epoch 68 = 0.17211467507166997\n",
      "Error on this batch = 0.1221382257403603\n",
      "Error on this batch = 0.14040147667579359\n",
      "Cost on val dataset after 69 epochs is = 0.17085895181601737\n",
      "Initial Cost on Val dataset for this epoch 69 = 0.17085895181601737\n",
      "Error on this batch = 0.11988561171117679\n",
      "Error on this batch = 0.13181691844330762\n",
      "Cost on val dataset after 70 epochs is = 0.18334575489190444\n",
      "Initial Cost on Val dataset for this epoch 70 = 0.18334575489190444\n",
      "Error on this batch = 0.1389611836431534\n",
      "Error on this batch = 0.12924006641537303\n",
      "Cost on val dataset after 71 epochs is = 0.16913508654046727\n",
      "Initial Cost on Val dataset for this epoch 71 = 0.16913508654046727\n",
      "Error on this batch = 0.11909275140558762\n",
      "Error on this batch = 0.12680384544045872\n",
      "Cost on val dataset after 72 epochs is = 0.17169298591346957\n",
      "Initial Cost on Val dataset for this epoch 72 = 0.17169298591346957\n",
      "Error on this batch = 0.12072035846401886\n",
      "Error on this batch = 0.1288185863886703\n",
      "Cost on val dataset after 73 epochs is = 0.16969856895022256\n",
      "Initial Cost on Val dataset for this epoch 73 = 0.16969856895022256\n",
      "Error on this batch = 0.12128915736562386\n",
      "Error on this batch = 0.13044408747909167\n",
      "Cost on val dataset after 74 epochs is = 0.19580773481587538\n",
      "Initial Cost on Val dataset for this epoch 74 = 0.19580773481587538\n",
      "Error on this batch = 0.14257096285518397\n",
      "Error on this batch = 0.13934938303935082\n",
      "Cost on val dataset after 75 epochs is = 0.16923328238545687\n",
      "Initial Cost on Val dataset for this epoch 75 = 0.16923328238545687\n",
      "Error on this batch = 0.1184506638157136\n",
      "Error on this batch = 0.12661541649087277\n",
      "Cost on val dataset after 76 epochs is = 0.1722662839977095\n",
      "Initial Cost on Val dataset for this epoch 76 = 0.1722662839977095\n",
      "Error on this batch = 0.12291118677109665\n",
      "Error on this batch = 0.12856586299252631\n",
      "Cost on val dataset after 77 epochs is = 0.1710630985500331\n",
      "Initial Cost on Val dataset for this epoch 77 = 0.1710630985500331\n",
      "Error on this batch = 0.12404932713491991\n",
      "Error on this batch = 0.12610081716921404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on val dataset after 78 epochs is = 0.17571191690667734\n",
      "Initial Cost on Val dataset for this epoch 78 = 0.17571191690667734\n",
      "Error on this batch = 0.12684773142167607\n",
      "Error on this batch = 0.14105368736740845\n",
      "Cost on val dataset after 79 epochs is = 0.17071996580169274\n",
      "Initial Cost on Val dataset for this epoch 79 = 0.17071996580169274\n",
      "Error on this batch = 0.11937390539726508\n",
      "Error on this batch = 0.13520875869920382\n",
      "Cost on val dataset after 80 epochs is = 0.17359984904281298\n",
      "Initial Cost on Val dataset for this epoch 80 = 0.17359984904281298\n",
      "Error on this batch = 0.11890805646444118\n",
      "Error on this batch = 0.12930286749541028\n",
      "Cost on val dataset after 81 epochs is = 0.18206433762599006\n",
      "Initial Cost on Val dataset for this epoch 81 = 0.18206433762599006\n",
      "Error on this batch = 0.12500100063738714\n",
      "Error on this batch = 0.13833093866411575\n",
      "Cost on val dataset after 82 epochs is = 0.17296344275606637\n",
      "Initial Cost on Val dataset for this epoch 82 = 0.17296344275606637\n",
      "Error on this batch = 0.11766013357344346\n",
      "Error on this batch = 0.15183750089211953\n",
      "Cost on val dataset after 83 epochs is = 0.17510192814410602\n",
      "Initial Cost on Val dataset for this epoch 83 = 0.17510192814410602\n",
      "Error on this batch = 0.11997286591654933\n",
      "Error on this batch = 0.13423090242565758\n",
      "Cost on val dataset after 84 epochs is = 0.18997354429502808\n",
      "Initial Cost on Val dataset for this epoch 84 = 0.18997354429502808\n",
      "Error on this batch = 0.1299139570942231\n",
      "Error on this batch = 0.12456430987591668\n",
      "Cost on val dataset after 85 epochs is = 0.174961225727428\n",
      "Initial Cost on Val dataset for this epoch 85 = 0.174961225727428\n",
      "Error on this batch = 0.12304462398694697\n",
      "Error on this batch = 0.124896005881625\n",
      "Cost on val dataset after 86 epochs is = 0.17494215919947212\n",
      "Initial Cost on Val dataset for this epoch 86 = 0.17494215919947212\n",
      "Error on this batch = 0.11932908052539981\n",
      "Error on this batch = 0.15159862395192047\n",
      "Cost on val dataset after 87 epochs is = 0.17322979178357215\n",
      "Initial Cost on Val dataset for this epoch 87 = 0.17322979178357215\n",
      "Error on this batch = 0.12375052671289517\n",
      "Error on this batch = 0.14468348183037105\n",
      "Cost on val dataset after 88 epochs is = 0.17295024113080507\n",
      "Initial Cost on Val dataset for this epoch 88 = 0.17295024113080507\n",
      "Error on this batch = 0.11936646106625481\n",
      "Error on this batch = 0.1367604440650226\n",
      "Cost on val dataset after 89 epochs is = 0.18660273015969053\n",
      "Initial Cost on Val dataset for this epoch 89 = 0.18660273015969053\n",
      "Error on this batch = 0.132507391889365\n",
      "Error on this batch = 0.13069647343348725\n",
      "Cost on val dataset after 90 epochs is = 0.17928728265885124\n",
      "Initial Cost on Val dataset for this epoch 90 = 0.17928728265885124\n",
      "Error on this batch = 0.13514513360534827\n",
      "Error on this batch = 0.1378144399021261\n",
      "Cost on val dataset after 91 epochs is = 0.18001635701237018\n",
      "Initial Cost on Val dataset for this epoch 91 = 0.18001635701237018\n",
      "Error on this batch = 0.11813392340859476\n",
      "Error on this batch = 0.14418849727722194\n",
      "Cost on val dataset after 92 epochs is = 0.17900294445481466\n",
      "Initial Cost on Val dataset for this epoch 92 = 0.17900294445481466\n",
      "Error on this batch = 0.1196078034023842\n",
      "Error on this batch = 0.12523290325106115\n",
      "Cost on val dataset after 93 epochs is = 0.1830140470223026\n",
      "Initial Cost on Val dataset for this epoch 93 = 0.1830140470223026\n",
      "Error on this batch = 0.13356172680299122\n",
      "Error on this batch = 0.14015517994582213\n",
      "Cost on val dataset after 94 epochs is = 0.17814298784361732\n",
      "Initial Cost on Val dataset for this epoch 94 = 0.17814298784361732\n",
      "Error on this batch = 0.11814250394758741\n",
      "Error on this batch = 0.13394867260836005\n",
      "Cost on val dataset after 95 epochs is = 0.16940886289482524\n",
      "Initial Cost on Val dataset for this epoch 95 = 0.16940886289482524\n",
      "Error on this batch = 0.11442132994030303\n",
      "Error on this batch = 0.12621813100184062\n",
      "Cost on val dataset after 96 epochs is = 0.18191212106586643\n",
      "Initial Cost on Val dataset for this epoch 96 = 0.18191212106586643\n",
      "Error on this batch = 0.11976038659275692\n",
      "Error on this batch = 0.1276379526515926\n",
      "Cost on val dataset after 97 epochs is = 0.1777290561675612\n",
      "Initial Cost on Val dataset for this epoch 97 = 0.1777290561675612\n",
      "Error on this batch = 0.11437347556961924\n",
      "Error on this batch = 0.1351801752276384\n",
      "Cost on val dataset after 98 epochs is = 0.18153205317815388\n",
      "Initial Cost on Val dataset for this epoch 98 = 0.18153205317815388\n",
      "Error on this batch = 0.12533286075606992\n",
      "Error on this batch = 0.13556750030268594\n",
      "Cost on val dataset after 99 epochs is = 0.18041789826890536\n",
      "Initial Cost on Val dataset for this epoch 99 = 0.18041789826890536\n",
      "Error on this batch = 0.12567282957789433\n",
      "Error on this batch = 0.1228767476969735\n",
      "Cost on val dataset after 100 epochs is = 0.1781989578864622\n",
      "Initial Cost on Val dataset for this epoch 100 = 0.1781989578864622\n",
      "Error on this batch = 0.11531151383253685\n",
      "Error on this batch = 0.1245992763925922\n",
      "Cost on val dataset after 101 epochs is = 0.17775215082802495\n",
      "Initial Cost on Val dataset for this epoch 101 = 0.17775215082802495\n",
      "Error on this batch = 0.12003033322692991\n",
      "Error on this batch = 0.12169793751384685\n",
      "Cost on val dataset after 102 epochs is = 0.1845131223539782\n",
      "Initial Cost on Val dataset for this epoch 102 = 0.1845131223539782\n",
      "Error on this batch = 0.12282093911434318\n",
      "Error on this batch = 0.12032771067021823\n",
      "Cost on val dataset after 103 epochs is = 0.16934006324014814\n",
      "Initial Cost on Val dataset for this epoch 103 = 0.16934006324014814\n",
      "Error on this batch = 0.11276460101103732\n",
      "Error on this batch = 0.12460488762319369\n",
      "Cost on val dataset after 104 epochs is = 0.1967180002514632\n",
      "Initial Cost on Val dataset for this epoch 104 = 0.1967180002514632\n",
      "Error on this batch = 0.13728407439772583\n",
      "Error on this batch = 0.13406262109751169\n",
      "Cost on val dataset after 105 epochs is = 0.18146475557486239\n",
      "Initial Cost on Val dataset for this epoch 105 = 0.18146475557486239\n",
      "Error on this batch = 0.13176638076290734\n",
      "Error on this batch = 0.14004603492753195\n",
      "Cost on val dataset after 106 epochs is = 0.17155052904243176\n",
      "Initial Cost on Val dataset for this epoch 106 = 0.17155052904243176\n",
      "Error on this batch = 0.12030557252767832\n",
      "Error on this batch = 0.11836321905055922\n",
      "Cost on val dataset after 107 epochs is = 0.1879873477985425\n",
      "Initial Cost on Val dataset for this epoch 107 = 0.1879873477985425\n",
      "Error on this batch = 0.12151097168440124\n",
      "Error on this batch = 0.1210528167226523\n",
      "Cost on val dataset after 108 epochs is = 0.18625761017032164\n",
      "Initial Cost on Val dataset for this epoch 108 = 0.18625761017032164\n",
      "Error on this batch = 0.12586803061987736\n",
      "Error on this batch = 0.11703140061036962\n",
      "Cost on val dataset after 109 epochs is = 0.17753549289615944\n",
      "Initial Cost on Val dataset for this epoch 109 = 0.17753549289615944\n",
      "Error on this batch = 0.11106784139892482\n",
      "Error on this batch = 0.11352228644862494\n",
      "Cost on val dataset after 110 epochs is = 0.17598927049558652\n",
      "Initial Cost on Val dataset for this epoch 110 = 0.17598927049558652\n",
      "Error on this batch = 0.11269765405378177\n",
      "Error on this batch = 0.12311914228128369\n",
      "Cost on val dataset after 111 epochs is = 0.1748293704237185\n",
      "Initial Cost on Val dataset for this epoch 111 = 0.1748293704237185\n",
      "Error on this batch = 0.11394262232766524\n",
      "Error on this batch = 0.11788134323394703\n",
      "Cost on val dataset after 112 epochs is = 0.1744610957674858\n",
      "Initial Cost on Val dataset for this epoch 112 = 0.1744610957674858\n",
      "Error on this batch = 0.11514834387033517\n",
      "Error on this batch = 0.11306197685808178\n",
      "Cost on val dataset after 113 epochs is = 0.17225135398465558\n",
      "Initial Cost on Val dataset for this epoch 113 = 0.17225135398465558\n",
      "Error on this batch = 0.11262024593738262\n",
      "Error on this batch = 0.11308481084132335\n",
      "Cost on val dataset after 114 epochs is = 0.1719762743657983\n",
      "Initial Cost on Val dataset for this epoch 114 = 0.1719762743657983\n",
      "Error on this batch = 0.11089839257253634\n",
      "Error on this batch = 0.11525515388953551\n",
      "Cost on val dataset after 115 epochs is = 0.1790696085900021\n",
      "Initial Cost on Val dataset for this epoch 115 = 0.1790696085900021\n",
      "Error on this batch = 0.11407893246587492\n",
      "Error on this batch = 0.1200738136742579\n",
      "Cost on val dataset after 116 epochs is = 0.17748823054897792\n",
      "Initial Cost on Val dataset for this epoch 116 = 0.17748823054897792\n",
      "Error on this batch = 0.1086632289404945\n",
      "Error on this batch = 0.11505073821969763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on val dataset after 117 epochs is = 0.17346832830303358\n",
      "Initial Cost on Val dataset for this epoch 117 = 0.17346832830303358\n",
      "Error on this batch = 0.11640368143605823\n",
      "Error on this batch = 0.1252202280535558\n",
      "Cost on val dataset after 118 epochs is = 0.19386412373853096\n",
      "Initial Cost on Val dataset for this epoch 118 = 0.19386412373853096\n",
      "Error on this batch = 0.1278755761306083\n",
      "Error on this batch = 0.11812837024460882\n",
      "Cost on val dataset after 119 epochs is = 0.18220522012941595\n",
      "Initial Cost on Val dataset for this epoch 119 = 0.18220522012941595\n",
      "Error on this batch = 0.13018862368046252\n",
      "Error on this batch = 0.11387555085776868\n",
      "Cost on val dataset after 120 epochs is = 0.17128109480563278\n",
      "Initial Cost on Val dataset for this epoch 120 = 0.17128109480563278\n",
      "Error on this batch = 0.1099070224880412\n",
      "Error on this batch = 0.11506330262544885\n",
      "Cost on val dataset after 121 epochs is = 0.17258227473435384\n",
      "Initial Cost on Val dataset for this epoch 121 = 0.17258227473435384\n",
      "Error on this batch = 0.10974196772201833\n",
      "Error on this batch = 0.11085437728472927\n",
      "Cost on val dataset after 122 epochs is = 0.17208506168719315\n",
      "Initial Cost on Val dataset for this epoch 122 = 0.17208506168719315\n",
      "Error on this batch = 0.11512641642812962\n",
      "Error on this batch = 0.11466947827154284\n",
      "Cost on val dataset after 123 epochs is = 0.17052837913589525\n",
      "Initial Cost on Val dataset for this epoch 123 = 0.17052837913589525\n",
      "Error on this batch = 0.10783875965361876\n",
      "Error on this batch = 0.11130765269692347\n",
      "Cost on val dataset after 124 epochs is = 0.17899729896038324\n",
      "Initial Cost on Val dataset for this epoch 124 = 0.17899729896038324\n",
      "Error on this batch = 0.1110768357834062\n",
      "Error on this batch = 0.16798186923857208\n",
      "Cost on val dataset after 125 epochs is = 0.17774769224869166\n",
      "Initial Cost on Val dataset for this epoch 125 = 0.17774769224869166\n",
      "Error on this batch = 0.1122395889658972\n",
      "Error on this batch = 0.11761214204574393\n",
      "Cost on val dataset after 126 epochs is = 0.1745716839246355\n",
      "Initial Cost on Val dataset for this epoch 126 = 0.1745716839246355\n",
      "Error on this batch = 0.1215116901171915\n",
      "Error on this batch = 0.1471356618849174\n",
      "Cost on val dataset after 127 epochs is = 0.1741514132114221\n",
      "Initial Cost on Val dataset for this epoch 127 = 0.1741514132114221\n",
      "Error on this batch = 0.10842851539599718\n",
      "Error on this batch = 0.111532883506445\n",
      "Cost on val dataset after 128 epochs is = 0.17655398845786363\n",
      "Initial Cost on Val dataset for this epoch 128 = 0.17655398845786363\n",
      "Error on this batch = 0.11439660568851226\n",
      "Error on this batch = 0.11828561664177775\n",
      "Cost on val dataset after 129 epochs is = 0.17762236229905723\n",
      "Initial Cost on Val dataset for this epoch 129 = 0.17762236229905723\n",
      "Error on this batch = 0.11265380593381856\n",
      "Error on this batch = 0.1225562573791068\n",
      "Cost on val dataset after 130 epochs is = 0.17345677865558762\n",
      "Initial Cost on Val dataset for this epoch 130 = 0.17345677865558762\n",
      "Error on this batch = 0.10553993989935478\n",
      "Error on this batch = 0.15301112158117178\n",
      "Cost on val dataset after 131 epochs is = 0.17694395897670556\n",
      "Initial Cost on Val dataset for this epoch 131 = 0.17694395897670556\n",
      "Error on this batch = 0.11365207176665897\n",
      "Error on this batch = 0.13026504615388826\n",
      "Cost on val dataset after 132 epochs is = 0.1808425469267667\n",
      "Initial Cost on Val dataset for this epoch 132 = 0.1808425469267667\n",
      "Error on this batch = 0.12453209193144016\n",
      "Error on this batch = 0.1221216979773822\n",
      "Cost on val dataset after 133 epochs is = 0.1752341307781077\n",
      "Initial Cost on Val dataset for this epoch 133 = 0.1752341307781077\n",
      "Error on this batch = 0.10878085173751068\n",
      "Error on this batch = 0.12838475421438986\n",
      "Cost on val dataset after 134 epochs is = 0.18867162238997875\n",
      "Initial Cost on Val dataset for this epoch 134 = 0.18867162238997875\n",
      "Error on this batch = 0.13501410693485358\n",
      "Error on this batch = 0.13144073858968547\n",
      "Cost on val dataset after 135 epochs is = 0.1822957516537747\n",
      "Initial Cost on Val dataset for this epoch 135 = 0.1822957516537747\n",
      "Error on this batch = 0.13224390599954916\n",
      "Error on this batch = 0.12956636346838526\n",
      "Cost on val dataset after 136 epochs is = 0.17071193663648243\n",
      "Initial Cost on Val dataset for this epoch 136 = 0.17071193663648243\n",
      "Error on this batch = 0.10750770774190066\n",
      "Error on this batch = 0.12111887379084924\n",
      "Cost on val dataset after 137 epochs is = 0.17225296801435247\n",
      "Initial Cost on Val dataset for this epoch 137 = 0.17225296801435247\n",
      "Error on this batch = 0.10174857967508714\n",
      "Error on this batch = 0.11966125621538976\n",
      "Cost on val dataset after 138 epochs is = 0.17854216095386954\n",
      "Initial Cost on Val dataset for this epoch 138 = 0.17854216095386954\n",
      "Error on this batch = 0.10484361496108133\n",
      "Error on this batch = 0.11918780572096185\n",
      "Cost on val dataset after 139 epochs is = 0.18285507049597607\n",
      "Initial Cost on Val dataset for this epoch 139 = 0.18285507049597607\n",
      "Error on this batch = 0.11176431889978879\n",
      "Error on this batch = 0.13057368392523955\n",
      "Cost on val dataset after 140 epochs is = 0.17887595566959524\n",
      "Initial Cost on Val dataset for this epoch 140 = 0.17887595566959524\n",
      "Error on this batch = 0.12249422692226762\n",
      "Error on this batch = 0.12898944685806332\n",
      "Cost on val dataset after 141 epochs is = 0.17418431406687288\n",
      "Initial Cost on Val dataset for this epoch 141 = 0.17418431406687288\n",
      "Error on this batch = 0.10430077935429473\n",
      "Error on this batch = 0.12553218298996682\n",
      "Cost on val dataset after 142 epochs is = 0.17937085299131267\n",
      "Initial Cost on Val dataset for this epoch 142 = 0.17937085299131267\n",
      "Error on this batch = 0.10755249969472111\n",
      "Error on this batch = 0.11988999569679713\n",
      "Cost on val dataset after 143 epochs is = 0.18027262848666145\n",
      "Initial Cost on Val dataset for this epoch 143 = 0.18027262848666145\n",
      "Error on this batch = 0.10619931396920987\n",
      "Error on this batch = 0.12221583587970643\n",
      "Cost on val dataset after 144 epochs is = 0.17892132541973643\n",
      "Initial Cost on Val dataset for this epoch 144 = 0.17892132541973643\n",
      "Error on this batch = 0.1108760832283965\n",
      "Error on this batch = 0.11649224507225878\n",
      "Cost on val dataset after 145 epochs is = 0.17739854054414772\n",
      "Initial Cost on Val dataset for this epoch 145 = 0.17739854054414772\n",
      "Error on this batch = 0.10737651441522462\n",
      "Error on this batch = 0.14234613144562666\n",
      "Cost on val dataset after 146 epochs is = 0.17210387374116626\n",
      "Initial Cost on Val dataset for this epoch 146 = 0.17210387374116626\n",
      "Error on this batch = 0.10934666013766034\n",
      "Error on this batch = 0.13463989729012846\n",
      "Cost on val dataset after 147 epochs is = 0.17584846002278495\n",
      "Initial Cost on Val dataset for this epoch 147 = 0.17584846002278495\n",
      "Error on this batch = 0.10219188486682079\n",
      "Error on this batch = 0.1185459077504379\n",
      "Cost on val dataset after 148 epochs is = 0.18085976405511664\n",
      "Initial Cost on Val dataset for this epoch 148 = 0.18085976405511664\n",
      "Error on this batch = 0.11426063793429717\n",
      "Error on this batch = 0.13388868406127383\n",
      "Cost on val dataset after 149 epochs is = 0.1823854524189169\n",
      "Initial Cost on Val dataset for this epoch 149 = 0.1823854524189169\n",
      "Error on this batch = 0.10968311031053361\n",
      "Error on this batch = 0.12974638058310256\n",
      "Cost on val dataset after 150 epochs is = 0.18075416720252305\n",
      "Initial Cost on Val dataset for this epoch 150 = 0.18075416720252305\n",
      "Error on this batch = 0.10296282800008999\n",
      "Error on this batch = 0.11852606721020185\n",
      "Cost on val dataset after 151 epochs is = 0.1779190327720155\n",
      "Initial Cost on Val dataset for this epoch 151 = 0.1779190327720155\n",
      "Error on this batch = 0.11001527121404632\n",
      "Error on this batch = 0.1297596659499184\n",
      "Cost on val dataset after 152 epochs is = 0.1859781397603947\n",
      "Initial Cost on Val dataset for this epoch 152 = 0.1859781397603947\n",
      "Error on this batch = 0.1142801993201116\n",
      "Error on this batch = 0.11437167803708077\n",
      "Cost on val dataset after 153 epochs is = 0.18542354675539047\n",
      "Initial Cost on Val dataset for this epoch 153 = 0.18542354675539047\n",
      "Error on this batch = 0.11618520363435597\n",
      "Error on this batch = 0.13635095524534233\n",
      "Cost on val dataset after 154 epochs is = 0.17655170229084952\n",
      "Initial Cost on Val dataset for this epoch 154 = 0.17655170229084952\n",
      "Error on this batch = 0.10663801813703132\n",
      "Error on this batch = 0.1173780521700668\n",
      "Cost on val dataset after 155 epochs is = 0.17832110188774963\n",
      "Initial Cost on Val dataset for this epoch 155 = 0.17832110188774963\n",
      "Error on this batch = 0.10810070885033418\n",
      "Error on this batch = 0.13157979831951924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on val dataset after 156 epochs is = 0.18718531452443748\n",
      "Initial Cost on Val dataset for this epoch 156 = 0.18718531452443748\n",
      "Error on this batch = 0.1189928592192255\n",
      "Error on this batch = 0.14599252671989046\n",
      "Cost on val dataset after 157 epochs is = 0.17791917162362486\n",
      "Initial Cost on Val dataset for this epoch 157 = 0.17791917162362486\n",
      "Error on this batch = 0.10747070376954404\n",
      "Error on this batch = 0.12292514827130865\n",
      "Cost on val dataset after 158 epochs is = 0.1788417500628872\n",
      "Initial Cost on Val dataset for this epoch 158 = 0.1788417500628872\n",
      "Error on this batch = 0.1107768016917987\n",
      "Error on this batch = 0.1365454641442829\n",
      "Cost on val dataset after 159 epochs is = 0.17595817524919286\n",
      "Initial Cost on Val dataset for this epoch 159 = 0.17595817524919286\n",
      "Error on this batch = 0.10979328859866513\n",
      "Error on this batch = 0.12722488990327632\n",
      "Cost on val dataset after 160 epochs is = 0.17807830946931144\n",
      "Initial Cost on Val dataset for this epoch 160 = 0.17807830946931144\n",
      "Error on this batch = 0.11174078067181699\n",
      "Error on this batch = 0.11390407382648121\n",
      "Cost on val dataset after 161 epochs is = 0.1802279810831117\n",
      "Initial Cost on Val dataset for this epoch 161 = 0.1802279810831117\n",
      "Error on this batch = 0.11142759360518849\n",
      "Error on this batch = 0.12760660837078144\n",
      "Cost on val dataset after 162 epochs is = 0.17967264942425604\n",
      "Initial Cost on Val dataset for this epoch 162 = 0.17967264942425604\n",
      "Error on this batch = 0.10688833568249102\n",
      "Error on this batch = 0.11641053400404758\n",
      "Cost on val dataset after 163 epochs is = 0.17851145496409693\n",
      "Initial Cost on Val dataset for this epoch 163 = 0.17851145496409693\n",
      "Error on this batch = 0.10272279192381245\n",
      "Error on this batch = 0.10669289943360853\n",
      "Cost on val dataset after 164 epochs is = 0.1846291562171486\n",
      "Initial Cost on Val dataset for this epoch 164 = 0.1846291562171486\n",
      "Error on this batch = 0.1232852762197124\n",
      "Error on this batch = 0.11169039143762383\n",
      "Cost on val dataset after 165 epochs is = 0.17895173404344125\n",
      "Initial Cost on Val dataset for this epoch 165 = 0.17895173404344125\n",
      "Error on this batch = 0.10406313864192694\n",
      "Error on this batch = 0.1114996858835751\n",
      "Cost on val dataset after 166 epochs is = 0.17547751692230423\n",
      "Initial Cost on Val dataset for this epoch 166 = 0.17547751692230423\n",
      "Error on this batch = 0.10216794436203702\n",
      "Error on this batch = 0.11468513661732711\n",
      "Cost on val dataset after 167 epochs is = 0.18594910729455638\n",
      "Initial Cost on Val dataset for this epoch 167 = 0.18594910729455638\n",
      "Error on this batch = 0.11893813047659371\n",
      "Error on this batch = 0.10876468125794161\n",
      "Cost on val dataset after 168 epochs is = 0.18000344725172956\n",
      "Initial Cost on Val dataset for this epoch 168 = 0.18000344725172956\n",
      "Error on this batch = 0.11260044214055238\n",
      "Error on this batch = 0.12830440969289977\n",
      "Cost on val dataset after 169 epochs is = 0.17416421089464845\n",
      "Initial Cost on Val dataset for this epoch 169 = 0.17416421089464845\n",
      "Error on this batch = 0.10449010234377169\n",
      "Error on this batch = 0.1139084023889003\n",
      "Cost on val dataset after 170 epochs is = 0.17462061746386467\n",
      "Initial Cost on Val dataset for this epoch 170 = 0.17462061746386467\n",
      "Error on this batch = 0.1030993650824615\n",
      "Error on this batch = 0.11474510039025063\n",
      "Cost on val dataset after 171 epochs is = 0.1730434574577181\n",
      "Initial Cost on Val dataset for this epoch 171 = 0.1730434574577181\n",
      "Error on this batch = 0.10125302497449258\n",
      "Error on this batch = 0.13231713849867305\n",
      "Cost on val dataset after 172 epochs is = 0.17381778838648837\n",
      "Initial Cost on Val dataset for this epoch 172 = 0.17381778838648837\n",
      "Error on this batch = 0.10318687501719208\n",
      "Error on this batch = 0.125717160662856\n",
      "Cost on val dataset after 173 epochs is = 0.21824961706132032\n",
      "Initial Cost on Val dataset for this epoch 173 = 0.21824961706132032\n",
      "Error on this batch = 0.1671089187244643\n",
      "Error on this batch = 0.14977188873579306\n",
      "Cost on val dataset after 174 epochs is = 0.173108575460026\n",
      "Initial Cost on Val dataset for this epoch 174 = 0.173108575460026\n",
      "Error on this batch = 0.1030359602344229\n",
      "Error on this batch = 0.12577671343193558\n",
      "Cost on val dataset after 175 epochs is = 0.17935435655722612\n",
      "Initial Cost on Val dataset for this epoch 175 = 0.17935435655722612\n",
      "Error on this batch = 0.10832317007368664\n",
      "Error on this batch = 0.11211076051710613\n",
      "Cost on val dataset after 176 epochs is = 0.17030224525802015\n",
      "Initial Cost on Val dataset for this epoch 176 = 0.17030224525802015\n",
      "Error on this batch = 0.10387233081994411\n",
      "Error on this batch = 0.11260590406127709\n",
      "Cost on val dataset after 177 epochs is = 0.1794002227939851\n",
      "Initial Cost on Val dataset for this epoch 177 = 0.1794002227939851\n",
      "Error on this batch = 0.11728983389304752\n",
      "Error on this batch = 0.1254415909451433\n",
      "Cost on val dataset after 178 epochs is = 0.16928994197835961\n",
      "Initial Cost on Val dataset for this epoch 178 = 0.16928994197835961\n",
      "Error on this batch = 0.10270561386986385\n",
      "Error on this batch = 0.13302503333328933\n",
      "Cost on val dataset after 179 epochs is = 0.17277570993308797\n",
      "Initial Cost on Val dataset for this epoch 179 = 0.17277570993308797\n",
      "Error on this batch = 0.10038552555778718\n",
      "Error on this batch = 0.11335370078657082\n",
      "Cost on val dataset after 180 epochs is = 0.16935990451579583\n",
      "Initial Cost on Val dataset for this epoch 180 = 0.16935990451579583\n",
      "Error on this batch = 0.10256605073117062\n",
      "Error on this batch = 0.1099905842813492\n",
      "Cost on val dataset after 181 epochs is = 0.17128516558352494\n",
      "Initial Cost on Val dataset for this epoch 181 = 0.17128516558352494\n",
      "Error on this batch = 0.10243852742472427\n",
      "Error on this batch = 0.10506424271484685\n",
      "Cost on val dataset after 182 epochs is = 0.16958997334615442\n",
      "Initial Cost on Val dataset for this epoch 182 = 0.16958997334615442\n",
      "Error on this batch = 0.1060275999272085\n",
      "Error on this batch = 0.12562381569321293\n",
      "Cost on val dataset after 183 epochs is = 0.18761209508422425\n",
      "Initial Cost on Val dataset for this epoch 183 = 0.18761209508422425\n",
      "Error on this batch = 0.11094075725196449\n",
      "Error on this batch = 0.11444572941608097\n",
      "Cost on val dataset after 184 epochs is = 0.17406894993936198\n",
      "Initial Cost on Val dataset for this epoch 184 = 0.17406894993936198\n",
      "Error on this batch = 0.10530360783318875\n",
      "Error on this batch = 0.11406034059418747\n",
      "Cost on val dataset after 185 epochs is = 0.1795459182518405\n",
      "Initial Cost on Val dataset for this epoch 185 = 0.1795459182518405\n",
      "Error on this batch = 0.10807399102091868\n",
      "Error on this batch = 0.10506899298725093\n",
      "Cost on val dataset after 186 epochs is = 0.17310726745190771\n",
      "Initial Cost on Val dataset for this epoch 186 = 0.17310726745190771\n",
      "Error on this batch = 0.11082681285737506\n",
      "Error on this batch = 0.10580513743367588\n",
      "Cost on val dataset after 187 epochs is = 0.17953162941414127\n",
      "Initial Cost on Val dataset for this epoch 187 = 0.17953162941414127\n",
      "Error on this batch = 0.1043301670781231\n",
      "Error on this batch = 0.11274925028398763\n",
      "Cost on val dataset after 188 epochs is = 0.1703123963635118\n",
      "Initial Cost on Val dataset for this epoch 188 = 0.1703123963635118\n",
      "Error on this batch = 0.10250901878095764\n",
      "Error on this batch = 0.10918582257714196\n",
      "Cost on val dataset after 189 epochs is = 0.176372311084773\n",
      "Initial Cost on Val dataset for this epoch 189 = 0.176372311084773\n",
      "Error on this batch = 0.10462088346019512\n",
      "Error on this batch = 0.11732534306197671\n",
      "Cost on val dataset after 190 epochs is = 0.18126290089540661\n",
      "Initial Cost on Val dataset for this epoch 190 = 0.18126290089540661\n",
      "Error on this batch = 0.09949869091943997\n",
      "Error on this batch = 0.10682545904972646\n",
      "Cost on val dataset after 191 epochs is = 0.18117592069693256\n",
      "Initial Cost on Val dataset for this epoch 191 = 0.18117592069693256\n",
      "Error on this batch = 0.10851152800943542\n",
      "Error on this batch = 0.10569609510846517\n",
      "Cost on val dataset after 192 epochs is = 0.16996593037962485\n",
      "Initial Cost on Val dataset for this epoch 192 = 0.16996593037962485\n",
      "Error on this batch = 0.10309749323257006\n",
      "Error on this batch = 0.11372408888153071\n",
      "Cost on val dataset after 193 epochs is = 0.18219359936104867\n",
      "Initial Cost on Val dataset for this epoch 193 = 0.18219359936104867\n",
      "Error on this batch = 0.10798445686245998\n",
      "Error on this batch = 0.11623001473184368\n",
      "Cost on val dataset after 194 epochs is = 0.17622237054042414\n",
      "Initial Cost on Val dataset for this epoch 194 = 0.17622237054042414\n",
      "Error on this batch = 0.10339560595728134\n",
      "Error on this batch = 0.1049907816549473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on val dataset after 195 epochs is = 0.17449404572403232\n",
      "Initial Cost on Val dataset for this epoch 195 = 0.17449404572403232\n",
      "Error on this batch = 0.09961967025986612\n",
      "Error on this batch = 0.10628889814059007\n",
      "Cost on val dataset after 196 epochs is = 0.18053805279680393\n",
      "Initial Cost on Val dataset for this epoch 196 = 0.18053805279680393\n",
      "Error on this batch = 0.10466983703835636\n",
      "Error on this batch = 0.13434426509760916\n",
      "Cost on val dataset after 197 epochs is = 0.1695872873587634\n",
      "Initial Cost on Val dataset for this epoch 197 = 0.1695872873587634\n",
      "Error on this batch = 0.09724940967225475\n",
      "Error on this batch = 0.13835654007544942\n",
      "Cost on val dataset after 198 epochs is = 0.17294476031317735\n",
      "Initial Cost on Val dataset for this epoch 198 = 0.17294476031317735\n",
      "Error on this batch = 0.10350448993562802\n",
      "Error on this batch = 0.11310556077806831\n",
      "Cost on val dataset after 199 epochs is = 0.1693269712509956\n",
      "Initial Cost on Val dataset for this epoch 199 = 0.1693269712509956\n",
      "Error on this batch = 0.09740320078201119\n",
      "Error on this batch = 0.12006396451798917\n",
      "Cost on val dataset after 200 epochs is = 0.17222513419071042\n",
      "Initial Cost on Val dataset for this epoch 200 = 0.17222513419071042\n",
      "Error on this batch = 0.09555670924476381\n",
      "Error on this batch = 0.17092716251757487\n",
      "Cost on val dataset after 201 epochs is = 0.18633936595852987\n",
      "Initial Cost on Val dataset for this epoch 201 = 0.18633936595852987\n",
      "Error on this batch = 0.10239151532056098\n",
      "Error on this batch = 0.1028960208111429\n",
      "Cost on val dataset after 202 epochs is = 0.18029936828079038\n",
      "Initial Cost on Val dataset for this epoch 202 = 0.18029936828079038\n",
      "Error on this batch = 0.103762493407955\n",
      "Error on this batch = 0.1044530918020358\n",
      "Cost on val dataset after 203 epochs is = 0.18058044523898803\n",
      "Initial Cost on Val dataset for this epoch 203 = 0.18058044523898803\n",
      "Error on this batch = 0.10863720254060936\n",
      "Error on this batch = 0.10814984741334566\n",
      "Cost on val dataset after 204 epochs is = 0.17313476412335774\n",
      "Initial Cost on Val dataset for this epoch 204 = 0.17313476412335774\n",
      "Error on this batch = 0.09674446422156432\n",
      "Error on this batch = 0.12479678194801565\n",
      "Cost on val dataset after 205 epochs is = 0.185088249281674\n",
      "Initial Cost on Val dataset for this epoch 205 = 0.185088249281674\n",
      "Error on this batch = 0.10394582588779258\n",
      "Error on this batch = 0.11245906187379817\n",
      "Cost on val dataset after 206 epochs is = 0.1746708918140481\n",
      "Initial Cost on Val dataset for this epoch 206 = 0.1746708918140481\n",
      "Error on this batch = 0.09534351671497636\n",
      "Error on this batch = 0.11167061646041879\n",
      "Cost on val dataset after 207 epochs is = 0.1745529738373718\n",
      "Initial Cost on Val dataset for this epoch 207 = 0.1745529738373718\n",
      "Error on this batch = 0.09086357582061115\n",
      "Error on this batch = 0.09662649064670042\n",
      "Cost on val dataset after 208 epochs is = 0.2052275183571205\n",
      "Initial Cost on Val dataset for this epoch 208 = 0.2052275183571205\n",
      "Error on this batch = 0.14013419064398172\n",
      "Error on this batch = 0.12717286952807524\n",
      "Cost on val dataset after 209 epochs is = 0.17545016144587486\n",
      "Initial Cost on Val dataset for this epoch 209 = 0.17545016144587486\n",
      "Error on this batch = 0.0935397028049031\n",
      "Error on this batch = 0.10248647129402644\n",
      "Cost on val dataset after 210 epochs is = 0.1761619102373865\n",
      "Initial Cost on Val dataset for this epoch 210 = 0.1761619102373865\n",
      "Error on this batch = 0.10417941912069072\n",
      "Error on this batch = 0.1074085908649309\n",
      "Cost on val dataset after 211 epochs is = 0.17302939400869738\n",
      "Initial Cost on Val dataset for this epoch 211 = 0.17302939400869738\n",
      "Error on this batch = 0.09495347892102578\n",
      "Error on this batch = 0.10468225818450357\n",
      "Cost on val dataset after 212 epochs is = 0.1790619204070177\n",
      "Initial Cost on Val dataset for this epoch 212 = 0.1790619204070177\n",
      "Error on this batch = 0.0913322378056148\n",
      "Error on this batch = 0.1264431062275235\n",
      "Cost on val dataset after 213 epochs is = 0.18118330529474103\n",
      "Initial Cost on Val dataset for this epoch 213 = 0.18118330529474103\n",
      "Error on this batch = 0.11239777226967171\n",
      "Error on this batch = 0.15682819983515534\n",
      "Cost on val dataset after 214 epochs is = 0.17537688842184318\n",
      "Initial Cost on Val dataset for this epoch 214 = 0.17537688842184318\n",
      "Error on this batch = 0.10169794603926956\n",
      "Error on this batch = 0.10307428415535831\n",
      "Cost on val dataset after 215 epochs is = 0.17496421865086004\n",
      "Initial Cost on Val dataset for this epoch 215 = 0.17496421865086004\n",
      "Error on this batch = 0.09178257336064057\n",
      "Error on this batch = 0.12177789382220276\n",
      "Cost on val dataset after 216 epochs is = 0.18913083714242765\n",
      "Initial Cost on Val dataset for this epoch 216 = 0.18913083714242765\n",
      "Error on this batch = 0.10863485017320479\n",
      "Error on this batch = 0.12901627709808972\n",
      "Cost on val dataset after 217 epochs is = 0.18224353360017925\n",
      "Initial Cost on Val dataset for this epoch 217 = 0.18224353360017925\n",
      "Error on this batch = 0.09919458579292531\n",
      "Error on this batch = 0.1252189934384821\n",
      "Cost on val dataset after 218 epochs is = 0.1918758916375385\n",
      "Initial Cost on Val dataset for this epoch 218 = 0.1918758916375385\n",
      "Error on this batch = 0.10424125879714619\n",
      "Error on this batch = 0.09886390063569993\n",
      "Cost on val dataset after 219 epochs is = 0.1767378295512222\n",
      "Initial Cost on Val dataset for this epoch 219 = 0.1767378295512222\n",
      "Error on this batch = 0.09614381692438469\n",
      "Error on this batch = 0.10922073434960769\n",
      "Cost on val dataset after 220 epochs is = 0.17587791401312822\n",
      "Initial Cost on Val dataset for this epoch 220 = 0.17587791401312822\n",
      "Error on this batch = 0.09674272305971346\n",
      "Error on this batch = 0.10353719800516703\n",
      "Cost on val dataset after 221 epochs is = 0.18753302975733863\n",
      "Initial Cost on Val dataset for this epoch 221 = 0.18753302975733863\n",
      "Error on this batch = 0.09781992380629202\n",
      "Error on this batch = 0.10986844624909531\n",
      "Cost on val dataset after 222 epochs is = 0.19662424369960765\n",
      "Initial Cost on Val dataset for this epoch 222 = 0.19662424369960765\n",
      "Error on this batch = 0.10917370421081429\n",
      "Error on this batch = 0.110401462801562\n",
      "Cost on val dataset after 223 epochs is = 0.18096839172575221\n",
      "Initial Cost on Val dataset for this epoch 223 = 0.18096839172575221\n",
      "Error on this batch = 0.0951478136286368\n",
      "Error on this batch = 0.09700586337027858\n",
      "Cost on val dataset after 224 epochs is = 0.17925345267441226\n",
      "Initial Cost on Val dataset for this epoch 224 = 0.17925345267441226\n",
      "Error on this batch = 0.0930773931289822\n",
      "Error on this batch = 0.11388573091053235\n",
      "Cost on val dataset after 225 epochs is = 0.18930376261553197\n",
      "Initial Cost on Val dataset for this epoch 225 = 0.18930376261553197\n",
      "Error on this batch = 0.09994841420347562\n",
      "Error on this batch = 0.10900341356751915\n",
      "Cost on val dataset after 226 epochs is = 0.18038698743493697\n",
      "Initial Cost on Val dataset for this epoch 226 = 0.18038698743493697\n",
      "Error on this batch = 0.09487012363219607\n",
      "Error on this batch = 0.17240606286581603\n",
      "Cost on val dataset after 227 epochs is = 0.20819260550416702\n",
      "Initial Cost on Val dataset for this epoch 227 = 0.20819260550416702\n",
      "Error on this batch = 0.13091319553874375\n",
      "Error on this batch = 0.10911482036173265\n",
      "Cost on val dataset after 228 epochs is = 0.1761116415895558\n",
      "Initial Cost on Val dataset for this epoch 228 = 0.1761116415895558\n",
      "Error on this batch = 0.09273410211564166\n",
      "Error on this batch = 0.10508822257668213\n",
      "Cost on val dataset after 229 epochs is = 0.17817539188435583\n",
      "Initial Cost on Val dataset for this epoch 229 = 0.17817539188435583\n",
      "Error on this batch = 0.09397261379774427\n",
      "Error on this batch = 0.12326729082714274\n",
      "Cost on val dataset after 230 epochs is = 0.1785289890125556\n",
      "Initial Cost on Val dataset for this epoch 230 = 0.1785289890125556\n",
      "Error on this batch = 0.089371055837961\n",
      "Error on this batch = 0.11266102207540657\n",
      "Cost on val dataset after 231 epochs is = 0.1813839339364583\n",
      "Initial Cost on Val dataset for this epoch 231 = 0.1813839339364583\n",
      "Error on this batch = 0.09209779290121092\n",
      "Error on this batch = 0.1272216647542524\n",
      "Cost on val dataset after 232 epochs is = 0.18672962826791512\n",
      "Initial Cost on Val dataset for this epoch 232 = 0.18672962826791512\n",
      "Error on this batch = 0.10237808239069343\n",
      "Error on this batch = 0.10560045409441923\n",
      "Cost on val dataset after 233 epochs is = 0.1870963552225391\n",
      "Initial Cost on Val dataset for this epoch 233 = 0.1870963552225391\n",
      "Error on this batch = 0.1035714865998603\n",
      "Error on this batch = 0.10335937600252729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on val dataset after 234 epochs is = 0.17429785607283532\n",
      "Initial Cost on Val dataset for this epoch 234 = 0.17429785607283532\n",
      "Error on this batch = 0.09809026155885338\n",
      "Error on this batch = 0.09450238606390665\n",
      "Cost on val dataset after 235 epochs is = 0.17967461893166084\n",
      "Initial Cost on Val dataset for this epoch 235 = 0.17967461893166084\n",
      "Error on this batch = 0.087674179861969\n",
      "Error on this batch = 0.09929369805021868\n",
      "Cost on val dataset after 236 epochs is = 0.17779162329875192\n",
      "Initial Cost on Val dataset for this epoch 236 = 0.17779162329875192\n",
      "Error on this batch = 0.10371088181869258\n",
      "Error on this batch = 0.10253702009543765\n",
      "Cost on val dataset after 237 epochs is = 0.18007990872230972\n",
      "Initial Cost on Val dataset for this epoch 237 = 0.18007990872230972\n",
      "Error on this batch = 0.0915985214088185\n",
      "Error on this batch = 0.10425594746781824\n",
      "Cost on val dataset after 238 epochs is = 0.180206124419515\n",
      "Initial Cost on Val dataset for this epoch 238 = 0.180206124419515\n",
      "Error on this batch = 0.10249378132595528\n",
      "Error on this batch = 0.11937029282418195\n",
      "Cost on val dataset after 239 epochs is = 0.18655419801233586\n",
      "Initial Cost on Val dataset for this epoch 239 = 0.18655419801233586\n",
      "Error on this batch = 0.09747661405636253\n",
      "Error on this batch = 0.10200571707945379\n",
      "Cost on val dataset after 240 epochs is = 0.18097734010522276\n",
      "Initial Cost on Val dataset for this epoch 240 = 0.18097734010522276\n",
      "Error on this batch = 0.09249854848916933\n",
      "Error on this batch = 0.09866592440168886\n",
      "Cost on val dataset after 241 epochs is = 0.17851169469140946\n",
      "Initial Cost on Val dataset for this epoch 241 = 0.17851169469140946\n",
      "Error on this batch = 0.1030149590408628\n",
      "Error on this batch = 0.17479399260537076\n",
      "Cost on val dataset after 242 epochs is = 0.18309697611667483\n",
      "Initial Cost on Val dataset for this epoch 242 = 0.18309697611667483\n",
      "Error on this batch = 0.09409082338560833\n",
      "Error on this batch = 0.1048876742490148\n",
      "Cost on val dataset after 243 epochs is = 0.18868148647664446\n",
      "Initial Cost on Val dataset for this epoch 243 = 0.18868148647664446\n",
      "Error on this batch = 0.10180043008890946\n",
      "Error on this batch = 0.10131768367858576\n",
      "Cost on val dataset after 244 epochs is = 0.17820709390494718\n",
      "Initial Cost on Val dataset for this epoch 244 = 0.17820709390494718\n",
      "Error on this batch = 0.10650531037245697\n",
      "Error on this batch = 0.09564512593746212\n",
      "Cost on val dataset after 245 epochs is = 0.1798583153784549\n",
      "Initial Cost on Val dataset for this epoch 245 = 0.1798583153784549\n",
      "Error on this batch = 0.09773124667219411\n",
      "Error on this batch = 0.1268999933726215\n",
      "Cost on val dataset after 246 epochs is = 0.19640472809641496\n",
      "Initial Cost on Val dataset for this epoch 246 = 0.19640472809641496\n",
      "Error on this batch = 0.11569633075553248\n",
      "Error on this batch = 0.10028567135444519\n",
      "Cost on val dataset after 247 epochs is = 0.17698151832897763\n",
      "Initial Cost on Val dataset for this epoch 247 = 0.17698151832897763\n",
      "Error on this batch = 0.09806304222098236\n",
      "Error on this batch = 0.10152403088696894\n",
      "Cost on val dataset after 248 epochs is = 0.18753478391761338\n",
      "Initial Cost on Val dataset for this epoch 248 = 0.18753478391761338\n",
      "Error on this batch = 0.09715540605143595\n",
      "Error on this batch = 0.09420789371182554\n",
      "Cost on val dataset after 249 epochs is = 0.2025183840744634\n",
      "Initial Cost on Val dataset for this epoch 249 = 0.2025183840744634\n",
      "Error on this batch = 0.10669520841649416\n",
      "Error on this batch = 0.12423720456030832\n",
      "Cost on val dataset after 250 epochs is = 0.1881471043536681\n",
      "Initial Cost on Val dataset for this epoch 250 = 0.1881471043536681\n",
      "Error on this batch = 0.11322184648316583\n",
      "Error on this batch = 0.11136389499514217\n",
      "Cost on val dataset after 251 epochs is = 0.1786533756992465\n",
      "Initial Cost on Val dataset for this epoch 251 = 0.1786533756992465\n",
      "Error on this batch = 0.0948238780993029\n",
      "Error on this batch = 0.10109717199919672\n",
      "Cost on val dataset after 252 epochs is = 0.18628321537183762\n",
      "Initial Cost on Val dataset for this epoch 252 = 0.18628321537183762\n",
      "Error on this batch = 0.10350384712019656\n",
      "Error on this batch = 0.09991320063989914\n",
      "Cost on val dataset after 253 epochs is = 0.2037738284037904\n",
      "Initial Cost on Val dataset for this epoch 253 = 0.2037738284037904\n",
      "Error on this batch = 0.1292131513459614\n",
      "Error on this batch = 0.10205510196107806\n",
      "Cost on val dataset after 254 epochs is = 0.18201785107716834\n",
      "Initial Cost on Val dataset for this epoch 254 = 0.18201785107716834\n",
      "Error on this batch = 0.09777553516976491\n",
      "Error on this batch = 0.0985619543013763\n",
      "Cost on val dataset after 255 epochs is = 0.18087160405968272\n",
      "Initial Cost on Val dataset for this epoch 255 = 0.18087160405968272\n",
      "Error on this batch = 0.09602026435462012\n",
      "Error on this batch = 0.11418468562253466\n",
      "Cost on val dataset after 256 epochs is = 0.18578938512788512\n",
      "Initial Cost on Val dataset for this epoch 256 = 0.18578938512788512\n",
      "Error on this batch = 0.10538681490124557\n",
      "Error on this batch = 0.09373807269439903\n",
      "Cost on val dataset after 257 epochs is = 0.18401228188564328\n",
      "Initial Cost on Val dataset for this epoch 257 = 0.18401228188564328\n",
      "Error on this batch = 0.10279982457109857\n",
      "Error on this batch = 0.09392785233231035\n",
      "Cost on val dataset after 258 epochs is = 0.2050504767320963\n",
      "Initial Cost on Val dataset for this epoch 258 = 0.2050504767320963\n",
      "Error on this batch = 0.15565107379543483\n",
      "Error on this batch = 0.10640317141173097\n",
      "Cost on val dataset after 259 epochs is = 0.18426433332844497\n",
      "Initial Cost on Val dataset for this epoch 259 = 0.18426433332844497\n",
      "Error on this batch = 0.09869815514044468\n",
      "Error on this batch = 0.09874911732581237\n",
      "Cost on val dataset after 260 epochs is = 0.17728948200824143\n",
      "Initial Cost on Val dataset for this epoch 260 = 0.17728948200824143\n",
      "Error on this batch = 0.10207626931802427\n",
      "Error on this batch = 0.09476486114648715\n",
      "Cost on val dataset after 261 epochs is = 0.1800074929262845\n",
      "Initial Cost on Val dataset for this epoch 261 = 0.1800074929262845\n",
      "Error on this batch = 0.09398996626151597\n",
      "Error on this batch = 0.10834135275505194\n",
      "Cost on val dataset after 262 epochs is = 0.18412510593223\n",
      "Initial Cost on Val dataset for this epoch 262 = 0.18412510593223\n",
      "Error on this batch = 0.0975501231007835\n",
      "Error on this batch = 0.10600789770857919\n",
      "Cost on val dataset after 263 epochs is = 0.19604823044368938\n",
      "Initial Cost on Val dataset for this epoch 263 = 0.19604823044368938\n",
      "Error on this batch = 0.1035443537404192\n",
      "Error on this batch = 0.0978883994997154\n",
      "Cost on val dataset after 264 epochs is = 0.18694098788012936\n",
      "Initial Cost on Val dataset for this epoch 264 = 0.18694098788012936\n",
      "Error on this batch = 0.09975221688811797\n",
      "Error on this batch = 0.10036931094814376\n",
      "Cost on val dataset after 265 epochs is = 0.17663837073380512\n",
      "Initial Cost on Val dataset for this epoch 265 = 0.17663837073380512\n",
      "Error on this batch = 0.09085355571430659\n",
      "Error on this batch = 0.11677873452120015\n",
      "Cost on val dataset after 266 epochs is = 0.1776431747531004\n",
      "Initial Cost on Val dataset for this epoch 266 = 0.1776431747531004\n",
      "Error on this batch = 0.09102873271897238\n",
      "Error on this batch = 0.09691365727488567\n",
      "Cost on val dataset after 267 epochs is = 0.18171645167961356\n",
      "Initial Cost on Val dataset for this epoch 267 = 0.18171645167961356\n",
      "Error on this batch = 0.10189107938305501\n",
      "Error on this batch = 0.1063952757508211\n",
      "Cost on val dataset after 268 epochs is = 0.19824300496460004\n",
      "Initial Cost on Val dataset for this epoch 268 = 0.19824300496460004\n",
      "Error on this batch = 0.10588876335152168\n",
      "Error on this batch = 0.10433961420293247\n",
      "Cost on val dataset after 269 epochs is = 0.18320014966779785\n",
      "Initial Cost on Val dataset for this epoch 269 = 0.18320014966779785\n",
      "Error on this batch = 0.09784760868421412\n",
      "Error on this batch = 0.09857564650222095\n",
      "Cost on val dataset after 270 epochs is = 0.19685751089967807\n",
      "Initial Cost on Val dataset for this epoch 270 = 0.19685751089967807\n",
      "Error on this batch = 0.1057396213582366\n",
      "Error on this batch = 0.0936859876555716\n",
      "Cost on val dataset after 271 epochs is = 0.18194752537410647\n",
      "Initial Cost on Val dataset for this epoch 271 = 0.18194752537410647\n",
      "Error on this batch = 0.09242554474547292\n",
      "Error on this batch = 0.1049288892609242\n",
      "Cost on val dataset after 272 epochs is = 0.1796547868946359\n",
      "Initial Cost on Val dataset for this epoch 272 = 0.1796547868946359\n",
      "Error on this batch = 0.0913478490377717\n",
      "Error on this batch = 0.11203531549341342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on val dataset after 273 epochs is = 0.1794032568299026\n",
      "Initial Cost on Val dataset for this epoch 273 = 0.1794032568299026\n",
      "Error on this batch = 0.10749155355626704\n",
      "Error on this batch = 0.10251263570190541\n",
      "Cost on val dataset after 274 epochs is = 0.17443228207858222\n",
      "Initial Cost on Val dataset for this epoch 274 = 0.17443228207858222\n",
      "Error on this batch = 0.09746134937219916\n",
      "Error on this batch = 0.0951351759673259\n",
      "Cost on val dataset after 275 epochs is = 0.18846918583324468\n",
      "Initial Cost on Val dataset for this epoch 275 = 0.18846918583324468\n",
      "Error on this batch = 0.10441632118530335\n",
      "Error on this batch = 0.11398850836894439\n",
      "Cost on val dataset after 276 epochs is = 0.1955207179650935\n",
      "Initial Cost on Val dataset for this epoch 276 = 0.1955207179650935\n",
      "Error on this batch = 0.10907498686185164\n",
      "Error on this batch = 0.10518829377802938\n",
      "Cost on val dataset after 277 epochs is = 0.19844719739477001\n",
      "Initial Cost on Val dataset for this epoch 277 = 0.19844719739477001\n",
      "Error on this batch = 0.10519266706349151\n",
      "Error on this batch = 0.1198662636248942\n",
      "Cost on val dataset after 278 epochs is = 0.18739264929310934\n",
      "Initial Cost on Val dataset for this epoch 278 = 0.18739264929310934\n",
      "Error on this batch = 0.09476801697030968\n",
      "Error on this batch = 0.10494446848241568\n",
      "Cost on val dataset after 279 epochs is = 0.17999835326284877\n",
      "Initial Cost on Val dataset for this epoch 279 = 0.17999835326284877\n",
      "Error on this batch = 0.09531045138993388\n",
      "Error on this batch = 0.11504370504280191\n",
      "Cost on val dataset after 280 epochs is = 0.18607506922873318\n",
      "Initial Cost on Val dataset for this epoch 280 = 0.18607506922873318\n",
      "Error on this batch = 0.10116963282901065\n",
      "Error on this batch = 0.0985641657015973\n",
      "Cost on val dataset after 281 epochs is = 0.1873522854494281\n",
      "Initial Cost on Val dataset for this epoch 281 = 0.1873522854494281\n",
      "Error on this batch = 0.09177434500245277\n",
      "Error on this batch = 0.10432808529340357\n",
      "Cost on val dataset after 282 epochs is = 0.1833087468741768\n",
      "Initial Cost on Val dataset for this epoch 282 = 0.1833087468741768\n",
      "Error on this batch = 0.09013305247824611\n",
      "Error on this batch = 0.1468809132526506\n",
      "Cost on val dataset after 283 epochs is = 0.1795411597006791\n",
      "Initial Cost on Val dataset for this epoch 283 = 0.1795411597006791\n",
      "Error on this batch = 0.09240871529590504\n",
      "Error on this batch = 0.10476759876319028\n",
      "Cost on val dataset after 284 epochs is = 0.1840085071357173\n",
      "Initial Cost on Val dataset for this epoch 284 = 0.1840085071357173\n",
      "Error on this batch = 0.0907905289752202\n",
      "Error on this batch = 0.10652192010949499\n",
      "Cost on val dataset after 285 epochs is = 0.19420057994182122\n",
      "Initial Cost on Val dataset for this epoch 285 = 0.19420057994182122\n",
      "Error on this batch = 0.11055587411946553\n",
      "Error on this batch = 0.1182769505445696\n",
      "Cost on val dataset after 286 epochs is = 0.1915760292564086\n",
      "Initial Cost on Val dataset for this epoch 286 = 0.1915760292564086\n",
      "Error on this batch = 0.1015907235972108\n",
      "Error on this batch = 0.10222379210850342\n",
      "Cost on val dataset after 287 epochs is = 0.17996525632008795\n",
      "Initial Cost on Val dataset for this epoch 287 = 0.17996525632008795\n",
      "Error on this batch = 0.09236320691556026\n",
      "Error on this batch = 0.09969071916311403\n",
      "Cost on val dataset after 288 epochs is = 0.19255956156275045\n",
      "Initial Cost on Val dataset for this epoch 288 = 0.19255956156275045\n",
      "Error on this batch = 0.0946290845541811\n",
      "Error on this batch = 0.11441389767519632\n",
      "Cost on val dataset after 289 epochs is = 0.17986037018823353\n",
      "Initial Cost on Val dataset for this epoch 289 = 0.17986037018823353\n",
      "Error on this batch = 0.08742184601144842\n",
      "Error on this batch = 0.10067659651450651\n",
      "Cost on val dataset after 290 epochs is = 0.18789855511696663\n",
      "Initial Cost on Val dataset for this epoch 290 = 0.18789855511696663\n",
      "Error on this batch = 0.08837138542415642\n",
      "Error on this batch = 0.12013223062054486\n",
      "Cost on val dataset after 291 epochs is = 0.17918263583467642\n",
      "Initial Cost on Val dataset for this epoch 291 = 0.17918263583467642\n",
      "Error on this batch = 0.09680004568000605\n",
      "Error on this batch = 0.10211069559473575\n",
      "Cost on val dataset after 292 epochs is = 0.18029562327103818\n",
      "Initial Cost on Val dataset for this epoch 292 = 0.18029562327103818\n",
      "Error on this batch = 0.08931817600504932\n",
      "Error on this batch = 0.10898985428163212\n",
      "Cost on val dataset after 293 epochs is = 0.18458888278835978\n",
      "Initial Cost on Val dataset for this epoch 293 = 0.18458888278835978\n",
      "Error on this batch = 0.09589968291798746\n",
      "Error on this batch = 0.10171969271312166\n",
      "Cost on val dataset after 294 epochs is = 0.17657045559541684\n",
      "Initial Cost on Val dataset for this epoch 294 = 0.17657045559541684\n",
      "Error on this batch = 0.09430991878066732\n",
      "Error on this batch = 0.10668643698016843\n",
      "Cost on val dataset after 295 epochs is = 0.18968106460374223\n",
      "Initial Cost on Val dataset for this epoch 295 = 0.18968106460374223\n",
      "Error on this batch = 0.093370653754695\n",
      "Error on this batch = 0.10288102810406027\n",
      "Cost on val dataset after 296 epochs is = 0.1824750856443414\n",
      "Initial Cost on Val dataset for this epoch 296 = 0.1824750856443414\n",
      "Error on this batch = 0.08509130439734952\n",
      "Error on this batch = 0.1041568906354662\n",
      "Cost on val dataset after 297 epochs is = 0.19689900870331176\n",
      "Initial Cost on Val dataset for this epoch 297 = 0.19689900870331176\n",
      "Error on this batch = 0.11305500431331708\n",
      "Error on this batch = 0.10829571709999034\n",
      "Cost on val dataset after 298 epochs is = 0.18143225975692115\n",
      "Initial Cost on Val dataset for this epoch 298 = 0.18143225975692115\n",
      "Error on this batch = 0.09036023330557275\n",
      "Error on this batch = 0.10332168428570061\n",
      "Cost on val dataset after 299 epochs is = 0.18358540733179826\n",
      "Initial Cost on Val dataset for this epoch 299 = 0.18358540733179826\n",
      "Error on this batch = 0.09516135501970578\n",
      "Error on this batch = 0.12251978635111582\n",
      "Cost on val dataset after 300 epochs is = 0.18720057881608873\n",
      "Initial Cost on Val dataset for this epoch 300 = 0.18720057881608873\n",
      "Error on this batch = 0.08890698186273435\n",
      "Error on this batch = 0.12159990322265214\n",
      "Cost on val dataset after 301 epochs is = 0.18873503323077084\n",
      "Initial Cost on Val dataset for this epoch 301 = 0.18873503323077084\n",
      "Error on this batch = 0.08560666762449731\n",
      "Error on this batch = 0.10410143311703575\n",
      "Cost on val dataset after 302 epochs is = 0.18190670494412856\n",
      "Initial Cost on Val dataset for this epoch 302 = 0.18190670494412856\n",
      "Error on this batch = 0.09514849137102506\n",
      "Error on this batch = 0.1295361001830946\n",
      "Cost on val dataset after 303 epochs is = 0.18079769925052794\n",
      "Initial Cost on Val dataset for this epoch 303 = 0.18079769925052794\n",
      "Error on this batch = 0.09531038777162638\n",
      "Error on this batch = 0.13103745352276008\n",
      "Cost on val dataset after 304 epochs is = 0.1852415604360591\n",
      "Initial Cost on Val dataset for this epoch 304 = 0.1852415604360591\n",
      "Error on this batch = 0.09629964471530343\n",
      "Error on this batch = 0.10979518416686534\n",
      "Cost on val dataset after 305 epochs is = 0.177478100671987\n",
      "Initial Cost on Val dataset for this epoch 305 = 0.177478100671987\n",
      "Error on this batch = 0.08427222643544337\n",
      "Error on this batch = 0.10331557804857477\n",
      "Cost on val dataset after 306 epochs is = 0.20571678005091668\n",
      "Initial Cost on Val dataset for this epoch 306 = 0.20571678005091668\n",
      "Error on this batch = 0.11142020387808507\n",
      "Error on this batch = 0.11096207334791078\n",
      "Cost on val dataset after 307 epochs is = 0.18310297559739552\n",
      "Initial Cost on Val dataset for this epoch 307 = 0.18310297559739552\n",
      "Error on this batch = 0.09397902874041454\n",
      "Error on this batch = 0.12197631978345257\n",
      "Cost on val dataset after 308 epochs is = 0.1804337260466381\n",
      "Initial Cost on Val dataset for this epoch 308 = 0.1804337260466381\n",
      "Error on this batch = 0.09449971996097709\n",
      "Error on this batch = 0.11009230967534027\n",
      "Cost on val dataset after 309 epochs is = 0.19046142856677364\n",
      "Initial Cost on Val dataset for this epoch 309 = 0.19046142856677364\n",
      "Error on this batch = 0.090801265362817\n",
      "Error on this batch = 0.11230066173852687\n",
      "Cost on val dataset after 310 epochs is = 0.1840098864714981\n",
      "Initial Cost on Val dataset for this epoch 310 = 0.1840098864714981\n",
      "Error on this batch = 0.09030368133332217\n",
      "Error on this batch = 0.11227708063469588\n",
      "Cost on val dataset after 311 epochs is = 0.19357152052545223\n",
      "Initial Cost on Val dataset for this epoch 311 = 0.19357152052545223\n",
      "Error on this batch = 0.10602872583374877\n",
      "Error on this batch = 0.10737598359209596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on val dataset after 312 epochs is = 0.17745305592385946\n",
      "Initial Cost on Val dataset for this epoch 312 = 0.17745305592385946\n",
      "Error on this batch = 0.08428816211299502\n",
      "Error on this batch = 0.11273997619902507\n",
      "Cost on val dataset after 313 epochs is = 0.1804739172412748\n",
      "Initial Cost on Val dataset for this epoch 313 = 0.1804739172412748\n",
      "Error on this batch = 0.08304988101485355\n",
      "Error on this batch = 0.10522286634663366\n",
      "Cost on val dataset after 314 epochs is = 0.18813899999682443\n",
      "Initial Cost on Val dataset for this epoch 314 = 0.18813899999682443\n",
      "Error on this batch = 0.09495436408939505\n",
      "Error on this batch = 0.11879314876921768\n",
      "Cost on val dataset after 315 epochs is = 0.18522791484243786\n",
      "Initial Cost on Val dataset for this epoch 315 = 0.18522791484243786\n",
      "Error on this batch = 0.09062374329351788\n",
      "Error on this batch = 0.11513873697792758\n",
      "Cost on val dataset after 316 epochs is = 0.181985623355961\n",
      "Initial Cost on Val dataset for this epoch 316 = 0.181985623355961\n",
      "Error on this batch = 0.10113353004973887\n",
      "Error on this batch = 0.10666613317071537\n",
      "Cost on val dataset after 317 epochs is = 0.1833497294247392\n",
      "Initial Cost on Val dataset for this epoch 317 = 0.1833497294247392\n",
      "Error on this batch = 0.0885873489809396\n",
      "Error on this batch = 0.1304051139940453\n",
      "Cost on val dataset after 318 epochs is = 0.18575028205564445\n",
      "Initial Cost on Val dataset for this epoch 318 = 0.18575028205564445\n",
      "Error on this batch = 0.08683650134665528\n",
      "Error on this batch = 0.1164198054934757\n",
      "Cost on val dataset after 319 epochs is = 0.18249187526352306\n",
      "Initial Cost on Val dataset for this epoch 319 = 0.18249187526352306\n",
      "Error on this batch = 0.09415515820069446\n",
      "Error on this batch = 0.12022018247001842\n",
      "Cost on val dataset after 320 epochs is = 0.1848229992325809\n",
      "Initial Cost on Val dataset for this epoch 320 = 0.1848229992325809\n",
      "Error on this batch = 0.10886314203441545\n",
      "Error on this batch = 0.10499236423766639\n",
      "Cost on val dataset after 321 epochs is = 0.21146117768664113\n",
      "Initial Cost on Val dataset for this epoch 321 = 0.21146117768664113\n",
      "Error on this batch = 0.10943871906991084\n",
      "Error on this batch = 0.11331207659945172\n",
      "Cost on val dataset after 322 epochs is = 0.18318191252889296\n",
      "Initial Cost on Val dataset for this epoch 322 = 0.18318191252889296\n",
      "Error on this batch = 0.09408712380761003\n",
      "Error on this batch = 0.11160573178945056\n",
      "Cost on val dataset after 323 epochs is = 0.19012905029627764\n",
      "Initial Cost on Val dataset for this epoch 323 = 0.19012905029627764\n",
      "Error on this batch = 0.09412489221133656\n",
      "Error on this batch = 0.10707678629288409\n",
      "Cost on val dataset after 324 epochs is = 0.18244916523653223\n",
      "Initial Cost on Val dataset for this epoch 324 = 0.18244916523653223\n",
      "Error on this batch = 0.09719876282901585\n",
      "Error on this batch = 0.12301433481604462\n",
      "Cost on val dataset after 325 epochs is = 0.1841375051193123\n",
      "Initial Cost on Val dataset for this epoch 325 = 0.1841375051193123\n",
      "Error on this batch = 0.08425644233894383\n",
      "Error on this batch = 0.14331078509375753\n",
      "Cost on val dataset after 326 epochs is = 0.18872592848638278\n",
      "Initial Cost on Val dataset for this epoch 326 = 0.18872592848638278\n",
      "Error on this batch = 0.09141645809571472\n",
      "Error on this batch = 0.113976058123613\n",
      "Cost on val dataset after 327 epochs is = 0.18718513256666847\n",
      "Initial Cost on Val dataset for this epoch 327 = 0.18718513256666847\n",
      "Error on this batch = 0.09576920379646708\n",
      "Error on this batch = 0.11929591224946673\n",
      "Cost on val dataset after 328 epochs is = 0.1869321383464582\n",
      "Initial Cost on Val dataset for this epoch 328 = 0.1869321383464582\n",
      "Error on this batch = 0.09572055994614231\n",
      "Error on this batch = 0.12223331636329272\n",
      "Cost on val dataset after 329 epochs is = 0.19857626171150766\n",
      "Initial Cost on Val dataset for this epoch 329 = 0.19857626171150766\n",
      "Error on this batch = 0.10809997128898925\n",
      "Error on this batch = 0.10375286065719337\n",
      "Cost on val dataset after 330 epochs is = 0.1876158457697307\n",
      "Initial Cost on Val dataset for this epoch 330 = 0.1876158457697307\n",
      "Error on this batch = 0.10990809895288552\n",
      "Error on this batch = 0.12798479088531775\n",
      "Cost on val dataset after 331 epochs is = 0.17861538690069595\n",
      "Initial Cost on Val dataset for this epoch 331 = 0.17861538690069595\n",
      "Error on this batch = 0.08847004414507864\n",
      "Error on this batch = 0.11517973553109463\n",
      "Cost on val dataset after 332 epochs is = 0.18104168223793884\n",
      "Initial Cost on Val dataset for this epoch 332 = 0.18104168223793884\n",
      "Error on this batch = 0.08372423045631415\n",
      "Error on this batch = 0.10861512477751958\n",
      "Cost on val dataset after 333 epochs is = 0.17938799954533227\n",
      "Initial Cost on Val dataset for this epoch 333 = 0.17938799954533227\n",
      "Error on this batch = 0.0873146812302263\n",
      "Error on this batch = 0.10803417265097931\n",
      "Cost on val dataset after 334 epochs is = 0.18743074662018622\n",
      "Initial Cost on Val dataset for this epoch 334 = 0.18743074662018622\n",
      "Error on this batch = 0.09634985705822455\n",
      "Error on this batch = 0.14539816312112058\n",
      "Cost on val dataset after 335 epochs is = 0.1842102414053513\n",
      "Initial Cost on Val dataset for this epoch 335 = 0.1842102414053513\n",
      "Error on this batch = 0.09318839279799963\n",
      "Error on this batch = 0.10673642444859155\n",
      "Cost on val dataset after 336 epochs is = 0.20543012033431768\n",
      "Initial Cost on Val dataset for this epoch 336 = 0.20543012033431768\n",
      "Error on this batch = 0.14377856908800302\n",
      "Error on this batch = 0.10210310280927141\n",
      "Cost on val dataset after 337 epochs is = 0.18554669290959025\n",
      "Initial Cost on Val dataset for this epoch 337 = 0.18554669290959025\n",
      "Error on this batch = 0.09232186955677761\n",
      "Error on this batch = 0.1352214067822908\n",
      "Cost on val dataset after 338 epochs is = 0.18269718392978412\n",
      "Initial Cost on Val dataset for this epoch 338 = 0.18269718392978412\n",
      "Error on this batch = 0.09333294186074036\n",
      "Error on this batch = 0.11222938148767364\n",
      "Cost on val dataset after 339 epochs is = 0.18504230313827422\n",
      "Initial Cost on Val dataset for this epoch 339 = 0.18504230313827422\n",
      "Error on this batch = 0.09693577811123197\n",
      "Error on this batch = 0.13175685377024338\n",
      "Cost on val dataset after 340 epochs is = 0.210509862824322\n",
      "Initial Cost on Val dataset for this epoch 340 = 0.210509862824322\n",
      "Error on this batch = 0.10387940725969368\n",
      "Error on this batch = 0.12228176594937505\n",
      "Cost on val dataset after 341 epochs is = 0.19393095951209166\n",
      "Initial Cost on Val dataset for this epoch 341 = 0.19393095951209166\n",
      "Error on this batch = 0.1036269520951029\n",
      "Error on this batch = 0.12278098837416647\n",
      "Cost on val dataset after 342 epochs is = 0.18468190947057628\n",
      "Initial Cost on Val dataset for this epoch 342 = 0.18468190947057628\n",
      "Error on this batch = 0.08866863992240302\n",
      "Error on this batch = 0.10032774640398437\n",
      "Cost on val dataset after 343 epochs is = 0.18806835754340065\n",
      "Initial Cost on Val dataset for this epoch 343 = 0.18806835754340065\n",
      "Error on this batch = 0.10810178402570418\n",
      "Error on this batch = 0.11606576504762041\n",
      "Cost on val dataset after 344 epochs is = 0.18309835720065742\n",
      "Initial Cost on Val dataset for this epoch 344 = 0.18309835720065742\n",
      "Error on this batch = 0.09488806168936278\n",
      "Error on this batch = 0.0987378252689355\n",
      "Cost on val dataset after 345 epochs is = 0.198535440641378\n",
      "Initial Cost on Val dataset for this epoch 345 = 0.198535440641378\n",
      "Error on this batch = 0.11464374449846186\n",
      "Error on this batch = 0.16110240002044462\n",
      "Cost on val dataset after 346 epochs is = 0.1885493955748992\n",
      "Initial Cost on Val dataset for this epoch 346 = 0.1885493955748992\n",
      "Error on this batch = 0.10436590248046099\n",
      "Error on this batch = 0.11320556671670957\n",
      "Cost on val dataset after 347 epochs is = 0.1906601284744528\n",
      "Initial Cost on Val dataset for this epoch 347 = 0.1906601284744528\n",
      "Error on this batch = 0.09742809076921198\n",
      "Error on this batch = 0.11064925801371683\n",
      "Cost on val dataset after 348 epochs is = 0.1824377226998552\n",
      "Initial Cost on Val dataset for this epoch 348 = 0.1824377226998552\n",
      "Error on this batch = 0.08864884392649018\n",
      "Error on this batch = 0.11545712132797899\n",
      "Cost on val dataset after 349 epochs is = 0.20649209825848688\n",
      "Initial Cost on Val dataset for this epoch 349 = 0.20649209825848688\n",
      "Error on this batch = 0.124853382826245\n",
      "Error on this batch = 0.11524023377726099\n",
      "Cost on val dataset after 350 epochs is = 0.1783231977479405\n",
      "Initial Cost on Val dataset for this epoch 350 = 0.1783231977479405\n",
      "Error on this batch = 0.08902569887318267\n",
      "Error on this batch = 0.09579821905626736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on val dataset after 351 epochs is = 0.19102907526778762\n",
      "Initial Cost on Val dataset for this epoch 351 = 0.19102907526778762\n",
      "Error on this batch = 0.10162905823296604\n",
      "Error on this batch = 0.11582360389696343\n",
      "Cost on val dataset after 352 epochs is = 0.18109399800277226\n",
      "Initial Cost on Val dataset for this epoch 352 = 0.18109399800277226\n",
      "Error on this batch = 0.09512541846009903\n",
      "Error on this batch = 0.12689898972613353\n",
      "Cost on val dataset after 353 epochs is = 0.18891022198013185\n",
      "Initial Cost on Val dataset for this epoch 353 = 0.18891022198013185\n",
      "Error on this batch = 0.0958284056140854\n",
      "Error on this batch = 0.11720381076938685\n",
      "Cost on val dataset after 354 epochs is = 0.19779009763539143\n",
      "Initial Cost on Val dataset for this epoch 354 = 0.19779009763539143\n",
      "Error on this batch = 0.11025598618762623\n",
      "Error on this batch = 0.09904606257822077\n",
      "Cost on val dataset after 355 epochs is = 0.17873214136228385\n",
      "Initial Cost on Val dataset for this epoch 355 = 0.17873214136228385\n",
      "Error on this batch = 0.08846532631515934\n",
      "Error on this batch = 0.09784887618302021\n",
      "Cost on val dataset after 356 epochs is = 0.1849352312749278\n",
      "Initial Cost on Val dataset for this epoch 356 = 0.1849352312749278\n",
      "Error on this batch = 0.08929652392328005\n",
      "Error on this batch = 0.11491621002134722\n",
      "Cost on val dataset after 357 epochs is = 0.18085007340667097\n",
      "Initial Cost on Val dataset for this epoch 357 = 0.18085007340667097\n",
      "Error on this batch = 0.0922076122776927\n",
      "Error on this batch = 0.11292898623178035\n",
      "Cost on val dataset after 358 epochs is = 0.19124669088057558\n",
      "Initial Cost on Val dataset for this epoch 358 = 0.19124669088057558\n",
      "Error on this batch = 0.10177926033870593\n",
      "Error on this batch = 0.10115649006629149\n",
      "Cost on val dataset after 359 epochs is = 0.18581668652345418\n",
      "Initial Cost on Val dataset for this epoch 359 = 0.18581668652345418\n",
      "Error on this batch = 0.0938669777509264\n",
      "Error on this batch = 0.11227398264574717\n",
      "Cost on val dataset after 360 epochs is = 0.17910212390753452\n",
      "Initial Cost on Val dataset for this epoch 360 = 0.17910212390753452\n",
      "Error on this batch = 0.09224859592572353\n",
      "Error on this batch = 0.10606944418565897\n",
      "Cost on val dataset after 361 epochs is = 0.18331690316958932\n",
      "Initial Cost on Val dataset for this epoch 361 = 0.18331690316958932\n",
      "Error on this batch = 0.09278742691575156\n",
      "Error on this batch = 0.10064690412035462\n",
      "Cost on val dataset after 362 epochs is = 0.1772904265930365\n",
      "Initial Cost on Val dataset for this epoch 362 = 0.1772904265930365\n",
      "Error on this batch = 0.08789771566854801\n",
      "Error on this batch = 0.1363638740859551\n",
      "Cost on val dataset after 363 epochs is = 0.17911322088387957\n",
      "Initial Cost on Val dataset for this epoch 363 = 0.17911322088387957\n",
      "Error on this batch = 0.08635001543452127\n",
      "Error on this batch = 0.10801612945499209\n",
      "Cost on val dataset after 364 epochs is = 0.17873293979655064\n",
      "Initial Cost on Val dataset for this epoch 364 = 0.17873293979655064\n",
      "Error on this batch = 0.08989451142394195\n",
      "Error on this batch = 0.11252754098241784\n",
      "Cost on val dataset after 365 epochs is = 0.1917427954151467\n",
      "Initial Cost on Val dataset for this epoch 365 = 0.1917427954151467\n",
      "Error on this batch = 0.08930375168491042\n",
      "Error on this batch = 0.10520308161714902\n",
      "Cost on val dataset after 366 epochs is = 0.20469524683220636\n",
      "Initial Cost on Val dataset for this epoch 366 = 0.20469524683220636\n",
      "Error on this batch = 0.13605873241842398\n",
      "Error on this batch = 0.10546715243135003\n",
      "Cost on val dataset after 367 epochs is = 0.18233535996178094\n",
      "Initial Cost on Val dataset for this epoch 367 = 0.18233535996178094\n",
      "Error on this batch = 0.09486020358459538\n",
      "Error on this batch = 0.11259575773439899\n",
      "Cost on val dataset after 368 epochs is = 0.17896238623429203\n",
      "Initial Cost on Val dataset for this epoch 368 = 0.17896238623429203\n",
      "Error on this batch = 0.08540815098890037\n",
      "Error on this batch = 0.10063382322510364\n",
      "Cost on val dataset after 369 epochs is = 0.1856680416960754\n",
      "Initial Cost on Val dataset for this epoch 369 = 0.1856680416960754\n",
      "Error on this batch = 0.09626500572318689\n",
      "Error on this batch = 0.11256907297840343\n",
      "Cost on val dataset after 370 epochs is = 0.1850930917774457\n",
      "Initial Cost on Val dataset for this epoch 370 = 0.1850930917774457\n",
      "Error on this batch = 0.09399501870148891\n",
      "Error on this batch = 0.10589239602537262\n",
      "Cost on val dataset after 371 epochs is = 0.18064779379731388\n",
      "Initial Cost on Val dataset for this epoch 371 = 0.18064779379731388\n",
      "Error on this batch = 0.08186972673201208\n",
      "Error on this batch = 0.11118643040956251\n",
      "Cost on val dataset after 372 epochs is = 0.18108542166469102\n",
      "Initial Cost on Val dataset for this epoch 372 = 0.18108542166469102\n",
      "Error on this batch = 0.08740280109476693\n",
      "Error on this batch = 0.1195884394861535\n",
      "Cost on val dataset after 373 epochs is = 0.17952542960065285\n",
      "Initial Cost on Val dataset for this epoch 373 = 0.17952542960065285\n",
      "Error on this batch = 0.08989216261015442\n",
      "Error on this batch = 0.10365984689584287\n",
      "Cost on val dataset after 374 epochs is = 0.1825508895740592\n",
      "Initial Cost on Val dataset for this epoch 374 = 0.1825508895740592\n",
      "Error on this batch = 0.08930506964249185\n",
      "Error on this batch = 0.1023219581662392\n",
      "Cost on val dataset after 375 epochs is = 0.18295745593212331\n",
      "Initial Cost on Val dataset for this epoch 375 = 0.18295745593212331\n",
      "Error on this batch = 0.0927877302512482\n",
      "Error on this batch = 0.10305437128995092\n",
      "Cost on val dataset after 376 epochs is = 0.17980548267087523\n",
      "Initial Cost on Val dataset for this epoch 376 = 0.17980548267087523\n",
      "Error on this batch = 0.09076686737109689\n",
      "Error on this batch = 0.12059879704578975\n",
      "Cost on val dataset after 377 epochs is = 0.18397661117187508\n",
      "Initial Cost on Val dataset for this epoch 377 = 0.18397661117187508\n",
      "Error on this batch = 0.0857587913407088\n",
      "Error on this batch = 0.10900967383449132\n",
      "Cost on val dataset after 378 epochs is = 0.18412443736037307\n",
      "Initial Cost on Val dataset for this epoch 378 = 0.18412443736037307\n",
      "Error on this batch = 0.08794283533926202\n",
      "Error on this batch = 0.11066076140295796\n",
      "Cost on val dataset after 379 epochs is = 0.19446875395307509\n",
      "Initial Cost on Val dataset for this epoch 379 = 0.19446875395307509\n",
      "Error on this batch = 0.10429203929759806\n",
      "Error on this batch = 0.1033945926611061\n",
      "Cost on val dataset after 380 epochs is = 0.18482415193305474\n",
      "Initial Cost on Val dataset for this epoch 380 = 0.18482415193305474\n",
      "Error on this batch = 0.09035589288253608\n",
      "Error on this batch = 0.09899128571536242\n",
      "Cost on val dataset after 381 epochs is = 0.1772321428987628\n",
      "Initial Cost on Val dataset for this epoch 381 = 0.1772321428987628\n",
      "Error on this batch = 0.10459683474381755\n",
      "Error on this batch = 0.10167641251298036\n",
      "Cost on val dataset after 382 epochs is = 0.18798929753138932\n",
      "Initial Cost on Val dataset for this epoch 382 = 0.18798929753138932\n",
      "Error on this batch = 0.09086211060786809\n",
      "Error on this batch = 0.10638557809633409\n",
      "Cost on val dataset after 383 epochs is = 0.1812754581651456\n",
      "Initial Cost on Val dataset for this epoch 383 = 0.1812754581651456\n",
      "Error on this batch = 0.08183577235234829\n",
      "Error on this batch = 0.1235609842893989\n",
      "Cost on val dataset after 384 epochs is = 0.18873737482605016\n",
      "Initial Cost on Val dataset for this epoch 384 = 0.18873737482605016\n",
      "Error on this batch = 0.08309329141660209\n",
      "Error on this batch = 0.10736331927112268\n",
      "Cost on val dataset after 385 epochs is = 0.18289995513705176\n",
      "Initial Cost on Val dataset for this epoch 385 = 0.18289995513705176\n",
      "Error on this batch = 0.07862092158787258\n",
      "Error on this batch = 0.10207072830447128\n",
      "Cost on val dataset after 386 epochs is = 0.19722653911378446\n",
      "Initial Cost on Val dataset for this epoch 386 = 0.19722653911378446\n",
      "Error on this batch = 0.08969596282897428\n",
      "Error on this batch = 0.10181456326678333\n",
      "Cost on val dataset after 387 epochs is = 0.19239298540946873\n",
      "Initial Cost on Val dataset for this epoch 387 = 0.19239298540946873\n",
      "Error on this batch = 0.08576305855979573\n",
      "Error on this batch = 0.12217153719884706\n",
      "Cost on val dataset after 388 epochs is = 0.17668799672948393\n",
      "Initial Cost on Val dataset for this epoch 388 = 0.17668799672948393\n",
      "Error on this batch = 0.08137837484134576\n",
      "Error on this batch = 0.11271193478198102\n",
      "Cost on val dataset after 389 epochs is = 0.1867636238321101\n",
      "Initial Cost on Val dataset for this epoch 389 = 0.1867636238321101\n",
      "Error on this batch = 0.08046638143485212\n",
      "Error on this batch = 0.09994732798402918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on val dataset after 390 epochs is = 0.18062297647130054\n",
      "Initial Cost on Val dataset for this epoch 390 = 0.18062297647130054\n",
      "Error on this batch = 0.07822642873154605\n",
      "Error on this batch = 0.10667117393795493\n",
      "Cost on val dataset after 391 epochs is = 0.18067176283797287\n",
      "Initial Cost on Val dataset for this epoch 391 = 0.18067176283797287\n",
      "Error on this batch = 0.08224742651117581\n",
      "Error on this batch = 0.0977666783807314\n",
      "Cost on val dataset after 392 epochs is = 0.18234079497319683\n",
      "Initial Cost on Val dataset for this epoch 392 = 0.18234079497319683\n",
      "Error on this batch = 0.07406342620465947\n",
      "Error on this batch = 0.10537613755826979\n",
      "Cost on val dataset after 393 epochs is = 0.1807730977333731\n",
      "Initial Cost on Val dataset for this epoch 393 = 0.1807730977333731\n",
      "Error on this batch = 0.07716312483841568\n",
      "Error on this batch = 0.10354323421747444\n",
      "Cost on val dataset after 394 epochs is = 0.17945029795989317\n",
      "Initial Cost on Val dataset for this epoch 394 = 0.17945029795989317\n",
      "Error on this batch = 0.07665956301748907\n",
      "Error on this batch = 0.09540621035175739\n",
      "Cost on val dataset after 395 epochs is = 0.1836758811024297\n",
      "Initial Cost on Val dataset for this epoch 395 = 0.1836758811024297\n",
      "Error on this batch = 0.08123348712445604\n",
      "Error on this batch = 0.12898042473161084\n",
      "Cost on val dataset after 396 epochs is = 0.18384780540675438\n",
      "Initial Cost on Val dataset for this epoch 396 = 0.18384780540675438\n",
      "Error on this batch = 0.07859012955701249\n",
      "Error on this batch = 0.09805868438925629\n",
      "Cost on val dataset after 397 epochs is = 0.1844490223479065\n",
      "Initial Cost on Val dataset for this epoch 397 = 0.1844490223479065\n",
      "Error on this batch = 0.08052192682906885\n",
      "Error on this batch = 0.14286056864836172\n",
      "Cost on val dataset after 398 epochs is = 0.18147550033393847\n",
      "Initial Cost on Val dataset for this epoch 398 = 0.18147550033393847\n",
      "Error on this batch = 0.08184431700652056\n",
      "Error on this batch = 0.09264785105976107\n",
      "Cost on val dataset after 399 epochs is = 0.18955163780398213\n",
      "Initial Cost on Val dataset for this epoch 399 = 0.18955163780398213\n",
      "Error on this batch = 0.09037596824178405\n",
      "Error on this batch = 0.11230976440161797\n",
      "Cost on val dataset after 400 epochs is = 0.18425261386651173\n",
      "Initial Cost on Val dataset for this epoch 400 = 0.18425261386651173\n",
      "Error on this batch = 0.0793275428596325\n",
      "Error on this batch = 0.10789485719924108\n",
      "Cost on val dataset after 401 epochs is = 0.18088966926467856\n",
      "Initial Cost on Val dataset for this epoch 401 = 0.18088966926467856\n",
      "Error on this batch = 0.07726448048634282\n",
      "Error on this batch = 0.12628513987482842\n",
      "Cost on val dataset after 402 epochs is = 0.18239143852824397\n",
      "Initial Cost on Val dataset for this epoch 402 = 0.18239143852824397\n",
      "Error on this batch = 0.0750545496120386\n",
      "Error on this batch = 0.10469851510185976\n",
      "Cost on val dataset after 403 epochs is = 0.18545292954917575\n",
      "Initial Cost on Val dataset for this epoch 403 = 0.18545292954917575\n",
      "Error on this batch = 0.09375371717229523\n",
      "Error on this batch = 0.13037153074487676\n",
      "Cost on val dataset after 404 epochs is = 0.18082629996673955\n",
      "Initial Cost on Val dataset for this epoch 404 = 0.18082629996673955\n",
      "Error on this batch = 0.07771946993926776\n",
      "Error on this batch = 0.10495107113804003\n",
      "Cost on val dataset after 405 epochs is = 0.18327796120532133\n",
      "Initial Cost on Val dataset for this epoch 405 = 0.18327796120532133\n",
      "Error on this batch = 0.08657723418158197\n",
      "Error on this batch = 0.09506571923218055\n",
      "Cost on val dataset after 406 epochs is = 0.19122910097001766\n",
      "Initial Cost on Val dataset for this epoch 406 = 0.19122910097001766\n",
      "Error on this batch = 0.08473340295700613\n",
      "Error on this batch = 0.12019032445542367\n",
      "Cost on val dataset after 407 epochs is = 0.18450158573188288\n",
      "Initial Cost on Val dataset for this epoch 407 = 0.18450158573188288\n",
      "Error on this batch = 0.08141602076536847\n",
      "Error on this batch = 0.11338619531907401\n",
      "Cost on val dataset after 408 epochs is = 0.19967091256928657\n",
      "Initial Cost on Val dataset for this epoch 408 = 0.19967091256928657\n",
      "Error on this batch = 0.11902414867898518\n",
      "Error on this batch = 0.1091396879441069\n",
      "Cost on val dataset after 409 epochs is = 0.18567604566683524\n",
      "Initial Cost on Val dataset for this epoch 409 = 0.18567604566683524\n",
      "Error on this batch = 0.07605435817387196\n",
      "Error on this batch = 0.09571221694478689\n",
      "Cost on val dataset after 410 epochs is = 0.1840135722169253\n",
      "Initial Cost on Val dataset for this epoch 410 = 0.1840135722169253\n",
      "Error on this batch = 0.07359528258970167\n",
      "Error on this batch = 0.1068862568566158\n",
      "Cost on val dataset after 411 epochs is = 0.18458546242947801\n",
      "Initial Cost on Val dataset for this epoch 411 = 0.18458546242947801\n",
      "Error on this batch = 0.07174256196186723\n",
      "Error on this batch = 0.09996356131558579\n",
      "Cost on val dataset after 412 epochs is = 0.18885191183979386\n",
      "Initial Cost on Val dataset for this epoch 412 = 0.18885191183979386\n",
      "Error on this batch = 0.07454419105714151\n",
      "Error on this batch = 0.1065519124881153\n",
      "Cost on val dataset after 413 epochs is = 0.1780973049811389\n",
      "Initial Cost on Val dataset for this epoch 413 = 0.1780973049811389\n",
      "Error on this batch = 0.08117933873405664\n",
      "Error on this batch = 0.10324418181264744\n",
      "Cost on val dataset after 414 epochs is = 0.1875559809041019\n",
      "Initial Cost on Val dataset for this epoch 414 = 0.1875559809041019\n",
      "Error on this batch = 0.08514560890304321\n",
      "Error on this batch = 0.1049797791226052\n",
      "Cost on val dataset after 415 epochs is = 0.1794728899335728\n",
      "Initial Cost on Val dataset for this epoch 415 = 0.1794728899335728\n",
      "Error on this batch = 0.0802313182645051\n",
      "Error on this batch = 0.09887371870299554\n",
      "Cost on val dataset after 416 epochs is = 0.18346326706157057\n",
      "Initial Cost on Val dataset for this epoch 416 = 0.18346326706157057\n",
      "Error on this batch = 0.0706379176655145\n",
      "Error on this batch = 0.12184648573110402\n",
      "Cost on val dataset after 417 epochs is = 0.18538153414202738\n",
      "Initial Cost on Val dataset for this epoch 417 = 0.18538153414202738\n",
      "Error on this batch = 0.07331232166237912\n",
      "Error on this batch = 0.10412148158323184\n",
      "Cost on val dataset after 418 epochs is = 0.18526066895475027\n",
      "Initial Cost on Val dataset for this epoch 418 = 0.18526066895475027\n",
      "Error on this batch = 0.0908071717151494\n",
      "Error on this batch = 0.09552170645347255\n",
      "Cost on val dataset after 419 epochs is = 0.19323639092904404\n",
      "Initial Cost on Val dataset for this epoch 419 = 0.19323639092904404\n",
      "Error on this batch = 0.08697579251071433\n",
      "Error on this batch = 0.10477729045724139\n",
      "Cost on val dataset after 420 epochs is = 0.17956379437644623\n",
      "Initial Cost on Val dataset for this epoch 420 = 0.17956379437644623\n",
      "Error on this batch = 0.08045123700404769\n",
      "Error on this batch = 0.10000330875222488\n",
      "Cost on val dataset after 421 epochs is = 0.18890636132680896\n",
      "Initial Cost on Val dataset for this epoch 421 = 0.18890636132680896\n",
      "Error on this batch = 0.08266387573386937\n",
      "Error on this batch = 0.13953563165248642\n",
      "Cost on val dataset after 422 epochs is = 0.18054801793395517\n",
      "Initial Cost on Val dataset for this epoch 422 = 0.18054801793395517\n",
      "Error on this batch = 0.07885915276156862\n",
      "Error on this batch = 0.10891350902352123\n",
      "Cost on val dataset after 423 epochs is = 0.1854785067589337\n",
      "Initial Cost on Val dataset for this epoch 423 = 0.1854785067589337\n",
      "Error on this batch = 0.0696566500480926\n",
      "Error on this batch = 0.1061425300485593\n",
      "Cost on val dataset after 424 epochs is = 0.18747328511600392\n",
      "Initial Cost on Val dataset for this epoch 424 = 0.18747328511600392\n",
      "Error on this batch = 0.08457638415618032\n",
      "Error on this batch = 0.11708141325126134\n",
      "Cost on val dataset after 425 epochs is = 0.18410990123127027\n",
      "Initial Cost on Val dataset for this epoch 425 = 0.18410990123127027\n",
      "Error on this batch = 0.07383675394370019\n",
      "Error on this batch = 0.11848870913935437\n",
      "Cost on val dataset after 426 epochs is = 0.1810087911948453\n",
      "Initial Cost on Val dataset for this epoch 426 = 0.1810087911948453\n",
      "Error on this batch = 0.07244499365035087\n",
      "Error on this batch = 0.10267255968239038\n",
      "Cost on val dataset after 427 epochs is = 0.18651068154589817\n",
      "Initial Cost on Val dataset for this epoch 427 = 0.18651068154589817\n",
      "Error on this batch = 0.07418399162508711\n",
      "Error on this batch = 0.0962292098303303\n",
      "Cost on val dataset after 428 epochs is = 0.1988323934377848\n",
      "Initial Cost on Val dataset for this epoch 428 = 0.1988323934377848\n",
      "Error on this batch = 0.10552560699198235\n",
      "Error on this batch = 0.1152378601711268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on val dataset after 429 epochs is = 0.21807814019541405\n",
      "Initial Cost on Val dataset for this epoch 429 = 0.21807814019541405\n",
      "Error on this batch = 0.12773954439119006\n",
      "Error on this batch = 0.09129091717468249\n",
      "Cost on val dataset after 430 epochs is = 0.1878991437613099\n",
      "Initial Cost on Val dataset for this epoch 430 = 0.1878991437613099\n",
      "Error on this batch = 0.08594863668853148\n",
      "Error on this batch = 0.11481649322994539\n",
      "Cost on val dataset after 431 epochs is = 0.1857191001969691\n",
      "Initial Cost on Val dataset for this epoch 431 = 0.1857191001969691\n",
      "Error on this batch = 0.0833179632745729\n",
      "Error on this batch = 0.10499757084464628\n",
      "Cost on val dataset after 432 epochs is = 0.18496264290704742\n",
      "Initial Cost on Val dataset for this epoch 432 = 0.18496264290704742\n",
      "Error on this batch = 0.07821998716864223\n",
      "Error on this batch = 0.10643340062010452\n",
      "Cost on val dataset after 433 epochs is = 0.1838953435533727\n",
      "Initial Cost on Val dataset for this epoch 433 = 0.1838953435533727\n",
      "Error on this batch = 0.07650351702347934\n",
      "Error on this batch = 0.0999678174529248\n",
      "Cost on val dataset after 434 epochs is = 0.18094542497794897\n",
      "Initial Cost on Val dataset for this epoch 434 = 0.18094542497794897\n",
      "Error on this batch = 0.07455245853207323\n",
      "Error on this batch = 0.09701077509332554\n",
      "Cost on val dataset after 435 epochs is = 0.18419692373749844\n",
      "Initial Cost on Val dataset for this epoch 435 = 0.18419692373749844\n",
      "Error on this batch = 0.07563235278200602\n",
      "Error on this batch = 0.18872784099365789\n",
      "Cost on val dataset after 436 epochs is = 0.1882641342917854\n",
      "Initial Cost on Val dataset for this epoch 436 = 0.1882641342917854\n",
      "Error on this batch = 0.1033160590297944\n",
      "Error on this batch = 0.13141963882076055\n",
      "Cost on val dataset after 437 epochs is = 0.18189905653909286\n",
      "Initial Cost on Val dataset for this epoch 437 = 0.18189905653909286\n",
      "Error on this batch = 0.08277704600027784\n",
      "Error on this batch = 0.10269161907446794\n",
      "Cost on val dataset after 438 epochs is = 0.183988561643243\n",
      "Initial Cost on Val dataset for this epoch 438 = 0.183988561643243\n",
      "Error on this batch = 0.07786497042985482\n",
      "Error on this batch = 0.10664309485411348\n",
      "Cost on val dataset after 439 epochs is = 0.1889750448775892\n",
      "Initial Cost on Val dataset for this epoch 439 = 0.1889750448775892\n",
      "Error on this batch = 0.07527684084031196\n",
      "Error on this batch = 0.11399874659682264\n",
      "Cost on val dataset after 440 epochs is = 0.18592194541931734\n",
      "Initial Cost on Val dataset for this epoch 440 = 0.18592194541931734\n",
      "Error on this batch = 0.08286830997011337\n",
      "Error on this batch = 0.13815950762731088\n",
      "Cost on val dataset after 441 epochs is = 0.2035447763092571\n",
      "Initial Cost on Val dataset for this epoch 441 = 0.2035447763092571\n",
      "Error on this batch = 0.1280260662708755\n",
      "Error on this batch = 0.11420236131581973\n",
      "Cost on val dataset after 442 epochs is = 0.18567307043728914\n",
      "Initial Cost on Val dataset for this epoch 442 = 0.18567307043728914\n",
      "Error on this batch = 0.0721139520277044\n",
      "Error on this batch = 0.11244294868124426\n",
      "Cost on val dataset after 443 epochs is = 0.18884022091384403\n",
      "Initial Cost on Val dataset for this epoch 443 = 0.18884022091384403\n",
      "Error on this batch = 0.07824703380882513\n",
      "Error on this batch = 0.12034347921247995\n",
      "Cost on val dataset after 444 epochs is = 0.18573748174813973\n",
      "Initial Cost on Val dataset for this epoch 444 = 0.18573748174813973\n",
      "Error on this batch = 0.08677871193839844\n",
      "Error on this batch = 0.1354841427955932\n",
      "Cost on val dataset after 445 epochs is = 0.18424893444617063\n",
      "Initial Cost on Val dataset for this epoch 445 = 0.18424893444617063\n",
      "Error on this batch = 0.08545962780889464\n",
      "Error on this batch = 0.12219538549475767\n",
      "Cost on val dataset after 446 epochs is = 0.18569884744691864\n",
      "Initial Cost on Val dataset for this epoch 446 = 0.18569884744691864\n",
      "Error on this batch = 0.06978368842464484\n",
      "Error on this batch = 0.11161649316182981\n",
      "Cost on val dataset after 447 epochs is = 0.1855473731061703\n",
      "Initial Cost on Val dataset for this epoch 447 = 0.1855473731061703\n",
      "Error on this batch = 0.06851316134787071\n",
      "Error on this batch = 0.09953036720863408\n",
      "Cost on val dataset after 448 epochs is = 0.1831012006113891\n",
      "Initial Cost on Val dataset for this epoch 448 = 0.1831012006113891\n",
      "Error on this batch = 0.07310732315077806\n",
      "Error on this batch = 0.11095191484763821\n",
      "Cost on val dataset after 449 epochs is = 0.1838315496099366\n",
      "Initial Cost on Val dataset for this epoch 449 = 0.1838315496099366\n",
      "Error on this batch = 0.0674841945130897\n",
      "Error on this batch = 0.1080246501927245\n",
      "Cost on val dataset after 450 epochs is = 0.18600445354661002\n",
      "Initial Cost on Val dataset for this epoch 450 = 0.18600445354661002\n",
      "Error on this batch = 0.07609406407175939\n",
      "Error on this batch = 0.1340269290994494\n",
      "Cost on val dataset after 451 epochs is = 0.1806816642029258\n",
      "Initial Cost on Val dataset for this epoch 451 = 0.1806816642029258\n",
      "Error on this batch = 0.0820411718249152\n",
      "Error on this batch = 0.12467620551113004\n",
      "Cost on val dataset after 452 epochs is = 0.18653815303252516\n",
      "Initial Cost on Val dataset for this epoch 452 = 0.18653815303252516\n",
      "Error on this batch = 0.07583350272167395\n",
      "Error on this batch = 0.10457215260176979\n",
      "Cost on val dataset after 453 epochs is = 0.1918264299245881\n",
      "Initial Cost on Val dataset for this epoch 453 = 0.1918264299245881\n",
      "Error on this batch = 0.08934074461582746\n",
      "Error on this batch = 0.12576585940620624\n",
      "Cost on val dataset after 454 epochs is = 0.18466740726611117\n",
      "Initial Cost on Val dataset for this epoch 454 = 0.18466740726611117\n",
      "Error on this batch = 0.07188990167252995\n",
      "Error on this batch = 0.12846511518982434\n",
      "Cost on val dataset after 455 epochs is = 0.18163746365167682\n",
      "Initial Cost on Val dataset for this epoch 455 = 0.18163746365167682\n",
      "Error on this batch = 0.07177994292962872\n",
      "Error on this batch = 0.10118452619282046\n",
      "Cost on val dataset after 456 epochs is = 0.1832325032842372\n",
      "Initial Cost on Val dataset for this epoch 456 = 0.1832325032842372\n",
      "Error on this batch = 0.06882084603611631\n",
      "Error on this batch = 0.1056160084270732\n",
      "Cost on val dataset after 457 epochs is = 0.18646353798886134\n",
      "Initial Cost on Val dataset for this epoch 457 = 0.18646353798886134\n",
      "Error on this batch = 0.07172755376062914\n",
      "Error on this batch = 0.10132596181072462\n",
      "Cost on val dataset after 458 epochs is = 0.18794559074361558\n",
      "Initial Cost on Val dataset for this epoch 458 = 0.18794559074361558\n",
      "Error on this batch = 0.07522139618463608\n",
      "Error on this batch = 0.1287011825365062\n",
      "Cost on val dataset after 459 epochs is = 0.2001898396168063\n",
      "Initial Cost on Val dataset for this epoch 459 = 0.2001898396168063\n",
      "Error on this batch = 0.09134800503568967\n",
      "Error on this batch = 0.11667211741908627\n",
      "Cost on val dataset after 460 epochs is = 0.18529497161686243\n",
      "Initial Cost on Val dataset for this epoch 460 = 0.18529497161686243\n",
      "Error on this batch = 0.0737506183265082\n",
      "Error on this batch = 0.11763238598206861\n",
      "Cost on val dataset after 461 epochs is = 0.184010776640553\n",
      "Initial Cost on Val dataset for this epoch 461 = 0.184010776640553\n",
      "Error on this batch = 0.07933386054105551\n",
      "Error on this batch = 0.09481073377607668\n",
      "Cost on val dataset after 462 epochs is = 0.18463932435169197\n",
      "Initial Cost on Val dataset for this epoch 462 = 0.18463932435169197\n",
      "Error on this batch = 0.07545295250117903\n",
      "Error on this batch = 0.10539512379279685\n",
      "Cost on val dataset after 463 epochs is = 0.19110476964312267\n",
      "Initial Cost on Val dataset for this epoch 463 = 0.19110476964312267\n",
      "Error on this batch = 0.09969999165830984\n",
      "Error on this batch = 0.11100327667980225\n",
      "Cost on val dataset after 464 epochs is = 0.1830809815597496\n",
      "Initial Cost on Val dataset for this epoch 464 = 0.1830809815597496\n",
      "Error on this batch = 0.07706433317122657\n",
      "Error on this batch = 0.1510952451245039\n",
      "Cost on val dataset after 465 epochs is = 0.1818078317724453\n",
      "Initial Cost on Val dataset for this epoch 465 = 0.1818078317724453\n",
      "Error on this batch = 0.07115669718736317\n",
      "Error on this batch = 0.13554513521565065\n",
      "Cost on val dataset after 466 epochs is = 0.1848998166150292\n",
      "Initial Cost on Val dataset for this epoch 466 = 0.1848998166150292\n",
      "Error on this batch = 0.07223133774830957\n",
      "Error on this batch = 0.11111049244677734\n",
      "Cost on val dataset after 467 epochs is = 0.1897878028806877\n",
      "Initial Cost on Val dataset for this epoch 467 = 0.1897878028806877\n",
      "Error on this batch = 0.08789516989436497\n",
      "Error on this batch = 0.11367017050736795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on val dataset after 468 epochs is = 0.18715747111741401\n",
      "Initial Cost on Val dataset for this epoch 468 = 0.18715747111741401\n",
      "Error on this batch = 0.08038194422658816\n",
      "Error on this batch = 0.11672068828325463\n",
      "Cost on val dataset after 469 epochs is = 0.19244529590231196\n",
      "Initial Cost on Val dataset for this epoch 469 = 0.19244529590231196\n",
      "Error on this batch = 0.07935803201597902\n",
      "Error on this batch = 0.11461609106186388\n",
      "Cost on val dataset after 470 epochs is = 0.18557663396731497\n",
      "Initial Cost on Val dataset for this epoch 470 = 0.18557663396731497\n",
      "Error on this batch = 0.07529887900944263\n",
      "Error on this batch = 0.11949101214387783\n",
      "Cost on val dataset after 471 epochs is = 0.18734820879042796\n",
      "Initial Cost on Val dataset for this epoch 471 = 0.18734820879042796\n",
      "Error on this batch = 0.07558552763738677\n",
      "Error on this batch = 0.10164068102228954\n",
      "Cost on val dataset after 472 epochs is = 0.19583924268536101\n",
      "Initial Cost on Val dataset for this epoch 472 = 0.19583924268536101\n",
      "Error on this batch = 0.09147223546258564\n",
      "Error on this batch = 0.10526702558434291\n",
      "Cost on val dataset after 473 epochs is = 0.18582534856917649\n",
      "Initial Cost on Val dataset for this epoch 473 = 0.18582534856917649\n",
      "Error on this batch = 0.07584716105620125\n",
      "Error on this batch = 0.10401110414181314\n",
      "Cost on val dataset after 474 epochs is = 0.18319373537689385\n",
      "Initial Cost on Val dataset for this epoch 474 = 0.18319373537689385\n",
      "Error on this batch = 0.07447480676637758\n",
      "Error on this batch = 0.13168926100357958\n",
      "Cost on val dataset after 475 epochs is = 0.1857158147130894\n",
      "Initial Cost on Val dataset for this epoch 475 = 0.1857158147130894\n",
      "Error on this batch = 0.07925379887143573\n",
      "Error on this batch = 0.12086333205339189\n",
      "Cost on val dataset after 476 epochs is = 0.19372878254067655\n",
      "Initial Cost on Val dataset for this epoch 476 = 0.19372878254067655\n",
      "Error on this batch = 0.08047813282141876\n",
      "Error on this batch = 0.11325406812817726\n",
      "Cost on val dataset after 477 epochs is = 0.1867236636411478\n",
      "Initial Cost on Val dataset for this epoch 477 = 0.1867236636411478\n",
      "Error on this batch = 0.07927078917318442\n",
      "Error on this batch = 0.1264340644530339\n",
      "Cost on val dataset after 478 epochs is = 0.19534990968321553\n",
      "Initial Cost on Val dataset for this epoch 478 = 0.19534990968321553\n",
      "Error on this batch = 0.09761788935180703\n",
      "Error on this batch = 0.09968104162168569\n",
      "Cost on val dataset after 479 epochs is = 0.18787387545555562\n",
      "Initial Cost on Val dataset for this epoch 479 = 0.18787387545555562\n",
      "Error on this batch = 0.07417717502169599\n",
      "Error on this batch = 0.09850310421443702\n",
      "Cost on val dataset after 480 epochs is = 0.19422211511964788\n",
      "Initial Cost on Val dataset for this epoch 480 = 0.19422211511964788\n",
      "Error on this batch = 0.07414907955677656\n",
      "Error on this batch = 0.13594721213712482\n",
      "Cost on val dataset after 481 epochs is = 0.18399559913007563\n",
      "Initial Cost on Val dataset for this epoch 481 = 0.18399559913007563\n",
      "Error on this batch = 0.0893566393613646\n",
      "Error on this batch = 0.1370083949162222\n",
      "Cost on val dataset after 482 epochs is = 0.18874095674064573\n",
      "Initial Cost on Val dataset for this epoch 482 = 0.18874095674064573\n",
      "Error on this batch = 0.08479478249220357\n",
      "Error on this batch = 0.09862267446904391\n",
      "Cost on val dataset after 483 epochs is = 0.1828092509600108\n",
      "Initial Cost on Val dataset for this epoch 483 = 0.1828092509600108\n",
      "Error on this batch = 0.07095961016153945\n",
      "Error on this batch = 0.13292557952066425\n",
      "Cost on val dataset after 484 epochs is = 0.18401076390151236\n",
      "Initial Cost on Val dataset for this epoch 484 = 0.18401076390151236\n",
      "Error on this batch = 0.07095242062501861\n",
      "Error on this batch = 0.11427712011710565\n",
      "Cost on val dataset after 485 epochs is = 0.20003299247049644\n",
      "Initial Cost on Val dataset for this epoch 485 = 0.20003299247049644\n",
      "Error on this batch = 0.08953591287589546\n",
      "Error on this batch = 0.12541913215923337\n",
      "Cost on val dataset after 486 epochs is = 0.19373941420051286\n",
      "Initial Cost on Val dataset for this epoch 486 = 0.19373941420051286\n",
      "Error on this batch = 0.07733010512664297\n",
      "Error on this batch = 0.13888591963576477\n",
      "Cost on val dataset after 487 epochs is = 0.19178491094432415\n",
      "Initial Cost on Val dataset for this epoch 487 = 0.19178491094432415\n",
      "Error on this batch = 0.07103017447899916\n",
      "Error on this batch = 0.10201125650251523\n",
      "Cost on val dataset after 488 epochs is = 0.18852841586257782\n",
      "Initial Cost on Val dataset for this epoch 488 = 0.18852841586257782\n",
      "Error on this batch = 0.07772275641575692\n",
      "Error on this batch = 0.10416347500481042\n",
      "Cost on val dataset after 489 epochs is = 0.18471180808374724\n",
      "Initial Cost on Val dataset for this epoch 489 = 0.18471180808374724\n",
      "Error on this batch = 0.0736041818103098\n",
      "Error on this batch = 0.1006158819290641\n",
      "Cost on val dataset after 490 epochs is = 0.17958047888586443\n",
      "Initial Cost on Val dataset for this epoch 490 = 0.17958047888586443\n",
      "Error on this batch = 0.07996202146009836\n",
      "Error on this batch = 0.0983937959625476\n",
      "Cost on val dataset after 491 epochs is = 0.18955327237288952\n",
      "Initial Cost on Val dataset for this epoch 491 = 0.18955327237288952\n",
      "Error on this batch = 0.0731199982450694\n",
      "Error on this batch = 0.1257934290450639\n",
      "Cost on val dataset after 492 epochs is = 0.18607638858393488\n",
      "Initial Cost on Val dataset for this epoch 492 = 0.18607638858393488\n",
      "Error on this batch = 0.06833736739076689\n",
      "Error on this batch = 0.1470321935426174\n",
      "Cost on val dataset after 493 epochs is = 0.2227602124477442\n",
      "Initial Cost on Val dataset for this epoch 493 = 0.2227602124477442\n",
      "Error on this batch = 0.13885841156538117\n",
      "Error on this batch = 0.10044809399774703\n",
      "Cost on val dataset after 494 epochs is = 0.18793881184082908\n",
      "Initial Cost on Val dataset for this epoch 494 = 0.18793881184082908\n",
      "Error on this batch = 0.06887650749597818\n",
      "Error on this batch = 0.12301465169185369\n",
      "Cost on val dataset after 495 epochs is = 0.18742583011960015\n",
      "Initial Cost on Val dataset for this epoch 495 = 0.18742583011960015\n",
      "Error on this batch = 0.08074318152099993\n",
      "Error on this batch = 0.12942965919596397\n",
      "Cost on val dataset after 496 epochs is = 0.1906091520273335\n",
      "Initial Cost on Val dataset for this epoch 496 = 0.1906091520273335\n",
      "Error on this batch = 0.07982828173504318\n",
      "Error on this batch = 0.11132479570176633\n",
      "Cost on val dataset after 497 epochs is = 0.18323195653162672\n",
      "Initial Cost on Val dataset for this epoch 497 = 0.18323195653162672\n",
      "Error on this batch = 0.07444322043331048\n",
      "Error on this batch = 0.11008120188314495\n",
      "Cost on val dataset after 498 epochs is = 0.18357550189440444\n",
      "Initial Cost on Val dataset for this epoch 498 = 0.18357550189440444\n",
      "Error on this batch = 0.07769667147975359\n",
      "Error on this batch = 0.09570766907605442\n",
      "Cost on val dataset after 499 epochs is = 0.18800742898178732\n",
      "Initial Cost on Val dataset for this epoch 499 = 0.18800742898178732\n",
      "Error on this batch = 0.07506874948337282\n",
      "Error on this batch = 0.10976232505325804\n",
      "Cost on val dataset after 500 epochs is = 0.18740929257756878\n",
      "Initial Cost on Val dataset for this epoch 500 = 0.18740929257756878\n",
      "Error on this batch = 0.07841902557391374\n",
      "Error on this batch = 0.11188767125047505\n",
      "Cost on val dataset after 501 epochs is = 0.19511635315211284\n",
      "Initial Cost on Val dataset for this epoch 501 = 0.19511635315211284\n",
      "Error on this batch = 0.08877463245496114\n",
      "Error on this batch = 0.12806942064224694\n",
      "Cost on val dataset after 502 epochs is = 0.1933549668102585\n",
      "Initial Cost on Val dataset for this epoch 502 = 0.1933549668102585\n",
      "Error on this batch = 0.08235691514547702\n",
      "Error on this batch = 0.12246289999565443\n",
      "Cost on val dataset after 503 epochs is = 0.18905508930790796\n",
      "Initial Cost on Val dataset for this epoch 503 = 0.18905508930790796\n",
      "Error on this batch = 0.07734051542137813\n",
      "Error on this batch = 0.10757123565559713\n",
      "Cost on val dataset after 504 epochs is = 0.195768307390941\n",
      "Initial Cost on Val dataset for this epoch 504 = 0.195768307390941\n",
      "Error on this batch = 0.08571393324367893\n",
      "Error on this batch = 0.10963045800067239\n",
      "Cost on val dataset after 505 epochs is = 0.1875994795275428\n",
      "Initial Cost on Val dataset for this epoch 505 = 0.1875994795275428\n",
      "Error on this batch = 0.0757878882645793\n",
      "Error on this batch = 0.13161863369764668\n",
      "Cost on val dataset after 506 epochs is = 0.18455388201366346\n",
      "Initial Cost on Val dataset for this epoch 506 = 0.18455388201366346\n",
      "Error on this batch = 0.07878522651951982\n",
      "Error on this batch = 0.10623337700047905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on val dataset after 507 epochs is = 0.18844576796160353\n",
      "Initial Cost on Val dataset for this epoch 507 = 0.18844576796160353\n",
      "Error on this batch = 0.07346388485028872\n",
      "Error on this batch = 0.11365981826940895\n",
      "Cost on val dataset after 508 epochs is = 0.19341953867948988\n",
      "Initial Cost on Val dataset for this epoch 508 = 0.19341953867948988\n",
      "Error on this batch = 0.07796382074689119\n",
      "Error on this batch = 0.11404982339326925\n",
      "Cost on val dataset after 509 epochs is = 0.19951344863388817\n",
      "Initial Cost on Val dataset for this epoch 509 = 0.19951344863388817\n",
      "Error on this batch = 0.08498632211280882\n",
      "Error on this batch = 0.08870500805694148\n",
      "Cost on val dataset after 510 epochs is = 0.18957565513756594\n",
      "Initial Cost on Val dataset for this epoch 510 = 0.18957565513756594\n",
      "Error on this batch = 0.08067559448526877\n",
      "Error on this batch = 0.11244229585706723\n",
      "Cost on val dataset after 511 epochs is = 0.19609619209850068\n",
      "Initial Cost on Val dataset for this epoch 511 = 0.19609619209850068\n",
      "Error on this batch = 0.08250451060025003\n",
      "Error on this batch = 0.12836045943381\n",
      "Cost on val dataset after 512 epochs is = 0.18139917705183514\n",
      "Initial Cost on Val dataset for this epoch 512 = 0.18139917705183514\n",
      "Error on this batch = 0.07256361217390993\n",
      "Error on this batch = 0.09910488694084357\n",
      "Cost on val dataset after 513 epochs is = 0.18998392789509708\n",
      "Initial Cost on Val dataset for this epoch 513 = 0.18998392789509708\n",
      "Error on this batch = 0.09199322885118504\n",
      "Error on this batch = 0.09472628815197943\n",
      "Cost on val dataset after 514 epochs is = 0.18436675872384792\n",
      "Initial Cost on Val dataset for this epoch 514 = 0.18436675872384792\n",
      "Error on this batch = 0.07821376024526658\n",
      "Error on this batch = 0.12612075038317075\n",
      "Cost on val dataset after 515 epochs is = 0.19081197216850787\n",
      "Initial Cost on Val dataset for this epoch 515 = 0.19081197216850787\n",
      "Error on this batch = 0.08122461711829207\n",
      "Error on this batch = 0.10398274308568609\n",
      "Cost on val dataset after 516 epochs is = 0.1877114179364911\n",
      "Initial Cost on Val dataset for this epoch 516 = 0.1877114179364911\n",
      "Error on this batch = 0.08781504299237672\n",
      "Error on this batch = 0.1164696970001391\n",
      "Cost on val dataset after 517 epochs is = 0.19101733881775682\n",
      "Initial Cost on Val dataset for this epoch 517 = 0.19101733881775682\n",
      "Error on this batch = 0.08384166734932348\n",
      "Error on this batch = 0.1437747600635103\n",
      "Cost on val dataset after 518 epochs is = 0.19108728837255365\n",
      "Initial Cost on Val dataset for this epoch 518 = 0.19108728837255365\n",
      "Error on this batch = 0.08171271852978176\n",
      "Error on this batch = 0.11102819352672373\n",
      "Cost on val dataset after 519 epochs is = 0.1872411385870599\n",
      "Initial Cost on Val dataset for this epoch 519 = 0.1872411385870599\n",
      "Error on this batch = 0.07911211223972753\n",
      "Error on this batch = 0.10001035766815004\n",
      "Cost on val dataset after 520 epochs is = 0.1922734229611957\n",
      "Initial Cost on Val dataset for this epoch 520 = 0.1922734229611957\n",
      "Error on this batch = 0.08311028143002488\n",
      "Error on this batch = 0.1021887810799611\n",
      "Cost on val dataset after 521 epochs is = 0.18867614385030038\n",
      "Initial Cost on Val dataset for this epoch 521 = 0.18867614385030038\n",
      "Error on this batch = 0.08155797224621136\n",
      "Error on this batch = 0.09812090199179661\n",
      "Cost on val dataset after 522 epochs is = 0.19194766929036586\n",
      "Initial Cost on Val dataset for this epoch 522 = 0.19194766929036586\n",
      "Error on this batch = 0.08705817071665975\n",
      "Error on this batch = 0.0988930551167962\n",
      "Cost on val dataset after 523 epochs is = 0.1903947225694346\n",
      "Initial Cost on Val dataset for this epoch 523 = 0.1903947225694346\n",
      "Error on this batch = 0.07508406236995133\n",
      "Error on this batch = 0.11104817408756411\n",
      "Cost on val dataset after 524 epochs is = 0.190826797046878\n",
      "Initial Cost on Val dataset for this epoch 524 = 0.190826797046878\n",
      "Error on this batch = 0.09480455354106418\n",
      "Error on this batch = 0.1780770629515353\n",
      "Cost on val dataset after 525 epochs is = 0.1959532746315026\n",
      "Initial Cost on Val dataset for this epoch 525 = 0.1959532746315026\n",
      "Error on this batch = 0.08043608268586044\n",
      "Error on this batch = 0.1017056021047253\n",
      "Cost on val dataset after 526 epochs is = 0.18429033813022852\n",
      "Initial Cost on Val dataset for this epoch 526 = 0.18429033813022852\n",
      "Error on this batch = 0.08303243863736155\n",
      "Error on this batch = 0.11136524850292866\n",
      "Cost on val dataset after 527 epochs is = 0.19510120685517648\n",
      "Initial Cost on Val dataset for this epoch 527 = 0.19510120685517648\n",
      "Error on this batch = 0.08002488323150012\n",
      "Error on this batch = 0.0907179122306304\n",
      "Cost on val dataset after 528 epochs is = 0.19348735607091586\n",
      "Initial Cost on Val dataset for this epoch 528 = 0.19348735607091586\n",
      "Error on this batch = 0.0780425777281782\n",
      "Error on this batch = 0.09484437310617125\n",
      "Cost on val dataset after 529 epochs is = 0.18577839326115322\n",
      "Initial Cost on Val dataset for this epoch 529 = 0.18577839326115322\n",
      "Error on this batch = 0.08356884196307691\n",
      "Error on this batch = 0.12408090356301114\n",
      "Cost on val dataset after 530 epochs is = 0.18608636559169833\n",
      "Initial Cost on Val dataset for this epoch 530 = 0.18608636559169833\n",
      "Error on this batch = 0.08321447259776987\n",
      "Error on this batch = 0.10714467700137327\n",
      "Cost on val dataset after 531 epochs is = 0.18876689332854624\n",
      "Initial Cost on Val dataset for this epoch 531 = 0.18876689332854624\n",
      "Error on this batch = 0.07637956100985897\n",
      "Error on this batch = 0.11670566716732571\n",
      "Cost on val dataset after 532 epochs is = 0.19434815489441748\n",
      "Initial Cost on Val dataset for this epoch 532 = 0.19434815489441748\n",
      "Error on this batch = 0.08235164766296295\n",
      "Error on this batch = 0.11436626709904715\n",
      "Cost on val dataset after 533 epochs is = 0.19120545932468552\n",
      "Initial Cost on Val dataset for this epoch 533 = 0.19120545932468552\n",
      "Error on this batch = 0.07614157731897733\n",
      "Error on this batch = 0.11175370669114262\n",
      "Cost on val dataset after 534 epochs is = 0.1916193441451634\n",
      "Initial Cost on Val dataset for this epoch 534 = 0.1916193441451634\n",
      "Error on this batch = 0.07223484042978041\n",
      "Error on this batch = 0.17116016390216107\n",
      "Cost on val dataset after 535 epochs is = 0.20335967297623603\n",
      "Initial Cost on Val dataset for this epoch 535 = 0.20335967297623603\n",
      "Error on this batch = 0.10459896634301458\n",
      "Error on this batch = 0.11523852500203095\n",
      "Cost on val dataset after 536 epochs is = 0.19350959424441352\n",
      "Initial Cost on Val dataset for this epoch 536 = 0.19350959424441352\n",
      "Error on this batch = 0.07566611691328273\n",
      "Error on this batch = 0.1219169308655374\n",
      "Cost on val dataset after 537 epochs is = 0.19229751107025084\n",
      "Initial Cost on Val dataset for this epoch 537 = 0.19229751107025084\n",
      "Error on this batch = 0.07558623980184632\n",
      "Error on this batch = 0.1119727718813663\n",
      "Cost on val dataset after 538 epochs is = 0.18676319441273231\n",
      "Initial Cost on Val dataset for this epoch 538 = 0.18676319441273231\n",
      "Error on this batch = 0.07808921174828491\n",
      "Error on this batch = 0.11568737061126841\n",
      "Cost on val dataset after 539 epochs is = 0.19834216974545363\n",
      "Initial Cost on Val dataset for this epoch 539 = 0.19834216974545363\n",
      "Error on this batch = 0.08455993992735021\n",
      "Error on this batch = 0.10748390004400532\n",
      "Cost on val dataset after 540 epochs is = 0.18547919182556025\n",
      "Initial Cost on Val dataset for this epoch 540 = 0.18547919182556025\n",
      "Error on this batch = 0.07394506795484647\n",
      "Error on this batch = 0.09459758742378263\n",
      "Cost on val dataset after 541 epochs is = 0.189333900687369\n",
      "Initial Cost on Val dataset for this epoch 541 = 0.189333900687369\n",
      "Error on this batch = 0.08004849780042501\n",
      "Error on this batch = 0.08812490122207756\n",
      "Cost on val dataset after 542 epochs is = 0.1906257000517744\n",
      "Initial Cost on Val dataset for this epoch 542 = 0.1906257000517744\n",
      "Error on this batch = 0.07596684597201417\n",
      "Error on this batch = 0.1296976684968716\n",
      "Cost on val dataset after 543 epochs is = 0.1878544169115487\n",
      "Initial Cost on Val dataset for this epoch 543 = 0.1878544169115487\n",
      "Error on this batch = 0.08251842586348451\n",
      "Error on this batch = 0.11780922962474391\n",
      "Cost on val dataset after 544 epochs is = 0.20365104519081795\n",
      "Initial Cost on Val dataset for this epoch 544 = 0.20365104519081795\n",
      "Error on this batch = 0.10415360750166362\n",
      "Error on this batch = 0.09365255389282733\n",
      "Cost on val dataset after 545 epochs is = 0.18526360200876674\n",
      "Initial Cost on Val dataset for this epoch 545 = 0.18526360200876674\n",
      "Error on this batch = 0.07594562296099469\n",
      "Error on this batch = 0.1153392104068702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on val dataset after 546 epochs is = 0.18760145720109836\n",
      "Initial Cost on Val dataset for this epoch 546 = 0.18760145720109836\n",
      "Error on this batch = 0.08332368074778053\n",
      "Error on this batch = 0.13243846844889368\n",
      "Cost on val dataset after 547 epochs is = 0.19236317280144394\n",
      "Initial Cost on Val dataset for this epoch 547 = 0.19236317280144394\n",
      "Error on this batch = 0.08600428553824506\n",
      "Error on this batch = 0.154725736090612\n",
      "Cost on val dataset after 548 epochs is = 0.19430680504111023\n",
      "Initial Cost on Val dataset for this epoch 548 = 0.19430680504111023\n",
      "Error on this batch = 0.07940968861643072\n",
      "Error on this batch = 0.11180328510863316\n",
      "Cost on val dataset after 549 epochs is = 0.1892831142243044\n",
      "Initial Cost on Val dataset for this epoch 549 = 0.1892831142243044\n",
      "Error on this batch = 0.08260105590834652\n",
      "Error on this batch = 0.13580293351964443\n",
      "Cost on val dataset after 550 epochs is = 0.1931692623137227\n",
      "Initial Cost on Val dataset for this epoch 550 = 0.1931692623137227\n",
      "Error on this batch = 0.08575956745971151\n",
      "Error on this batch = 0.10272368879882723\n",
      "Cost on val dataset after 551 epochs is = 0.20698675947076975\n",
      "Initial Cost on Val dataset for this epoch 551 = 0.20698675947076975\n",
      "Error on this batch = 0.09394710481212137\n",
      "Error on this batch = 0.10756276845040275\n",
      "Cost on val dataset after 552 epochs is = 0.19234647863310106\n",
      "Initial Cost on Val dataset for this epoch 552 = 0.19234647863310106\n",
      "Error on this batch = 0.08585747070696327\n",
      "Error on this batch = 0.10102334626159686\n",
      "Cost on val dataset after 553 epochs is = 0.19270674540623528\n",
      "Initial Cost on Val dataset for this epoch 553 = 0.19270674540623528\n",
      "Error on this batch = 0.08498956165729632\n",
      "Error on this batch = 0.10680899648892635\n",
      "Cost on val dataset after 554 epochs is = 0.19115630714058182\n",
      "Initial Cost on Val dataset for this epoch 554 = 0.19115630714058182\n",
      "Error on this batch = 0.08276042166974971\n",
      "Error on this batch = 0.11196050631782063\n",
      "Cost on val dataset after 555 epochs is = 0.19312354417115402\n",
      "Initial Cost on Val dataset for this epoch 555 = 0.19312354417115402\n",
      "Error on this batch = 0.08602117718314124\n",
      "Error on this batch = 0.11175115361161796\n",
      "Cost on val dataset after 556 epochs is = 0.1951649778150966\n",
      "Initial Cost on Val dataset for this epoch 556 = 0.1951649778150966\n",
      "Error on this batch = 0.0954543241918511\n",
      "Error on this batch = 0.10621944129327744\n",
      "Cost on val dataset after 557 epochs is = 0.20069171852637493\n",
      "Initial Cost on Val dataset for this epoch 557 = 0.20069171852637493\n",
      "Error on this batch = 0.09862998383183534\n",
      "Error on this batch = 0.08764142964428878\n",
      "Cost on val dataset after 558 epochs is = 0.1911443978130939\n",
      "Initial Cost on Val dataset for this epoch 558 = 0.1911443978130939\n",
      "Error on this batch = 0.07971457687655667\n",
      "Error on this batch = 0.11310587530579333\n",
      "Cost on val dataset after 559 epochs is = 0.19302938386821494\n",
      "Initial Cost on Val dataset for this epoch 559 = 0.19302938386821494\n",
      "Error on this batch = 0.08235600898622558\n",
      "Error on this batch = 0.1021841417003045\n",
      "Cost on val dataset after 560 epochs is = 0.18619755224573575\n",
      "Initial Cost on Val dataset for this epoch 560 = 0.18619755224573575\n",
      "Error on this batch = 0.0747814903968693\n",
      "Error on this batch = 0.10290586695613946\n",
      "Cost on val dataset after 561 epochs is = 0.19387837600373733\n",
      "Initial Cost on Val dataset for this epoch 561 = 0.19387837600373733\n",
      "Error on this batch = 0.07621846196905742\n",
      "Error on this batch = 0.11488132917704327\n",
      "Cost on val dataset after 562 epochs is = 0.18923614505792105\n",
      "Initial Cost on Val dataset for this epoch 562 = 0.18923614505792105\n",
      "Error on this batch = 0.08608599077006918\n",
      "Error on this batch = 0.10409421128294107\n",
      "Cost on val dataset after 563 epochs is = 0.19017924020911833\n",
      "Initial Cost on Val dataset for this epoch 563 = 0.19017924020911833\n",
      "Error on this batch = 0.0858601492191392\n",
      "Error on this batch = 0.12049502629470542\n",
      "Cost on val dataset after 564 epochs is = 0.19643176370505522\n",
      "Initial Cost on Val dataset for this epoch 564 = 0.19643176370505522\n",
      "Error on this batch = 0.08178263278619965\n",
      "Error on this batch = 0.09103367781655998\n",
      "Cost on val dataset after 565 epochs is = 0.19266211622750998\n",
      "Initial Cost on Val dataset for this epoch 565 = 0.19266211622750998\n",
      "Error on this batch = 0.0913029566643262\n",
      "Error on this batch = 0.11666404717366981\n",
      "Cost on val dataset after 566 epochs is = 0.1924646631909867\n",
      "Initial Cost on Val dataset for this epoch 566 = 0.1924646631909867\n",
      "Error on this batch = 0.09120147413871213\n",
      "Error on this batch = 0.173249694850456\n",
      "Cost on val dataset after 567 epochs is = 0.23336846847400092\n",
      "Initial Cost on Val dataset for this epoch 567 = 0.23336846847400092\n",
      "Error on this batch = 0.11520209442261341\n",
      "Error on this batch = 0.08866261874622038\n",
      "Cost on val dataset after 568 epochs is = 0.20102593659440343\n",
      "Initial Cost on Val dataset for this epoch 568 = 0.20102593659440343\n",
      "Error on this batch = 0.10420797721522122\n",
      "Error on this batch = 0.09059341802148367\n",
      "Cost on val dataset after 569 epochs is = 0.1949743436121152\n",
      "Initial Cost on Val dataset for this epoch 569 = 0.1949743436121152\n",
      "Error on this batch = 0.0929247135925354\n",
      "Error on this batch = 0.10368305930031971\n",
      "Cost on val dataset after 570 epochs is = 0.18910228639435672\n",
      "Initial Cost on Val dataset for this epoch 570 = 0.18910228639435672\n",
      "Error on this batch = 0.07541399172558269\n",
      "Error on this batch = 0.11627410720491846\n",
      "Cost on val dataset after 571 epochs is = 0.19320099050209638\n",
      "Initial Cost on Val dataset for this epoch 571 = 0.19320099050209638\n",
      "Error on this batch = 0.07568549718376524\n",
      "Error on this batch = 0.0887531050267758\n",
      "Cost on val dataset after 572 epochs is = 0.19313757252807237\n",
      "Initial Cost on Val dataset for this epoch 572 = 0.19313757252807237\n",
      "Error on this batch = 0.08478760346047778\n",
      "Error on this batch = 0.14466280676910365\n",
      "Cost on val dataset after 573 epochs is = 0.19726264193887952\n",
      "Initial Cost on Val dataset for this epoch 573 = 0.19726264193887952\n",
      "Error on this batch = 0.08903015738595055\n",
      "Error on this batch = 0.11169958667123522\n",
      "Cost on val dataset after 574 epochs is = 0.18979214995461252\n",
      "Initial Cost on Val dataset for this epoch 574 = 0.18979214995461252\n",
      "Error on this batch = 0.07766438716652935\n",
      "Error on this batch = 0.10029506415769335\n",
      "Cost on val dataset after 575 epochs is = 0.19762809738882306\n",
      "Initial Cost on Val dataset for this epoch 575 = 0.19762809738882306\n",
      "Error on this batch = 0.08429701054913157\n",
      "Error on this batch = 0.11701917015626068\n",
      "Cost on val dataset after 576 epochs is = 0.1920497261142635\n",
      "Initial Cost on Val dataset for this epoch 576 = 0.1920497261142635\n",
      "Error on this batch = 0.08491559469186168\n",
      "Error on this batch = 0.10914259478264411\n",
      "Cost on val dataset after 577 epochs is = 0.19291195010797976\n",
      "Initial Cost on Val dataset for this epoch 577 = 0.19291195010797976\n",
      "Error on this batch = 0.09118699901016535\n",
      "Error on this batch = 0.08597009670474465\n",
      "Cost on val dataset after 578 epochs is = 0.20231012502201387\n",
      "Initial Cost on Val dataset for this epoch 578 = 0.20231012502201387\n",
      "Error on this batch = 0.08242277922728387\n",
      "Error on this batch = 0.100572056271884\n",
      "Cost on val dataset after 579 epochs is = 0.19169966894731658\n",
      "Initial Cost on Val dataset for this epoch 579 = 0.19169966894731658\n",
      "Error on this batch = 0.08087002767512626\n",
      "Error on this batch = 0.10394964804039472\n",
      "Cost on val dataset after 580 epochs is = 0.20993142725364078\n",
      "Initial Cost on Val dataset for this epoch 580 = 0.20993142725364078\n",
      "Error on this batch = 0.09421605342792122\n",
      "Error on this batch = 0.09609211754073407\n",
      "Cost on val dataset after 581 epochs is = 0.19242388612895003\n",
      "Initial Cost on Val dataset for this epoch 581 = 0.19242388612895003\n",
      "Error on this batch = 0.0805874413683613\n",
      "Error on this batch = 0.08450441609552879\n",
      "Cost on val dataset after 582 epochs is = 0.19403517865535458\n",
      "Initial Cost on Val dataset for this epoch 582 = 0.19403517865535458\n",
      "Error on this batch = 0.08153429795719891\n",
      "Error on this batch = 0.1007051388871023\n",
      "Cost on val dataset after 583 epochs is = 0.18869619084608424\n",
      "Initial Cost on Val dataset for this epoch 583 = 0.18869619084608424\n",
      "Error on this batch = 0.07063137084371164\n",
      "Error on this batch = 0.08959556411951156\n",
      "Cost on val dataset after 584 epochs is = 0.20142637526799861\n",
      "Initial Cost on Val dataset for this epoch 584 = 0.20142637526799861\n",
      "Error on this batch = 0.09727102320678714\n",
      "Error on this batch = 0.10452973113691365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on val dataset after 585 epochs is = 0.1927852739560482\n",
      "Initial Cost on Val dataset for this epoch 585 = 0.1927852739560482\n",
      "Error on this batch = 0.08562932994680543\n",
      "Error on this batch = 0.10858216012020919\n",
      "Cost on val dataset after 586 epochs is = 0.18980390209928616\n",
      "Initial Cost on Val dataset for this epoch 586 = 0.18980390209928616\n",
      "Error on this batch = 0.07461452873602144\n",
      "Error on this batch = 0.09970625740849627\n",
      "Cost on val dataset after 587 epochs is = 0.1947691097592097\n",
      "Initial Cost on Val dataset for this epoch 587 = 0.1947691097592097\n",
      "Error on this batch = 0.08991001331261284\n",
      "Error on this batch = 0.09080280991130098\n",
      "Cost on val dataset after 588 epochs is = 0.21387490926157776\n",
      "Initial Cost on Val dataset for this epoch 588 = 0.21387490926157776\n",
      "Error on this batch = 0.10436199146950419\n",
      "Error on this batch = 0.09250930237003374\n",
      "Cost on val dataset after 589 epochs is = 0.19862635308374307\n",
      "Initial Cost on Val dataset for this epoch 589 = 0.19862635308374307\n",
      "Error on this batch = 0.0857682809337216\n",
      "Error on this batch = 0.11685270987256811\n",
      "Cost on val dataset after 590 epochs is = 0.21221505322804016\n",
      "Initial Cost on Val dataset for this epoch 590 = 0.21221505322804016\n",
      "Error on this batch = 0.10436863089290216\n",
      "Error on this batch = 0.11929679591844027\n",
      "Cost on val dataset after 591 epochs is = 0.19627638082614923\n",
      "Initial Cost on Val dataset for this epoch 591 = 0.19627638082614923\n",
      "Error on this batch = 0.09581029232609382\n",
      "Error on this batch = 0.08817307498395119\n",
      "Cost on val dataset after 592 epochs is = 0.19144526473209297\n",
      "Initial Cost on Val dataset for this epoch 592 = 0.19144526473209297\n",
      "Error on this batch = 0.08166098680709538\n",
      "Error on this batch = 0.09846545290923037\n",
      "Cost on val dataset after 593 epochs is = 0.19653548514526503\n",
      "Initial Cost on Val dataset for this epoch 593 = 0.19653548514526503\n",
      "Error on this batch = 0.08551637214725315\n",
      "Error on this batch = 0.09781530318902285\n",
      "Cost on val dataset after 594 epochs is = 0.19178819562222957\n",
      "Initial Cost on Val dataset for this epoch 594 = 0.19178819562222957\n",
      "Error on this batch = 0.07502861973197048\n",
      "Error on this batch = 0.09249425247026061\n",
      "Cost on val dataset after 595 epochs is = 0.20400294308152658\n",
      "Initial Cost on Val dataset for this epoch 595 = 0.20400294308152658\n",
      "Error on this batch = 0.10023720333750401\n",
      "Error on this batch = 0.09377058881445147\n",
      "Cost on val dataset after 596 epochs is = 0.2021767619469064\n",
      "Initial Cost on Val dataset for this epoch 596 = 0.2021767619469064\n",
      "Error on this batch = 0.09249567797514165\n",
      "Error on this batch = 0.09866346003363306\n",
      "Cost on val dataset after 597 epochs is = 0.20122995268801502\n",
      "Initial Cost on Val dataset for this epoch 597 = 0.20122995268801502\n",
      "Error on this batch = 0.10055373547447165\n",
      "Error on this batch = 0.09593226280819238\n",
      "Cost on val dataset after 598 epochs is = 0.20239040596676328\n",
      "Initial Cost on Val dataset for this epoch 598 = 0.20239040596676328\n",
      "Error on this batch = 0.08193013763704436\n",
      "Error on this batch = 0.11189388469040676\n",
      "Cost on val dataset after 599 epochs is = 0.20968941267261593\n",
      "Initial Cost on Val dataset for this epoch 599 = 0.20968941267261593\n",
      "Error on this batch = 0.10154062844049022\n",
      "Error on this batch = 0.08541595592904258\n",
      "Cost on val dataset after 600 epochs is = 0.19668838806497707\n",
      "Initial Cost on Val dataset for this epoch 600 = 0.19668838806497707\n",
      "Error on this batch = 0.08152456790682716\n",
      "Error on this batch = 0.15024446504744748\n",
      "Cost on val dataset after 601 epochs is = 0.19579174705810556\n",
      "Initial Cost on Val dataset for this epoch 601 = 0.19579174705810556\n",
      "Error on this batch = 0.08212582667033441\n",
      "Error on this batch = 0.1172551004391095\n",
      "Cost on val dataset after 602 epochs is = 0.20289273226519203\n",
      "Initial Cost on Val dataset for this epoch 602 = 0.20289273226519203\n",
      "Error on this batch = 0.08110362647773314\n",
      "Error on this batch = 0.09168846908063111\n",
      "Cost on val dataset after 603 epochs is = 0.1979601362496347\n",
      "Initial Cost on Val dataset for this epoch 603 = 0.1979601362496347\n",
      "Error on this batch = 0.08805839108898038\n",
      "Error on this batch = 0.133598849933755\n",
      "Cost on val dataset after 604 epochs is = 0.1970442108404819\n",
      "Initial Cost on Val dataset for this epoch 604 = 0.1970442108404819\n",
      "Error on this batch = 0.0788177665055015\n",
      "Error on this batch = 0.09616736907431125\n",
      "Cost on val dataset after 605 epochs is = 0.21687386622556182\n",
      "Initial Cost on Val dataset for this epoch 605 = 0.21687386622556182\n",
      "Error on this batch = 0.09091928644974212\n",
      "Error on this batch = 0.10479537514030643\n",
      "Cost on val dataset after 606 epochs is = 0.19205282780077385\n",
      "Initial Cost on Val dataset for this epoch 606 = 0.19205282780077385\n",
      "Error on this batch = 0.07384899693467999\n",
      "Error on this batch = 0.12902321422060092\n",
      "Cost on val dataset after 607 epochs is = 0.19885529161984342\n",
      "Initial Cost on Val dataset for this epoch 607 = 0.19885529161984342\n",
      "Error on this batch = 0.077797955524674\n",
      "Error on this batch = 0.09282618849409748\n",
      "Cost on val dataset after 608 epochs is = 0.20400347255000684\n",
      "Initial Cost on Val dataset for this epoch 608 = 0.20400347255000684\n",
      "Error on this batch = 0.08309817067585888\n",
      "Error on this batch = 0.10781180359179343\n",
      "Cost on val dataset after 609 epochs is = 0.19048818510564886\n",
      "Initial Cost on Val dataset for this epoch 609 = 0.19048818510564886\n",
      "Error on this batch = 0.07263390652642374\n",
      "Error on this batch = 0.09561505550248126\n",
      "Cost on val dataset after 610 epochs is = 0.19200477123726226\n",
      "Initial Cost on Val dataset for this epoch 610 = 0.19200477123726226\n",
      "Error on this batch = 0.0755272298638504\n",
      "Error on this batch = 0.11153817023746902\n",
      "Cost on val dataset after 611 epochs is = 0.19582750136837856\n",
      "Initial Cost on Val dataset for this epoch 611 = 0.19582750136837856\n",
      "Error on this batch = 0.07448562113353535\n",
      "Error on this batch = 0.10470450957271207\n",
      "Cost on val dataset after 612 epochs is = 0.20595726457845234\n",
      "Initial Cost on Val dataset for this epoch 612 = 0.20595726457845234\n",
      "Error on this batch = 0.08533483164316691\n",
      "Error on this batch = 0.09928033766448836\n",
      "Cost on val dataset after 613 epochs is = 0.1985885355970172\n",
      "Initial Cost on Val dataset for this epoch 613 = 0.1985885355970172\n",
      "Error on this batch = 0.08500708262423866\n",
      "Error on this batch = 0.09296920582493928\n",
      "Cost on val dataset after 614 epochs is = 0.212273468635497\n",
      "Initial Cost on Val dataset for this epoch 614 = 0.212273468635497\n",
      "Error on this batch = 0.11129133132665013\n",
      "Error on this batch = 0.09251789264699793\n",
      "Cost on val dataset after 615 epochs is = 0.19979316704209252\n",
      "Initial Cost on Val dataset for this epoch 615 = 0.19979316704209252\n",
      "Error on this batch = 0.0838733266887794\n",
      "Error on this batch = 0.09106156119600613\n",
      "Cost on val dataset after 616 epochs is = 0.2246244331587469\n",
      "Initial Cost on Val dataset for this epoch 616 = 0.2246244331587469\n",
      "Error on this batch = 0.10120461065294728\n",
      "Error on this batch = 0.09364818240688712\n",
      "Cost on val dataset after 617 epochs is = 0.19746874519608273\n",
      "Initial Cost on Val dataset for this epoch 617 = 0.19746874519608273\n",
      "Error on this batch = 0.08980360750529066\n",
      "Error on this batch = 0.09774399885151348\n",
      "Cost on val dataset after 618 epochs is = 0.1969880170183708\n",
      "Initial Cost on Val dataset for this epoch 618 = 0.1969880170183708\n",
      "Error on this batch = 0.0781973793396218\n",
      "Error on this batch = 0.10361687889876926\n",
      "Cost on val dataset after 619 epochs is = 0.19128685810590423\n",
      "Initial Cost on Val dataset for this epoch 619 = 0.19128685810590423\n",
      "Error on this batch = 0.08324065905886772\n",
      "Error on this batch = 0.0946410280319688\n",
      "Cost on val dataset after 620 epochs is = 0.204251644388671\n",
      "Initial Cost on Val dataset for this epoch 620 = 0.204251644388671\n",
      "Error on this batch = 0.07752364238185105\n",
      "Error on this batch = 0.09507602705082785\n",
      "Cost on val dataset after 621 epochs is = 0.22036963490556538\n",
      "Initial Cost on Val dataset for this epoch 621 = 0.22036963490556538\n",
      "Error on this batch = 0.11786649857367504\n",
      "Error on this batch = 0.09619780267371025\n",
      "Cost on val dataset after 622 epochs is = 0.19401486409944038\n",
      "Initial Cost on Val dataset for this epoch 622 = 0.19401486409944038\n",
      "Error on this batch = 0.06891513431978111\n",
      "Error on this batch = 0.10860786546250996\n",
      "Cost on val dataset after 623 epochs is = 0.1950084809529125\n",
      "Initial Cost on Val dataset for this epoch 623 = 0.1950084809529125\n",
      "Error on this batch = 0.07050232849805331\n",
      "Error on this batch = 0.09068279307970667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on val dataset after 624 epochs is = 0.18844865548028575\n",
      "Initial Cost on Val dataset for this epoch 624 = 0.18844865548028575\n",
      "Error on this batch = 0.06897053535988774\n",
      "Error on this batch = 0.09554740289652959\n",
      "Cost on val dataset after 625 epochs is = 0.1908480190175003\n",
      "Initial Cost on Val dataset for this epoch 625 = 0.1908480190175003\n",
      "Error on this batch = 0.06718357634576698\n",
      "Error on this batch = 0.11991811304792746\n",
      "Cost on val dataset after 626 epochs is = 0.2057493477321587\n",
      "Initial Cost on Val dataset for this epoch 626 = 0.2057493477321587\n",
      "Error on this batch = 0.09478914851797184\n",
      "Error on this batch = 0.10334368041375687\n",
      "Cost on val dataset after 627 epochs is = 0.1953246746951714\n",
      "Initial Cost on Val dataset for this epoch 627 = 0.1953246746951714\n",
      "Error on this batch = 0.0841185855753606\n",
      "Error on this batch = 0.13819261396735144\n",
      "Cost on val dataset after 628 epochs is = 0.19471060795074593\n",
      "Initial Cost on Val dataset for this epoch 628 = 0.19471060795074593\n",
      "Error on this batch = 0.07365516934348552\n",
      "Error on this batch = 0.0963103750576505\n",
      "Cost on val dataset after 629 epochs is = 0.18938562197527256\n",
      "Initial Cost on Val dataset for this epoch 629 = 0.18938562197527256\n",
      "Error on this batch = 0.06802975814217288\n",
      "Error on this batch = 0.10195652677296156\n",
      "Cost on val dataset after 630 epochs is = 0.18725565969918914\n",
      "Initial Cost on Val dataset for this epoch 630 = 0.18725565969918914\n",
      "Error on this batch = 0.06682949466286159\n",
      "Error on this batch = 0.09913794609767727\n",
      "Cost on val dataset after 631 epochs is = 0.19297679988154975\n",
      "Initial Cost on Val dataset for this epoch 631 = 0.19297679988154975\n",
      "Error on this batch = 0.06975570661870158\n",
      "Error on this batch = 0.10167166619484604\n",
      "Cost on val dataset after 632 epochs is = 0.19398778820240525\n",
      "Initial Cost on Val dataset for this epoch 632 = 0.19398778820240525\n",
      "Error on this batch = 0.07754797280794172\n",
      "Error on this batch = 0.09251253103828738\n",
      "Cost on val dataset after 633 epochs is = 0.1931826444722959\n",
      "Initial Cost on Val dataset for this epoch 633 = 0.1931826444722959\n",
      "Error on this batch = 0.08905089786547853\n",
      "Error on this batch = 0.10800918050405191\n",
      "Cost on val dataset after 634 epochs is = 0.1959946498103746\n",
      "Initial Cost on Val dataset for this epoch 634 = 0.1959946498103746\n",
      "Error on this batch = 0.07766846526069125\n",
      "Error on this batch = 0.09596356582311423\n",
      "Cost on val dataset after 635 epochs is = 0.19324402888380326\n",
      "Initial Cost on Val dataset for this epoch 635 = 0.19324402888380326\n",
      "Error on this batch = 0.10091699712339487\n",
      "Error on this batch = 0.10336930750219135\n",
      "Cost on val dataset after 636 epochs is = 0.19147738747104634\n",
      "Initial Cost on Val dataset for this epoch 636 = 0.19147738747104634\n",
      "Error on this batch = 0.06631576586076628\n",
      "Error on this batch = 0.09219851478430552\n",
      "Cost on val dataset after 637 epochs is = 0.19246329693087652\n",
      "Initial Cost on Val dataset for this epoch 637 = 0.19246329693087652\n",
      "Error on this batch = 0.07878980956240976\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-a4a48a88c907>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_class_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'constant'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-89-48ff9c56ec0b>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(mini_batch, X_valid, valid_class_enc, theta, lr, act_fn, lr_mode, cost_fn)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;31m#Backward Propagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackward_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;31m#Theta Update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-86-0bb1905ca34a>\u001b[0m in \u001b[0;36mbackward_prop\u001b[0;34m(fm, Y_b, theta, batch_size, act_fn, cost_fn)\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0mdelta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mfm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;32melif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact_fn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                 \u001b[0mdelta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mderiv_relu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0;32melif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact_fn\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'softplus'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0mdelta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mderiv_softplus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "arch=[100, 100]\n",
    "lr=0.6\n",
    "theta = theta_init(n, r, arch, 'uniform')\n",
    "print(\"Training the network with {} hidden layer with {} units\".format(len(arch), arch))\n",
    "print(\"The parameters of the layers are of the shape:\")\n",
    "\n",
    "for j in range(len(theta)):\n",
    "    print(\"theta between layer {} and layer {} is {}\".format(j, j+1,theta[j].shape))\n",
    "    \n",
    "start = time.time()\n",
    "epoch, theta = training(mini_batch, X_valid, valid_class_enc, theta, lr, 'relu', 'adaptive')\n",
    "\n",
    "epochs.append(epoch)\n",
    "train_time.append(time.time()-start)\n",
    "train_accuracy.append(calc_accuracy(X_train, theta, train_class_enc, 'relu'))\n",
    "valid_accuracy.append(calc_accuracy(X_valid, theta, valid_class_enc, 'relu'))\n",
    "test_accuracy.append(calc_accuracy(X_test, theta, test_actual_class_enc, 'relu'))\n",
    "\n",
    "print(\"\\n------------------------------------------------------------------------------\")\n",
    "print(\"The stats for number of units in the hidden layer = {} with ReLU are as below:\".format(arch))\n",
    "print(\"------------------------------------------------------------------------------\")\n",
    "print(\"The number of epochs with ReLU is = {}\".format(epochs[-1]))\n",
    "print(\"The training time with ReLU is = {:2.3f}sec\".format(train_time[-1]))\n",
    "print(\"The training accuracy with ReLU is = {:2.3f}%\".format(train_accuracy[-1]))\n",
    "print(\"The validation accuracy with ReLU is = {:2.3f}%\".format(valid_accuracy[-1]))\n",
    "print(\"The test accuracy with ReLU is = {:2.3f}%\".format(test_accuracy[-1]))\n",
    "print(\"------------------------------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------Running Part D-----------------------------------------------------\n",
      "------------------Training a 100x100 hidden layer network with SoftPlus activation------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"----------------------------Running Part D-----------------------------------------------------\")\n",
    "print(\"------------------Training a 100x100 hidden layer network with SoftPlus activation------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the network with 2 hidden layer with [100, 100] units\n",
      "The parameters of the layers are of the shape:\n",
      "theta between layer 0 and layer 1 is (785, 100)\n",
      "theta between layer 1 and layer 2 is (101, 100)\n",
      "theta between layer 2 and layer 3 is (101, 26)\n",
      "Initial Cost on Val dataset for this epoch 1 = 3.28396125009747\n",
      "learning rate for this epoch =  0.6\n",
      "Error on this batch = 3.299332606831042\n",
      "Error on this batch = 0.48046714236879146\n",
      "Cost on val dataset after 2 epochs is = 0.48015313989335623\n",
      "Initial Cost on Val dataset for this epoch 2 = 0.48015313989335623\n",
      "learning rate for this epoch =  0.5045378491522288\n",
      "Error on this batch = 0.4803573418193883\n",
      "Error on this batch = 0.4800857965810846\n",
      "Cost on val dataset after 3 epochs is = 0.47935118216244477\n",
      "Initial Cost on Val dataset for this epoch 3 = 0.47935118216244477\n",
      "learning rate for this epoch =  0.4559014113909555\n",
      "Error on this batch = 0.4796120968500499\n",
      "Error on this batch = 0.47909629203916504\n",
      "Cost on val dataset after 4 epochs is = 0.47821151941857476\n",
      "Initial Cost on Val dataset for this epoch 4 = 0.47821151941857476\n",
      "learning rate for this epoch =  0.42426406871192845\n",
      "Error on this batch = 0.478520359167879\n",
      "Error on this batch = 0.4775196763852671\n",
      "Cost on val dataset after 5 epochs is = 0.4760110628484433\n",
      "Initial Cost on Val dataset for this epoch 5 = 0.4760110628484433\n",
      "learning rate for this epoch =  0.40124418298585324\n",
      "Error on this batch = 0.4765181756126063\n",
      "Error on this batch = 0.473764198193189\n",
      "Cost on val dataset after 6 epochs is = 0.46754000524374306\n",
      "Initial Cost on Val dataset for this epoch 6 = 0.46754000524374306\n",
      "learning rate for this epoch =  0.38336586254776345\n",
      "Error on this batch = 0.46902541706977646\n",
      "Error on this batch = 0.46164266486383837\n",
      "Cost on val dataset after 7 epochs is = 0.4534958587663454\n",
      "Initial Cost on Val dataset for this epoch 7 = 0.4534958587663454\n",
      "learning rate for this epoch =  0.3688728917707586\n",
      "Error on this batch = 0.4582424788171892\n",
      "Error on this batch = 0.4381250196906282\n",
      "Cost on val dataset after 8 epochs is = 0.4261898558138839\n",
      "Initial Cost on Val dataset for this epoch 8 = 0.4261898558138839\n",
      "learning rate for this epoch =  0.35676213450081634\n",
      "Error on this batch = 0.43358758116075297\n",
      "Error on this batch = 0.40237907259891237\n",
      "Cost on val dataset after 9 epochs is = 0.3955955031643565\n",
      "Initial Cost on Val dataset for this epoch 9 = 0.3955955031643565\n",
      "learning rate for this epoch =  0.34641016151377546\n",
      "Error on this batch = 0.4034762487318176\n",
      "Error on this batch = 0.3672735121821971\n",
      "Cost on val dataset after 10 epochs is = 0.35311670944367407\n",
      "Initial Cost on Val dataset for this epoch 10 = 0.35311670944367407\n",
      "learning rate for this epoch =  0.33740479511420945\n",
      "Error on this batch = 0.35454176351682587\n",
      "Error on this batch = 0.3207635252302927\n",
      "Cost on val dataset after 11 epochs is = 0.3051179587585634\n",
      "Initial Cost on Val dataset for this epoch 11 = 0.3051179587585634\n",
      "learning rate for this epoch =  0.32946029206566746\n",
      "Error on this batch = 0.29554976338616423\n",
      "Error on this batch = 0.27636904811706114\n",
      "Cost on val dataset after 12 epochs is = 0.2650173152074029\n",
      "Initial Cost on Val dataset for this epoch 12 = 0.2650173152074029\n",
      "learning rate for this epoch =  0.32237097954706256\n",
      "Error on this batch = 0.25024034023729663\n",
      "Error on this batch = 0.23511791752640157\n",
      "Cost on val dataset after 13 epochs is = 0.23586128398660822\n",
      "Initial Cost on Val dataset for this epoch 13 = 0.23586128398660822\n",
      "learning rate for this epoch =  0.315984232708756\n",
      "Error on this batch = 0.219852438836435\n",
      "Error on this batch = 0.20730296115532312\n",
      "Cost on val dataset after 14 epochs is = 0.21567184131247347\n",
      "Initial Cost on Val dataset for this epoch 14 = 0.21567184131247347\n",
      "learning rate for this epoch =  0.31018389237430233\n",
      "Error on this batch = 0.1992345184669125\n",
      "Error on this batch = 0.1830369702704054\n",
      "Cost on val dataset after 15 epochs is = 0.20077636971955284\n",
      "Initial Cost on Val dataset for this epoch 15 = 0.20077636971955284\n",
      "learning rate for this epoch =  0.30487964889276886\n",
      "Error on this batch = 0.18338523291138287\n",
      "Error on this batch = 0.1664074608848041\n",
      "Cost on val dataset after 16 epochs is = 0.1894009213349263\n",
      "Initial Cost on Val dataset for this epoch 16 = 0.1894009213349263\n",
      "learning rate for this epoch =  0.3\n",
      "Error on this batch = 0.17239072074710593\n",
      "Error on this batch = 0.156105952568238\n",
      "Cost on val dataset after 17 epochs is = 0.18033447551574106\n",
      "Initial Cost on Val dataset for this epoch 17 = 0.18033447551574106\n",
      "learning rate for this epoch =  0.2954874363032714\n",
      "Error on this batch = 0.16484740804950676\n",
      "Error on this batch = 0.1485595498242252\n",
      "Cost on val dataset after 18 epochs is = 0.1728865141084296\n",
      "Initial Cost on Val dataset for this epoch 18 = 0.1728865141084296\n",
      "learning rate for this epoch =  0.29129506302439406\n",
      "Error on this batch = 0.1587513321124054\n",
      "Error on this batch = 0.14278764467923866\n",
      "Cost on val dataset after 19 epochs is = 0.1667031579035976\n",
      "Initial Cost on Val dataset for this epoch 19 = 0.1667031579035976\n",
      "learning rate for this epoch =  0.2873841752661448\n",
      "Error on this batch = 0.15330513420151717\n",
      "Error on this batch = 0.1381938970080863\n",
      "Cost on val dataset after 20 epochs is = 0.16155041091726133\n",
      "Initial Cost on Val dataset for this epoch 20 = 0.16155041091726133\n",
      "learning rate for this epoch =  0.28372248270095274\n",
      "Error on this batch = 0.14848372722143371\n",
      "Error on this batch = 0.1342734202742093\n",
      "Cost on val dataset after 21 epochs is = 0.15722803506502828\n",
      "Initial Cost on Val dataset for this epoch 21 = 0.15722803506502828\n",
      "learning rate for this epoch =  0.28028278663692\n",
      "Error on this batch = 0.14434717978995035\n",
      "Error on this batch = 0.13069147377184467\n",
      "Cost on val dataset after 22 epochs is = 0.1535403888343946\n",
      "Initial Cost on Val dataset for this epoch 22 = 0.1535403888343946\n",
      "learning rate for this epoch =  0.27704197856646157\n",
      "Error on this batch = 0.14082668755447483\n",
      "Error on this batch = 0.12732858907935787\n",
      "Cost on val dataset after 23 epochs is = 0.15032526431958446\n",
      "Initial Cost on Val dataset for this epoch 23 = 0.15032526431958446\n",
      "learning rate for this epoch =  0.27398027129803876\n",
      "Error on this batch = 0.13778461756950908\n",
      "Error on this batch = 0.12415720954311557\n",
      "Cost on val dataset after 24 epochs is = 0.14747048188978595\n",
      "Initial Cost on Val dataset for this epoch 24 = 0.14747048188978595\n",
      "learning rate for this epoch =  0.27108060108295345\n",
      "Error on this batch = 0.13509405287514667\n",
      "Error on this batch = 0.12115855703461925\n",
      "Cost on val dataset after 25 epochs is = 0.14490345707217203\n",
      "Initial Cost on Val dataset for this epoch 25 = 0.14490345707217203\n",
      "learning rate for this epoch =  0.2683281572999747\n",
      "Error on this batch = 0.1326724901232593\n",
      "Error on this batch = 0.1183211778670401\n",
      "Cost on val dataset after 26 epochs is = 0.14257499860936998\n",
      "Initial Cost on Val dataset for this epoch 26 = 0.14257499860936998\n",
      "learning rate for this epoch =  0.2657100085614884\n",
      "Error on this batch = 0.1304718447297448\n",
      "Error on this batch = 0.11564185724436372\n",
      "Cost on val dataset after 27 epochs is = 0.14044957686272028\n",
      "Initial Cost on Val dataset for this epoch 27 = 0.14044957686272028\n",
      "learning rate for this epoch =  0.2632148025904985\n",
      "Error on this batch = 0.12845747693286266\n",
      "Error on this batch = 0.11311699014199608\n",
      "Cost on val dataset after 28 epochs is = 0.13850079329358742\n",
      "Initial Cost on Val dataset for this epoch 28 = 0.13850079329358742\n",
      "learning rate for this epoch =  0.26083252316699485\n",
      "Error on this batch = 0.12659706165179976\n",
      "Error on this batch = 0.1107383686874994\n",
      "Cost on val dataset after 29 epochs is = 0.13670834904812537\n",
      "Initial Cost on Val dataset for this epoch 29 = 0.13670834904812537\n",
      "learning rate for this epoch =  0.2585542916753436\n",
      "Error on this batch = 0.12485863713531099\n",
      "Error on this batch = 0.10849386131582034\n",
      "Cost on val dataset after 30 epochs is = 0.1350554101546688\n",
      "Initial Cost on Val dataset for this epoch 30 = 0.1350554101546688\n",
      "learning rate for this epoch =  0.2563722038377404\n",
      "Error on this batch = 0.12321278882976203\n",
      "Error on this batch = 0.10636812573901498\n",
      "Cost on val dataset after 31 epochs is = 0.13352676340016714\n",
      "Initial Cost on Val dataset for this epoch 31 = 0.13352676340016714\n",
      "learning rate for this epoch =  0.254279194449013\n",
      "Error on this batch = 0.12163446748404855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on this batch = 0.10434430334406755\n",
      "Cost on val dataset after 32 epochs is = 0.1321078508185103\n",
      "Initial Cost on Val dataset for this epoch 32 = 0.1321078508185103\n",
      "learning rate for this epoch =  0.2522689245761144\n",
      "Error on this batch = 0.12010243912140457\n",
      "Error on this batch = 0.10240658915659272\n",
      "Cost on val dataset after 33 epochs is = 0.13078434640530417\n",
      "Initial Cost on Val dataset for this epoch 33 = 0.13078434640530417\n",
      "learning rate for this epoch =  0.2503356869166904\n",
      "Error on this batch = 0.1185979305801395\n",
      "Error on this batch = 0.10054073869378925\n",
      "Cost on val dataset after 34 epochs is = 0.12954234627349512\n",
      "Initial Cost on Val dataset for this epoch 34 = 0.12954234627349512\n",
      "learning rate for this epoch =  0.2484743259399312\n",
      "Error on this batch = 0.11710430282875446\n",
      "Error on this batch = 0.09873357957564027\n",
      "Cost on val dataset after 35 epochs is = 0.12836908324647928\n",
      "Initial Cost on Val dataset for this epoch 35 = 0.12836908324647928\n",
      "learning rate for this epoch =  0.2466801701403118\n",
      "Error on this batch = 0.11560823868777818\n",
      "Error on this batch = 0.09697378875548505\n",
      "Cost on val dataset after 36 epochs is = 0.1272536865663435\n",
      "Initial Cost on Val dataset for this epoch 36 = 0.1272536865663435\n",
      "learning rate for this epoch =  0.24494897427831783\n",
      "Error on this batch = 0.11410251060372348\n",
      "Error on this batch = 0.09525340076602415\n",
      "Cost on val dataset after 37 epochs is = 0.12618769861709725\n",
      "Initial Cost on Val dataset for this epoch 37 = 0.12618769861709725\n",
      "learning rate for this epoch =  0.2432768699032619\n",
      "Error on this batch = 0.11258959787427636\n",
      "Error on this batch = 0.0935685205482417\n",
      "Cost on val dataset after 38 epochs is = 0.12516520962898317\n",
      "Initial Cost on Val dataset for this epoch 38 = 0.12516520962898317\n",
      "learning rate for this epoch =  0.24166032278194638\n",
      "Error on this batch = 0.11108427107099918\n",
      "Error on this batch = 0.09191849972660754\n",
      "Cost on val dataset after 39 epochs is = 0.1241825225712256\n",
      "Initial Cost on Val dataset for this epoch 39 = 0.1241825225712256\n",
      "learning rate for this epoch =  0.24009609611534996\n",
      "Error on this batch = 0.1096127522560502\n",
      "Error on this batch = 0.09030408979676921\n",
      "Cost on val dataset after 40 epochs is = 0.12323747228879682\n",
      "Initial Cost on Val dataset for this epoch 40 = 0.12323747228879682\n",
      "learning rate for this epoch =  0.23858121863011517\n",
      "Error on this batch = 0.1082068269112596\n",
      "Error on this batch = 0.08872572450681719\n",
      "Cost on val dataset after 41 epochs is = 0.12232870226661874\n",
      "Initial Cost on Val dataset for this epoch 41 = 0.12232870226661874\n",
      "learning rate for this epoch =  0.23711295679464287\n",
      "Error on this batch = 0.10689410635435664\n",
      "Error on this batch = 0.08718268137259218\n",
      "Cost on val dataset after 42 epochs is = 0.12145516605278639\n",
      "Initial Cost on Val dataset for this epoch 42 = 0.12145516605278639\n",
      "learning rate for this epoch =  0.2356887905403078\n",
      "Error on this batch = 0.10568969767713485\n",
      "Error on this batch = 0.08567316562871127\n",
      "Cost on val dataset after 43 epochs is = 0.1206160014640861\n",
      "Initial Cost on Val dataset for this epoch 43 = 0.1206160014640861\n",
      "learning rate for this epoch =  0.23430639197370967\n",
      "Error on this batch = 0.10459465099567103\n",
      "Error on this batch = 0.08419542412190686\n",
      "Cost on val dataset after 44 epochs is = 0.11981090874938179\n",
      "Initial Cost on Val dataset for this epoch 44 = 0.11981090874938179\n",
      "learning rate for this epoch =  0.23296360665133395\n",
      "Error on this batch = 0.10360066020773992\n",
      "Error on this batch = 0.08275102077194493\n",
      "Cost on val dataset after 45 epochs is = 0.11904119942412938\n",
      "Initial Cost on Val dataset for this epoch 45 = 0.11904119942412938\n",
      "learning rate for this epoch =  0.23165843705765382\n",
      "Error on this batch = 0.1026957661601399\n",
      "Error on this batch = 0.08134957839675779\n",
      "Cost on val dataset after 46 epochs is = 0.11831000066121722\n",
      "Initial Cost on Val dataset for this epoch 46 = 0.11831000066121722\n",
      "learning rate for this epoch =  0.23038902798476096\n",
      "Error on this batch = 0.10186318857611397\n",
      "Error on this batch = 0.07999839693461802\n",
      "Cost on val dataset after 47 epochs is = 0.11761016430704509\n",
      "Initial Cost on Val dataset for this epoch 47 = 0.11761016430704509\n",
      "learning rate for this epoch =  0.229153653558572\n",
      "Error on this batch = 0.10106417960305361\n",
      "Error on this batch = 0.07870424538385644\n",
      "Cost on val dataset after 48 epochs is = 0.1169200169326275\n",
      "Initial Cost on Val dataset for this epoch 48 = 0.1169200169326275\n",
      "learning rate for this epoch =  0.22795070569547776\n",
      "Error on this batch = 0.10027008032062819\n",
      "Error on this batch = 0.07747470260709406\n",
      "Cost on val dataset after 49 epochs is = 0.11624377575904511\n",
      "Initial Cost on Val dataset for this epoch 49 = 0.11624377575904511\n",
      "learning rate for this epoch =  0.2267786838055363\n",
      "Error on this batch = 0.09949357895359012\n",
      "Error on this batch = 0.07629769100633686\n",
      "Cost on val dataset after 50 epochs is = 0.11558893756838584\n",
      "Initial Cost on Val dataset for this epoch 50 = 0.11558893756838584\n",
      "learning rate for this epoch =  0.2256361855851836\n",
      "Error on this batch = 0.09873866245312568\n",
      "Error on this batch = 0.07516303196210916\n",
      "Cost on val dataset after 51 epochs is = 0.11495674315918232\n",
      "Initial Cost on Val dataset for this epoch 51 = 0.11495674315918232\n",
      "learning rate for this epoch =  0.22452189876492748\n",
      "Error on this batch = 0.09800170672101154\n",
      "Error on this batch = 0.07406381673870248\n",
      "Cost on val dataset after 52 epochs is = 0.11434692623101808\n",
      "Initial Cost on Val dataset for this epoch 52 = 0.11434692623101808\n",
      "learning rate for this epoch =  0.22343459369638943\n",
      "Error on this batch = 0.09727820635913852\n",
      "Error on this batch = 0.07299493750836522\n",
      "Cost on val dataset after 53 epochs is = 0.11375889497817063\n",
      "Initial Cost on Val dataset for this epoch 53 = 0.11375889497817063\n",
      "learning rate for this epoch =  0.2223731166789908\n",
      "Error on this batch = 0.09656432661151775\n",
      "Error on this batch = 0.07195233405067906\n",
      "Cost on val dataset after 54 epochs is = 0.11319192836315346\n",
      "Initial Cost on Val dataset for this epoch 54 = 0.11319192836315346\n",
      "learning rate for this epoch =  0.22133638394006433\n",
      "Error on this batch = 0.09585713301846753\n",
      "Error on this batch = 0.07093268920148296\n",
      "Cost on val dataset after 55 epochs is = 0.11264519500438945\n",
      "Initial Cost on Val dataset for this epoch 55 = 0.11264519500438945\n",
      "learning rate for this epoch =  0.2203233761936155\n",
      "Error on this batch = 0.09515454448730097\n",
      "Error on this batch = 0.06993348158264343\n",
      "Cost on val dataset after 56 epochs is = 0.11211773991901341\n",
      "Initial Cost on Val dataset for this epoch 56 = 0.11211773991901341\n",
      "learning rate for this epoch =  0.21933313371270743\n",
      "Error on this batch = 0.09445521124613543\n",
      "Error on this batch = 0.06895310955221286\n",
      "Cost on val dataset after 57 epochs is = 0.11160852240924206\n",
      "Initial Cost on Val dataset for this epoch 57 = 0.11160852240924206\n",
      "learning rate for this epoch =  0.21836475185876858\n",
      "Error on this batch = 0.09375836397824577\n",
      "Error on this batch = 0.06799076072471343\n",
      "Cost on val dataset after 58 epochs is = 0.1111165063824915\n",
      "Initial Cost on Val dataset for this epoch 58 = 0.1111165063824915\n",
      "learning rate for this epoch =  0.21741737701825978\n",
      "Error on this batch = 0.09306365931759224\n",
      "Error on this batch = 0.06704606739848978\n",
      "Cost on val dataset after 59 epochs is = 0.11064073530525036\n",
      "Initial Cost on Val dataset for this epoch 59 = 0.11064073530525036\n",
      "learning rate for this epoch =  0.2164902029032644\n",
      "Error on this batch = 0.09237104246309376\n",
      "Error on this batch = 0.0661188286109663\n",
      "Cost on val dataset after 60 epochs is = 0.11018036116103726\n",
      "Initial Cost on Val dataset for this epoch 60 = 0.11018036116103726\n",
      "learning rate for this epoch =  0.21558246717785054\n",
      "Error on this batch = 0.09168064974910628\n",
      "Error on this batch = 0.06520892788740765\n",
      "Cost on val dataset after 61 epochs is = 0.10973465843137317\n",
      "Initial Cost on Val dataset for this epoch 61 = 0.10973465843137317\n",
      "learning rate for this epoch =  0.2146934483766157\n",
      "Error on this batch = 0.09099276696496236\n",
      "Error on this batch = 0.06431639028569176\n",
      "Cost on val dataset after 62 epochs is = 0.10930305387971036\n",
      "Initial Cost on Val dataset for this epoch 62 = 0.10930305387971036\n",
      "learning rate for this epoch =  0.21382246308577726\n",
      "Error on this batch = 0.0903078352726564\n",
      "Error on this batch = 0.06344148235073253\n",
      "Cost on val dataset after 63 epochs is = 0.10888516714969734\n",
      "Initial Cost on Val dataset for this epoch 63 = 0.10888516714969734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate for this epoch =  0.21296886336060317\n",
      "Error on this batch = 0.08962646828125856\n",
      "Error on this batch = 0.06258478346947928\n",
      "Cost on val dataset after 64 epochs is = 0.10848083478438392\n",
      "Initial Cost on Val dataset for this epoch 64 = 0.10848083478438392\n",
      "learning rate for this epoch =  0.21213203435596423\n",
      "Error on this batch = 0.08894943361236536\n",
      "Error on this batch = 0.06174718748375312\n",
      "Cost on val dataset after 65 epochs is = 0.10809009175983073\n",
      "Initial Cost on Val dataset for this epoch 65 = 0.10809009175983073\n",
      "learning rate for this epoch =  0.21131139214939415\n",
      "Error on this batch = 0.08827756772508064\n",
      "Error on this batch = 0.06092981812505838\n",
      "Cost on val dataset after 66 epochs is = 0.10771309556793335\n",
      "Initial Cost on Val dataset for this epoch 66 = 0.10771309556793335\n",
      "learning rate for this epoch =  0.21050638173832112\n",
      "Error on this batch = 0.0876116225170163\n",
      "Error on this batch = 0.060133858310809564\n",
      "Cost on val dataset after 67 epochs is = 0.10734998743594827\n",
      "Initial Cost on Val dataset for this epoch 67 = 0.10734998743594827\n",
      "learning rate for this epoch =  0.20971647519513073\n",
      "Error on this batch = 0.08695207555593022\n",
      "Error on this batch = 0.05936030058873287\n",
      "Cost on val dataset after 68 epochs is = 0.10700070133867737\n",
      "Initial Cost on Val dataset for this epoch 68 = 0.10700070133867737\n",
      "learning rate for this epoch =  0.2089411699654712\n",
      "Error on this batch = 0.08629896825514621\n",
      "Error on this batch = 0.05860963196636419\n",
      "Cost on val dataset after 69 epochs is = 0.10666477000793316\n",
      "Initial Cost on Val dataset for this epoch 69 = 0.10666477000793316\n",
      "learning rate for this epoch =  0.2081799872967546\n",
      "Error on this batch = 0.08565184134574672\n",
      "Error on this batch = 0.057881495069528825\n",
      "Cost on val dataset after 70 epochs is = 0.10634122679056954\n",
      "Initial Cost on Val dataset for this epoch 70 = 0.10634122679056954\n",
      "learning rate for this epoch =  0.2074324707851646\n",
      "Error on this batch = 0.08500976591222116\n",
      "Error on this batch = 0.05717443943541287\n",
      "Cost on val dataset after 71 epochs is = 0.10602869847857371\n",
      "Initial Cost on Val dataset for this epoch 71 = 0.10602869847857371\n",
      "learning rate for this epoch =  0.2066981850306836\n",
      "Error on this batch = 0.084371370444062\n",
      "Error on this batch = 0.05648593360692518\n",
      "Cost on val dataset after 72 epochs is = 0.10572566492564144\n",
      "Initial Cost on Val dataset for this epoch 72 = 0.10572566492564144\n",
      "learning rate for this epoch =  0.20597671439071177\n",
      "Error on this batch = 0.08373480363931114\n",
      "Error on this batch = 0.055812702952853746\n",
      "Cost on val dataset after 73 epochs is = 0.10543071502863097\n",
      "Initial Cost on Val dataset for this epoch 73 = 0.10543071502863097\n",
      "learning rate for this epoch =  0.20526766182379289\n",
      "Error on this batch = 0.08309768840475477\n",
      "Error on this batch = 0.05515121926872664\n",
      "Cost on val dataset after 74 epochs is = 0.10514266316086067\n",
      "Initial Cost on Val dataset for this epoch 74 = 0.10514266316086067\n",
      "learning rate for this epoch =  0.20457064781579723\n",
      "Error on this batch = 0.08245709525533551\n",
      "Error on this batch = 0.05449814165256397\n",
      "Cost on val dataset after 75 epochs is = 0.10486057312354746\n",
      "Initial Cost on Val dataset for this epoch 75 = 0.10486057312354746\n",
      "learning rate for this epoch =  0.2038853093816547\n",
      "Error on this batch = 0.08180952854096084\n",
      "Error on this batch = 0.05385082679696217\n",
      "Cost on val dataset after 76 epochs is = 0.10458381724044695\n",
      "Initial Cost on Val dataset for this epoch 76 = 0.10458381724044695\n",
      "learning rate for this epoch =  0.20321129913639427\n",
      "Error on this batch = 0.08115106406046806\n",
      "Error on this batch = 0.05320826981207308\n",
      "Cost on val dataset after 77 epochs is = 0.1043122215462824\n",
      "Initial Cost on Val dataset for this epoch 77 = 0.1043122215462824\n",
      "learning rate for this epoch =  0.2025482844298358\n",
      "Error on this batch = 0.08047797213365279\n",
      "Error on this batch = 0.052572352696975216\n",
      "Cost on val dataset after 78 epochs is = 0.10404619865218709\n",
      "Initial Cost on Val dataset for this epoch 78 = 0.10404619865218709\n",
      "learning rate for this epoch =  0.20189594653980908\n",
      "Error on this batch = 0.07978807775262196\n",
      "Error on this batch = 0.05194789442421922\n",
      "Cost on val dataset after 79 epochs is = 0.10378663849971952\n",
      "Initial Cost on Val dataset for this epoch 79 = 0.10378663849971952\n",
      "learning rate for this epoch =  0.20125397991924746\n",
      "Error on this batch = 0.0790823035219342\n",
      "Error on this batch = 0.05134011090823622\n",
      "Cost on val dataset after 80 epochs is = 0.10353459944199453\n",
      "Initial Cost on Val dataset for this epoch 80 = 0.10353459944199453\n",
      "learning rate for this epoch =  0.20062209149292662\n",
      "Error on this batch = 0.07836553334978057\n",
      "Error on this batch = 0.050752459886375105\n",
      "Cost on val dataset after 81 epochs is = 0.10329135130277478\n",
      "Initial Cost on Val dataset for this epoch 81 = 0.10329135130277478\n",
      "learning rate for this epoch =  0.19999999999999998\n",
      "Error on this batch = 0.07764816786662553\n",
      "Error on this batch = 0.050188801925269706\n",
      "Cost on val dataset after 82 epochs is = 0.10305817872469797\n",
      "Initial Cost on Val dataset for this epoch 82 = 0.10305817872469797\n",
      "learning rate for this epoch =  0.19938743537882408\n",
      "Error on this batch = 0.07694628686804109\n",
      "Error on this batch = 0.0496555415969108\n",
      "Cost on val dataset after 83 epochs is = 0.10283466445426198\n",
      "Initial Cost on Val dataset for this epoch 83 = 0.10283466445426198\n",
      "learning rate for this epoch =  0.1987841381908741\n",
      "Error on this batch = 0.07627090845457364\n",
      "Error on this batch = 0.04915774087245278\n",
      "Cost on val dataset after 84 epochs is = 0.10261799226273445\n",
      "Initial Cost on Val dataset for this epoch 84 = 0.10261799226273445\n",
      "learning rate for this epoch =  0.19818985908082842\n",
      "Error on this batch = 0.07561621424439731\n",
      "Error on this batch = 0.048694864215163815\n",
      "Cost on val dataset after 85 epochs is = 0.10240527950958835\n",
      "Initial Cost on Val dataset for this epoch 85 = 0.10240527950958835\n",
      "learning rate for this epoch =  0.1976043582701508\n",
      "Error on this batch = 0.07496643086390992\n",
      "Error on this batch = 0.04826280340428543\n",
      "Cost on val dataset after 86 epochs is = 0.10219533578902115\n",
      "Initial Cost on Val dataset for this epoch 86 = 0.10219533578902115\n",
      "learning rate for this epoch =  0.19702740508172414\n",
      "Error on this batch = 0.0743078604622047\n",
      "Error on this batch = 0.047857599597175976\n",
      "Cost on val dataset after 87 epochs is = 0.10198839485446526\n",
      "Initial Cost on Val dataset for this epoch 87 = 0.10198839485446526\n",
      "learning rate for this epoch =  0.19645877749329657\n",
      "Error on this batch = 0.0736325818086403\n",
      "Error on this batch = 0.04747643204803882\n",
      "Cost on val dataset after 88 epochs is = 0.10178530105574536\n",
      "Initial Cost on Val dataset for this epoch 88 = 0.10178530105574536\n",
      "learning rate for this epoch =  0.19589826171768313\n",
      "Error on this batch = 0.07293738277246115\n",
      "Error on this batch = 0.04711714855856462\n",
      "Cost on val dataset after 89 epochs is = 0.10158696006710369\n",
      "Initial Cost on Val dataset for this epoch 89 = 0.10158696006710369\n",
      "learning rate for this epoch =  0.1953456518078377\n",
      "Error on this batch = 0.0722219012163967\n",
      "Error on this batch = 0.04677799445944439\n",
      "Cost on val dataset after 90 epochs is = 0.10139413484840448\n",
      "Initial Cost on Val dataset for this epoch 90 = 0.10139413484840448\n",
      "learning rate for this epoch =  0.19480074928505933\n",
      "Error on this batch = 0.07148734670953422\n",
      "Error on this batch = 0.04645759896639273\n",
      "Cost on val dataset after 91 epochs is = 0.10120743732342474\n",
      "Initial Cost on Val dataset for this epoch 91 = 0.10120743732342474\n",
      "learning rate for this epoch =  0.19426336278873857\n",
      "Error on this batch = 0.07073589329782193\n",
      "Error on this batch = 0.04615497517500036\n",
      "Cost on val dataset after 92 epochs is = 0.1010273907602785\n",
      "Initial Cost on Val dataset for this epoch 92 = 0.1010273907602785\n",
      "learning rate for this epoch =  0.1937333077461732\n",
      "Error on this batch = 0.06997045966481645\n",
      "Error on this batch = 0.045869462823942844\n",
      "Cost on val dataset after 93 epochs is = 0.10085450212899734\n",
      "Initial Cost on Val dataset for this epoch 93 = 0.10085450212899734\n",
      "learning rate for this epoch =  0.19321040606110043\n",
      "Error on this batch = 0.06919463424140064\n",
      "Error on this batch = 0.04560060849663653\n",
      "Cost on val dataset after 94 epochs is = 0.10068931012582781\n",
      "Initial Cost on Val dataset for this epoch 94 = 0.10068931012582781\n",
      "learning rate for this epoch =  0.1926944858196948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on this batch = 0.06841258971054139\n",
      "Error on this batch = 0.04534799120614217\n",
      "Cost on val dataset after 95 epochs is = 0.1005323838088031\n",
      "Initial Cost on Val dataset for this epoch 95 = 0.1005323838088031\n",
      "learning rate for this epoch =  0.1921853810128792\n",
      "Error on this batch = 0.06762888709166365\n",
      "Error on this batch = 0.04511102424869412\n",
      "Cost on val dataset after 96 epochs is = 0.1003842609329091\n",
      "Initial Cost on Val dataset for this epoch 96 = 0.1003842609329091\n",
      "learning rate for this epoch =  0.19168293127388172\n",
      "Error on this batch = 0.06684812082045732\n",
      "Error on this batch = 0.04488879404139251\n",
      "Cost on val dataset after 97 epochs is = 0.10024533817393907\n",
      "Initial Cost on Val dataset for this epoch 97 = 0.10024533817393907\n",
      "learning rate for this epoch =  0.19118698163005315\n",
      "Error on this batch = 0.06607443319167944\n",
      "Error on this batch = 0.04468000316295303\n",
      "Cost on val dataset after 98 epochs is = 0.10011574687832497\n",
      "Initial Cost on Val dataset for this epoch 98 = 0.10011574687832497\n",
      "learning rate for this epoch =  0.19069738226803112\n",
      "Error on this batch = 0.065311019855547\n",
      "Error on this batch = 0.04448304549102534\n",
      "Cost on val dataset after 99 epochs is = 0.09999525269747747\n",
      "Initial Cost on Val dataset for this epoch 99 = 0.09999525269747747\n",
      "learning rate for this epoch =  0.19021398831140582\n",
      "Error on this batch = 0.06455979802336934\n",
      "Error on this batch = 0.04429617652830682\n",
      "Cost on val dataset after 100 epochs is = 0.0998832014482479\n",
      "Initial Cost on Val dataset for this epoch 100 = 0.0998832014482479\n",
      "learning rate for this epoch =  0.18973665961010275\n",
      "Error on this batch = 0.06382135534358013\n",
      "Error on this batch = 0.044117703949690446\n",
      "Cost on val dataset after 101 epochs is = 0.09977851170663007\n",
      "Initial Cost on Val dataset for this epoch 101 = 0.09977851170663007\n",
      "learning rate for this epoch =  0.1892652605407543\n",
      "Error on this batch = 0.0630951579986507\n",
      "Error on this batch = 0.043946137398085394\n",
      "Cost on val dataset after 102 epochs is = 0.09967970793760668\n",
      "Initial Cost on Val dataset for this epoch 102 = 0.09967970793760668\n",
      "learning rate for this epoch =  0.18879965981738495\n",
      "Error on this batch = 0.06237987344003955\n",
      "Error on this batch = 0.043780277131841495\n",
      "Cost on val dataset after 103 epochs is = 0.09958500003615506\n",
      "Initial Cost on Val dataset for this epoch 103 = 0.09958500003615506\n",
      "learning rate for this epoch =  0.1883397303117814\n",
      "Error on this batch = 0.06167364775869968\n",
      "Error on this batch = 0.04361925189333312\n",
      "Cost on val dataset after 104 epochs is = 0.09949242688210715\n",
      "Initial Cost on Val dataset for this epoch 104 = 0.09949242688210715\n",
      "learning rate for this epoch =  0.18788534888296407\n",
      "Error on this batch = 0.06097426757240628\n",
      "Error on this batch = 0.04346252460963847\n",
      "Cost on val dataset after 105 epochs is = 0.0994000719002277\n",
      "Initial Cost on Val dataset for this epoch 105 = 0.0994000719002277\n",
      "learning rate for this epoch =  0.18743639621521535\n",
      "Error on this batch = 0.06027924990641669\n",
      "Error on this batch = 0.043309876731466995\n",
      "Cost on val dataset after 106 epochs is = 0.09930632806275012\n",
      "Initial Cost on Val dataset for this epoch 106 = 0.09930632806275012\n",
      "learning rate for this epoch =  0.18699275666415935\n",
      "Error on this batch = 0.05958594985512566\n",
      "Error on this batch = 0.043161370285235715\n",
      "Cost on val dataset after 107 epochs is = 0.09921015991893267\n",
      "Initial Cost on Val dataset for this epoch 107 = 0.09921015991893267\n",
      "learning rate for this epoch =  0.18655431811042028\n",
      "Error on this batch = 0.0588917193188753\n",
      "Error on this batch = 0.04301728183735978\n",
      "Cost on val dataset after 108 epochs is = 0.09911130184376199\n",
      "Initial Cost on Val dataset for this epoch 108 = 0.09911130184376199\n",
      "learning rate for this epoch =  0.18612097182041992\n",
      "Error on this batch = 0.05819405184301662\n",
      "Error on this batch = 0.042878005881372305\n",
      "Cost on val dataset after 109 epochs is = 0.09901034646467785\n",
      "Initial Cost on Val dataset for this epoch 109 = 0.09901034646467785\n",
      "learning rate for this epoch =  0.1856926123139029\n",
      "Error on this batch = 0.05749063037513514\n",
      "Error on this batch = 0.04274392811699952\n",
      "Cost on val dataset after 110 epochs is = 0.09890870249649825\n",
      "Initial Cost on Val dataset for this epoch 110 = 0.09890870249649825\n",
      "learning rate for this epoch =  0.18526913723780689\n",
      "Error on this batch = 0.056779305951212805\n",
      "Error on this batch = 0.04261527286374195\n",
      "Cost on val dataset after 111 epochs is = 0.09880842205833253\n",
      "Initial Cost on Val dataset for this epoch 111 = 0.09880842205833253\n",
      "learning rate for this epoch =  0.1848504472461183\n",
      "Error on this batch = 0.05605817115047101\n",
      "Error on this batch = 0.04249194815130825\n",
      "Cost on val dataset after 112 epochs is = 0.09871190017126423\n",
      "Initial Cost on Val dataset for this epoch 112 = 0.09871190017126423\n",
      "learning rate for this epoch =  0.1844364458853793\n",
      "Error on this batch = 0.05532588696227432\n",
      "Error on this batch = 0.042373446065359636\n",
      "Cost on val dataset after 113 epochs is = 0.09862144217391687\n",
      "Initial Cost on Val dataset for this epoch 113 = 0.09862144217391687\n",
      "learning rate for this epoch =  0.18402703948553187\n",
      "Error on this batch = 0.05458220811657183\n",
      "Error on this batch = 0.04225887552890477\n",
      "Cost on val dataset after 114 epochs is = 0.09853873425577346\n",
      "Initial Cost on Val dataset for this epoch 114 = 0.09853873425577346\n",
      "learning rate for this epoch =  0.18362213705580538\n",
      "Error on this batch = 0.053828383314096034\n",
      "Error on this batch = 0.04214716524431534\n",
      "Cost on val dataset after 115 epochs is = 0.09846437620042993\n",
      "Initial Cost on Val dataset for this epoch 115 = 0.09846437620042993\n",
      "learning rate for this epoch =  0.18322165018537329\n",
      "Error on this batch = 0.05306716032167025\n",
      "Error on this batch = 0.042037360081695035\n",
      "Cost on val dataset after 116 epochs is = 0.09839773856388365\n",
      "Initial Cost on Val dataset for this epoch 116 = 0.09839773856388365\n",
      "learning rate for this epoch =  0.18282549294852\n",
      "Error on this batch = 0.05230265698860176\n",
      "Error on this batch = 0.04192884290541126\n",
      "Cost on val dataset after 117 epochs is = 0.098337312146008\n",
      "Initial Cost on Val dataset for this epoch 117 = 0.098337312146008\n",
      "learning rate for this epoch =  0.1824335818140776\n",
      "Error on this batch = 0.05154065027496516\n",
      "Error on this batch = 0.04182138227837513\n",
      "Cost on val dataset after 118 epochs is = 0.09828144772084496\n",
      "Initial Cost on Val dataset for this epoch 118 = 0.09828144772084496\n",
      "learning rate for this epoch =  0.18204583555890436\n",
      "Error on this batch = 0.050789091372571614\n",
      "Error on this batch = 0.04171505127898136\n",
      "Cost on val dataset after 119 epochs is = 0.09822912734557908\n",
      "Initial Cost on Val dataset for this epoch 119 = 0.09822912734557908\n",
      "learning rate for this epoch =  0.1816621751851926\n",
      "Error on this batch = 0.05005779381948292\n",
      "Error on this batch = 0.041610095263212354\n",
      "Cost on val dataset after 120 epochs is = 0.09818031160966187\n",
      "Initial Cost on Val dataset for this epoch 120 = 0.09818031160966187\n",
      "learning rate for this epoch =  0.18128252384140608\n",
      "Error on this batch = 0.04935670103035895\n",
      "Error on this batch = 0.04150678300300298\n",
      "Cost on val dataset after 121 epochs is = 0.09813562342423598\n",
      "Initial Cost on Val dataset for this epoch 121 = 0.09813562342423598\n",
      "learning rate for this epoch =  0.18090680674665816\n",
      "Error on this batch = 0.04869334789738117\n",
      "Error on this batch = 0.04140527670631671\n",
      "Cost on val dataset after 122 epochs is = 0.09809557966889143\n",
      "Initial Cost on Val dataset for this epoch 122 = 0.09809557966889143\n",
      "learning rate for this epoch =  0.18053495111835455\n",
      "Error on this batch = 0.04807066200679781\n",
      "Error on this batch = 0.04130557103997525\n",
      "Cost on val dataset after 123 epochs is = 0.09805979346945294\n",
      "Initial Cost on Val dataset for this epoch 123 = 0.09805979346945294\n",
      "learning rate for this epoch =  0.1801668861029339\n",
      "Error on this batch = 0.047485880159343255\n",
      "Error on this batch = 0.04120750747630554\n",
      "Cost on val dataset after 124 epochs is = 0.0980263376136913\n",
      "Initial Cost on Val dataset for this epoch 124 = 0.0980263376136913\n",
      "learning rate for this epoch =  0.17980254270954982\n",
      "Error on this batch = 0.04693068275408907\n",
      "Error on this batch = 0.04111080717470411\n",
      "Cost on val dataset after 125 epochs is = 0.09799117509874136\n",
      "Initial Cost on Val dataset for this epoch 125 = 0.09799117509874136\n",
      "learning rate for this epoch =  0.17944185374654648\n",
      "Error on this batch = 0.04639216775366603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on this batch = 0.04101506116395503\n",
      "Cost on val dataset after 126 epochs is = 0.09794768679686619\n",
      "Initial Cost on Val dataset for this epoch 126 = 0.09794768679686619\n",
      "learning rate for this epoch =  0.17908475376058935\n",
      "Error on this batch = 0.04585434481343254\n",
      "Error on this batch = 0.04091967329995286\n",
      "Cost on val dataset after 127 epochs is = 0.09788700994686296\n",
      "Initial Cost on Val dataset for this epoch 127 = 0.09788700994686296\n",
      "learning rate for this epoch =  0.17873117897831953\n",
      "Error on this batch = 0.045300577141981276\n",
      "Error on this batch = 0.0408238173706268\n",
      "Cost on val dataset after 128 epochs is = 0.09780061280686961\n",
      "Initial Cost on Val dataset for this epoch 128 = 0.09780061280686961\n",
      "learning rate for this epoch =  0.17838106725040817\n",
      "Error on this batch = 0.04471807822468664\n",
      "Error on this batch = 0.040726499020713076\n",
      "Cost on val dataset after 129 epochs is = 0.0976853539037743\n",
      "Initial Cost on Val dataset for this epoch 129 = 0.0976853539037743\n",
      "learning rate for this epoch =  0.1780343579978945\n",
      "Error on this batch = 0.04410394987325465\n",
      "Error on this batch = 0.040626782343408115\n",
      "Cost on val dataset after 130 epochs is = 0.09754716765324134\n",
      "Initial Cost on Val dataset for this epoch 130 = 0.09754716765324134\n",
      "learning rate for this epoch =  0.17769099216069748\n",
      "Error on this batch = 0.043467865274114635\n",
      "Error on this batch = 0.04052414441114218\n",
      "Cost on val dataset after 131 epochs is = 0.09739829944042148\n",
      "Initial Cost on Val dataset for this epoch 131 = 0.09739829944042148\n",
      "learning rate for this epoch =  0.17735091214819665\n",
      "Error on this batch = 0.04282677274809183\n",
      "Error on this batch = 0.04041880297300818\n",
      "Cost on val dataset after 132 epochs is = 0.09725014856770743\n",
      "Initial Cost on Val dataset for this epoch 132 = 0.09725014856770743\n",
      "learning rate for this epoch =  0.17701406179178425\n",
      "Error on this batch = 0.04219613823257543\n",
      "Error on this batch = 0.040311830238440205\n",
      "Cost on val dataset after 133 epochs is = 0.0971085718127101\n",
      "Initial Cost on Val dataset for this epoch 133 = 0.0971085718127101\n",
      "learning rate for this epoch =  0.1766803862992956\n",
      "Error on this batch = 0.041585627649499975\n",
      "Error on this batch = 0.04020492364461757\n",
      "Cost on val dataset after 134 epochs is = 0.09697331221211437\n",
      "Initial Cost on Val dataset for this epoch 134 = 0.09697331221211437\n",
      "learning rate for this epoch =  0.17634983221122996\n",
      "Error on this batch = 0.040999469846572455\n",
      "Error on this batch = 0.040099644941977285\n",
      "Cost on val dataset after 135 epochs is = 0.09683963288963686\n",
      "Initial Cost on Val dataset for this epoch 135 = 0.09683963288963686\n",
      "learning rate for this epoch =  0.17602234735867867\n",
      "Error on this batch = 0.040438992617235325\n",
      "Error on this batch = 0.03999596473659915\n",
      "Cost on val dataset after 136 epochs is = 0.09670240266646729\n",
      "Initial Cost on Val dataset for this epoch 136 = 0.09670240266646729\n",
      "learning rate for this epoch =  0.17569788082288185\n",
      "Error on this batch = 0.039907229677106415\n",
      "Error on this batch = 0.039891297563758324\n",
      "Cost on val dataset after 137 epochs is = 0.09656038843319915\n",
      "Initial Cost on Val dataset for this epoch 137 = 0.09656038843319915\n",
      "learning rate for this epoch =  0.17537638289633925\n",
      "Error on this batch = 0.039409790415340415\n",
      "Error on this batch = 0.039782307231794996\n",
      "Cost on val dataset after 138 epochs is = 0.09641605403145323\n",
      "Initial Cost on Val dataset for this epoch 138 = 0.09641605403145323\n",
      "learning rate for this epoch =  0.1750578050454048\n",
      "Error on this batch = 0.03894907111858342\n",
      "Error on this batch = 0.03966790543271878\n",
      "Cost on val dataset after 139 epochs is = 0.09627272536553431\n",
      "Initial Cost on Val dataset for this epoch 139 = 0.09627272536553431\n",
      "learning rate for this epoch =  0.17474209987429748\n",
      "Error on this batch = 0.03852160045811596\n",
      "Error on this batch = 0.039549752871208724\n",
      "Cost on val dataset after 140 epochs is = 0.09613266595299425\n",
      "Initial Cost on Val dataset for this epoch 140 = 0.09613266595299425\n",
      "learning rate for this epoch =  0.17442922109046577\n",
      "Error on this batch = 0.038120490043135564\n",
      "Error on this batch = 0.03943094153934292\n",
      "Cost on val dataset after 141 epochs is = 0.09599663130307215\n",
      "Initial Cost on Val dataset for this epoch 141 = 0.09599663130307215\n",
      "learning rate for this epoch =  0.17411912347124506\n",
      "Error on this batch = 0.03773924263919568\n",
      "Error on this batch = 0.03931475620251046\n",
      "Cost on val dataset after 142 epochs is = 0.09586497994333398\n",
      "Initial Cost on Val dataset for this epoch 142 = 0.09586497994333398\n",
      "learning rate for this epoch =  0.17381176283175084\n",
      "Error on this batch = 0.037376121971204025\n",
      "Error on this batch = 0.039203405429275966\n",
      "Cost on val dataset after 143 epochs is = 0.09573890442888693\n",
      "Initial Cost on Val dataset for this epoch 143 = 0.09573890442888693\n",
      "learning rate for this epoch =  0.17350709599395428\n",
      "Error on this batch = 0.03703400516547614\n",
      "Error on this batch = 0.03909678960674457\n",
      "Cost on val dataset after 144 epochs is = 0.0956201656935234\n",
      "Initial Cost on Val dataset for this epoch 144 = 0.0956201656935234\n",
      "learning rate for this epoch =  0.17320508075688773\n",
      "Error on this batch = 0.036712868492122366\n",
      "Error on this batch = 0.038991795472090265\n",
      "Cost on val dataset after 145 epochs is = 0.09551053923418683\n",
      "Initial Cost on Val dataset for this epoch 145 = 0.09551053923418683\n",
      "learning rate for this epoch =  0.17290567586793207\n",
      "Error on this batch = 0.03640308064217708\n",
      "Error on this batch = 0.03888325750800472\n",
      "Cost on val dataset after 146 epochs is = 0.0954110603861697\n",
      "Initial Cost on Val dataset for this epoch 146 = 0.0954110603861697\n",
      "learning rate for this epoch =  0.1726088409951392\n",
      "Error on this batch = 0.036087929671464056\n",
      "Error on this batch = 0.038768485918717135\n",
      "Cost on val dataset after 147 epochs is = 0.09532127867804202\n",
      "Initial Cost on Val dataset for this epoch 147 = 0.09532127867804202\n",
      "learning rate for this epoch =  0.1723145367005454\n",
      "Error on this batch = 0.035757468086025715\n",
      "Error on this batch = 0.038649608142655854\n",
      "Cost on val dataset after 148 epochs is = 0.09523972085298832\n",
      "Initial Cost on Val dataset for this epoch 148 = 0.09523972085298832\n",
      "learning rate for this epoch =  0.172022724414434\n",
      "Error on this batch = 0.03541471059358167\n",
      "Error on this batch = 0.03852997540057854\n",
      "Cost on val dataset after 149 epochs is = 0.09516509239488939\n",
      "Initial Cost on Val dataset for this epoch 149 = 0.09516509239488939\n",
      "learning rate for this epoch =  0.171733366410507\n",
      "Error on this batch = 0.0350672171214604\n",
      "Error on this batch = 0.03841193879516444\n",
      "Cost on val dataset after 150 epochs is = 0.09509667794403434\n",
      "Initial Cost on Val dataset for this epoch 150 = 0.09509667794403434\n",
      "learning rate for this epoch =  0.17144642578192795\n",
      "Error on this batch = 0.03472118267158321\n",
      "Error on this batch = 0.03829702429643216\n",
      "Cost on val dataset after 151 epochs is = 0.09503410091549774\n",
      "Initial Cost on Val dataset for this epoch 151 = 0.09503410091549774\n",
      "learning rate for this epoch =  0.1711618664182\n",
      "Error on this batch = 0.034380772055304244\n",
      "Error on this batch = 0.038186198806662554\n",
      "Cost on val dataset after 152 epochs is = 0.09497705442833075\n",
      "Initial Cost on Val dataset for this epoch 152 = 0.09497705442833075\n",
      "learning rate for this epoch =  0.1708796529828442\n",
      "Error on this batch = 0.03404858997125983\n",
      "Error on this batch = 0.038079996794162824\n",
      "Cost on val dataset after 153 epochs is = 0.09492515122654985\n",
      "Initial Cost on Val dataset for this epoch 153 = 0.09492515122654985\n",
      "learning rate for this epoch =  0.17059975089184612\n",
      "Error on this batch = 0.033726116894987744\n",
      "Error on this batch = 0.03797863919975338\n",
      "Cost on val dataset after 154 epochs is = 0.09487786615189707\n",
      "Initial Cost on Val dataset for this epoch 154 = 0.09487786615189707\n",
      "learning rate for this epoch =  0.17032212629283866\n",
      "Error on this batch = 0.0334140325903745\n",
      "Error on this batch = 0.037882144827609314\n",
      "Cost on val dataset after 155 epochs is = 0.09483452975164026\n",
      "Initial Cost on Val dataset for this epoch 155 = 0.09483452975164026\n",
      "learning rate for this epoch =  0.17004674604499187\n",
      "Error on this batch = 0.03311244052764775\n",
      "Error on this batch = 0.03779041173264246\n",
      "Cost on val dataset after 156 epochs is = 0.09479435122162747\n",
      "Initial Cost on Val dataset for this epoch 156 = 0.09479435122162747\n",
      "learning rate for this epoch =  0.16977357769958104\n",
      "Error on this batch = 0.032821017199862454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on this batch = 0.037703260147842636\n",
      "Cost on val dataset after 157 epochs is = 0.0947564660527291\n",
      "Initial Cost on Val dataset for this epoch 157 = 0.0947564660527291\n",
      "learning rate for this epoch =  0.16950258948120644\n",
      "Error on this batch = 0.032539116403830025\n",
      "Error on this batch = 0.03762044160545364\n",
      "Cost on val dataset after 158 epochs is = 0.09472000826697088\n",
      "Initial Cost on Val dataset for this epoch 158 = 0.09472000826697088\n",
      "learning rate for this epoch =  0.16923375026963824\n",
      "Error on this batch = 0.032265861288964255\n",
      "Error on this batch = 0.03754162754133115\n",
      "Cost on val dataset after 159 epochs is = 0.09468420024478322\n",
      "Initial Cost on Val dataset for this epoch 159 = 0.09468420024478322\n",
      "learning rate for this epoch =  0.16896702958226256\n",
      "Error on this batch = 0.03200025946624348\n",
      "Error on this batch = 0.03746639511759581\n",
      "Cost on val dataset after 160 epochs is = 0.0946484421915136\n",
      "Initial Cost on Val dataset for this epoch 160 = 0.0946484421915136\n",
      "learning rate for this epoch =  0.16870239755710473\n",
      "Error on this batch = 0.03174136910793141\n",
      "Error on this batch = 0.03739422565271571\n",
      "Cost on val dataset after 161 epochs is = 0.0946123821900179\n",
      "Initial Cost on Val dataset for this epoch 161 = 0.0946123821900179\n",
      "learning rate for this epoch =  0.16843982493640752\n",
      "Error on this batch = 0.03148851016922353\n",
      "Error on this batch = 0.03732451873572265\n",
      "Cost on val dataset after 162 epochs is = 0.09457596347304438\n",
      "Initial Cost on Val dataset for this epoch 162 = 0.09457596347304438\n",
      "learning rate for this epoch =  0.1681792830507429\n",
      "Error on this batch = 0.031241462018503485\n",
      "Error on this batch = 0.03725661059639562\n",
      "Cost on val dataset after 163 epochs is = 0.09453945782097134\n",
      "Initial Cost on Val dataset for this epoch 163 = 0.09453945782097134\n",
      "learning rate for this epoch =  0.1679207438036363\n",
      "Error on this batch = 0.031000567101005322\n",
      "Error on this batch = 0.037189785682217966\n",
      "Cost on val dataset after 164 epochs is = 0.09450347357454794\n",
      "Initial Cost on Val dataset for this epoch 164 = 0.09450347357454794\n",
      "learning rate for this epoch =  0.16766417965668484\n",
      "Error on this batch = 0.030766699184014104\n",
      "Error on this batch = 0.03712328605406892\n",
      "Cost on val dataset after 165 epochs is = 0.09446887036823189\n",
      "Initial Cost on Val dataset for this epoch 165 = 0.09446887036823189\n",
      "learning rate for this epoch =  0.1674095636151496\n",
      "Error on this batch = 0.03054107039927263\n",
      "Error on this batch = 0.0370563370174442\n",
      "Cost on val dataset after 166 epochs is = 0.09443643412402909\n",
      "Initial Cost on Val dataset for this epoch 166 = 0.09443643412402909\n",
      "learning rate for this epoch =  0.16715686921400502\n",
      "Error on this batch = 0.030324700143652385\n",
      "Error on this batch = 0.036988213963987264\n",
      "Cost on val dataset after 167 epochs is = 0.09440621366599557\n",
      "Initial Cost on Val dataset for this epoch 167 = 0.09440621366599557\n",
      "learning rate for this epoch =  0.16690607050442752\n",
      "Error on this batch = 0.030117280215856256\n",
      "Error on this batch = 0.03691836071261557\n",
      "Cost on val dataset after 168 epochs is = 0.09437713654407782\n",
      "Initial Cost on Val dataset for this epoch 168 = 0.09437713654407782\n",
      "learning rate for this epoch =  0.16665714204070745\n",
      "Error on this batch = 0.029916702515124235\n",
      "Error on this batch = 0.036846472801266364\n",
      "Cost on val dataset after 169 epochs is = 0.09434816588231179\n",
      "Initial Cost on Val dataset for this epoch 169 = 0.09434816588231179\n",
      "learning rate for this epoch =  0.16641005886756874\n",
      "Error on this batch = 0.029722407103131437\n",
      "Error on this batch = 0.03677238576426633\n",
      "Cost on val dataset after 170 epochs is = 0.09431944872335522\n",
      "Initial Cost on Val dataset for this epoch 170 = 0.09431944872335522\n",
      "learning rate for this epoch =  0.1661647965078805\n",
      "Error on this batch = 0.029537166374812546\n",
      "Error on this batch = 0.036696004445681406\n",
      "Cost on val dataset after 171 epochs is = 0.09429130091109676\n",
      "Initial Cost on Val dataset for this epoch 171 = 0.09429130091109676\n",
      "learning rate for this epoch =  0.1659213309507473\n",
      "Error on this batch = 0.02936190939371054\n",
      "Error on this batch = 0.036617565312830065\n",
      "Cost on val dataset after 172 epochs is = 0.09426333813813857\n",
      "Initial Cost on Val dataset for this epoch 172 = 0.09426333813813857\n",
      "learning rate for this epoch =  0.16567963863996335\n",
      "Error on this batch = 0.029194518467647465\n",
      "Error on this batch = 0.036537689938697805\n",
      "Cost on val dataset after 173 epochs is = 0.09423471868255492\n",
      "Initial Cost on Val dataset for this epoch 173 = 0.09423471868255492\n",
      "learning rate for this epoch =  0.16543969646281814\n",
      "Error on this batch = 0.02903241537716689\n",
      "Error on this batch = 0.03645714209915358\n",
      "Cost on val dataset after 174 epochs is = 0.09420459894192795\n",
      "Initial Cost on Val dataset for this epoch 174 = 0.09420459894192795\n",
      "learning rate for this epoch =  0.1652014817392402\n",
      "Error on this batch = 0.02887372474447939\n",
      "Error on this batch = 0.03637663229203362\n",
      "Cost on val dataset after 175 epochs is = 0.09417235749621335\n",
      "Initial Cost on Val dataset for this epoch 175 = 0.09417235749621335\n",
      "learning rate for this epoch =  0.1649649722112678\n",
      "Error on this batch = 0.028717300749812123\n",
      "Error on this batch = 0.03629672559979917\n",
      "Cost on val dataset after 176 epochs is = 0.09413763698809757\n",
      "Initial Cost on Val dataset for this epoch 176 = 0.09413763698809757\n",
      "learning rate for this epoch =  0.16473014603283373\n",
      "Error on this batch = 0.028562546058866174\n",
      "Error on this batch = 0.0362178010953861\n",
      "Cost on val dataset after 177 epochs is = 0.09410031000216337\n",
      "Initial Cost on Val dataset for this epoch 177 = 0.09410031000216337\n",
      "learning rate for this epoch =  0.16449698175985428\n",
      "Error on this batch = 0.028409269010281672\n",
      "Error on this batch = 0.03614003793497771\n",
      "Cost on val dataset after 178 epochs is = 0.09406042224191792\n",
      "Initial Cost on Val dataset for this epoch 178 = 0.09406042224191792\n",
      "learning rate for this epoch =  0.164265458340611\n",
      "Error on this batch = 0.0282575729079828\n",
      "Error on this batch = 0.03606342845748329\n",
      "Cost on val dataset after 179 epochs is = 0.09401814496936907\n",
      "Initial Cost on Val dataset for this epoch 179 = 0.09401814496936907\n",
      "learning rate for this epoch =  0.16403555510641493\n",
      "Error on this batch = 0.028107744696347442\n",
      "Error on this batch = 0.03598782489108234\n",
      "Cost on val dataset after 180 epochs is = 0.0939737566588\n",
      "Initial Cost on Val dataset for this epoch 180 = 0.0939737566588\n",
      "learning rate for this epoch =  0.163807251762544\n",
      "Error on this batch = 0.02796013973170869\n",
      "Error on this batch = 0.0359130162567163\n",
      "Cost on val dataset after 181 epochs is = 0.09392764372333948\n",
      "Initial Cost on Val dataset for this epoch 181 = 0.09392764372333948\n",
      "learning rate for this epoch =  0.1635805283794437\n",
      "Error on this batch = 0.027815088014712552\n",
      "Error on this batch = 0.03583881136316745\n",
      "Cost on val dataset after 182 epochs is = 0.09388028860399222\n",
      "Initial Cost on Val dataset for this epoch 182 = 0.09388028860399222\n",
      "learning rate for this epoch =  0.1633553653841821\n",
      "Error on this batch = 0.027672849166045937\n",
      "Error on this batch = 0.0357650926034999\n",
      "Cost on val dataset after 183 epochs is = 0.09383223401725616\n",
      "Initial Cost on Val dataset for this epoch 183 = 0.09383223401725616\n",
      "learning rate for this epoch =  0.16313174355215057\n",
      "Error on this batch = 0.02753361714013544\n",
      "Error on this batch = 0.035691822648211254\n",
      "Cost on val dataset after 184 epochs is = 0.09378404312575535\n",
      "Initial Cost on Val dataset for this epoch 184 = 0.09378404312575535\n",
      "learning rate for this epoch =  0.16290964399900174\n",
      "Error on this batch = 0.027397550254211814\n",
      "Error on this batch = 0.03561901707940963\n",
      "Cost on val dataset after 185 epochs is = 0.09373627156206653\n",
      "Initial Cost on Val dataset for this epoch 185 = 0.09373627156206653\n",
      "learning rate for this epoch =  0.1626890481728167\n",
      "Error on this batch = 0.027264800697643886\n",
      "Error on this batch = 0.03554670947870125\n",
      "Cost on val dataset after 186 epochs is = 0.09368944816059417\n",
      "Initial Cost on Val dataset for this epoch 186 = 0.09368944816059417\n",
      "learning rate for this epoch =  0.1624699378464939\n",
      "Error on this batch = 0.027135530358069943\n",
      "Error on this batch = 0.03547492770880741\n",
      "Cost on val dataset after 187 epochs is = 0.09364405895300651\n",
      "Initial Cost on Val dataset for this epoch 187 = 0.09364405895300651\n",
      "learning rate for this epoch =  0.16225229511035183\n",
      "Error on this batch = 0.027009909203181907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on this batch = 0.035403687617094594\n",
      "Cost on val dataset after 188 epochs is = 0.09360053594192663\n",
      "Initial Cost on Val dataset for this epoch 188 = 0.09360053594192663\n",
      "learning rate for this epoch =  0.16203610236493907\n",
      "Error on this batch = 0.026888096126812245\n",
      "Error on this batch = 0.035333002477572775\n",
      "Cost on val dataset after 189 epochs is = 0.0935592535045018\n",
      "Initial Cost on Val dataset for this epoch 189 = 0.0935592535045018\n",
      "learning rate for this epoch =  0.16182134231404424\n",
      "Error on this batch = 0.02677020444139574\n",
      "Error on this batch = 0.035262902651171116\n",
      "Cost on val dataset after 190 epochs is = 0.09352053176853066\n",
      "Initial Cost on Val dataset for this epoch 190 = 0.09352053176853066\n",
      "learning rate for this epoch =  0.1616079979578994\n",
      "Error on this batch = 0.026656257106231648\n",
      "Error on this batch = 0.03519345786768918\n",
      "Cost on val dataset after 191 epochs is = 0.09348464356273628\n",
      "Initial Cost on Val dataset for this epoch 191 = 0.09348464356273628\n",
      "learning rate for this epoch =  0.16139605258657097\n",
      "Error on this batch = 0.026546140289168622\n",
      "Error on this batch = 0.03512479346574253\n",
      "Cost on val dataset after 192 epochs is = 0.09345182120339326\n",
      "Initial Cost on Val dataset for this epoch 192 = 0.09345182120339326\n",
      "learning rate for this epoch =  0.16118548977353128\n",
      "Error on this batch = 0.026439567081999717\n",
      "Error on this batch = 0.03505709255354378\n",
      "Cost on val dataset after 193 epochs is = 0.09342226035555014\n",
      "Initial Cost on Val dataset for this epoch 193 = 0.09342226035555014\n",
      "learning rate for this epoch =  0.1609762933694058\n",
      "Error on this batch = 0.026336064177332853\n",
      "Error on this batch = 0.03499057953360075\n",
      "Cost on val dataset after 194 epochs is = 0.09339611954145648\n",
      "Initial Cost on Val dataset for this epoch 194 = 0.09339611954145648\n",
      "learning rate for this epoch =  0.16076844749588948\n",
      "Error on this batch = 0.02623499061207661\n",
      "Error on this batch = 0.03492548703159374\n",
      "Cost on val dataset after 195 epochs is = 0.09337351538784235\n",
      "Initial Cost on Val dataset for this epoch 195 = 0.09337351538784235\n",
      "learning rate for this epoch =  0.16056193653982745\n",
      "Error on this batch = 0.026135588434209545\n",
      "Error on this batch = 0.03486201602496174\n",
      "Cost on val dataset after 196 epochs is = 0.09335451530228989\n",
      "Initial Cost on Val dataset for this epoch 196 = 0.09335451530228989\n",
      "learning rate for this epoch =  0.16035674514745463\n",
      "Error on this batch = 0.026037053353689582\n",
      "Error on this batch = 0.03480030376410663\n",
      "Cost on val dataset after 197 epochs is = 0.0933391303971883\n",
      "Initial Cost on Val dataset for this epoch 197 = 0.0933391303971883\n",
      "learning rate for this epoch =  0.1601528582187888\n",
      "Error on this batch = 0.02593860573183501\n",
      "Error on this batch = 0.03474041214697617\n",
      "Cost on val dataset after 198 epochs is = 0.09332731154335047\n",
      "Initial Cost on Val dataset for this epoch 198 = 0.09332731154335047\n",
      "learning rate for this epoch =  0.15995026090217312\n",
      "Error on this batch = 0.02583954457987042\n",
      "Error on this batch = 0.034682340663245326\n",
      "Cost on val dataset after 199 epochs is = 0.09331895050536625\n",
      "Initial Cost on Val dataset for this epoch 199 = 0.09331895050536625\n",
      "learning rate for this epoch =  0.15974893858896244\n",
      "Error on this batch = 0.025739278920327166\n",
      "Error on this batch = 0.034626057666544474\n",
      "Cost on val dataset after 200 epochs is = 0.0933138868155394\n",
      "Initial Cost on Val dataset for this epoch 200 = 0.0933138868155394\n",
      "learning rate for this epoch =  0.15954887690834965\n",
      "Error on this batch = 0.02563734368669437\n",
      "Error on this batch = 0.034571537366885834\n",
      "Cost on val dataset after 201 epochs is = 0.09331191975693556\n",
      "Initial Cost on Val dataset for this epoch 201 = 0.09331191975693556\n",
      "learning rate for this epoch =  0.15935006172232735\n",
      "Error on this batch = 0.025533412290530372\n",
      "Error on this batch = 0.03451878991327772\n",
      "Cost on val dataset after 202 epochs is = 0.09331282346611175\n",
      "Initial Cost on Val dataset for this epoch 202 = 0.09331282346611175\n",
      "learning rate for this epoch =  0.1591524791207806\n",
      "Error on this batch = 0.025427313273750397\n",
      "Error on this batch = 0.0344678764567763\n",
      "Cost on val dataset after 203 epochs is = 0.09331636181287928\n",
      "Initial Cost on Val dataset for this epoch 203 = 0.09331636181287928\n",
      "learning rate for this epoch =  0.15895611541670698\n",
      "Error on this batch = 0.025319049854402788\n",
      "Error on this batch = 0.03441890673655946\n",
      "Cost on val dataset after 204 epochs is = 0.09332229881269387\n",
      "Initial Cost on Val dataset for this epoch 204 = 0.09332229881269387\n",
      "learning rate for this epoch =  0.15876095714155977\n",
      "Error on this batch = 0.025208815303429246\n",
      "Error on this batch = 0.034372020881168315\n",
      "Cost on val dataset after 205 epochs is = 0.09333040012526848\n",
      "Initial Cost on Val dataset for this epoch 205 = 0.09333040012526848\n",
      "learning rate for this epoch =  0.15856699104071065\n",
      "Error on this batch = 0.025096995960947343\n",
      "Error on this batch = 0.03432735876728005\n",
      "Cost on val dataset after 206 epochs is = 0.09334042141782276\n",
      "Initial Cost on Val dataset for this epoch 206 = 0.09334042141782276\n",
      "learning rate for this epoch =  0.15837420406902836\n",
      "Error on this batch = 0.02498415491861972\n",
      "Error on this batch = 0.03428502008261227\n",
      "Cost on val dataset after 207 epochs is = 0.0933520794373421\n",
      "Initial Cost on Val dataset for this epoch 207 = 0.0933520794373421\n",
      "learning rate for this epoch =  0.15818258338656938\n",
      "Error on this batch = 0.024870989283375073\n",
      "Error on this batch = 0.034245018173388105\n",
      "Cost on val dataset after 208 epochs is = 0.09336500056611069\n",
      "Initial Cost on Val dataset for this epoch 208 = 0.09336500056611069\n",
      "learning rate for this epoch =  0.157992116354378\n",
      "Error on this batch = 0.024758249790485548\n",
      "Error on this batch = 0.03420723337256538\n",
      "Cost on val dataset after 209 epochs is = 0.0933786379872725\n",
      "Initial Cost on Val dataset for this epoch 209 = 0.0933786379872725\n",
      "learning rate for this epoch =  0.15780279053039173\n",
      "Error on this batch = 0.024646604002381303\n",
      "Error on this batch = 0.03417137817995748\n",
      "Cost on val dataset after 210 epochs is = 0.0933921455743962\n",
      "Initial Cost on Val dataset for this epoch 210 = 0.0933921455743962\n",
      "learning rate for this epoch =  0.1576145936654495\n",
      "Error on this batch = 0.02453642551168026\n",
      "Error on this batch = 0.03413699405135916\n",
      "Cost on val dataset after 211 epochs is = 0.09340422480950057\n",
      "cost initial= 0.0933921455743962 , cost final=0.09340422480950057 , change in cost= 1.2079235104378583e-05\n",
      "\n",
      "------------------------------------------------------------------------------\n",
      "The stats for number of units in the hidden layer = [100, 100] with softplus are as below:\n",
      "------------------------------------------------------------------------------\n",
      "The number of epochs with softplus is = 211\n",
      "The training time with softplus is = 89.693sec\n",
      "The training accuracy with softplus is = 94.968%\n",
      "The validation accuracy with softplus is = 89.795%\n",
      "The test accuracy with softplus is = 87.908%\n",
      "------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "arch=[100, 100]\n",
    "lr=0.6\n",
    "theta = theta_init(n, r, arch, 'normal')\n",
    "print(\"Training the network with {} hidden layer with {} units\".format(len(arch), arch))\n",
    "print(\"The parameters of the layers are of the shape:\")\n",
    "\n",
    "for j in range(len(theta)):\n",
    "    print(\"theta between layer {} and layer {} is {}\".format(j, j+1,theta[j].shape))\n",
    "    \n",
    "start = time.time()\n",
    "epoch, theta = training(mini_batch, X_valid, valid_class_enc, theta, lr, 'softplus', 'adaptive')\n",
    "\n",
    "epochs.append(epoch)\n",
    "train_time.append(time.time()-start)\n",
    "train_accuracy.append(calc_accuracy(X_train, theta, train_class_enc, 'softplus'))\n",
    "valid_accuracy.append(calc_accuracy(X_valid, theta, valid_class_enc, 'softplus'))\n",
    "test_accuracy.append(calc_accuracy(X_test, theta, test_actual_class_enc, 'softplus'))\n",
    "\n",
    "print(\"\\n------------------------------------------------------------------------------\")\n",
    "print(\"The stats for number of units in the hidden layer = {} with softplus are as below:\".format(arch))\n",
    "print(\"------------------------------------------------------------------------------\")\n",
    "print(\"The number of epochs with softplus is = {}\".format(epochs[-1]))\n",
    "print(\"The training time with softplus is = {:2.3f}sec\".format(train_time[-1]))\n",
    "print(\"The training accuracy with softplus is = {:2.3f}%\".format(train_accuracy[-1]))\n",
    "print(\"The validation accuracy with softplus is = {:2.3f}%\".format(valid_accuracy[-1]))\n",
    "print(\"The test accuracy with softplus is = {:2.3f}%\".format(test_accuracy[-1]))\n",
    "print(\"------------------------------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------Plotting Graphs for Part D - ReLU of 2 Hidden Layer ARCH ------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"------------------Plotting Graphs for Part D - ReLU of 2 Hidden Layer ARCH ------------------\")\n",
    "x=[0,1,2]\n",
    "data = [' ','ReLU', ' ' ,' ' ,' ','Sigmoid', ' ',' ',' ','Softplus']\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_title(\"Accuracy and Epochs for Neural Network [100,100]\")\n",
    "ax.plot(x, train_accuracy, marker='o', label='Train Accuracy')\n",
    "ax.plot(x, valid_accuracy, marker='o',label='Validation Accuracy')\n",
    "ax.plot(x, test_accuracy, marker='o', label='Test Accuracy')\n",
    "ax.set_xlabel(\"Activation Function\")\n",
    "ax.set_xticklabels(data)\n",
    "ax.set_ylabel(\"Accuracy (%)\")\n",
    "ax.legend()\n",
    "ax1=ax.twinx()\n",
    "ax1.set_ylabel(\"Number of Epochs\")\n",
    "ax1.plot(x, epochs, marker='*', color='m',label='Epochs')\n",
    "ax1.tick_params(axis='y', labelcolor='m', labelsize=12)\n",
    "ax1.legend()\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "#plt.savefig(\"plots/partd/comp_diff_act.png\", dpi=1000, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part F - Binary Cross Entropy With ReLU activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=[]\n",
    "train_accuracy = []\n",
    "valid_accuracy = []\n",
    "test_accuracy = []\n",
    "train_time = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------Running Part F-----------------------------------------------------\n",
      "------------------Training a 100x100 hidden layer network with ReLU activation and Cross Entropy------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"----------------------------Running Part F-----------------------------------------------------\")\n",
    "print(\"------------------Training a 100x100 hidden layer network with ReLU activation and Cross Entropy------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the network with 2 hidden layer with [100, 100] units\n",
      "The parameters of the layers are of the shape:\n",
      "theta between layer 0 and layer 1 is (785, 100)\n",
      "theta between layer 1 and layer 2 is (101, 100)\n",
      "theta between layer 2 and layer 3 is (101, 26)\n",
      "Initial Cost on Val dataset for this epoch 1 = 17.901383893296945\n",
      "learning rate for this epoch =  0.1\n",
      "Error on this batch = 17.89721578625675\n",
      "Error on this batch = 4.604033332206689\n",
      "Cost on val dataset after 2 epochs is = 4.171266974682238\n",
      "Initial Cost on Val dataset for this epoch 2 = 4.171266974682238\n",
      "learning rate for this epoch =  0.08408964152537146\n",
      "Error on this batch = 4.157265536554206\n",
      "Error on this batch = 3.474939642069099\n",
      "Cost on val dataset after 3 epochs is = 2.8478910470909926\n",
      "Initial Cost on Val dataset for this epoch 3 = 2.8478910470909926\n",
      "learning rate for this epoch =  0.07598356856515927\n",
      "Error on this batch = 2.774500042844477\n",
      "Error on this batch = 2.6790828682956356\n",
      "Cost on val dataset after 4 epochs is = 2.2040228179440255\n",
      "Initial Cost on Val dataset for this epoch 4 = 2.2040228179440255\n",
      "learning rate for this epoch =  0.07071067811865475\n",
      "Error on this batch = 2.1155773816588574\n",
      "Error on this batch = 2.2881330181942445\n",
      "Cost on val dataset after 5 epochs is = 1.8861889088055086\n",
      "Initial Cost on Val dataset for this epoch 5 = 1.8861889088055086\n",
      "learning rate for this epoch =  0.0668740304976422\n",
      "Error on this batch = 1.754164818806454\n",
      "Error on this batch = 2.015463651966893\n",
      "Cost on val dataset after 6 epochs is = 1.699219783334616\n",
      "Initial Cost on Val dataset for this epoch 6 = 1.699219783334616\n",
      "learning rate for this epoch =  0.06389431042462725\n",
      "Error on this batch = 1.5305504698600063\n",
      "Error on this batch = 1.8336445271220867\n",
      "Cost on val dataset after 7 epochs is = 1.5817035213588488\n",
      "Initial Cost on Val dataset for this epoch 7 = 1.5817035213588488\n",
      "learning rate for this epoch =  0.06147881529512644\n",
      "Error on this batch = 1.393411534253587\n",
      "Error on this batch = 1.7177173762148914\n",
      "Cost on val dataset after 8 epochs is = 1.4876664589062583\n",
      "Initial Cost on Val dataset for this epoch 8 = 1.4876664589062583\n",
      "learning rate for this epoch =  0.05946035575013606\n",
      "Error on this batch = 1.2761286613093268\n",
      "Error on this batch = 1.6289050182842173\n",
      "Cost on val dataset after 9 epochs is = 1.422604124373947\n",
      "Initial Cost on Val dataset for this epoch 9 = 1.422604124373947\n",
      "learning rate for this epoch =  0.05773502691896258\n",
      "Error on this batch = 1.1974401121902194\n",
      "Error on this batch = 1.5552322645261427\n",
      "Cost on val dataset after 10 epochs is = 1.3725259601355417\n",
      "Initial Cost on Val dataset for this epoch 10 = 1.3725259601355417\n",
      "learning rate for this epoch =  0.05623413251903491\n",
      "Error on this batch = 1.1370975743645209\n",
      "Error on this batch = 1.5026302679897452\n",
      "Cost on val dataset after 11 epochs is = 1.3326236121836055\n",
      "Initial Cost on Val dataset for this epoch 11 = 1.3326236121836055\n",
      "learning rate for this epoch =  0.05491004867761125\n",
      "Error on this batch = 1.0888923788288196\n",
      "Error on this batch = 1.460469338035764\n",
      "Cost on val dataset after 12 epochs is = 1.3002288689185864\n",
      "Initial Cost on Val dataset for this epoch 12 = 1.3002288689185864\n",
      "learning rate for this epoch =  0.0537284965911771\n",
      "Error on this batch = 1.0507538252371893\n",
      "Error on this batch = 1.4244396049094856\n",
      "Cost on val dataset after 13 epochs is = 1.273117398970834\n",
      "Initial Cost on Val dataset for this epoch 13 = 1.273117398970834\n",
      "learning rate for this epoch =  0.052664038784792665\n",
      "Error on this batch = 1.018989259159506\n",
      "Error on this batch = 1.3901729422065867\n",
      "Cost on val dataset after 14 epochs is = 1.2497065582810822\n",
      "Initial Cost on Val dataset for this epoch 14 = 1.2497065582810822\n",
      "learning rate for this epoch =  0.05169731539571706\n",
      "Error on this batch = 0.9938784428097175\n",
      "Error on this batch = 1.3610976738420058\n",
      "Cost on val dataset after 15 epochs is = 1.2300432078422268\n",
      "Initial Cost on Val dataset for this epoch 15 = 1.2300432078422268\n",
      "learning rate for this epoch =  0.05081327481546148\n",
      "Error on this batch = 0.9729204550346094\n",
      "Error on this batch = 1.3376602868608825\n",
      "Cost on val dataset after 16 epochs is = 1.2129163287057763\n",
      "Initial Cost on Val dataset for this epoch 16 = 1.2129163287057763\n",
      "learning rate for this epoch =  0.05\n",
      "Error on this batch = 0.9536727046513841\n",
      "Error on this batch = 1.3164892170538758\n",
      "Cost on val dataset after 17 epochs is = 1.197661272725907\n",
      "Initial Cost on Val dataset for this epoch 17 = 1.197661272725907\n",
      "learning rate for this epoch =  0.049247906050545236\n",
      "Error on this batch = 0.9374361704176275\n",
      "Error on this batch = 1.2972431618316704\n",
      "Cost on val dataset after 18 epochs is = 1.1841694400551483\n",
      "Initial Cost on Val dataset for this epoch 18 = 1.1841694400551483\n",
      "learning rate for this epoch =  0.048549177170732344\n",
      "Error on this batch = 0.9234819023584052\n",
      "Error on this batch = 1.2805743919437385\n",
      "Cost on val dataset after 19 epochs is = 1.1712288775946966\n",
      "Initial Cost on Val dataset for this epoch 19 = 1.1712288775946966\n",
      "learning rate for this epoch =  0.04789736254435747\n",
      "Error on this batch = 0.9104971374630492\n",
      "Error on this batch = 1.2644882724140858\n",
      "Cost on val dataset after 20 epochs is = 1.1597060058332262\n",
      "Initial Cost on Val dataset for this epoch 20 = 1.1597060058332262\n",
      "learning rate for this epoch =  0.047287080450158794\n",
      "Error on this batch = 0.8985239319695894\n",
      "Error on this batch = 1.2511405391626804\n",
      "Cost on val dataset after 21 epochs is = 1.1493385561191891\n",
      "Initial Cost on Val dataset for this epoch 21 = 1.1493385561191891\n",
      "learning rate for this epoch =  0.04671379777282001\n",
      "Error on this batch = 0.8875831216165618\n",
      "Error on this batch = 1.2386718359015116\n",
      "Cost on val dataset after 22 epochs is = 1.1403035641737094\n",
      "Initial Cost on Val dataset for this epoch 22 = 1.1403035641737094\n",
      "learning rate for this epoch =  0.04617366309441026\n",
      "Error on this batch = 0.8776541100492576\n",
      "Error on this batch = 1.2265250611019343\n",
      "Cost on val dataset after 23 epochs is = 1.132312168331301\n",
      "Initial Cost on Val dataset for this epoch 23 = 1.132312168331301\n",
      "learning rate for this epoch =  0.04566337854967313\n",
      "Error on this batch = 0.8689909761625018\n",
      "Error on this batch = 1.2165047901057002\n",
      "Cost on val dataset after 24 epochs is = 1.1248569108577087\n",
      "Initial Cost on Val dataset for this epoch 24 = 1.1248569108577087\n",
      "learning rate for this epoch =  0.04518010018049225\n",
      "Error on this batch = 0.8617387721550864\n",
      "Error on this batch = 1.2070469924445457\n",
      "Cost on val dataset after 25 epochs is = 1.1180005572633336\n",
      "Initial Cost on Val dataset for this epoch 25 = 1.1180005572633336\n",
      "learning rate for this epoch =  0.044721359549995794\n",
      "Error on this batch = 0.8552776212816522\n",
      "Error on this batch = 1.1980218418955106\n",
      "Cost on val dataset after 26 epochs is = 1.1120500415646468\n",
      "Initial Cost on Val dataset for this epoch 26 = 1.1120500415646468\n",
      "learning rate for this epoch =  0.04428500142691474\n",
      "Error on this batch = 0.849746873825257\n",
      "Error on this batch = 1.190627404878493\n",
      "Cost on val dataset after 27 epochs is = 1.1062264758338503\n",
      "Initial Cost on Val dataset for this epoch 27 = 1.1062264758338503\n",
      "learning rate for this epoch =  0.043869133765083085\n",
      "Error on this batch = 0.8443886786168893\n",
      "Error on this batch = 1.1828574479098266\n",
      "Cost on val dataset after 28 epochs is = 1.1009719258842023\n",
      "Initial Cost on Val dataset for this epoch 28 = 1.1009719258842023\n",
      "learning rate for this epoch =  0.043472087194499145\n",
      "Error on this batch = 0.8391220274307224\n",
      "Error on this batch = 1.1752798334577812\n",
      "Cost on val dataset after 29 epochs is = 1.095976094897564\n",
      "Initial Cost on Val dataset for this epoch 29 = 1.095976094897564\n",
      "learning rate for this epoch =  0.04309238194589061\n",
      "Error on this batch = 0.8339274551174343\n",
      "Error on this batch = 1.1694010850237464\n",
      "Cost on val dataset after 30 epochs is = 1.0915249394005708\n",
      "Initial Cost on Val dataset for this epoch 30 = 1.0915249394005708\n",
      "learning rate for this epoch =  0.042728700639623404\n",
      "Error on this batch = 0.8300343388539426\n",
      "Error on this batch = 1.1638204102865377\n",
      "Cost on val dataset after 31 epochs is = 1.0871472106427018\n",
      "Initial Cost on Val dataset for this epoch 31 = 1.0871472106427018\n",
      "learning rate for this epoch =  0.04237986574150217\n",
      "Error on this batch = 0.8257995855656249\n",
      "Error on this batch = 1.1573300936301267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on val dataset after 32 epochs is = 1.083082037159882\n",
      "Initial Cost on Val dataset for this epoch 32 = 1.083082037159882\n",
      "learning rate for this epoch =  0.04204482076268573\n",
      "Error on this batch = 0.8217006095139037\n",
      "Error on this batch = 1.1509390146976388\n",
      "Cost on val dataset after 33 epochs is = 1.0795678766906542\n",
      "Initial Cost on Val dataset for this epoch 33 = 1.0795678766906542\n",
      "learning rate for this epoch =  0.04172261448611506\n",
      "Error on this batch = 0.8181938630064265\n",
      "Error on this batch = 1.1446990640640704\n",
      "Cost on val dataset after 34 epochs is = 1.0761646018987852\n",
      "Initial Cost on Val dataset for this epoch 34 = 1.0761646018987852\n",
      "learning rate for this epoch =  0.041412387656655204\n",
      "Error on this batch = 0.8143973712134508\n",
      "Error on this batch = 1.1389130440668354\n",
      "Cost on val dataset after 35 epochs is = 1.0730155920483009\n",
      "Initial Cost on Val dataset for this epoch 35 = 1.0730155920483009\n",
      "learning rate for this epoch =  0.04111336169005197\n",
      "Error on this batch = 0.810668052379808\n",
      "Error on this batch = 1.132854177953819\n",
      "Cost on val dataset after 36 epochs is = 1.0698084960812462\n",
      "Initial Cost on Val dataset for this epoch 36 = 1.0698084960812462\n",
      "learning rate for this epoch =  0.040824829046386304\n",
      "Error on this batch = 0.8070801818916479\n",
      "Error on this batch = 1.1268965978961059\n",
      "Cost on val dataset after 37 epochs is = 1.0665993290611095\n",
      "Initial Cost on Val dataset for this epoch 37 = 1.0665993290611095\n",
      "learning rate for this epoch =  0.040546144983876986\n",
      "Error on this batch = 0.8033802333481547\n",
      "Error on this batch = 1.1207269196707454\n",
      "Cost on val dataset after 38 epochs is = 1.0638167326589634\n",
      "Initial Cost on Val dataset for this epoch 38 = 1.0638167326589634\n",
      "learning rate for this epoch =  0.04027672046365773\n",
      "Error on this batch = 0.8003212117979976\n",
      "Error on this batch = 1.1148095142152188\n",
      "Cost on val dataset after 39 epochs is = 1.0616410873073792\n",
      "Initial Cost on Val dataset for this epoch 39 = 1.0616410873073792\n",
      "learning rate for this epoch =  0.040016016019225\n",
      "Error on this batch = 0.7983471320613806\n",
      "Error on this batch = 1.109709788198642\n",
      "Cost on val dataset after 40 epochs is = 1.0592731546244043\n",
      "Initial Cost on Val dataset for this epoch 40 = 1.0592731546244043\n",
      "learning rate for this epoch =  0.03976353643835253\n",
      "Error on this batch = 0.7965085914944757\n",
      "Error on this batch = 1.1049480832523029\n",
      "Cost on val dataset after 41 epochs is = 1.056686331386001\n",
      "Initial Cost on Val dataset for this epoch 41 = 1.056686331386001\n",
      "learning rate for this epoch =  0.03951882613244048\n",
      "Error on this batch = 0.7934206652361553\n",
      "Error on this batch = 1.1000890974883935\n",
      "Cost on val dataset after 42 epochs is = 1.0544217596755034\n",
      "Initial Cost on Val dataset for this epoch 42 = 1.0544217596755034\n",
      "learning rate for this epoch =  0.0392814650900513\n",
      "Error on this batch = 0.7911898589855543\n",
      "Error on this batch = 1.0950261134145\n",
      "Cost on val dataset after 43 epochs is = 1.0524372723406148\n",
      "Initial Cost on Val dataset for this epoch 43 = 1.0524372723406148\n",
      "learning rate for this epoch =  0.03905106532895161\n",
      "Error on this batch = 0.7886576332458484\n",
      "Error on this batch = 1.091324602549052\n",
      "Cost on val dataset after 44 epochs is = 1.050412315246289\n",
      "Initial Cost on Val dataset for this epoch 44 = 1.050412315246289\n",
      "learning rate for this epoch =  0.03882726777522233\n",
      "Error on this batch = 0.7865703379462733\n",
      "Error on this batch = 1.0869660519788777\n",
      "Cost on val dataset after 45 epochs is = 1.0485556565252476\n",
      "Initial Cost on Val dataset for this epoch 45 = 1.0485556565252476\n",
      "learning rate for this epoch =  0.03860973950960897\n",
      "Error on this batch = 0.7840780219584538\n",
      "Error on this batch = 1.0827661277800347\n",
      "Cost on val dataset after 46 epochs is = 1.0465941753528787\n",
      "Initial Cost on Val dataset for this epoch 46 = 1.0465941753528787\n",
      "learning rate for this epoch =  0.0383981713307935\n",
      "Error on this batch = 0.7810991949953697\n",
      "Error on this batch = 1.079106819171264\n",
      "Cost on val dataset after 47 epochs is = 1.0446877156161392\n",
      "Initial Cost on Val dataset for this epoch 47 = 1.0446877156161392\n",
      "learning rate for this epoch =  0.03819227559309534\n",
      "Error on this batch = 0.778441243021037\n",
      "Error on this batch = 1.0753952979366463\n",
      "Cost on val dataset after 48 epochs is = 1.0432379862537278\n",
      "Initial Cost on Val dataset for this epoch 48 = 1.0432379862537278\n",
      "learning rate for this epoch =  0.037991784282579634\n",
      "Error on this batch = 0.7756339835936694\n",
      "Error on this batch = 1.0718233671112574\n",
      "Cost on val dataset after 49 epochs is = 1.0416102243098004\n",
      "Initial Cost on Val dataset for this epoch 49 = 1.0416102243098004\n",
      "learning rate for this epoch =  0.03779644730092272\n",
      "Error on this batch = 0.7732823449634312\n",
      "Error on this batch = 1.0688176885007534\n",
      "Cost on val dataset after 50 epochs is = 1.0400712409914417\n",
      "Initial Cost on Val dataset for this epoch 50 = 1.0400712409914417\n",
      "learning rate for this epoch =  0.03760603093086394\n",
      "Error on this batch = 0.7709889193008174\n",
      "Error on this batch = 1.0666524952899936\n",
      "Cost on val dataset after 51 epochs is = 1.038523796143003\n",
      "Initial Cost on Val dataset for this epoch 51 = 1.038523796143003\n",
      "learning rate for this epoch =  0.03742031646082125\n",
      "Error on this batch = 0.7691530508010922\n",
      "Error on this batch = 1.0634012566135485\n",
      "Cost on val dataset after 52 epochs is = 1.0371553287018391\n",
      "Initial Cost on Val dataset for this epoch 52 = 1.0371553287018391\n",
      "learning rate for this epoch =  0.03723909894939824\n",
      "Error on this batch = 0.7676060626651185\n",
      "Error on this batch = 1.0609682750976983\n",
      "Cost on val dataset after 53 epochs is = 1.0359812553032017\n",
      "Initial Cost on Val dataset for this epoch 53 = 1.0359812553032017\n",
      "learning rate for this epoch =  0.037062186113165134\n",
      "Error on this batch = 0.7656269669586883\n",
      "Error on this batch = 1.057970293056345\n",
      "Cost on val dataset after 54 epochs is = 1.034943901154386\n",
      "Initial Cost on Val dataset for this epoch 54 = 1.034943901154386\n",
      "learning rate for this epoch =  0.03688939732334406\n",
      "Error on this batch = 0.7639012079832719\n",
      "Error on this batch = 1.0565926145488056\n",
      "Cost on val dataset after 55 epochs is = 1.034113831584653\n",
      "Initial Cost on Val dataset for this epoch 55 = 1.034113831584653\n",
      "learning rate for this epoch =  0.03672056269893592\n",
      "Error on this batch = 0.7623221619594722\n",
      "Error on this batch = 1.055003124386783\n",
      "Cost on val dataset after 56 epochs is = 1.033265237054787\n",
      "Initial Cost on Val dataset for this epoch 56 = 1.033265237054787\n",
      "learning rate for this epoch =  0.03655552228545124\n",
      "Error on this batch = 0.7608823427351191\n",
      "Error on this batch = 1.0524151408511222\n",
      "Cost on val dataset after 57 epochs is = 1.0325839945623423\n",
      "Initial Cost on Val dataset for this epoch 57 = 1.0325839945623423\n",
      "learning rate for this epoch =  0.036394125309794766\n",
      "Error on this batch = 0.7593073528692518\n",
      "Error on this batch = 1.050785765128238\n",
      "Cost on val dataset after 58 epochs is = 1.0322248770084088\n",
      "Initial Cost on Val dataset for this epoch 58 = 1.0322248770084088\n",
      "learning rate for this epoch =  0.036236229503043296\n",
      "Error on this batch = 0.7585730244935783\n",
      "Error on this batch = 1.049072012904364\n",
      "Cost on val dataset after 59 epochs is = 1.0318107302586341\n",
      "Initial Cost on Val dataset for this epoch 59 = 1.0318107302586341\n",
      "learning rate for this epoch =  0.036081700483877405\n",
      "Error on this batch = 0.7573126379534177\n",
      "Error on this batch = 1.0471705541612089\n",
      "Cost on val dataset after 60 epochs is = 1.031524899182814\n",
      "Initial Cost on Val dataset for this epoch 60 = 1.031524899182814\n",
      "learning rate for this epoch =  0.03593041119630843\n",
      "Error on this batch = 0.7560998604781579\n",
      "Error on this batch = 1.0457146110527575\n",
      "Cost on val dataset after 61 epochs is = 1.0310694227757855\n",
      "Initial Cost on Val dataset for this epoch 61 = 1.0310694227757855\n",
      "learning rate for this epoch =  0.03578224139610262\n",
      "Error on this batch = 0.7550080179292911\n",
      "Error on this batch = 1.0446510721889513\n",
      "Cost on val dataset after 62 epochs is = 1.0308060489378477\n",
      "Initial Cost on Val dataset for this epoch 62 = 1.0308060489378477\n",
      "learning rate for this epoch =  0.03563707718096288\n",
      "Error on this batch = 0.7540290949818318\n",
      "Error on this batch = 1.0436034008841433\n",
      "Cost on val dataset after 63 epochs is = 1.0305380984918793\n",
      "Initial Cost on Val dataset for this epoch 63 = 1.0305380984918793\n",
      "learning rate for this epoch =  0.03549481056010053\n",
      "Error on this batch = 0.753282669621821\n",
      "Error on this batch = 1.042814613063983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on val dataset after 64 epochs is = 1.0301677233153075\n",
      "Initial Cost on Val dataset for this epoch 64 = 1.0301677233153075\n",
      "learning rate for this epoch =  0.035355339059327376\n",
      "Error on this batch = 0.7525712346792088\n",
      "Error on this batch = 1.0414964167249556\n",
      "Cost on val dataset after 65 epochs is = 1.0298608329336356\n",
      "Initial Cost on Val dataset for this epoch 65 = 1.0298608329336356\n",
      "learning rate for this epoch =  0.03521856535823236\n",
      "Error on this batch = 0.751912065722261\n",
      "Error on this batch = 1.040432927302541\n",
      "Cost on val dataset after 66 epochs is = 1.0297112432127649\n",
      "Initial Cost on Val dataset for this epoch 66 = 1.0297112432127649\n",
      "learning rate for this epoch =  0.035084396956386855\n",
      "Error on this batch = 0.7513912564437039\n",
      "Error on this batch = 1.0396461140236573\n",
      "Cost on val dataset after 67 epochs is = 1.0295036157879063\n",
      "Initial Cost on Val dataset for this epoch 67 = 1.0295036157879063\n",
      "learning rate for this epoch =  0.034952745865855124\n",
      "Error on this batch = 0.7506656190064607\n",
      "Error on this batch = 1.0377075948620607\n",
      "Cost on val dataset after 68 epochs is = 1.0293529406078088\n",
      "Initial Cost on Val dataset for this epoch 68 = 1.0293529406078088\n",
      "learning rate for this epoch =  0.03482352832757854\n",
      "Error on this batch = 0.7499521351931696\n",
      "Error on this batch = 1.0367119940084957\n",
      "Cost on val dataset after 69 epochs is = 1.0293108871294305\n",
      "Initial Cost on Val dataset for this epoch 69 = 1.0293108871294305\n",
      "learning rate for this epoch =  0.034696664549459105\n",
      "Error on this batch = 0.7487609705449094\n",
      "Error on this batch = 1.0365688648245202\n",
      "Cost on val dataset after 70 epochs is = 1.0293492713941812\n",
      "Initial Cost on Val dataset for this epoch 70 = 1.0293492713941812\n",
      "learning rate for this epoch =  0.034572078464194106\n",
      "Error on this batch = 0.7474779619114932\n",
      "Error on this batch = 1.0363433283742955\n",
      "Cost on val dataset after 71 epochs is = 1.0295276437477614\n",
      "Initial Cost on Val dataset for this epoch 71 = 1.0295276437477614\n",
      "learning rate for this epoch =  0.03444969750511394\n",
      "Error on this batch = 0.7455343652788995\n",
      "Error on this batch = 1.0357485865328935\n",
      "Cost on val dataset after 72 epochs is = 1.0297330437153411\n",
      "Initial Cost on Val dataset for this epoch 72 = 1.0297330437153411\n",
      "learning rate for this epoch =  0.03432945239845196\n",
      "Error on this batch = 0.7435538098330864\n",
      "Error on this batch = 1.0357569664645936\n",
      "Cost on val dataset after 73 epochs is = 1.030074270643092\n",
      "Initial Cost on Val dataset for this epoch 73 = 1.030074270643092\n",
      "learning rate for this epoch =  0.03421127697063215\n",
      "Error on this batch = 0.7426886099407493\n",
      "Error on this batch = 1.0359663459558468\n",
      "Cost on val dataset after 74 epochs is = 1.03024358926659\n",
      "Initial Cost on Val dataset for this epoch 74 = 1.03024358926659\n",
      "learning rate for this epoch =  0.03409510796929954\n",
      "Error on this batch = 0.7409055597359462\n",
      "Error on this batch = 1.0360820827512434\n",
      "Cost on val dataset after 75 epochs is = 1.0303219465255073\n",
      "Initial Cost on Val dataset for this epoch 75 = 1.0303219465255073\n",
      "learning rate for this epoch =  0.033980884896942454\n",
      "Error on this batch = 0.739160724620628\n",
      "Error on this batch = 1.0348105899015319\n",
      "Cost on val dataset after 76 epochs is = 1.0303724432650128\n",
      "Initial Cost on Val dataset for this epoch 76 = 1.0303724432650128\n",
      "learning rate for this epoch =  0.033868549856065716\n",
      "Error on this batch = 0.737236994192969\n",
      "Error on this batch = 1.0341623466155536\n",
      "Cost on val dataset after 77 epochs is = 1.0305708118322778\n",
      "Initial Cost on Val dataset for this epoch 77 = 1.0305708118322778\n",
      "learning rate for this epoch =  0.03375804740497264\n",
      "Error on this batch = 0.7353157712802986\n",
      "Error on this batch = 1.0338951895943727\n",
      "Cost on val dataset after 78 epochs is = 1.0310548419278427\n",
      "Initial Cost on Val dataset for this epoch 78 = 1.0310548419278427\n",
      "learning rate for this epoch =  0.03364932442330151\n",
      "Error on this batch = 0.7335667178308038\n",
      "Error on this batch = 1.033884415185547\n",
      "Cost on val dataset after 79 epochs is = 1.0311947513106106\n",
      "cost initial= 1.0310548419278427 , cost final=1.0311947513106106 , change in cost= 0.00013990938276786657\n",
      "\n",
      "------------------------------------------------------------------------------\n",
      "The stats for number of units in the hidden layer = [100, 100] with ReLU are as below:\n",
      "------------------------------------------------------------------------------\n",
      "The number of epochs with ReLU is = 79\n",
      "The training time with ReLU is = 16.790sec\n",
      "The training accuracy with ReLU is = 88.661%\n",
      "The validation accuracy with ReLU is = 85.436%\n",
      "The test accuracy with ReLU is = 83.385%\n",
      "------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "arch=[100, 100]\n",
    "lr=0.1\n",
    "theta = theta_init(n, r, arch, 'normal')\n",
    "print(\"Training the network with {} hidden layer with {} units\".format(len(arch), arch))\n",
    "print(\"The parameters of the layers are of the shape:\")\n",
    "\n",
    "for j in range(len(theta)):\n",
    "    print(\"theta between layer {} and layer {} is {}\".format(j, j+1,theta[j].shape))\n",
    "    \n",
    "start = time.time()\n",
    "epoch, theta = training(mini_batch, X_valid, valid_class_enc, theta, lr, 'relu', 'adaptive', 'entropy')\n",
    "\n",
    "epochs.append(epoch)\n",
    "train_time.append(time.time()-start)\n",
    "train_accuracy.append(calc_accuracy(X_train, theta, train_class_enc, 'relu'))\n",
    "valid_accuracy.append(calc_accuracy(X_valid, theta, valid_class_enc, 'relu'))\n",
    "test_accuracy.append(calc_accuracy(X_test, theta, test_actual_class_enc, 'relu'))\n",
    "\n",
    "print(\"\\n------------------------------------------------------------------------------\")\n",
    "print(\"The stats for number of units in the hidden layer = {} with ReLU are as below:\".format(arch))\n",
    "print(\"------------------------------------------------------------------------------\")\n",
    "print(\"The number of epochs with ReLU is = {}\".format(epochs[-1]))\n",
    "print(\"The training time with ReLU is = {:2.3f}sec\".format(train_time[-1]))\n",
    "print(\"The training accuracy with ReLU is = {:2.3f}%\".format(train_accuracy[-1]))\n",
    "print(\"The validation accuracy with ReLU is = {:2.3f}%\".format(valid_accuracy[-1]))\n",
    "print(\"The test accuracy with ReLU is = {:2.3f}%\".format(test_accuracy[-1]))\n",
    "print(\"------------------------------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------Running Part F-----------------------------------------------------\n",
      "------------------Training a 100x100 hidden layer network with Softplus activation and Cross Entropy------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"----------------------------Running Part F-----------------------------------------------------\")\n",
    "print(\"------------------Training a 100x100 hidden layer network with Softplus activation and Cross Entropy------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the network with 2 hidden layer with [100, 100] units\n",
      "The parameters of the layers are of the shape:\n",
      "theta between layer 0 and layer 1 is (785, 100)\n",
      "theta between layer 1 and layer 2 is (101, 100)\n",
      "theta between layer 2 and layer 3 is (101, 26)\n",
      "Initial Cost on Val dataset for this epoch 1 = 19.81503930292858\n",
      "learning rate for this epoch =  0.1\n",
      "Error on this batch = 19.794280492670666\n",
      "Error on this batch = 4.175317200313914\n",
      "Cost on val dataset after 2 epochs is = 4.024354564890312\n",
      "Initial Cost on Val dataset for this epoch 2 = 4.024354564890312\n",
      "learning rate for this epoch =  0.08408964152537146\n",
      "Error on this batch = 4.051570739139761\n",
      "Error on this batch = 3.4029525993912\n",
      "Cost on val dataset after 3 epochs is = 2.9019077904745707\n",
      "Initial Cost on Val dataset for this epoch 3 = 2.9019077904745707\n",
      "learning rate for this epoch =  0.07598356856515927\n",
      "Error on this batch = 2.939627005348293\n",
      "Error on this batch = 2.645088093797996\n",
      "Cost on val dataset after 4 epochs is = 2.180768819383366\n",
      "Initial Cost on Val dataset for this epoch 4 = 2.180768819383366\n",
      "learning rate for this epoch =  0.07071067811865475\n",
      "Error on this batch = 2.183160605921156\n",
      "Error on this batch = 2.2109997141826585\n",
      "Cost on val dataset after 5 epochs is = 1.8151391120957396\n",
      "Initial Cost on Val dataset for this epoch 5 = 1.8151391120957396\n",
      "learning rate for this epoch =  0.0668740304976422\n",
      "Error on this batch = 1.7056436571446687\n",
      "Error on this batch = 1.9419549862038767\n",
      "Cost on val dataset after 6 epochs is = 1.5947659128193645\n",
      "Initial Cost on Val dataset for this epoch 6 = 1.5947659128193645\n",
      "learning rate for this epoch =  0.06389431042462725\n",
      "Error on this batch = 1.4186687270400646\n",
      "Error on this batch = 1.757283772649388\n",
      "Cost on val dataset after 7 epochs is = 1.4608112218876002\n",
      "Initial Cost on Val dataset for this epoch 7 = 1.4608112218876002\n",
      "learning rate for this epoch =  0.06147881529512644\n",
      "Error on this batch = 1.263012749531799\n",
      "Error on this batch = 1.6353944437397439\n",
      "Cost on val dataset after 8 epochs is = 1.3738471788646582\n",
      "Initial Cost on Val dataset for this epoch 8 = 1.3738471788646582\n",
      "learning rate for this epoch =  0.05946035575013606\n",
      "Error on this batch = 1.1671087519484054\n",
      "Error on this batch = 1.5402006186395398\n",
      "Cost on val dataset after 9 epochs is = 1.3086103301282008\n",
      "Initial Cost on Val dataset for this epoch 9 = 1.3086103301282008\n",
      "learning rate for this epoch =  0.05773502691896258\n",
      "Error on this batch = 1.0960765681666444\n",
      "Error on this batch = 1.4617006945685973\n",
      "Cost on val dataset after 10 epochs is = 1.2557603414382288\n",
      "Initial Cost on Val dataset for this epoch 10 = 1.2557603414382288\n",
      "learning rate for this epoch =  0.05623413251903491\n",
      "Error on this batch = 1.0387711111558608\n",
      "Error on this batch = 1.3978562776030845\n",
      "Cost on val dataset after 11 epochs is = 1.211612959325901\n",
      "Initial Cost on Val dataset for this epoch 11 = 1.211612959325901\n",
      "learning rate for this epoch =  0.05491004867761125\n",
      "Error on this batch = 0.9909991242506396\n",
      "Error on this batch = 1.3461805057436362\n",
      "Cost on val dataset after 12 epochs is = 1.1741266291400536\n",
      "Initial Cost on Val dataset for this epoch 12 = 1.1741266291400536\n",
      "learning rate for this epoch =  0.0537284965911771\n",
      "Error on this batch = 0.9504466209399789\n",
      "Error on this batch = 1.3035323999385344\n",
      "Cost on val dataset after 13 epochs is = 1.1418542492847117\n",
      "Initial Cost on Val dataset for this epoch 13 = 1.1418542492847117\n",
      "learning rate for this epoch =  0.052664038784792665\n",
      "Error on this batch = 0.9154033958052614\n",
      "Error on this batch = 1.2672033047914208\n",
      "Cost on val dataset after 14 epochs is = 1.113692245742254\n",
      "Initial Cost on Val dataset for this epoch 14 = 1.113692245742254\n",
      "learning rate for this epoch =  0.05169731539571706\n",
      "Error on this batch = 0.8844787509160691\n",
      "Error on this batch = 1.235290533610019\n",
      "Cost on val dataset after 15 epochs is = 1.0887824103037318\n",
      "Initial Cost on Val dataset for this epoch 15 = 1.0887824103037318\n",
      "learning rate for this epoch =  0.05081327481546148\n",
      "Error on this batch = 0.8565550235668147\n",
      "Error on this batch = 1.206577446312441\n",
      "Cost on val dataset after 16 epochs is = 1.0664662405637126\n",
      "Initial Cost on Val dataset for this epoch 16 = 1.0664662405637126\n",
      "learning rate for this epoch =  0.05\n",
      "Error on this batch = 0.8308115137541748\n",
      "Error on this batch = 1.180313514476632\n",
      "Cost on val dataset after 17 epochs is = 1.0462541879992329\n",
      "Initial Cost on Val dataset for this epoch 17 = 1.0462541879992329\n",
      "learning rate for this epoch =  0.049247906050545236\n",
      "Error on this batch = 0.8067263050640163\n",
      "Error on this batch = 1.1560307611646456\n",
      "Cost on val dataset after 18 epochs is = 1.027784838273554\n",
      "Initial Cost on Val dataset for this epoch 18 = 1.027784838273554\n",
      "learning rate for this epoch =  0.048549177170732344\n",
      "Error on this batch = 0.7840134212094205\n",
      "Error on this batch = 1.1334192423186769\n",
      "Cost on val dataset after 19 epochs is = 1.0107816739363182\n",
      "Initial Cost on Val dataset for this epoch 19 = 1.0107816739363182\n",
      "learning rate for this epoch =  0.04789736254435747\n",
      "Error on this batch = 0.7625255289054397\n",
      "Error on this batch = 1.1122536813542225\n",
      "Cost on val dataset after 20 epochs is = 0.9950240257989524\n",
      "Initial Cost on Val dataset for this epoch 20 = 0.9950240257989524\n",
      "learning rate for this epoch =  0.047287080450158794\n",
      "Error on this batch = 0.7421750718557846\n",
      "Error on this batch = 1.092355650003341\n",
      "Cost on val dataset after 21 epochs is = 0.9803330227766361\n",
      "Initial Cost on Val dataset for this epoch 21 = 0.9803330227766361\n",
      "learning rate for this epoch =  0.04671379777282001\n",
      "Error on this batch = 0.7228935777816361\n",
      "Error on this batch = 1.0735764527891596\n",
      "Cost on val dataset after 22 epochs is = 0.9665640748050499\n",
      "Initial Cost on Val dataset for this epoch 22 = 0.9665640748050499\n",
      "learning rate for this epoch =  0.04617366309441026\n",
      "Error on this batch = 0.7046174283095725\n",
      "Error on this batch = 1.055790052045333\n",
      "Cost on val dataset after 23 epochs is = 0.9536006640829484\n",
      "Initial Cost on Val dataset for this epoch 23 = 0.9536006640829484\n",
      "learning rate for this epoch =  0.04566337854967313\n",
      "Error on this batch = 0.6872845040710616\n",
      "Error on this batch = 1.0388898226474519\n",
      "Cost on val dataset after 24 epochs is = 0.9413485950739896\n",
      "Initial Cost on Val dataset for this epoch 24 = 0.9413485950739896\n",
      "learning rate for this epoch =  0.04518010018049225\n",
      "Error on this batch = 0.6708338538456735\n",
      "Error on this batch = 1.0227862096932843\n",
      "Cost on val dataset after 25 epochs is = 0.9297311174517937\n",
      "Initial Cost on Val dataset for this epoch 25 = 0.9297311174517937\n",
      "learning rate for this epoch =  0.044721359549995794\n",
      "Error on this batch = 0.6552060468642907\n",
      "Error on this batch = 1.0074042838588322\n",
      "Cost on val dataset after 26 epochs is = 0.9186851141159775\n",
      "Initial Cost on Val dataset for this epoch 26 = 0.9186851141159775\n",
      "learning rate for this epoch =  0.04428500142691474\n",
      "Error on this batch = 0.6403437477495788\n",
      "Error on this batch = 0.9926811052123786\n",
      "Cost on val dataset after 27 epochs is = 0.9081582341754629\n",
      "Initial Cost on Val dataset for this epoch 27 = 0.9081582341754629\n",
      "learning rate for this epoch =  0.043869133765083085\n",
      "Error on this batch = 0.6261923742468564\n",
      "Error on this batch = 0.9785631253474005\n",
      "Cost on val dataset after 28 epochs is = 0.8981067422748793\n",
      "Initial Cost on Val dataset for this epoch 28 = 0.8981067422748793\n",
      "learning rate for this epoch =  0.043472087194499145\n",
      "Error on this batch = 0.6127006932788553\n",
      "Error on this batch = 0.9650038766386039\n",
      "Cost on val dataset after 29 epochs is = 0.8884938724352862\n",
      "Initial Cost on Val dataset for this epoch 29 = 0.8884938724352862\n",
      "learning rate for this epoch =  0.04309238194589061\n",
      "Error on this batch = 0.5998212337018817\n",
      "Error on this batch = 0.95196210235771\n",
      "Cost on val dataset after 30 epochs is = 0.8792885295932316\n",
      "Initial Cost on Val dataset for this epoch 30 = 0.8792885295932316\n",
      "learning rate for this epoch =  0.042728700639623404\n",
      "Error on this batch = 0.5875104630310596\n",
      "Error on this batch = 0.9394003729403363\n",
      "Cost on val dataset after 31 epochs is = 0.8704642353409163\n",
      "Initial Cost on Val dataset for this epoch 31 = 0.8704642353409163\n",
      "learning rate for this epoch =  0.04237986574150217\n",
      "Error on this batch = 0.5757287447451864\n",
      "Error on this batch = 0.9272841522164496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost on val dataset after 32 epochs is = 0.8619982543747718\n",
      "Initial Cost on Val dataset for this epoch 32 = 0.8619982543747718\n",
      "learning rate for this epoch =  0.04204482076268573\n",
      "Error on this batch = 0.564440134197126\n",
      "Error on this batch = 0.9155812301188323\n",
      "Cost on val dataset after 33 epochs is = 0.8538708642457267\n",
      "Initial Cost on Val dataset for this epoch 33 = 0.8538708642457267\n",
      "learning rate for this epoch =  0.04172261448611506\n",
      "Error on this batch = 0.5536120816443589\n",
      "Error on this batch = 0.9042614203676581\n",
      "Cost on val dataset after 34 epochs is = 0.8460647459155728\n",
      "Initial Cost on Val dataset for this epoch 34 = 0.8460647459155728\n",
      "learning rate for this epoch =  0.041412387656655204\n",
      "Error on this batch = 0.5432151008384287\n",
      "Error on this batch = 0.8932964255127908\n",
      "Cost on val dataset after 35 epochs is = 0.8385644798365265\n",
      "Initial Cost on Val dataset for this epoch 35 = 0.8385644798365265\n",
      "learning rate for this epoch =  0.04111336169005197\n",
      "Error on this batch = 0.5332224432901284\n",
      "Error on this batch = 0.882659789589944\n",
      "Cost on val dataset after 36 epochs is = 0.8313561350415861\n",
      "Initial Cost on Val dataset for this epoch 36 = 0.8313561350415861\n",
      "learning rate for this epoch =  0.040824829046386304\n",
      "Error on this batch = 0.5236098003524023\n",
      "Error on this batch = 0.8723268824104956\n",
      "Cost on val dataset after 37 epochs is = 0.8244269395728652\n",
      "Initial Cost on Val dataset for this epoch 37 = 0.8244269395728652\n",
      "learning rate for this epoch =  0.040546144983876986\n",
      "Error on this batch = 0.5143550414971988\n",
      "Error on this batch = 0.8622748822480821\n",
      "Cost on val dataset after 38 epochs is = 0.8177650210130687\n",
      "Initial Cost on Val dataset for this epoch 38 = 0.8177650210130687\n",
      "learning rate for this epoch =  0.04027672046365773\n",
      "Error on this batch = 0.5054379881323555\n",
      "Error on this batch = 0.8524827412963037\n",
      "Cost on val dataset after 39 epochs is = 0.8113592065707516\n",
      "Initial Cost on Val dataset for this epoch 39 = 0.8113592065707516\n",
      "learning rate for this epoch =  0.040016016019225\n",
      "Error on this batch = 0.49684021709719867\n",
      "Error on this batch = 0.8429311297661312\n",
      "Cost on val dataset after 40 epochs is = 0.8051988731899224\n",
      "Initial Cost on Val dataset for this epoch 40 = 0.8051988731899224\n",
      "learning rate for this epoch =  0.03976353643835253\n",
      "Error on this batch = 0.4885448855557945\n",
      "Error on this batch = 0.8336023609329187\n",
      "Cost on val dataset after 41 epochs is = 0.7992738393567818\n",
      "Initial Cost on Val dataset for this epoch 41 = 0.7992738393567818\n",
      "learning rate for this epoch =  0.03951882613244048\n",
      "Error on this batch = 0.48053656870611927\n",
      "Error on this batch = 0.8244803024714236\n",
      "Cost on val dataset after 42 epochs is = 0.7935742914699111\n",
      "Initial Cost on Val dataset for this epoch 42 = 0.7935742914699111\n",
      "learning rate for this epoch =  0.0392814650900513\n",
      "Error on this batch = 0.4728011031628492\n",
      "Error on this batch = 0.815550280324945\n",
      "Cost on val dataset after 43 epochs is = 0.7880907386718171\n",
      "Initial Cost on Val dataset for this epoch 43 = 0.7880907386718171\n",
      "learning rate for this epoch =  0.03905106532895161\n",
      "Error on this batch = 0.46532543169521995\n",
      "Error on this batch = 0.8067989809225796\n",
      "Cost on val dataset after 44 epochs is = 0.7828139908061222\n",
      "Initial Cost on Val dataset for this epoch 44 = 0.7828139908061222\n",
      "learning rate for this epoch =  0.03882726777522233\n",
      "Error on this batch = 0.4580974486681244\n",
      "Error on this batch = 0.798214356312802\n",
      "Cost on val dataset after 45 epochs is = 0.7777351546131134\n",
      "Initial Cost on Val dataset for this epoch 45 = 0.7777351546131134\n",
      "learning rate for this epoch =  0.03860973950960897\n",
      "Error on this batch = 0.45110584926161656\n",
      "Error on this batch = 0.7897855352231801\n",
      "Cost on val dataset after 46 epochs is = 0.7728456434169734\n",
      "Initial Cost on Val dataset for this epoch 46 = 0.7728456434169734\n",
      "learning rate for this epoch =  0.0383981713307935\n",
      "Error on this batch = 0.4443399884211794\n",
      "Error on this batch = 0.7815027416392137\n",
      "Cost on val dataset after 47 epochs is = 0.7681371954833152\n",
      "Initial Cost on Val dataset for this epoch 47 = 0.7681371954833152\n",
      "learning rate for this epoch =  0.03819227559309534\n",
      "Error on this batch = 0.4377897567384438\n",
      "Error on this batch = 0.7733572214392944\n",
      "Cost on val dataset after 48 epochs is = 0.7636018961152244\n",
      "Initial Cost on Val dataset for this epoch 48 = 0.7636018961152244\n",
      "learning rate for this epoch =  0.037991784282579634\n",
      "Error on this batch = 0.43144547972738745\n",
      "Error on this batch = 0.7653411768162613\n",
      "Cost on val dataset after 49 epochs is = 0.7592321986487192\n",
      "Initial Cost on Val dataset for this epoch 49 = 0.7592321986487192\n",
      "learning rate for this epoch =  0.03779644730092272\n",
      "Error on this batch = 0.4252978444911974\n",
      "Error on this batch = 0.7574477073775208\n",
      "Cost on val dataset after 50 epochs is = 0.7550209400382458\n",
      "Initial Cost on Val dataset for this epoch 50 = 0.7550209400382458\n",
      "learning rate for this epoch =  0.03760603093086394\n",
      "Error on this batch = 0.4193378543381791\n",
      "Error on this batch = 0.7496707558116427\n",
      "Cost on val dataset after 51 epochs is = 0.7509613478280944\n",
      "Initial Cost on Val dataset for this epoch 51 = 0.7509613478280944\n",
      "learning rate for this epoch =  0.03742031646082125\n",
      "Error on this batch = 0.4135568084934101\n",
      "Error on this batch = 0.7420050550569313\n",
      "Cost on val dataset after 52 epochs is = 0.747047036939446\n",
      "Initial Cost on Val dataset for this epoch 52 = 0.747047036939446\n",
      "learning rate for this epoch =  0.03723909894939824\n",
      "Error on this batch = 0.40794630152093336\n",
      "Error on this batch = 0.7344460734782431\n",
      "Cost on val dataset after 53 epochs is = 0.7432719965903717\n",
      "Initial Cost on Val dataset for this epoch 53 = 0.7432719965903717\n",
      "learning rate for this epoch =  0.037062186113165134\n",
      "Error on this batch = 0.40249823587668987\n",
      "Error on this batch = 0.7269899550462182\n",
      "Cost on val dataset after 54 epochs is = 0.73963056936663\n",
      "Initial Cost on Val dataset for this epoch 54 = 0.73963056936663\n",
      "learning rate for this epoch =  0.03688939732334406\n",
      "Error on this batch = 0.39720484114238674\n",
      "Error on this batch = 0.7196334529239737\n",
      "Cost on val dataset after 55 epochs is = 0.7361174255400672\n",
      "Initial Cost on Val dataset for this epoch 55 = 0.7361174255400672\n",
      "learning rate for this epoch =  0.03672056269893592\n",
      "Error on this batch = 0.3920586945684902\n",
      "Error on this batch = 0.7123738567642736\n",
      "Cost on val dataset after 56 epochs is = 0.732727535962796\n",
      "Initial Cost on Val dataset for this epoch 56 = 0.732727535962796\n",
      "learning rate for this epoch =  0.03655552228545124\n",
      "Error on this batch = 0.3870527390548233\n",
      "Error on this batch = 0.7052089157627738\n",
      "Cost on val dataset after 57 epochs is = 0.7294561463322637\n",
      "Initial Cost on Val dataset for this epoch 57 = 0.7294561463322637\n",
      "learning rate for this epoch =  0.036394125309794766\n",
      "Error on this batch = 0.3821802961627724\n",
      "Error on this batch = 0.6981367605926966\n",
      "Cost on val dataset after 58 epochs is = 0.7262987546493297\n",
      "Initial Cost on Val dataset for this epoch 58 = 0.7262987546493297\n",
      "learning rate for this epoch =  0.036236229503043296\n",
      "Error on this batch = 0.3774350729229525\n",
      "Error on this batch = 0.6911558276020116\n",
      "Cost on val dataset after 59 epochs is = 0.7232510926626842\n",
      "Initial Cost on Val dataset for this epoch 59 = 0.7232510926626842\n",
      "learning rate for this epoch =  0.036081700483877405\n",
      "Error on this batch = 0.37281116201639747\n",
      "Error on this batch = 0.6842647882490406\n",
      "Cost on val dataset after 60 epochs is = 0.720309111283709\n",
      "Initial Cost on Val dataset for this epoch 60 = 0.720309111283709\n",
      "learning rate for this epoch =  0.03593041119630843\n",
      "Error on this batch = 0.36830303542911735\n",
      "Error on this batch = 0.6774624859968607\n",
      "Cost on val dataset after 61 epochs is = 0.7174689694660624\n",
      "Initial Cost on Val dataset for this epoch 61 = 0.7174689694660624\n",
      "learning rate for this epoch =  0.03578224139610262\n",
      "Error on this batch = 0.363905532005148\n",
      "Error on this batch = 0.6707478820588517\n",
      "Cost on val dataset after 62 epochs is = 0.7147270258333436\n",
      "Initial Cost on Val dataset for this epoch 62 = 0.7147270258333436\n",
      "learning rate for this epoch =  0.03563707718096288\n",
      "Error on this batch = 0.3596138395267414\n",
      "Error on this batch = 0.6641200106472736\n",
      "Cost on val dataset after 63 epochs is = 0.7120798323040333\n",
      "Initial Cost on Val dataset for this epoch 63 = 0.7120798323040333\n",
      "learning rate for this epoch =  0.03549481056010053\n",
      "Error on this batch = 0.3554234720745421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on this batch = 0.6575779437753928\n",
      "Cost on val dataset after 64 epochs is = 0.7095241290087071\n",
      "Initial Cost on Val dataset for this epoch 64 = 0.7095241290087071\n",
      "learning rate for this epoch =  0.035355339059327376\n",
      "Error on this batch = 0.35133024348788433\n",
      "Error on this batch = 0.651120765194832\n",
      "Cost on val dataset after 65 epochs is = 0.7070568398602558\n",
      "Initial Cost on Val dataset for this epoch 65 = 0.7070568398602558\n",
      "learning rate for this epoch =  0.03521856535823236\n",
      "Error on this batch = 0.3473302377733606\n",
      "Error on this batch = 0.6447475526927045\n",
      "Cost on val dataset after 66 epochs is = 0.7046750682018554\n",
      "Initial Cost on Val dataset for this epoch 66 = 0.7046750682018554\n",
      "learning rate for this epoch =  0.035084396956386855\n",
      "Error on this batch = 0.34341977731816914\n",
      "Error on this batch = 0.6384573677114791\n",
      "Cost on val dataset after 67 epochs is = 0.7023760920229639\n",
      "Initial Cost on Val dataset for this epoch 67 = 0.7023760920229639\n",
      "learning rate for this epoch =  0.034952745865855124\n",
      "Error on this batch = 0.33959538977407905\n",
      "Error on this batch = 0.6322492510770363\n",
      "Cost on val dataset after 68 epochs is = 0.7001573583132087\n",
      "Initial Cost on Val dataset for this epoch 68 = 0.7001573583132087\n",
      "learning rate for this epoch =  0.03482352832757854\n",
      "Error on this batch = 0.335853774504486\n",
      "Error on this batch = 0.6261222235146665\n",
      "Cost on val dataset after 69 epochs is = 0.6980164762280278\n",
      "Initial Cost on Val dataset for this epoch 69 = 0.6980164762280278\n",
      "learning rate for this epoch =  0.034696664549459105\n",
      "Error on this batch = 0.3321917695376546\n",
      "Error on this batch = 0.6200752895830152\n",
      "Cost on val dataset after 70 epochs is = 0.6959512088706691\n",
      "Initial Cost on Val dataset for this epoch 70 = 0.6959512088706691\n",
      "learning rate for this epoch =  0.034572078464194106\n",
      "Error on this batch = 0.32860632003825246\n",
      "Error on this batch = 0.6141074436458784\n",
      "Cost on val dataset after 71 epochs is = 0.6939594636458822\n",
      "Initial Cost on Val dataset for this epoch 71 = 0.6939594636458822\n",
      "learning rate for this epoch =  0.03444969750511394\n",
      "Error on this batch = 0.32509444937816573\n",
      "Error on this batch = 0.6082176765219431\n",
      "Cost on val dataset after 72 epochs is = 0.6920392812977493\n",
      "Initial Cost on Val dataset for this epoch 72 = 0.6920392812977493\n",
      "learning rate for this epoch =  0.03432945239845196\n",
      "Error on this batch = 0.3216532339271841\n",
      "Error on this batch = 0.6024049815075343\n",
      "Cost on val dataset after 73 epochs is = 0.6901888238906301\n",
      "Initial Cost on Val dataset for this epoch 73 = 0.6901888238906301\n",
      "learning rate for this epoch =  0.03421127697063215\n",
      "Error on this batch = 0.318279782658463\n",
      "Error on this batch = 0.5966683585777134\n",
      "Cost on val dataset after 74 epochs is = 0.6884063621117599\n",
      "Initial Cost on Val dataset for this epoch 74 = 0.6884063621117599\n",
      "learning rate for this epoch =  0.03409510796929954\n",
      "Error on this batch = 0.3149712225369282\n",
      "Error on this batch = 0.591006815767114\n",
      "Cost on val dataset after 75 epochs is = 0.6866902623538139\n",
      "Initial Cost on Val dataset for this epoch 75 = 0.6866902623538139\n",
      "learning rate for this epoch =  0.033980884896942454\n",
      "Error on this batch = 0.3117246904057037\n",
      "Error on this batch = 0.5854193670387337\n",
      "Cost on val dataset after 76 epochs is = 0.6850389740685472\n",
      "Initial Cost on Val dataset for this epoch 76 = 0.6850389740685472\n",
      "learning rate for this epoch =  0.033868549856065716\n",
      "Error on this batch = 0.30853733170379516\n",
      "Error on this batch = 0.5799050263675006\n",
      "Cost on val dataset after 77 epochs is = 0.6834510178678594\n",
      "Initial Cost on Val dataset for this epoch 77 = 0.6834510178678594\n",
      "learning rate for this epoch =  0.03375804740497264\n",
      "Error on this batch = 0.30540630586754386\n",
      "Error on this batch = 0.5744627982595009\n",
      "Cost on val dataset after 78 epochs is = 0.6819249747918824\n",
      "Initial Cost on Val dataset for this epoch 78 = 0.6819249747918824\n",
      "learning rate for this epoch =  0.03364932442330151\n",
      "Error on this batch = 0.3023287977503097\n",
      "Error on this batch = 0.5690916654263328\n",
      "Cost on val dataset after 79 epochs is = 0.6804594770752812\n",
      "Initial Cost on Val dataset for this epoch 79 = 0.6804594770752812\n",
      "learning rate for this epoch =  0.03354232998654124\n",
      "Error on this batch = 0.29930203391824317\n",
      "Error on this batch = 0.5637905747517188\n",
      "Cost on val dataset after 80 epochs is = 0.679053200634969\n",
      "Initial Cost on Val dataset for this epoch 80 = 0.679053200634969\n",
      "learning rate for this epoch =  0.0334370152488211\n",
      "Error on this batch = 0.2963233023164036\n",
      "Error on this batch = 0.5585584229529326\n",
      "Cost on val dataset after 81 epochs is = 0.6777048593858446\n",
      "Initial Cost on Val dataset for this epoch 81 = 0.6777048593858446\n",
      "learning rate for this epoch =  0.03333333333333333\n",
      "Error on this batch = 0.29338997359161306\n",
      "Error on this batch = 0.5533940434178725\n",
      "Cost on val dataset after 82 epochs is = 0.6764132013736177\n",
      "Initial Cost on Val dataset for this epoch 82 = 0.6764132013736177\n",
      "learning rate for this epoch =  0.03323123922980402\n",
      "Error on this batch = 0.29049952231339654\n",
      "Error on this batch = 0.548296195594857\n",
      "Cost on val dataset after 83 epochs is = 0.6751770065998076\n",
      "Initial Cost on Val dataset for this epoch 83 = 0.6751770065998076\n",
      "learning rate for this epoch =  0.033130689698479016\n",
      "Error on this batch = 0.2876495464358215\n",
      "Error on this batch = 0.5432635580580952\n",
      "Cost on val dataset after 84 epochs is = 0.6739950863070576\n",
      "Initial Cost on Val dataset for this epoch 84 = 0.6739950863070576\n",
      "learning rate for this epoch =  0.03303164318013808\n",
      "Error on this batch = 0.28483778357160966\n",
      "Error on this batch = 0.5382947260073537\n",
      "Cost on val dataset after 85 epochs is = 0.6728662833987044\n",
      "Initial Cost on Val dataset for this epoch 85 = 0.6728662833987044\n",
      "learning rate for this epoch =  0.032934059711691804\n",
      "Error on this batch = 0.2820621229951083\n",
      "Error on this batch = 0.5333882135241713\n",
      "Cost on val dataset after 86 epochs is = 0.6717894735947594\n",
      "Initial Cost on Val dataset for this epoch 86 = 0.6717894735947594\n",
      "learning rate for this epoch =  0.03283790084695403\n",
      "Error on this batch = 0.2793206127443727\n",
      "Error on this batch = 0.5285424604394556\n",
      "Cost on val dataset after 87 epochs is = 0.6707635668896909\n",
      "Initial Cost on Val dataset for this epoch 87 = 0.6707635668896909\n",
      "learning rate for this epoch =  0.0327431295822161\n",
      "Error on this batch = 0.2766114617278695\n",
      "Error on this batch = 0.5237558432186309\n",
      "Cost on val dataset after 88 epochs is = 0.6697875088868642\n",
      "Initial Cost on Val dataset for this epoch 88 = 0.6697875088868642\n",
      "learning rate for this epoch =  0.032649710286280526\n",
      "Error on this batch = 0.2739330372948091\n",
      "Error on this batch = 0.5190266889018003\n",
      "Cost on val dataset after 89 epochs is = 0.6688602816444292\n",
      "Initial Cost on Val dataset for this epoch 89 = 0.6688602816444292\n",
      "learning rate for this epoch =  0.03255760863463962\n",
      "Error on this batch = 0.2712838592036177\n",
      "Error on this batch = 0.5143532909067196\n",
      "Cost on val dataset after 90 epochs is = 0.6679809037712128\n",
      "Initial Cost on Val dataset for this epoch 90 = 0.6679809037712128\n",
      "learning rate for this epoch =  0.03246679154750989\n",
      "Error on this batch = 0.2686625912199312\n",
      "Error on this batch = 0.5097339254479143\n",
      "Cost on val dataset after 91 epochs is = 0.6671484296417182\n",
      "Initial Cost on Val dataset for this epoch 91 = 0.6671484296417182\n",
      "learning rate for this epoch =  0.03237722713145643\n",
      "Error on this batch = 0.26606803162924264\n",
      "Error on this batch = 0.5051668674431565\n",
      "Cost on val dataset after 92 epochs is = 0.666361947733479\n",
      "Initial Cost on Val dataset for this epoch 92 = 0.666361947733479\n",
      "learning rate for this epoch =  0.032288884624362205\n",
      "Error on this batch = 0.2634991037604885\n",
      "Error on this batch = 0.500650405026113\n",
      "Cost on val dataset after 93 epochs is = 0.6656205782054536\n",
      "Initial Cost on Val dataset for this epoch 93 = 0.6656205782054536\n",
      "learning rate for this epoch =  0.03220173434351674\n",
      "Error on this batch = 0.2609548472524764\n",
      "Error on this batch = 0.4961828520945838\n",
      "Cost on val dataset after 94 epochs is = 0.6649234699167269\n",
      "Initial Cost on Val dataset for this epoch 94 = 0.6649234699167269\n",
      "learning rate for this epoch =  0.0321157476366158\n",
      "Error on this batch = 0.25843441036251596\n",
      "Error on this batch = 0.4917625586295195\n",
      "Cost on val dataset after 95 epochs is = 0.6642697971234827\n",
      "Initial Cost on Val dataset for this epoch 95 = 0.6642697971234827\n",
      "learning rate for this epoch =  0.03203089683547987\n",
      "Error on this batch = 0.2559370432253678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on this batch = 0.4873879187714366\n",
      "Cost on val dataset after 96 epochs is = 0.6636587560904479\n",
      "Initial Cost on Val dataset for this epoch 96 = 0.6636587560904479\n",
      "learning rate for this epoch =  0.03194715521231362\n",
      "Error on this batch = 0.25346209169954426\n",
      "Error on this batch = 0.48305737681519106\n",
      "Cost on val dataset after 97 epochs is = 0.6630895618182764\n",
      "Initial Cost on Val dataset for this epoch 97 = 0.6630895618182764\n",
      "learning rate for this epoch =  0.031864496938342195\n",
      "Error on this batch = 0.2510089913201687\n",
      "Error on this batch = 0.47876943138302963\n",
      "Cost on val dataset after 98 epochs is = 0.6625614450315347\n",
      "Initial Cost on Val dataset for this epoch 98 = 0.6625614450315347\n",
      "learning rate for this epoch =  0.031782897044671854\n",
      "Error on this batch = 0.24857726090304874\n",
      "Error on this batch = 0.4745226380764392\n",
      "Cost on val dataset after 99 epochs is = 0.6620736495047799\n",
      "Initial Cost on Val dataset for this epoch 99 = 0.6620736495047799\n",
      "learning rate for this epoch =  0.0317023313852343\n",
      "Error on this batch = 0.24616649547403302\n",
      "Error on this batch = 0.4703156109115726\n",
      "Cost on val dataset after 100 epochs is = 0.6616254297379166\n",
      "Initial Cost on Val dataset for this epoch 100 = 0.6616254297379166\n",
      "learning rate for this epoch =  0.03162277660168379\n",
      "Error on this batch = 0.24377635838001369\n",
      "Error on this batch = 0.46614702282997655\n",
      "Cost on val dataset after 101 epochs is = 0.6612160489359875\n",
      "Initial Cost on Val dataset for this epoch 101 = 0.6612160489359875\n",
      "learning rate for this epoch =  0.03154421009012572\n",
      "Error on this batch = 0.24140657262453755\n",
      "Error on this batch = 0.46201560555810944\n",
      "Cost on val dataset after 102 epochs is = 0.6608447772097171\n",
      "Initial Cost on Val dataset for this epoch 102 = 0.6608447772097171\n",
      "learning rate for this epoch =  0.03146660996956416\n",
      "Error on this batch = 0.23905691162518905\n",
      "Error on this batch = 0.45792014907051026\n",
      "Cost on val dataset after 103 epochs is = 0.6605108898953872\n",
      "Initial Cost on Val dataset for this epoch 103 = 0.6605108898953872\n",
      "learning rate for this epoch =  0.03138995505196357\n",
      "Error on this batch = 0.2367271896906883\n",
      "Error on this batch = 0.4538595008918893\n",
      "Cost on val dataset after 104 epochs is = 0.660213665896556\n",
      "Initial Cost on Val dataset for this epoch 104 = 0.660213665896556\n",
      "learning rate for this epoch =  0.03131422481382735\n",
      "Error on this batch = 0.23441725255707052\n",
      "Error on this batch = 0.4498325654495985\n",
      "Cost on val dataset after 105 epochs is = 0.6599523859733016\n",
      "Initial Cost on Val dataset for this epoch 105 = 0.6599523859733016\n",
      "learning rate for this epoch =  0.03123939936920256\n",
      "Error on this batch = 0.23212696831043272\n",
      "Error on this batch = 0.4458383036564683\n",
      "Cost on val dataset after 106 epochs is = 0.6597263309421239\n",
      "Initial Cost on Val dataset for this epoch 106 = 0.6597263309421239\n",
      "learning rate for this epoch =  0.031165459444026558\n",
      "Error on this batch = 0.22985621897205719\n",
      "Error on this batch = 0.4418757328625806\n",
      "Cost on val dataset after 107 epochs is = 0.6595347797947124\n",
      "Initial Cost on Val dataset for this epoch 107 = 0.6595347797947124\n",
      "learning rate for this epoch =  0.03109238635173672\n",
      "Error on this batch = 0.2276048929469517\n",
      "Error on this batch = 0.43794392726289033\n",
      "Cost on val dataset after 108 epochs is = 0.6593770077891953\n",
      "Initial Cost on Val dataset for this epoch 108 = 0.6593770077891953\n",
      "learning rate for this epoch =  0.031020161970069987\n",
      "Error on this batch = 0.22537287845515067\n",
      "Error on this batch = 0.4340420187872627\n",
      "Cost on val dataset after 109 epochs is = 0.6592522846060409\n",
      "Initial Cost on Val dataset for this epoch 109 = 0.6592522846060409\n",
      "learning rate for this epoch =  0.030948768718983822\n",
      "Error on this batch = 0.22316005798987681\n",
      "Error on this batch = 0.430169198433845\n",
      "Cost on val dataset after 110 epochs is = 0.6591598726866676\n",
      "Initial Cost on Val dataset for this epoch 110 = 0.6591598726866676\n",
      "learning rate for this epoch =  0.030878189539634483\n",
      "Error on this batch = 0.2209663037871055\n",
      "Error on this batch = 0.42632471794095644\n",
      "Cost on val dataset after 111 epochs is = 0.6590990258820351\n",
      "Initial Cost on Val dataset for this epoch 111 = 0.6590990258820351\n",
      "learning rate for this epoch =  0.03080840787435305\n",
      "Error on this batch = 0.21879147425199452\n",
      "Error on this batch = 0.4225078916333017\n",
      "Cost on val dataset after 112 epochs is = 0.6590689885298519\n",
      "Initial Cost on Val dataset for this epoch 112 = 0.6590689885298519\n",
      "learning rate for this epoch =  0.03073940764756322\n",
      "Error on this batch = 0.21663541126960512\n",
      "Error on this batch = 0.4187180982328417\n",
      "Cost on val dataset after 113 epochs is = 0.6590689950541168\n",
      "Initial Cost on Val dataset for this epoch 113 = 0.6590689950541168\n",
      "learning rate for this epoch =  0.030671173247588647\n",
      "Error on this batch = 0.2144979383272848\n",
      "Error on this batch = 0.4149547824001269\n",
      "Cost on val dataset after 114 epochs is = 0.6590982701438248\n",
      "Initial Cost on Val dataset for this epoch 114 = 0.6590982701438248\n",
      "learning rate for this epoch =  0.0306036895093009\n",
      "Error on this batch = 0.21237885938836493\n",
      "Error on this batch = 0.41121745577345353\n",
      "Cost on val dataset after 115 epochs is = 0.6591560295249443\n",
      "Initial Cost on Val dataset for this epoch 115 = 0.6591560295249443\n",
      "learning rate for this epoch =  0.030536941697562214\n",
      "Error on this batch = 0.21027795847434008\n",
      "Error on this batch = 0.4075056973026557\n",
      "Cost on val dataset after 116 epochs is = 0.6592414812978483\n",
      "Initial Cost on Val dataset for this epoch 116 = 0.6592414812978483\n",
      "learning rate for this epoch =  0.03047091549142\n",
      "Error on this batch = 0.20819499992844406\n",
      "Error on this batch = 0.40381915272879076\n",
      "Cost on val dataset after 117 epochs is = 0.6593538277772112\n",
      "Initial Cost on Val dataset for this epoch 117 = 0.6593538277772112\n",
      "learning rate for this epoch =  0.030405596969012936\n",
      "Error on this batch = 0.20612972934197135\n",
      "Error on this batch = 0.40015753313345437\n",
      "Cost on val dataset after 118 epochs is = 0.6594922677468957\n",
      "Initial Cost on Val dataset for this epoch 118 = 0.6594922677468957\n",
      "learning rate for this epoch =  0.030340972593150727\n",
      "Error on this batch = 0.20408187512280965\n",
      "Error on this batch = 0.3965206125617627\n",
      "Cost on val dataset after 119 epochs is = 0.6596559990300415\n",
      "Initial Cost on Val dataset for this epoch 119 = 0.6596559990300415\n",
      "learning rate for this epoch =  0.030277029197532102\n",
      "Error on this batch = 0.20205115067345372\n",
      "Error on this batch = 0.39290822480005605\n",
      "Cost on val dataset after 120 epochs is = 0.6598442212733892\n",
      "Initial Cost on Val dataset for this epoch 120 = 0.6598442212733892\n",
      "learning rate for this epoch =  0.030213753973567684\n",
      "Error on this batch = 0.20003725712592144\n",
      "Error on this batch = 0.38932025945328674\n",
      "Cost on val dataset after 121 epochs is = 0.660056138852186\n",
      "Initial Cost on Val dataset for this epoch 121 = 0.660056138852186\n",
      "learning rate for this epoch =  0.030151134457776365\n",
      "Error on this batch = 0.19803988655783117\n",
      "Error on this batch = 0.38575665751096955\n",
      "Cost on val dataset after 122 epochs is = 0.6602909638140824\n",
      "cost initial= 0.660056138852186 , cost final=0.6602909638140824 , change in cost= 0.00023482496189641822\n",
      "\n",
      "------------------------------------------------------------------------------\n",
      "The stats for number of units in the hidden layer = [100, 100] with softplus are as below:\n",
      "------------------------------------------------------------------------------\n",
      "The number of epochs with softplus is = 122\n",
      "The training time with softplus is = 52.935sec\n",
      "The training accuracy with softplus is = 97.041%\n",
      "The validation accuracy with softplus is = 91.333%\n",
      "The test accuracy with softplus is = 89.800%\n",
      "------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "arch=[100, 100]\n",
    "lr=0.1\n",
    "theta = theta_init(n, r, arch, 'normal')\n",
    "print(\"Training the network with {} hidden layer with {} units\".format(len(arch), arch))\n",
    "print(\"The parameters of the layers are of the shape:\")\n",
    "\n",
    "for j in range(len(theta)):\n",
    "    print(\"theta between layer {} and layer {} is {}\".format(j, j+1,theta[j].shape))\n",
    "    \n",
    "start = time.time()\n",
    "epoch, theta = training(mini_batch, X_valid, valid_class_enc, theta, lr, 'softplus', 'adaptive', 'entropy')\n",
    "\n",
    "epochs.append(epoch)\n",
    "train_time.append(time.time()-start)\n",
    "train_accuracy.append(calc_accuracy(X_train, theta, train_class_enc, 'softplus'))\n",
    "valid_accuracy.append(calc_accuracy(X_valid, theta, valid_class_enc, 'softplus'))\n",
    "test_accuracy.append(calc_accuracy(X_test, theta, test_actual_class_enc, 'softplus'))\n",
    "\n",
    "print(\"\\n------------------------------------------------------------------------------\")\n",
    "print(\"The stats for number of units in the hidden layer = {} with softplus are as below:\".format(arch))\n",
    "print(\"------------------------------------------------------------------------------\")\n",
    "print(\"The number of epochs with softplus is = {}\".format(epochs[-1]))\n",
    "print(\"The training time with softplus is = {:2.3f}sec\".format(train_time[-1]))\n",
    "print(\"The training accuracy with softplus is = {:2.3f}%\".format(train_accuracy[-1]))\n",
    "print(\"The validation accuracy with softplus is = {:2.3f}%\".format(valid_accuracy[-1]))\n",
    "print(\"The test accuracy with softplus is = {:2.3f}%\".format(test_accuracy[-1]))\n",
    "print(\"------------------------------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------Plotting Graphs for Part F - ReLU with Cross Entropy ------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"------------------Plotting Graphs for Part F - ReLU with Cross Entropy ------------------\")\n",
    "x=[0,1]\n",
    "data = [' ','ReLU', ' ' ,' ' ,' ',' ','Softplus']\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_title(\"Accuracy and Epochs for Neural Network [100,100]\\n with Cross-Entropy\")\n",
    "ax.plot(x, train_accuracy, marker='o', label='Train Accuracy')\n",
    "ax.plot(x, valid_accuracy, marker='o',label='Validation Accuracy')\n",
    "ax.plot(x, test_accuracy, marker='o', label='Test Accuracy')\n",
    "ax.set_xlabel(\"Activation Function\")\n",
    "ax.set_xticklabels(data)\n",
    "ax.set_ylabel(\"Accuracy (%)\")\n",
    "ax.legend()\n",
    "ax1=ax.twinx()\n",
    "ax1.set_ylabel(\"Number of Epochs\")\n",
    "ax1.plot(x, epochs, marker='*', color='m',label='Epochs')\n",
    "ax1.tick_params(axis='y', labelcolor='m', labelsize=12)\n",
    "plt.legend(loc=4)\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "#plt.savefig(\"plots/partf/comp_diff_act.png\", dpi=1000, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part E - MLP Classifier using SKLEARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------Running Part E-----------------------------------------------------\n",
      "------------------Training a 100x100 hidden layer network with MLP Classifier------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"----------------------------Running Part E-----------------------------------------------------\")\n",
    "print(\"------------------Training a 100x100 hidden layer network with MLP Classifier------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier as mlp_classifier\n",
    "PATH = os.getcwd()\n",
    "os.chdir('Alphabets/')\n",
    "X_train = pd.read_csv('train.csv', sep=',', header=None, index_col=False)\n",
    "X_test = pd.read_csv('test.csv', sep=',', header=None, index_col=False)\n",
    "train_class = X_train[X_train.columns[-1]]\n",
    "test_actual_class = X_test[X_test.columns[-1]]\n",
    "\n",
    "X_train = X_train.drop(X_train.columns[-1], axis=1)\n",
    "X_test = X_test.drop(X_test.columns[-1], axis=1)\n",
    "\n",
    "os.chdir('../')\n",
    "\n",
    "X_train = X_train/255\n",
    "X_test = X_test/255\n",
    "\n",
    "m = X_train.shape[0] # Number of Training Samples\n",
    "n = X_train.shape[1] # Number of input features\n",
    "train_class_enc = pd.get_dummies(train_class).to_numpy()\n",
    "test_actual_class_enc = pd.get_dummies(test_actual_class).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = []\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "train_time = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=[]\n",
    "#Classifier - logistic with constant LR\n",
    "clf.append(mlp_classifier(hidden_layer_sizes=(100, 100), activation='logistic', solver='sgd', \n",
    "                     batch_size=100, learning_rate_init=0.1, learning_rate='constant', max_iter=400,\n",
    "                     tol=1e-5, verbose=False))\n",
    "#Classifier - logistic with early_stopping =False\n",
    "clf.append(mlp_classifier(hidden_layer_sizes=(100, 100), activation='logistic', solver='sgd', \n",
    "                     batch_size=100, learning_rate_init=0.1, learning_rate='constant', max_iter=400,\n",
    "                     early_stopping=False, tol=1e-5, verbose=False))\n",
    "#Classifier - logistic with invscaling with lr0=0.3 and p=5\n",
    "clf.append(mlp_classifier(hidden_layer_sizes=(100, 100), activation='logistic', solver='sgd', \n",
    "                     batch_size=100, learning_rate_init=0.3, learning_rate='invscaling', max_iter=400,\n",
    "                     power_t=(1/5), tol=1e-5, verbose=False))\n",
    "#Classifier - logistic with invscaling with pow(1/3)\n",
    "clf.append(mlp_classifier(hidden_layer_sizes=(100, 100), activation='logistic', solver='sgd', \n",
    "                     batch_size=100, learning_rate_init=0.3, learning_rate='invscaling', max_iter=400,\n",
    "                     power_t=(1/4), tol=1e-5, verbose=False))\n",
    "\n",
    "\n",
    "#Classifier ReLU with constant LR (1e-5) lr=0.03\n",
    "clf.append(mlp_classifier(hidden_layer_sizes=(100, 100), activation='relu', solver='sgd', \n",
    "                     batch_size=100, learning_rate_init=0.03, learning_rate='constant', max_iter=400,\n",
    "                     tol=1e-5, verbose=False))\n",
    "#Classifier ReLU with constant LR (1e-5) lr=0.1\n",
    "clf.append(mlp_classifier(hidden_layer_sizes=(100, 100), activation='relu', solver='sgd', \n",
    "                     batch_size=100, learning_rate_init=0.1, learning_rate='constant', max_iter=400,\n",
    "                     tol=1e-5, verbose=False))\n",
    "#Classifier ReLU with constant LR (1e-5) with early stopping\n",
    "clf.append(mlp_classifier(hidden_layer_sizes=(100, 100), activation='relu', solver='sgd', \n",
    "                     batch_size=100, learning_rate_init=0.1, learning_rate='constant', max_iter=400,\n",
    "                     early_stopping=False, tol=1e-5, verbose=False))\n",
    "\n",
    "#Classifier ReLU with constant LR (1e-6) with invscaling sqrt\n",
    "clf.append(mlp_classifier(hidden_layer_sizes=(100, 100), activation='relu', solver='sgd', \n",
    "                     batch_size=100, learning_rate_init=0.1, learning_rate='invscaling', max_iter=400,\n",
    "                     power_t=(1/2), tol=1e-5, verbose=False))\n",
    "clf.append(mlp_classifier(hidden_layer_sizes=(100, 100), activation='relu', solver='sgd', \n",
    "                     batch_size=100, learning_rate_init=0.1, learning_rate='invscaling', max_iter=400,\n",
    "                     power_t=(1/3), tol=1e-5, verbose=False))\n",
    "#Classifier ReLU with constant LR (1e-6) with invscaling pow(1/3)\n",
    "clf.append(mlp_classifier(hidden_layer_sizes=(100, 100), activation='relu', solver='sgd', \n",
    "                     batch_size=100, learning_rate_init=0.1, learning_rate='invscaling', max_iter=400,\n",
    "                     power_t=(1/4), tol=1e-5, verbose=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Training the classifier with following params--------------\n",
      "-------------------------------------------------------------------\n",
      "MLPClassifier(activation='logistic', alpha=0.0001, batch_size=100, beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(100, 100), learning_rate='constant',\n",
      "              learning_rate_init=0.1, max_fun=15000, max_iter=400, momentum=0.9,\n",
      "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "              random_state=None, shuffle=True, solver='sgd', tol=1e-05,\n",
      "              validation_fraction=0.1, verbose=True, warm_start=False)\n",
      "-------------------------------------------------------------------\n",
      "Iteration 1, loss = 4.40204009\n",
      "Iteration 2, loss = 3.66074777\n",
      "Iteration 3, loss = 2.62139016\n",
      "Iteration 4, loss = 1.97130534\n",
      "Iteration 5, loss = 1.48572083\n",
      "Iteration 6, loss = 1.21746981\n",
      "Iteration 7, loss = 1.04363075\n",
      "Iteration 8, loss = 0.92358638\n",
      "Iteration 9, loss = 0.82895469\n",
      "Iteration 10, loss = 0.74692766\n",
      "Iteration 11, loss = 0.67857800\n",
      "Iteration 12, loss = 0.62064775\n",
      "Iteration 13, loss = 0.56697164\n",
      "Iteration 14, loss = 0.51872707\n",
      "Iteration 15, loss = 0.48015339\n",
      "Iteration 16, loss = 0.44150042\n",
      "Iteration 17, loss = 0.41049340\n",
      "Iteration 18, loss = 0.37763241\n",
      "Iteration 19, loss = 0.34948171\n",
      "Iteration 20, loss = 0.32447954\n",
      "Iteration 21, loss = 0.30236718\n",
      "Iteration 22, loss = 0.27835123\n",
      "Iteration 23, loss = 0.25934719\n",
      "Iteration 24, loss = 0.24183724\n",
      "Iteration 25, loss = 0.22467190\n",
      "Iteration 26, loss = 0.20778959\n",
      "Iteration 27, loss = 0.19313224\n",
      "Iteration 28, loss = 0.18029302\n",
      "Iteration 29, loss = 0.16893995\n",
      "Iteration 30, loss = 0.15827005\n",
      "Iteration 31, loss = 0.14669585\n",
      "Iteration 32, loss = 0.13758022\n",
      "Iteration 33, loss = 0.12783293\n",
      "Iteration 34, loss = 0.11980845\n",
      "Iteration 35, loss = 0.11111068\n",
      "Iteration 36, loss = 0.10504885\n",
      "Iteration 37, loss = 0.09708357\n",
      "Iteration 38, loss = 0.09173372\n",
      "Iteration 39, loss = 0.08545294\n",
      "Iteration 40, loss = 0.08105341\n",
      "Iteration 41, loss = 0.07609224\n",
      "Iteration 42, loss = 0.07196706\n",
      "Iteration 43, loss = 0.06803188\n",
      "Iteration 44, loss = 0.06378161\n",
      "Iteration 45, loss = 0.06081301\n",
      "Iteration 46, loss = 0.05742576\n",
      "Iteration 47, loss = 0.05458759\n",
      "Iteration 48, loss = 0.05200551\n",
      "Iteration 49, loss = 0.04933576\n",
      "Iteration 50, loss = 0.04712343\n",
      "Iteration 51, loss = 0.04491332\n",
      "Iteration 52, loss = 0.04320393\n",
      "Iteration 53, loss = 0.04142540\n",
      "Iteration 54, loss = 0.03960232\n",
      "Iteration 55, loss = 0.03798973\n",
      "Iteration 56, loss = 0.03656130\n",
      "Iteration 57, loss = 0.03511367\n",
      "Iteration 58, loss = 0.03399215\n",
      "Iteration 59, loss = 0.03280153\n",
      "Iteration 60, loss = 0.03180340\n",
      "Iteration 61, loss = 0.03078779\n",
      "Iteration 62, loss = 0.02986183\n",
      "Iteration 63, loss = 0.02891720\n",
      "Iteration 64, loss = 0.02808212\n",
      "Iteration 65, loss = 0.02723299\n",
      "Iteration 66, loss = 0.02651760\n",
      "Iteration 67, loss = 0.02583327\n",
      "Iteration 68, loss = 0.02509525\n",
      "Iteration 69, loss = 0.02447914\n",
      "Iteration 70, loss = 0.02382518\n",
      "Iteration 71, loss = 0.02335943\n",
      "Iteration 72, loss = 0.02277192\n",
      "Iteration 73, loss = 0.02231830\n",
      "Iteration 74, loss = 0.02189824\n",
      "Iteration 75, loss = 0.02131874\n",
      "Iteration 76, loss = 0.02090527\n",
      "Iteration 77, loss = 0.02056057\n",
      "Iteration 78, loss = 0.02015735\n",
      "Iteration 79, loss = 0.01978273\n",
      "Iteration 80, loss = 0.01937606\n",
      "Iteration 81, loss = 0.01904981\n",
      "Iteration 82, loss = 0.01866519\n",
      "Iteration 83, loss = 0.01840304\n",
      "Iteration 84, loss = 0.01808720\n",
      "Iteration 85, loss = 0.01778201\n",
      "Iteration 86, loss = 0.01752614\n",
      "Iteration 87, loss = 0.01724211\n",
      "Iteration 88, loss = 0.01700247\n",
      "Iteration 89, loss = 0.01670989\n",
      "Iteration 90, loss = 0.01654650\n",
      "Iteration 91, loss = 0.01631125\n",
      "Iteration 92, loss = 0.01606307\n",
      "Iteration 93, loss = 0.01584243\n",
      "Iteration 94, loss = 0.01561394\n",
      "Iteration 95, loss = 0.01545457\n",
      "Iteration 96, loss = 0.01524160\n",
      "Iteration 97, loss = 0.01506932\n",
      "Iteration 98, loss = 0.01488436\n",
      "Iteration 99, loss = 0.01473324\n",
      "Iteration 100, loss = 0.01455704\n",
      "Iteration 101, loss = 0.01440066\n",
      "Iteration 102, loss = 0.01425662\n",
      "Iteration 103, loss = 0.01410303\n",
      "Iteration 104, loss = 0.01393855\n",
      "Iteration 105, loss = 0.01380999\n",
      "Iteration 106, loss = 0.01368839\n",
      "Iteration 107, loss = 0.01353903\n",
      "Iteration 108, loss = 0.01341992\n",
      "Iteration 109, loss = 0.01330407\n",
      "Iteration 110, loss = 0.01316626\n",
      "Iteration 111, loss = 0.01304702\n",
      "Iteration 112, loss = 0.01294387\n",
      "Iteration 113, loss = 0.01284021\n",
      "Iteration 114, loss = 0.01271476\n",
      "Iteration 115, loss = 0.01260906\n",
      "Iteration 116, loss = 0.01251042\n",
      "Iteration 117, loss = 0.01241642\n",
      "Iteration 118, loss = 0.01232576\n",
      "Iteration 119, loss = 0.01222111\n",
      "Iteration 120, loss = 0.01213973\n",
      "Iteration 121, loss = 0.01204958\n",
      "Iteration 122, loss = 0.01196323\n",
      "Iteration 123, loss = 0.01187167\n",
      "Iteration 124, loss = 0.01179343\n",
      "Iteration 125, loss = 0.01172101\n",
      "Iteration 126, loss = 0.01162321\n",
      "Iteration 127, loss = 0.01156576\n",
      "Iteration 128, loss = 0.01147804\n",
      "Iteration 129, loss = 0.01140943\n",
      "Iteration 130, loss = 0.01133584\n",
      "Iteration 131, loss = 0.01126395\n",
      "Iteration 132, loss = 0.01120298\n",
      "Iteration 133, loss = 0.01114407\n",
      "Iteration 134, loss = 0.01107196\n",
      "Iteration 135, loss = 0.01099460\n",
      "Iteration 136, loss = 0.01095187\n",
      "Iteration 137, loss = 0.01088946\n",
      "Iteration 138, loss = 0.01082675\n",
      "Iteration 139, loss = 0.01076342\n",
      "Iteration 140, loss = 0.01069870\n",
      "Iteration 141, loss = 0.01065665\n",
      "Iteration 142, loss = 0.01059737\n",
      "Iteration 143, loss = 0.01053905\n",
      "Iteration 144, loss = 0.01049493\n",
      "Iteration 145, loss = 0.01043424\n",
      "Iteration 146, loss = 0.01038339\n",
      "Iteration 147, loss = 0.01034109\n",
      "Iteration 148, loss = 0.01029803\n",
      "Iteration 149, loss = 0.01024317\n",
      "Iteration 150, loss = 0.01021626\n",
      "Iteration 151, loss = 0.01014558\n",
      "Iteration 152, loss = 0.01011473\n",
      "Iteration 153, loss = 0.01007082\n",
      "Iteration 154, loss = 0.01002634\n",
      "Iteration 155, loss = 0.00998114\n",
      "Iteration 156, loss = 0.00994086\n",
      "Iteration 157, loss = 0.00989858\n",
      "Iteration 158, loss = 0.00986134\n",
      "Iteration 159, loss = 0.00981633\n",
      "Iteration 160, loss = 0.00979050\n",
      "Iteration 161, loss = 0.00974905\n",
      "Iteration 162, loss = 0.00970911\n",
      "Iteration 163, loss = 0.00967642\n",
      "Iteration 164, loss = 0.00963878\n",
      "Iteration 165, loss = 0.00960936\n",
      "Iteration 166, loss = 0.00957371\n",
      "Iteration 167, loss = 0.00953582\n",
      "Iteration 168, loss = 0.00949659\n",
      "Iteration 169, loss = 0.00946822\n",
      "Iteration 170, loss = 0.00944037\n",
      "Iteration 171, loss = 0.00940468\n",
      "Iteration 172, loss = 0.00937364\n",
      "Iteration 173, loss = 0.00934377\n",
      "Iteration 174, loss = 0.00931584\n",
      "Iteration 175, loss = 0.00928949\n",
      "Iteration 176, loss = 0.00925717\n",
      "Iteration 177, loss = 0.00922908\n",
      "Iteration 178, loss = 0.00919094\n",
      "Iteration 179, loss = 0.00917554\n",
      "Iteration 180, loss = 0.00914835\n",
      "Iteration 181, loss = 0.00911896\n",
      "Iteration 182, loss = 0.00908949\n",
      "Iteration 183, loss = 0.00907392\n",
      "Iteration 184, loss = 0.00904644\n",
      "Iteration 185, loss = 0.00900770\n",
      "Iteration 186, loss = 0.00899690\n",
      "Iteration 187, loss = 0.00896435\n",
      "Iteration 188, loss = 0.00894361\n",
      "Iteration 189, loss = 0.00891941\n",
      "Iteration 190, loss = 0.00890139\n",
      "Iteration 191, loss = 0.00887427\n",
      "Iteration 192, loss = 0.00885376\n",
      "Iteration 193, loss = 0.00882811\n",
      "Iteration 194, loss = 0.00880976\n",
      "Iteration 195, loss = 0.00877956\n",
      "Iteration 196, loss = 0.00876240\n",
      "Iteration 197, loss = 0.00874443\n",
      "Iteration 198, loss = 0.00872387\n",
      "Iteration 199, loss = 0.00870664\n",
      "Iteration 200, loss = 0.00868290\n",
      "Iteration 201, loss = 0.00866127\n",
      "Iteration 202, loss = 0.00864172\n",
      "Iteration 203, loss = 0.00862401\n",
      "Iteration 204, loss = 0.00860221\n",
      "Iteration 205, loss = 0.00858625\n",
      "Iteration 206, loss = 0.00856700\n",
      "Iteration 207, loss = 0.00854957\n",
      "Iteration 208, loss = 0.00853101\n",
      "Iteration 209, loss = 0.00851226\n",
      "Iteration 210, loss = 0.00849268\n",
      "Iteration 211, loss = 0.00848000\n",
      "Iteration 212, loss = 0.00846560\n",
      "Iteration 213, loss = 0.00844677\n",
      "Iteration 214, loss = 0.00842751\n",
      "Iteration 215, loss = 0.00841426\n",
      "Iteration 216, loss = 0.00839427\n",
      "Iteration 217, loss = 0.00838103\n",
      "Iteration 218, loss = 0.00836681\n",
      "Iteration 219, loss = 0.00835262\n",
      "Iteration 220, loss = 0.00833742\n",
      "Iteration 221, loss = 0.00831973\n",
      "Iteration 222, loss = 0.00830835\n",
      "Iteration 223, loss = 0.00829356\n",
      "Iteration 224, loss = 0.00827775\n",
      "Iteration 225, loss = 0.00826039\n",
      "Iteration 226, loss = 0.00824949\n",
      "Iteration 227, loss = 0.00823476\n",
      "Iteration 228, loss = 0.00822110\n",
      "Iteration 229, loss = 0.00820431\n",
      "Iteration 230, loss = 0.00819250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 231, loss = 0.00818206\n",
      "Iteration 232, loss = 0.00816669\n",
      "Iteration 233, loss = 0.00815445\n",
      "Iteration 234, loss = 0.00814171\n",
      "Iteration 235, loss = 0.00812549\n",
      "Iteration 236, loss = 0.00811395\n",
      "Iteration 237, loss = 0.00810097\n",
      "Iteration 238, loss = 0.00809088\n",
      "Iteration 239, loss = 0.00807933\n",
      "Iteration 240, loss = 0.00806483\n",
      "Iteration 241, loss = 0.00805517\n",
      "Iteration 242, loss = 0.00804530\n",
      "Iteration 243, loss = 0.00803136\n",
      "Iteration 244, loss = 0.00802000\n",
      "Iteration 245, loss = 0.00800937\n",
      "Iteration 246, loss = 0.00799755\n",
      "Iteration 247, loss = 0.00798497\n",
      "Iteration 248, loss = 0.00797602\n",
      "Iteration 249, loss = 0.00796509\n",
      "Iteration 250, loss = 0.00795512\n",
      "Iteration 251, loss = 0.00794356\n",
      "Iteration 252, loss = 0.00793086\n",
      "Iteration 253, loss = 0.00792437\n",
      "Iteration 254, loss = 0.00791360\n",
      "Iteration 255, loss = 0.00790178\n",
      "Iteration 256, loss = 0.00789237\n",
      "Iteration 257, loss = 0.00788448\n",
      "Iteration 258, loss = 0.00787165\n",
      "Iteration 259, loss = 0.00786263\n",
      "Iteration 260, loss = 0.00785138\n",
      "Iteration 261, loss = 0.00784491\n",
      "Iteration 262, loss = 0.00783444\n",
      "Iteration 263, loss = 0.00782734\n",
      "Iteration 264, loss = 0.00781706\n",
      "Iteration 265, loss = 0.00780829\n",
      "Iteration 266, loss = 0.00779826\n",
      "Iteration 267, loss = 0.00778883\n",
      "Iteration 268, loss = 0.00778039\n",
      "Iteration 269, loss = 0.00777285\n",
      "Iteration 270, loss = 0.00776264\n",
      "Iteration 271, loss = 0.00775491\n",
      "Iteration 272, loss = 0.00774889\n",
      "Iteration 273, loss = 0.00773878\n",
      "Iteration 274, loss = 0.00773122\n",
      "Iteration 275, loss = 0.00772343\n",
      "Iteration 276, loss = 0.00771506\n",
      "Iteration 277, loss = 0.00770548\n",
      "Iteration 278, loss = 0.00769916\n",
      "Iteration 279, loss = 0.00769061\n",
      "Iteration 280, loss = 0.00768370\n",
      "Iteration 281, loss = 0.00767341\n",
      "Iteration 282, loss = 0.00766925\n",
      "Iteration 283, loss = 0.00766000\n",
      "Iteration 284, loss = 0.00765195\n",
      "Iteration 285, loss = 0.00764505\n",
      "Iteration 286, loss = 0.00763791\n",
      "Iteration 287, loss = 0.00763160\n",
      "Iteration 288, loss = 0.00762389\n",
      "Iteration 289, loss = 0.00761408\n",
      "Iteration 290, loss = 0.00760967\n",
      "Iteration 291, loss = 0.00759982\n",
      "Iteration 292, loss = 0.00759626\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
      "The training Accuracy achieved is = 100.000\n",
      "The test Accuracy achieved is = 88.385\n",
      "The number of epochs is = 292\n",
      "The training time achieved is = 126.136\n",
      "--------Training the classifier with following params--------------\n",
      "-------------------------------------------------------------------\n",
      "MLPClassifier(activation='logistic', alpha=0.0001, batch_size=100, beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(100, 100), learning_rate='constant',\n",
      "              learning_rate_init=0.1, max_fun=15000, max_iter=400, momentum=0.9,\n",
      "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "              random_state=None, shuffle=True, solver='sgd', tol=1e-05,\n",
      "              validation_fraction=0.1, verbose=True, warm_start=False)\n",
      "-------------------------------------------------------------------\n",
      "Iteration 1, loss = 4.43052265\n",
      "Validation score: 0.000000\n",
      "Iteration 2, loss = 3.87209300\n",
      "Validation score: 0.000000\n",
      "Iteration 3, loss = 2.90003073\n",
      "Validation score: 0.102308\n",
      "Iteration 4, loss = 2.18054327\n",
      "Validation score: 0.357692\n",
      "Iteration 5, loss = 1.71302583\n",
      "Validation score: 0.546923\n",
      "Iteration 6, loss = 1.37670859\n",
      "Validation score: 0.677692\n",
      "Iteration 7, loss = 1.15313912\n",
      "Validation score: 0.743846\n",
      "Iteration 8, loss = 1.00005646\n",
      "Validation score: 0.775385\n",
      "Iteration 9, loss = 0.88637088\n",
      "Validation score: 0.791538\n",
      "Iteration 10, loss = 0.79859101\n",
      "Validation score: 0.811538\n",
      "Iteration 11, loss = 0.71923573\n",
      "Validation score: 0.819231\n",
      "Iteration 12, loss = 0.65559563\n",
      "Validation score: 0.825385\n",
      "Iteration 13, loss = 0.59621500\n",
      "Validation score: 0.826154\n",
      "Iteration 14, loss = 0.54909431\n",
      "Validation score: 0.840000\n",
      "Iteration 15, loss = 0.50323063\n",
      "Validation score: 0.850769\n",
      "Iteration 16, loss = 0.46458070\n",
      "Validation score: 0.859231\n",
      "Iteration 17, loss = 0.42659393\n",
      "Validation score: 0.862308\n",
      "Iteration 18, loss = 0.39623864\n",
      "Validation score: 0.862308\n",
      "Iteration 19, loss = 0.36547911\n",
      "Validation score: 0.863846\n",
      "Iteration 20, loss = 0.34069016\n",
      "Validation score: 0.862308\n",
      "Iteration 21, loss = 0.31472795\n",
      "Validation score: 0.876154\n",
      "Iteration 22, loss = 0.29222887\n",
      "Validation score: 0.873077\n",
      "Iteration 23, loss = 0.27142155\n",
      "Validation score: 0.876154\n",
      "Iteration 24, loss = 0.25149500\n",
      "Validation score: 0.876923\n",
      "Iteration 25, loss = 0.23390881\n",
      "Validation score: 0.875385\n",
      "Iteration 26, loss = 0.21576859\n",
      "Validation score: 0.880000\n",
      "Iteration 27, loss = 0.20144206\n",
      "Validation score: 0.877692\n",
      "Iteration 28, loss = 0.18705575\n",
      "Validation score: 0.885385\n",
      "Iteration 29, loss = 0.17211738\n",
      "Validation score: 0.884615\n",
      "Iteration 30, loss = 0.16180736\n",
      "Validation score: 0.883077\n",
      "Iteration 31, loss = 0.15108189\n",
      "Validation score: 0.888462\n",
      "Iteration 32, loss = 0.13983224\n",
      "Validation score: 0.891538\n",
      "Iteration 33, loss = 0.13059033\n",
      "Validation score: 0.891538\n",
      "Iteration 34, loss = 0.12108886\n",
      "Validation score: 0.893077\n",
      "Iteration 35, loss = 0.11371775\n",
      "Validation score: 0.891538\n",
      "Iteration 36, loss = 0.10588745\n",
      "Validation score: 0.892308\n",
      "Iteration 37, loss = 0.09993055\n",
      "Validation score: 0.894615\n",
      "Iteration 38, loss = 0.09394810\n",
      "Validation score: 0.893846\n",
      "Iteration 39, loss = 0.08754417\n",
      "Validation score: 0.893846\n",
      "Iteration 40, loss = 0.08257828\n",
      "Validation score: 0.896154\n",
      "Iteration 41, loss = 0.07844358\n",
      "Validation score: 0.893077\n",
      "Iteration 42, loss = 0.07382629\n",
      "Validation score: 0.893077\n",
      "Iteration 43, loss = 0.06919545\n",
      "Validation score: 0.896923\n",
      "Iteration 44, loss = 0.06590698\n",
      "Validation score: 0.894615\n",
      "Iteration 45, loss = 0.06304306\n",
      "Validation score: 0.892308\n",
      "Iteration 46, loss = 0.05990018\n",
      "Validation score: 0.892308\n",
      "Iteration 47, loss = 0.05678746\n",
      "Validation score: 0.893077\n",
      "Iteration 48, loss = 0.05417734\n",
      "Validation score: 0.895385\n",
      "Iteration 49, loss = 0.05201814\n",
      "Validation score: 0.900000\n",
      "Iteration 50, loss = 0.04983751\n",
      "Validation score: 0.899231\n",
      "Iteration 51, loss = 0.04764378\n",
      "Validation score: 0.898462\n",
      "Iteration 52, loss = 0.04587903\n",
      "Validation score: 0.899231\n",
      "Iteration 53, loss = 0.04373366\n",
      "Validation score: 0.897692\n",
      "Iteration 54, loss = 0.04240932\n",
      "Validation score: 0.898462\n",
      "Iteration 55, loss = 0.04083702\n",
      "Validation score: 0.898462\n",
      "Iteration 56, loss = 0.03926248\n",
      "Validation score: 0.898462\n",
      "Iteration 57, loss = 0.03803006\n",
      "Validation score: 0.896154\n",
      "Iteration 58, loss = 0.03664878\n",
      "Validation score: 0.897692\n",
      "Iteration 59, loss = 0.03552114\n",
      "Validation score: 0.897692\n",
      "Iteration 60, loss = 0.03427841\n",
      "Validation score: 0.896154\n",
      "Validation score did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
      "The training Accuracy achieved is = 98.838\n",
      "The test Accuracy achieved is = 87.815\n",
      "The number of epochs is = 60\n",
      "The training time achieved is = 22.613\n",
      "--------Training the classifier with following params--------------\n",
      "-------------------------------------------------------------------\n",
      "MLPClassifier(activation='logistic', alpha=0.0001, batch_size=100, beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(100, 100), learning_rate='invscaling',\n",
      "              learning_rate_init=0.3, max_fun=15000, max_iter=400, momentum=0.9,\n",
      "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.2,\n",
      "              random_state=None, shuffle=True, solver='sgd', tol=1e-05,\n",
      "              validation_fraction=0.1, verbose=True, warm_start=False)\n",
      "-------------------------------------------------------------------\n",
      "Iteration 1, loss = 4.42801389\n",
      "Iteration 2, loss = 4.02596292\n",
      "Iteration 3, loss = 3.87454169\n",
      "Iteration 4, loss = 3.79304929\n",
      "Iteration 5, loss = 3.72809097\n",
      "Iteration 6, loss = 3.66152124\n",
      "Iteration 7, loss = 3.57633795\n",
      "Iteration 8, loss = 3.45514887\n",
      "Iteration 9, loss = 3.32199629\n",
      "Iteration 10, loss = 3.20917575\n",
      "Iteration 11, loss = 3.10525199\n",
      "Iteration 12, loss = 3.00536485\n",
      "Iteration 13, loss = 2.92012477\n",
      "Iteration 14, loss = 2.84166494\n",
      "Iteration 15, loss = 2.76640765\n",
      "Iteration 16, loss = 2.69109637\n",
      "Iteration 17, loss = 2.61113309\n",
      "Iteration 18, loss = 2.52343505\n",
      "Iteration 19, loss = 2.43415827\n",
      "Iteration 20, loss = 2.34216664\n",
      "Iteration 21, loss = 2.24796703\n",
      "Iteration 22, loss = 2.15390429\n",
      "Iteration 23, loss = 2.05949510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 24, loss = 1.96731984\n",
      "Iteration 25, loss = 1.87869472\n",
      "Iteration 26, loss = 1.79595583\n",
      "Iteration 27, loss = 1.72311480\n",
      "Iteration 28, loss = 1.65537157\n",
      "Iteration 29, loss = 1.59668851\n",
      "Iteration 30, loss = 1.53917002\n",
      "Iteration 31, loss = 1.48533925\n",
      "Iteration 32, loss = 1.43538380\n",
      "Iteration 33, loss = 1.38690275\n",
      "Iteration 34, loss = 1.34124156\n",
      "Iteration 35, loss = 1.29574277\n",
      "Iteration 36, loss = 1.25359031\n",
      "Iteration 37, loss = 1.21260901\n",
      "Iteration 38, loss = 1.17481083\n",
      "Iteration 39, loss = 1.13796467\n",
      "Iteration 40, loss = 1.10518988\n",
      "Iteration 41, loss = 1.07163165\n",
      "Iteration 42, loss = 1.04040277\n",
      "Iteration 43, loss = 1.00928791\n",
      "Iteration 44, loss = 0.98259498\n",
      "Iteration 45, loss = 0.95702545\n",
      "Iteration 46, loss = 0.93129600\n",
      "Iteration 47, loss = 0.90744259\n",
      "Iteration 48, loss = 0.88489825\n",
      "Iteration 49, loss = 0.86163559\n",
      "Iteration 50, loss = 0.84010861\n",
      "Iteration 51, loss = 0.82035608\n",
      "Iteration 52, loss = 0.80119383\n",
      "Iteration 53, loss = 0.78230972\n",
      "Iteration 54, loss = 0.76437147\n",
      "Iteration 55, loss = 0.74674593\n",
      "Iteration 56, loss = 0.73032045\n",
      "Iteration 57, loss = 0.71486781\n",
      "Iteration 58, loss = 0.69872644\n",
      "Iteration 59, loss = 0.68433118\n",
      "Iteration 60, loss = 0.67068505\n",
      "Iteration 61, loss = 0.65667782\n",
      "Iteration 62, loss = 0.64221628\n",
      "Iteration 63, loss = 0.62835683\n",
      "Iteration 64, loss = 0.61736321\n",
      "Iteration 65, loss = 0.60394175\n",
      "Iteration 66, loss = 0.59247090\n",
      "Iteration 67, loss = 0.58065916\n",
      "Iteration 68, loss = 0.56935173\n",
      "Iteration 69, loss = 0.55807900\n",
      "Iteration 70, loss = 0.54791753\n",
      "Iteration 71, loss = 0.53697121\n",
      "Iteration 72, loss = 0.52728448\n",
      "Iteration 73, loss = 0.51798119\n",
      "Iteration 74, loss = 0.50710346\n",
      "Iteration 75, loss = 0.49875019\n",
      "Iteration 76, loss = 0.49018189\n",
      "Iteration 77, loss = 0.48087214\n",
      "Iteration 78, loss = 0.47134677\n",
      "Iteration 79, loss = 0.46389234\n",
      "Iteration 80, loss = 0.45640911\n",
      "Iteration 81, loss = 0.44867696\n",
      "Iteration 82, loss = 0.44055118\n",
      "Iteration 83, loss = 0.43226412\n",
      "Iteration 84, loss = 0.42547985\n",
      "Iteration 85, loss = 0.41915436\n",
      "Iteration 86, loss = 0.41163913\n",
      "Iteration 87, loss = 0.40520965\n",
      "Iteration 88, loss = 0.39848776\n",
      "Iteration 89, loss = 0.39232861\n",
      "Iteration 90, loss = 0.38600247\n",
      "Iteration 91, loss = 0.37965624\n",
      "Iteration 92, loss = 0.37382535\n",
      "Iteration 93, loss = 0.36878828\n",
      "Iteration 94, loss = 0.36244755\n",
      "Iteration 95, loss = 0.35704318\n",
      "Iteration 96, loss = 0.35168623\n",
      "Iteration 97, loss = 0.34669781\n",
      "Iteration 98, loss = 0.34124396\n",
      "Iteration 99, loss = 0.33653188\n",
      "Iteration 100, loss = 0.33195351\n",
      "Iteration 101, loss = 0.32695590\n",
      "Iteration 102, loss = 0.32187908\n",
      "Iteration 103, loss = 0.31693791\n",
      "Iteration 104, loss = 0.31303903\n",
      "Iteration 105, loss = 0.30834405\n",
      "Iteration 106, loss = 0.30374883\n",
      "Iteration 107, loss = 0.29995082\n",
      "Iteration 108, loss = 0.29544999\n",
      "Iteration 109, loss = 0.29138400\n",
      "Iteration 110, loss = 0.28768476\n",
      "Iteration 111, loss = 0.28356550\n",
      "Iteration 112, loss = 0.27987562\n",
      "Iteration 113, loss = 0.27610100\n",
      "Iteration 114, loss = 0.27281977\n",
      "Iteration 115, loss = 0.26977477\n",
      "Iteration 116, loss = 0.26569487\n",
      "Iteration 117, loss = 0.26224303\n",
      "Iteration 118, loss = 0.25869619\n",
      "Iteration 119, loss = 0.25558058\n",
      "Iteration 120, loss = 0.25294445\n",
      "Iteration 121, loss = 0.24902542\n",
      "Iteration 122, loss = 0.24605263\n",
      "Iteration 123, loss = 0.24288517\n",
      "Iteration 124, loss = 0.23951310\n",
      "Iteration 125, loss = 0.23689188\n",
      "Iteration 126, loss = 0.23428508\n",
      "Iteration 127, loss = 0.23140076\n",
      "Iteration 128, loss = 0.22838908\n",
      "Iteration 129, loss = 0.22594893\n",
      "Iteration 130, loss = 0.22344797\n",
      "Iteration 131, loss = 0.22029702\n",
      "Iteration 132, loss = 0.21799959\n",
      "Iteration 133, loss = 0.21537239\n",
      "Iteration 134, loss = 0.21320504\n",
      "Iteration 135, loss = 0.21023807\n",
      "Iteration 136, loss = 0.20819698\n",
      "Iteration 137, loss = 0.20592935\n",
      "Iteration 138, loss = 0.20318257\n",
      "Iteration 139, loss = 0.20112463\n",
      "Iteration 140, loss = 0.19938497\n",
      "Iteration 141, loss = 0.19695483\n",
      "Iteration 142, loss = 0.19448832\n",
      "Iteration 143, loss = 0.19219015\n",
      "Iteration 144, loss = 0.19016994\n",
      "Iteration 145, loss = 0.18821283\n",
      "Iteration 146, loss = 0.18599074\n",
      "Iteration 147, loss = 0.18422964\n",
      "Iteration 148, loss = 0.18261870\n",
      "Iteration 149, loss = 0.18049157\n",
      "Iteration 150, loss = 0.17862336\n",
      "Iteration 151, loss = 0.17658817\n",
      "Iteration 152, loss = 0.17512091\n",
      "Iteration 153, loss = 0.17277945\n",
      "Iteration 154, loss = 0.17143461\n",
      "Iteration 155, loss = 0.16953045\n",
      "Iteration 156, loss = 0.16759579\n",
      "Iteration 157, loss = 0.16593310\n",
      "Iteration 158, loss = 0.16474714\n",
      "Iteration 159, loss = 0.16282972\n",
      "Iteration 160, loss = 0.16139491\n",
      "Iteration 161, loss = 0.15995103\n",
      "Iteration 162, loss = 0.15811383\n",
      "Iteration 163, loss = 0.15674313\n",
      "Iteration 164, loss = 0.15517774\n",
      "Iteration 165, loss = 0.15376795\n",
      "Iteration 166, loss = 0.15241225\n",
      "Iteration 167, loss = 0.15077035\n",
      "Iteration 168, loss = 0.14931782\n",
      "Iteration 169, loss = 0.14800046\n",
      "Iteration 170, loss = 0.14655841\n",
      "Iteration 171, loss = 0.14512184\n",
      "Iteration 172, loss = 0.14430608\n",
      "Iteration 173, loss = 0.14262715\n",
      "Iteration 174, loss = 0.14147225\n",
      "Iteration 175, loss = 0.14007233\n",
      "Iteration 176, loss = 0.13867286\n",
      "Iteration 177, loss = 0.13779971\n",
      "Iteration 178, loss = 0.13614189\n",
      "Iteration 179, loss = 0.13521686\n",
      "Iteration 180, loss = 0.13402851\n",
      "Iteration 181, loss = 0.13298224\n",
      "Iteration 182, loss = 0.13158191\n",
      "Iteration 183, loss = 0.13091976\n",
      "Iteration 184, loss = 0.12946209\n",
      "Iteration 185, loss = 0.12843424\n",
      "Iteration 186, loss = 0.12722025\n",
      "Iteration 187, loss = 0.12606506\n",
      "Iteration 188, loss = 0.12513385\n",
      "Iteration 189, loss = 0.12397712\n",
      "Iteration 190, loss = 0.12303982\n",
      "Iteration 191, loss = 0.12192599\n",
      "Iteration 192, loss = 0.12118540\n",
      "Iteration 193, loss = 0.12011229\n",
      "Iteration 194, loss = 0.11902348\n",
      "Iteration 195, loss = 0.11833788\n",
      "Iteration 196, loss = 0.11725797\n",
      "Iteration 197, loss = 0.11604579\n",
      "Iteration 198, loss = 0.11549879\n",
      "Iteration 199, loss = 0.11453971\n",
      "Iteration 200, loss = 0.11348562\n",
      "Iteration 201, loss = 0.11271014\n",
      "Iteration 202, loss = 0.11184574\n",
      "Iteration 203, loss = 0.11078856\n",
      "Iteration 204, loss = 0.11020917\n",
      "Iteration 205, loss = 0.10930323\n",
      "Iteration 206, loss = 0.10850250\n",
      "Iteration 207, loss = 0.10744229\n",
      "Iteration 208, loss = 0.10683273\n",
      "Iteration 209, loss = 0.10587065\n",
      "Iteration 210, loss = 0.10535131\n",
      "Iteration 211, loss = 0.10467204\n",
      "Iteration 212, loss = 0.10368789\n",
      "Iteration 213, loss = 0.10302804\n",
      "Iteration 214, loss = 0.10248892\n",
      "Iteration 215, loss = 0.10157965\n",
      "Iteration 216, loss = 0.10076664\n",
      "Iteration 217, loss = 0.10022637\n",
      "Iteration 218, loss = 0.09941799\n",
      "Iteration 219, loss = 0.09870604\n",
      "Iteration 220, loss = 0.09805520\n",
      "Iteration 221, loss = 0.09727673\n",
      "Iteration 222, loss = 0.09672400\n",
      "Iteration 223, loss = 0.09610762\n",
      "Iteration 224, loss = 0.09546185\n",
      "Iteration 225, loss = 0.09481178\n",
      "Iteration 226, loss = 0.09422092\n",
      "Iteration 227, loss = 0.09356621\n",
      "Iteration 228, loss = 0.09292427\n",
      "Iteration 229, loss = 0.09234703\n",
      "Iteration 230, loss = 0.09154780\n",
      "Iteration 231, loss = 0.09116365\n",
      "Iteration 232, loss = 0.09047250\n",
      "Iteration 233, loss = 0.09004131\n",
      "Iteration 234, loss = 0.08936616\n",
      "Iteration 235, loss = 0.08885564\n",
      "Iteration 236, loss = 0.08820695\n",
      "Iteration 237, loss = 0.08749100\n",
      "Iteration 238, loss = 0.08727527\n",
      "Iteration 239, loss = 0.08652396\n",
      "Iteration 240, loss = 0.08608779\n",
      "Iteration 241, loss = 0.08554535\n",
      "Iteration 242, loss = 0.08501101\n",
      "Iteration 243, loss = 0.08450900\n",
      "Iteration 244, loss = 0.08386397\n",
      "Iteration 245, loss = 0.08343973\n",
      "Iteration 246, loss = 0.08293502\n",
      "Iteration 247, loss = 0.08245518\n",
      "Iteration 248, loss = 0.08190525\n",
      "Iteration 249, loss = 0.08142579\n",
      "Iteration 250, loss = 0.08087739\n",
      "Iteration 251, loss = 0.08058237\n",
      "Iteration 252, loss = 0.07996684\n",
      "Iteration 253, loss = 0.07941003\n",
      "Iteration 254, loss = 0.07909255\n",
      "Iteration 255, loss = 0.07873914\n",
      "Iteration 256, loss = 0.07825658\n",
      "Iteration 257, loss = 0.07781730\n",
      "Iteration 258, loss = 0.07733891\n",
      "Iteration 259, loss = 0.07699726\n",
      "Iteration 260, loss = 0.07645686\n",
      "Iteration 261, loss = 0.07600619\n",
      "Iteration 262, loss = 0.07564447\n",
      "Iteration 263, loss = 0.07522895\n",
      "Iteration 264, loss = 0.07477167\n",
      "Iteration 265, loss = 0.07448077\n",
      "Iteration 266, loss = 0.07391180\n",
      "Iteration 267, loss = 0.07354199\n",
      "Iteration 268, loss = 0.07312457\n",
      "Iteration 269, loss = 0.07277911\n",
      "Iteration 270, loss = 0.07235955\n",
      "Iteration 271, loss = 0.07199347\n",
      "Iteration 272, loss = 0.07160507\n",
      "Iteration 273, loss = 0.07124282\n",
      "Iteration 274, loss = 0.07087873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 275, loss = 0.07055748\n",
      "Iteration 276, loss = 0.07021660\n",
      "Iteration 277, loss = 0.06981383\n",
      "Iteration 278, loss = 0.06944030\n",
      "Iteration 279, loss = 0.06910058\n",
      "Iteration 280, loss = 0.06874581\n",
      "Iteration 281, loss = 0.06836807\n",
      "Iteration 282, loss = 0.06819712\n",
      "Iteration 283, loss = 0.06769244\n",
      "Iteration 284, loss = 0.06745178\n",
      "Iteration 285, loss = 0.06713470\n",
      "Iteration 286, loss = 0.06673338\n",
      "Iteration 287, loss = 0.06650850\n",
      "Iteration 288, loss = 0.06613529\n",
      "Iteration 289, loss = 0.06580907\n",
      "Iteration 290, loss = 0.06552046\n",
      "Iteration 291, loss = 0.06518692\n",
      "Iteration 292, loss = 0.06489363\n",
      "Iteration 293, loss = 0.06458532\n",
      "Iteration 294, loss = 0.06433693\n",
      "Iteration 295, loss = 0.06392932\n",
      "Iteration 296, loss = 0.06369468\n",
      "Iteration 297, loss = 0.06342810\n",
      "Iteration 298, loss = 0.06310827\n",
      "Iteration 299, loss = 0.06288065\n",
      "Iteration 300, loss = 0.06253422\n",
      "Iteration 301, loss = 0.06231723\n",
      "Iteration 302, loss = 0.06201085\n",
      "Iteration 303, loss = 0.06171358\n",
      "Iteration 304, loss = 0.06139961\n",
      "Iteration 305, loss = 0.06118094\n",
      "Iteration 306, loss = 0.06090997\n",
      "Iteration 307, loss = 0.06057598\n",
      "Iteration 308, loss = 0.06035722\n",
      "Iteration 309, loss = 0.06011941\n",
      "Iteration 310, loss = 0.05984132\n",
      "Iteration 311, loss = 0.05958326\n",
      "Iteration 312, loss = 0.05936960\n",
      "Iteration 313, loss = 0.05909377\n",
      "Iteration 314, loss = 0.05888245\n",
      "Iteration 315, loss = 0.05860528\n",
      "Iteration 316, loss = 0.05829017\n",
      "Iteration 317, loss = 0.05816493\n",
      "Iteration 318, loss = 0.05783940\n",
      "Iteration 319, loss = 0.05761489\n",
      "Iteration 320, loss = 0.05744142\n",
      "Iteration 321, loss = 0.05720725\n",
      "Iteration 322, loss = 0.05693940\n",
      "Iteration 323, loss = 0.05673341\n",
      "Iteration 324, loss = 0.05655516\n",
      "Iteration 325, loss = 0.05628114\n",
      "Iteration 326, loss = 0.05610384\n",
      "Iteration 327, loss = 0.05579303\n",
      "Iteration 328, loss = 0.05562795\n",
      "Iteration 329, loss = 0.05542267\n",
      "Iteration 330, loss = 0.05517187\n",
      "Iteration 331, loss = 0.05501162\n",
      "Iteration 332, loss = 0.05473151\n",
      "Iteration 333, loss = 0.05454223\n",
      "Iteration 334, loss = 0.05433628\n",
      "Iteration 335, loss = 0.05415459\n",
      "Iteration 336, loss = 0.05392846\n",
      "Iteration 337, loss = 0.05372121\n",
      "Iteration 338, loss = 0.05352291\n",
      "Iteration 339, loss = 0.05331461\n",
      "Iteration 340, loss = 0.05313169\n",
      "Iteration 341, loss = 0.05292070\n",
      "Iteration 342, loss = 0.05275136\n",
      "Iteration 343, loss = 0.05254004\n",
      "Iteration 344, loss = 0.05235367\n",
      "Iteration 345, loss = 0.05218249\n",
      "Iteration 346, loss = 0.05199756\n",
      "Iteration 347, loss = 0.05180812\n",
      "Iteration 348, loss = 0.05163243\n",
      "Iteration 349, loss = 0.05145765\n",
      "Iteration 350, loss = 0.05131974\n",
      "Iteration 351, loss = 0.05101733\n",
      "Iteration 352, loss = 0.05090283\n",
      "Iteration 353, loss = 0.05066494\n",
      "Iteration 354, loss = 0.05053734\n",
      "Iteration 355, loss = 0.05036446\n",
      "Iteration 356, loss = 0.05017553\n",
      "Iteration 357, loss = 0.05003100\n",
      "Iteration 358, loss = 0.04979142\n",
      "Iteration 359, loss = 0.04967742\n",
      "Iteration 360, loss = 0.04947698\n",
      "Iteration 361, loss = 0.04937774\n",
      "Iteration 362, loss = 0.04914525\n",
      "Iteration 363, loss = 0.04897842\n",
      "Iteration 364, loss = 0.04882686\n",
      "Iteration 365, loss = 0.04863865\n",
      "Iteration 366, loss = 0.04852485\n",
      "Iteration 367, loss = 0.04836738\n",
      "Iteration 368, loss = 0.04819779\n",
      "Iteration 369, loss = 0.04795770\n",
      "Iteration 370, loss = 0.04786714\n",
      "Iteration 371, loss = 0.04768174\n",
      "Iteration 372, loss = 0.04754309\n",
      "Iteration 373, loss = 0.04740596\n",
      "Iteration 374, loss = 0.04723682\n",
      "Iteration 375, loss = 0.04705886\n",
      "Iteration 376, loss = 0.04692453\n",
      "Iteration 377, loss = 0.04673927\n",
      "Iteration 378, loss = 0.04661722\n",
      "Iteration 379, loss = 0.04649142\n",
      "Iteration 380, loss = 0.04633523\n",
      "Iteration 381, loss = 0.04618253\n",
      "Iteration 382, loss = 0.04603491\n",
      "Iteration 383, loss = 0.04594162\n",
      "Iteration 384, loss = 0.04576532\n",
      "Iteration 385, loss = 0.04564341\n",
      "Iteration 386, loss = 0.04547156\n",
      "Iteration 387, loss = 0.04534423\n",
      "Iteration 388, loss = 0.04518295\n",
      "Iteration 389, loss = 0.04503119\n",
      "Iteration 390, loss = 0.04490098\n",
      "Iteration 391, loss = 0.04477583\n",
      "Iteration 392, loss = 0.04464936\n",
      "Iteration 393, loss = 0.04448325\n",
      "Iteration 394, loss = 0.04434118\n",
      "Iteration 395, loss = 0.04421369\n",
      "Iteration 396, loss = 0.04409371\n",
      "Iteration 397, loss = 0.04392514\n",
      "Iteration 398, loss = 0.04381288\n",
      "Iteration 399, loss = 0.04363538\n",
      "Iteration 400, loss = 0.04354570\n",
      "The training Accuracy achieved is = 99.823\n",
      "The test Accuracy achieved is = 85.600\n",
      "The number of epochs is = 400\n",
      "The training time achieved is = 164.379\n",
      "--------Training the classifier with following params--------------\n",
      "-------------------------------------------------------------------\n",
      "MLPClassifier(activation='logistic', alpha=0.0001, batch_size=100, beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(100, 100), learning_rate='invscaling',\n",
      "              learning_rate_init=0.3, max_fun=15000, max_iter=400, momentum=0.9,\n",
      "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.25,\n",
      "              random_state=None, shuffle=True, solver='sgd', tol=1e-05,\n",
      "              validation_fraction=0.1, verbose=True, warm_start=False)\n",
      "-------------------------------------------------------------------\n",
      "Iteration 1, loss = 4.41860791\n",
      "Iteration 2, loss = 3.92305683\n",
      "Iteration 3, loss = 3.69283761\n",
      "Iteration 4, loss = 3.52263524\n",
      "Iteration 5, loss = 3.40990751\n",
      "Iteration 6, loss = 3.33047392\n",
      "Iteration 7, loss = 3.26896842\n",
      "Iteration 8, loss = 3.21575387\n",
      "Iteration 9, loss = 3.16705091\n",
      "Iteration 10, loss = 3.12102187\n",
      "Iteration 11, loss = 3.07501018\n",
      "Iteration 12, loss = 3.02956877\n",
      "Iteration 13, loss = 2.98388107\n",
      "Iteration 14, loss = 2.93721307\n",
      "Iteration 15, loss = 2.88950653\n",
      "Iteration 16, loss = 2.84111614\n",
      "Iteration 17, loss = 2.79153026\n",
      "Iteration 18, loss = 2.74044094\n",
      "Iteration 19, loss = 2.68632518\n",
      "Iteration 20, loss = 2.62973349\n",
      "Iteration 21, loss = 2.56902273\n",
      "Iteration 22, loss = 2.50463604\n",
      "Iteration 23, loss = 2.43767358\n",
      "Iteration 24, loss = 2.36900746\n",
      "Iteration 25, loss = 2.29983465\n",
      "Iteration 26, loss = 2.23232025\n",
      "Iteration 27, loss = 2.16772469\n",
      "Iteration 28, loss = 2.10575973\n",
      "Iteration 29, loss = 2.04804989\n",
      "Iteration 30, loss = 1.99381292\n",
      "Iteration 31, loss = 1.94346792\n",
      "Iteration 32, loss = 1.89556825\n",
      "Iteration 33, loss = 1.85023173\n",
      "Iteration 34, loss = 1.80778357\n",
      "Iteration 35, loss = 1.76731965\n",
      "Iteration 36, loss = 1.72812936\n",
      "Iteration 37, loss = 1.69083523\n",
      "Iteration 38, loss = 1.65586008\n",
      "Iteration 39, loss = 1.62215866\n",
      "Iteration 40, loss = 1.58899498\n",
      "Iteration 41, loss = 1.55745925\n",
      "Iteration 42, loss = 1.52657202\n",
      "Iteration 43, loss = 1.49693108\n",
      "Iteration 44, loss = 1.46752700\n",
      "Iteration 45, loss = 1.44051159\n",
      "Iteration 46, loss = 1.41330953\n",
      "Iteration 47, loss = 1.38694901\n",
      "Iteration 48, loss = 1.36150273\n",
      "Iteration 49, loss = 1.33667015\n",
      "Iteration 50, loss = 1.31282337\n",
      "Iteration 51, loss = 1.28877891\n",
      "Iteration 52, loss = 1.26721150\n",
      "Iteration 53, loss = 1.24516909\n",
      "Iteration 54, loss = 1.22419181\n",
      "Iteration 55, loss = 1.20307213\n",
      "Iteration 56, loss = 1.18291382\n",
      "Iteration 57, loss = 1.16416743\n",
      "Iteration 58, loss = 1.14518266\n",
      "Iteration 59, loss = 1.12677190\n",
      "Iteration 60, loss = 1.10965175\n",
      "Iteration 61, loss = 1.09278054\n",
      "Iteration 62, loss = 1.07546648\n",
      "Iteration 63, loss = 1.05977865\n",
      "Iteration 64, loss = 1.04454725\n",
      "Iteration 65, loss = 1.02883183\n",
      "Iteration 66, loss = 1.01350632\n",
      "Iteration 67, loss = 0.99961127\n",
      "Iteration 68, loss = 0.98503293\n",
      "Iteration 69, loss = 0.97109510\n",
      "Iteration 70, loss = 0.95849640\n",
      "Iteration 71, loss = 0.94539227\n",
      "Iteration 72, loss = 0.93305307\n",
      "Iteration 73, loss = 0.92072734\n",
      "Iteration 74, loss = 0.90938124\n",
      "Iteration 75, loss = 0.89802033\n",
      "Iteration 76, loss = 0.88724909\n",
      "Iteration 77, loss = 0.87712277\n",
      "Iteration 78, loss = 0.86611978\n",
      "Iteration 79, loss = 0.85633038\n",
      "Iteration 80, loss = 0.84719305\n",
      "Iteration 81, loss = 0.83716606\n",
      "Iteration 82, loss = 0.82779693\n",
      "Iteration 83, loss = 0.81883388\n",
      "Iteration 84, loss = 0.80975669\n",
      "Iteration 85, loss = 0.80141231\n",
      "Iteration 86, loss = 0.79222655\n",
      "Iteration 87, loss = 0.78455586\n",
      "Iteration 88, loss = 0.77649669\n",
      "Iteration 89, loss = 0.76861530\n",
      "Iteration 90, loss = 0.76101663\n",
      "Iteration 91, loss = 0.75308322\n",
      "Iteration 92, loss = 0.74604972\n",
      "Iteration 93, loss = 0.73890633\n",
      "Iteration 94, loss = 0.73138225\n",
      "Iteration 95, loss = 0.72439787\n",
      "Iteration 96, loss = 0.71756736\n",
      "Iteration 97, loss = 0.71112637\n",
      "Iteration 98, loss = 0.70416714\n",
      "Iteration 99, loss = 0.69719153\n",
      "Iteration 100, loss = 0.69147355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 101, loss = 0.68473413\n",
      "Iteration 102, loss = 0.67802450\n",
      "Iteration 103, loss = 0.67195478\n",
      "Iteration 104, loss = 0.66561303\n",
      "Iteration 105, loss = 0.65989320\n",
      "Iteration 106, loss = 0.65461498\n",
      "Iteration 107, loss = 0.64822412\n",
      "Iteration 108, loss = 0.64220850\n",
      "Iteration 109, loss = 0.63660359\n",
      "Iteration 110, loss = 0.63122280\n",
      "Iteration 111, loss = 0.62552928\n",
      "Iteration 112, loss = 0.62031232\n",
      "Iteration 113, loss = 0.61445731\n",
      "Iteration 114, loss = 0.60911162\n",
      "Iteration 115, loss = 0.60465334\n",
      "Iteration 116, loss = 0.59933084\n",
      "Iteration 117, loss = 0.59382760\n",
      "Iteration 118, loss = 0.58915062\n",
      "Iteration 119, loss = 0.58460169\n",
      "Iteration 120, loss = 0.57980956\n",
      "Iteration 121, loss = 0.57469338\n",
      "Iteration 122, loss = 0.56973948\n",
      "Iteration 123, loss = 0.56566116\n",
      "Iteration 124, loss = 0.56092946\n",
      "Iteration 125, loss = 0.55613936\n",
      "Iteration 126, loss = 0.55142383\n",
      "Iteration 127, loss = 0.54755226\n",
      "Iteration 128, loss = 0.54314335\n",
      "Iteration 129, loss = 0.53878096\n",
      "Iteration 130, loss = 0.53458791\n",
      "Iteration 131, loss = 0.53047494\n",
      "Iteration 132, loss = 0.52620569\n",
      "Iteration 133, loss = 0.52290687\n",
      "Iteration 134, loss = 0.51840489\n",
      "Iteration 135, loss = 0.51454013\n",
      "Iteration 136, loss = 0.51046183\n",
      "Iteration 137, loss = 0.50721535\n",
      "Iteration 138, loss = 0.50291761\n",
      "Iteration 139, loss = 0.49900886\n",
      "Iteration 140, loss = 0.49532599\n",
      "Iteration 141, loss = 0.49179771\n",
      "Iteration 142, loss = 0.48806758\n",
      "Iteration 143, loss = 0.48463251\n",
      "Iteration 144, loss = 0.48066276\n",
      "Iteration 145, loss = 0.47771127\n",
      "Iteration 146, loss = 0.47416149\n",
      "Iteration 147, loss = 0.47072447\n",
      "Iteration 148, loss = 0.46711550\n",
      "Iteration 149, loss = 0.46394600\n",
      "Iteration 150, loss = 0.46026839\n",
      "Iteration 151, loss = 0.45756959\n",
      "Iteration 152, loss = 0.45453937\n",
      "Iteration 153, loss = 0.45102427\n",
      "Iteration 154, loss = 0.44820941\n",
      "Iteration 155, loss = 0.44492132\n",
      "Iteration 156, loss = 0.44186554\n",
      "Iteration 157, loss = 0.43850080\n",
      "Iteration 158, loss = 0.43598419\n",
      "Iteration 159, loss = 0.43259973\n",
      "Iteration 160, loss = 0.42948696\n",
      "Iteration 161, loss = 0.42656767\n",
      "Iteration 162, loss = 0.42376721\n",
      "Iteration 163, loss = 0.42077531\n",
      "Iteration 164, loss = 0.41867372\n",
      "Iteration 165, loss = 0.41559242\n",
      "Iteration 166, loss = 0.41272623\n",
      "Iteration 167, loss = 0.41003484\n",
      "Iteration 168, loss = 0.40665734\n",
      "Iteration 169, loss = 0.40415167\n",
      "Iteration 170, loss = 0.40193775\n",
      "Iteration 171, loss = 0.39929994\n",
      "Iteration 172, loss = 0.39662760\n",
      "Iteration 173, loss = 0.39393069\n",
      "Iteration 174, loss = 0.39077793\n",
      "Iteration 175, loss = 0.38911379\n",
      "Iteration 176, loss = 0.38653894\n",
      "Iteration 177, loss = 0.38341717\n",
      "Iteration 178, loss = 0.38167682\n",
      "Iteration 179, loss = 0.37903440\n",
      "Iteration 180, loss = 0.37613452\n",
      "Iteration 181, loss = 0.37437713\n",
      "Iteration 182, loss = 0.37185902\n",
      "Iteration 183, loss = 0.36967618\n",
      "Iteration 184, loss = 0.36733007\n",
      "Iteration 185, loss = 0.36459175\n",
      "Iteration 186, loss = 0.36309490\n",
      "Iteration 187, loss = 0.36032343\n",
      "Iteration 188, loss = 0.35797884\n",
      "Iteration 189, loss = 0.35607148\n",
      "Iteration 190, loss = 0.35364556\n",
      "Iteration 191, loss = 0.35130935\n",
      "Iteration 192, loss = 0.34994444\n",
      "Iteration 193, loss = 0.34762320\n",
      "Iteration 194, loss = 0.34557453\n",
      "Iteration 195, loss = 0.34333824\n",
      "Iteration 196, loss = 0.34143923\n",
      "Iteration 197, loss = 0.33903238\n",
      "Iteration 198, loss = 0.33741833\n",
      "Iteration 199, loss = 0.33517691\n",
      "Iteration 200, loss = 0.33341192\n",
      "Iteration 201, loss = 0.33124917\n",
      "Iteration 202, loss = 0.32954148\n",
      "Iteration 203, loss = 0.32748036\n",
      "Iteration 204, loss = 0.32606919\n",
      "Iteration 205, loss = 0.32355451\n",
      "Iteration 206, loss = 0.32174350\n",
      "Iteration 207, loss = 0.32009954\n",
      "Iteration 208, loss = 0.31843028\n",
      "Iteration 209, loss = 0.31630607\n",
      "Iteration 210, loss = 0.31480616\n",
      "Iteration 211, loss = 0.31303979\n",
      "Iteration 212, loss = 0.31104779\n",
      "Iteration 213, loss = 0.30949666\n",
      "Iteration 214, loss = 0.30738320\n",
      "Iteration 215, loss = 0.30549870\n",
      "Iteration 216, loss = 0.30399633\n",
      "Iteration 217, loss = 0.30268424\n",
      "Iteration 218, loss = 0.30070662\n",
      "Iteration 219, loss = 0.29925437\n",
      "Iteration 220, loss = 0.29748293\n",
      "Iteration 221, loss = 0.29578487\n",
      "Iteration 222, loss = 0.29417353\n",
      "Iteration 223, loss = 0.29258435\n",
      "Iteration 224, loss = 0.29102059\n",
      "Iteration 225, loss = 0.28938000\n",
      "Iteration 226, loss = 0.28765632\n",
      "Iteration 227, loss = 0.28628326\n",
      "Iteration 228, loss = 0.28493913\n",
      "Iteration 229, loss = 0.28321343\n",
      "Iteration 230, loss = 0.28187198\n",
      "Iteration 231, loss = 0.28013442\n",
      "Iteration 232, loss = 0.27882805\n",
      "Iteration 233, loss = 0.27702538\n",
      "Iteration 234, loss = 0.27540364\n",
      "Iteration 235, loss = 0.27416667\n",
      "Iteration 236, loss = 0.27254522\n",
      "Iteration 237, loss = 0.27109239\n",
      "Iteration 238, loss = 0.27007164\n",
      "Iteration 239, loss = 0.26841797\n",
      "Iteration 240, loss = 0.26746394\n",
      "Iteration 241, loss = 0.26588663\n",
      "Iteration 242, loss = 0.26454229\n",
      "Iteration 243, loss = 0.26263406\n",
      "Iteration 244, loss = 0.26183711\n",
      "Iteration 245, loss = 0.26041082\n",
      "Iteration 246, loss = 0.25894190\n",
      "Iteration 247, loss = 0.25784742\n",
      "Iteration 248, loss = 0.25619616\n",
      "Iteration 249, loss = 0.25484157\n",
      "Iteration 250, loss = 0.25382218\n",
      "Iteration 251, loss = 0.25210428\n",
      "Iteration 252, loss = 0.25094923\n",
      "Iteration 253, loss = 0.25007843\n",
      "Iteration 254, loss = 0.24836344\n",
      "Iteration 255, loss = 0.24739915\n",
      "Iteration 256, loss = 0.24597038\n",
      "Iteration 257, loss = 0.24477255\n",
      "Iteration 258, loss = 0.24354855\n",
      "Iteration 259, loss = 0.24237610\n",
      "Iteration 260, loss = 0.24117301\n",
      "Iteration 261, loss = 0.24014043\n",
      "Iteration 262, loss = 0.23883540\n",
      "Iteration 263, loss = 0.23757238\n",
      "Iteration 264, loss = 0.23650233\n",
      "Iteration 265, loss = 0.23532055\n",
      "Iteration 266, loss = 0.23383227\n",
      "Iteration 267, loss = 0.23300357\n",
      "Iteration 268, loss = 0.23186999\n",
      "Iteration 269, loss = 0.23057254\n",
      "Iteration 270, loss = 0.22957197\n",
      "Iteration 271, loss = 0.22866173\n",
      "Iteration 272, loss = 0.22714703\n",
      "Iteration 273, loss = 0.22660764\n",
      "Iteration 274, loss = 0.22521611\n",
      "Iteration 275, loss = 0.22422350\n",
      "Iteration 276, loss = 0.22318779\n",
      "Iteration 277, loss = 0.22183830\n",
      "Iteration 278, loss = 0.22086281\n",
      "Iteration 279, loss = 0.21969117\n",
      "Iteration 280, loss = 0.21887058\n",
      "Iteration 281, loss = 0.21793776\n",
      "Iteration 282, loss = 0.21663860\n",
      "Iteration 283, loss = 0.21571251\n",
      "Iteration 284, loss = 0.21465214\n",
      "Iteration 285, loss = 0.21358979\n",
      "Iteration 286, loss = 0.21265332\n",
      "Iteration 287, loss = 0.21175628\n",
      "Iteration 288, loss = 0.21048716\n",
      "Iteration 289, loss = 0.20979676\n",
      "Iteration 290, loss = 0.20883081\n",
      "Iteration 291, loss = 0.20784543\n",
      "Iteration 292, loss = 0.20690848\n",
      "Iteration 293, loss = 0.20576436\n",
      "Iteration 294, loss = 0.20528777\n",
      "Iteration 295, loss = 0.20400591\n",
      "Iteration 296, loss = 0.20323107\n",
      "Iteration 297, loss = 0.20226888\n",
      "Iteration 298, loss = 0.20115194\n",
      "Iteration 299, loss = 0.20050120\n",
      "Iteration 300, loss = 0.19951151\n",
      "Iteration 301, loss = 0.19860446\n",
      "Iteration 302, loss = 0.19784666\n",
      "Iteration 303, loss = 0.19706907\n",
      "Iteration 304, loss = 0.19615698\n",
      "Iteration 305, loss = 0.19507382\n",
      "Iteration 306, loss = 0.19433078\n",
      "Iteration 307, loss = 0.19335057\n",
      "Iteration 308, loss = 0.19244227\n",
      "Iteration 309, loss = 0.19174385\n",
      "Iteration 310, loss = 0.19085076\n",
      "Iteration 311, loss = 0.18975689\n",
      "Iteration 312, loss = 0.18916620\n",
      "Iteration 313, loss = 0.18853216\n",
      "Iteration 314, loss = 0.18773461\n",
      "Iteration 315, loss = 0.18683164\n",
      "Iteration 316, loss = 0.18614903\n",
      "Iteration 317, loss = 0.18510848\n",
      "Iteration 318, loss = 0.18443225\n",
      "Iteration 319, loss = 0.18361306\n",
      "Iteration 320, loss = 0.18293076\n",
      "Iteration 321, loss = 0.18191071\n",
      "Iteration 322, loss = 0.18122172\n",
      "Iteration 323, loss = 0.18035109\n",
      "Iteration 324, loss = 0.17964035\n",
      "Iteration 325, loss = 0.17885454\n",
      "Iteration 326, loss = 0.17822525\n",
      "Iteration 327, loss = 0.17737744\n",
      "Iteration 328, loss = 0.17680891\n",
      "Iteration 329, loss = 0.17587024\n",
      "Iteration 330, loss = 0.17523060\n",
      "Iteration 331, loss = 0.17447983\n",
      "Iteration 332, loss = 0.17380686\n",
      "Iteration 333, loss = 0.17324573\n",
      "Iteration 334, loss = 0.17232112\n",
      "Iteration 335, loss = 0.17160025\n",
      "Iteration 336, loss = 0.17090692\n",
      "Iteration 337, loss = 0.17021806\n",
      "Iteration 338, loss = 0.16947458\n",
      "Iteration 339, loss = 0.16884348\n",
      "Iteration 340, loss = 0.16797283\n",
      "Iteration 341, loss = 0.16745488\n",
      "Iteration 342, loss = 0.16668086\n",
      "Iteration 343, loss = 0.16617954\n",
      "Iteration 344, loss = 0.16522123\n",
      "Iteration 345, loss = 0.16490228\n",
      "Iteration 346, loss = 0.16404482\n",
      "Iteration 347, loss = 0.16346339\n",
      "Iteration 348, loss = 0.16283265\n",
      "Iteration 349, loss = 0.16219554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 350, loss = 0.16151394\n",
      "Iteration 351, loss = 0.16078277\n",
      "Iteration 352, loss = 0.16020924\n",
      "Iteration 353, loss = 0.15941608\n",
      "Iteration 354, loss = 0.15892590\n",
      "Iteration 355, loss = 0.15828927\n",
      "Iteration 356, loss = 0.15772549\n",
      "Iteration 357, loss = 0.15699518\n",
      "Iteration 358, loss = 0.15629608\n",
      "Iteration 359, loss = 0.15576000\n",
      "Iteration 360, loss = 0.15509468\n",
      "Iteration 361, loss = 0.15463720\n",
      "Iteration 362, loss = 0.15393751\n",
      "Iteration 363, loss = 0.15324088\n",
      "Iteration 364, loss = 0.15275656\n",
      "Iteration 365, loss = 0.15217961\n",
      "Iteration 366, loss = 0.15172107\n",
      "Iteration 367, loss = 0.15089837\n",
      "Iteration 368, loss = 0.15059529\n",
      "Iteration 369, loss = 0.14990252\n",
      "Iteration 370, loss = 0.14923178\n",
      "Iteration 371, loss = 0.14867130\n",
      "Iteration 372, loss = 0.14816523\n",
      "Iteration 373, loss = 0.14754876\n",
      "Iteration 374, loss = 0.14700140\n",
      "Iteration 375, loss = 0.14642965\n",
      "Iteration 376, loss = 0.14601514\n",
      "Iteration 377, loss = 0.14524276\n",
      "Iteration 378, loss = 0.14477154\n",
      "Iteration 379, loss = 0.14436931\n",
      "Iteration 380, loss = 0.14373423\n",
      "Iteration 381, loss = 0.14319209\n",
      "Iteration 382, loss = 0.14270156\n",
      "Iteration 383, loss = 0.14206675\n",
      "Iteration 384, loss = 0.14156596\n",
      "Iteration 385, loss = 0.14097878\n",
      "Iteration 386, loss = 0.14038958\n",
      "Iteration 387, loss = 0.14003108\n",
      "Iteration 388, loss = 0.13944313\n",
      "Iteration 389, loss = 0.13893834\n",
      "Iteration 390, loss = 0.13835843\n",
      "Iteration 391, loss = 0.13793786\n",
      "Iteration 392, loss = 0.13734127\n",
      "Iteration 393, loss = 0.13696359\n",
      "Iteration 394, loss = 0.13630891\n",
      "Iteration 395, loss = 0.13594968\n",
      "Iteration 396, loss = 0.13531299\n",
      "Iteration 397, loss = 0.13499004\n",
      "Iteration 398, loss = 0.13444737\n",
      "Iteration 399, loss = 0.13395994\n",
      "Iteration 400, loss = 0.13351160\n",
      "The training Accuracy achieved is = 98.754\n",
      "The test Accuracy achieved is = 85.985\n",
      "The number of epochs is = 400\n",
      "The training time achieved is = 161.802\n",
      "--------Training the classifier with following params--------------\n",
      "-------------------------------------------------------------------\n",
      "MLPClassifier(activation='relu', alpha=0.0001, batch_size=100, beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(100, 100), learning_rate='constant',\n",
      "              learning_rate_init=0.03, max_fun=15000, max_iter=400,\n",
      "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
      "              power_t=0.5, random_state=None, shuffle=True, solver='sgd',\n",
      "              tol=1e-05, validation_fraction=0.1, verbose=True,\n",
      "              warm_start=False)\n",
      "-------------------------------------------------------------------\n",
      "Iteration 1, loss = 2.50816383\n",
      "Iteration 2, loss = 1.03550767\n",
      "Iteration 3, loss = 0.80031625\n",
      "Iteration 4, loss = 0.65562967\n",
      "Iteration 5, loss = 0.54995677\n",
      "Iteration 6, loss = 0.46658296\n",
      "Iteration 7, loss = 0.40855145\n",
      "Iteration 8, loss = 0.35310345\n",
      "Iteration 9, loss = 0.31109136\n",
      "Iteration 10, loss = 0.27328861\n",
      "Iteration 11, loss = 0.24035228\n",
      "Iteration 12, loss = 0.21033184\n",
      "Iteration 13, loss = 0.18015944\n",
      "Iteration 14, loss = 0.15634685\n",
      "Iteration 15, loss = 0.13872468\n",
      "Iteration 16, loss = 0.11490303\n",
      "Iteration 17, loss = 0.09998127\n",
      "Iteration 18, loss = 0.08469757\n",
      "Iteration 19, loss = 0.07205677\n",
      "Iteration 20, loss = 0.06152100\n",
      "Iteration 21, loss = 0.04817851\n",
      "Iteration 22, loss = 0.04214471\n",
      "Iteration 23, loss = 0.03556062\n",
      "Iteration 24, loss = 0.02900015\n",
      "Iteration 25, loss = 0.02438611\n",
      "Iteration 26, loss = 0.02088990\n",
      "Iteration 27, loss = 0.01700693\n",
      "Iteration 28, loss = 0.01508868\n",
      "Iteration 29, loss = 0.01333514\n",
      "Iteration 30, loss = 0.01200468\n",
      "Iteration 31, loss = 0.01096737\n",
      "Iteration 32, loss = 0.01002612\n",
      "Iteration 33, loss = 0.00930630\n",
      "Iteration 34, loss = 0.00878092\n",
      "Iteration 35, loss = 0.00816508\n",
      "Iteration 36, loss = 0.00774515\n",
      "Iteration 37, loss = 0.00730698\n",
      "Iteration 38, loss = 0.00699837\n",
      "Iteration 39, loss = 0.00667722\n",
      "Iteration 40, loss = 0.00632899\n",
      "Iteration 41, loss = 0.00607510\n",
      "Iteration 42, loss = 0.00585758\n",
      "Iteration 43, loss = 0.00559460\n",
      "Iteration 44, loss = 0.00540806\n",
      "Iteration 45, loss = 0.00521687\n",
      "Iteration 46, loss = 0.00502819\n",
      "Iteration 47, loss = 0.00483502\n",
      "Iteration 48, loss = 0.00470609\n",
      "Iteration 49, loss = 0.00457549\n",
      "Iteration 50, loss = 0.00442761\n",
      "Iteration 51, loss = 0.00432187\n",
      "Iteration 52, loss = 0.00417088\n",
      "Iteration 53, loss = 0.00406345\n",
      "Iteration 54, loss = 0.00397302\n",
      "Iteration 55, loss = 0.00384805\n",
      "Iteration 56, loss = 0.00378185\n",
      "Iteration 57, loss = 0.00368852\n",
      "Iteration 58, loss = 0.00359066\n",
      "Iteration 59, loss = 0.00352527\n",
      "Iteration 60, loss = 0.00345755\n",
      "Iteration 61, loss = 0.00339083\n",
      "Iteration 62, loss = 0.00331779\n",
      "Iteration 63, loss = 0.00325677\n",
      "Iteration 64, loss = 0.00318057\n",
      "Iteration 65, loss = 0.00312447\n",
      "Iteration 66, loss = 0.00306888\n",
      "Iteration 67, loss = 0.00302831\n",
      "Iteration 68, loss = 0.00296707\n",
      "Iteration 69, loss = 0.00291042\n",
      "Iteration 70, loss = 0.00286782\n",
      "Iteration 71, loss = 0.00282112\n",
      "Iteration 72, loss = 0.00278308\n",
      "Iteration 73, loss = 0.00274146\n",
      "Iteration 74, loss = 0.00270168\n",
      "Iteration 75, loss = 0.00265672\n",
      "Iteration 76, loss = 0.00263024\n",
      "Iteration 77, loss = 0.00258818\n",
      "Iteration 78, loss = 0.00255951\n",
      "Iteration 79, loss = 0.00251788\n",
      "Iteration 80, loss = 0.00248601\n",
      "Iteration 81, loss = 0.00245561\n",
      "Iteration 82, loss = 0.00242803\n",
      "Iteration 83, loss = 0.00239440\n",
      "Iteration 84, loss = 0.00237409\n",
      "Iteration 85, loss = 0.00234379\n",
      "Iteration 86, loss = 0.00231379\n",
      "Iteration 87, loss = 0.00229338\n",
      "Iteration 88, loss = 0.00226435\n",
      "Iteration 89, loss = 0.00223859\n",
      "Iteration 90, loss = 0.00221807\n",
      "Iteration 91, loss = 0.00219839\n",
      "Iteration 92, loss = 0.00217207\n",
      "Iteration 93, loss = 0.00215391\n",
      "Iteration 94, loss = 0.00213435\n",
      "Iteration 95, loss = 0.00211181\n",
      "Iteration 96, loss = 0.00209314\n",
      "Iteration 97, loss = 0.00207582\n",
      "Iteration 98, loss = 0.00205340\n",
      "Iteration 99, loss = 0.00203770\n",
      "Iteration 100, loss = 0.00202272\n",
      "Iteration 101, loss = 0.00200419\n",
      "Iteration 102, loss = 0.00198849\n",
      "Iteration 103, loss = 0.00197368\n",
      "Iteration 104, loss = 0.00195758\n",
      "Iteration 105, loss = 0.00194029\n",
      "Iteration 106, loss = 0.00192265\n",
      "Iteration 107, loss = 0.00191210\n",
      "Iteration 108, loss = 0.00189567\n",
      "Iteration 109, loss = 0.00188651\n",
      "Iteration 110, loss = 0.00187229\n",
      "Iteration 111, loss = 0.00185412\n",
      "Iteration 112, loss = 0.00184323\n",
      "Iteration 113, loss = 0.00182986\n",
      "Iteration 114, loss = 0.00181990\n",
      "Iteration 115, loss = 0.00180794\n",
      "Iteration 116, loss = 0.00179747\n",
      "Iteration 117, loss = 0.00178439\n",
      "Iteration 118, loss = 0.00177357\n",
      "Iteration 119, loss = 0.00176334\n",
      "Iteration 120, loss = 0.00175028\n",
      "Iteration 121, loss = 0.00173917\n",
      "Iteration 122, loss = 0.00172981\n",
      "Iteration 123, loss = 0.00172139\n",
      "Iteration 124, loss = 0.00171105\n",
      "Iteration 125, loss = 0.00170145\n",
      "Iteration 126, loss = 0.00168975\n",
      "Iteration 127, loss = 0.00168404\n",
      "Iteration 128, loss = 0.00167474\n",
      "Iteration 129, loss = 0.00166369\n",
      "Iteration 130, loss = 0.00165704\n",
      "Iteration 131, loss = 0.00164781\n",
      "Iteration 132, loss = 0.00163885\n",
      "Iteration 133, loss = 0.00163204\n",
      "Iteration 134, loss = 0.00162407\n",
      "Iteration 135, loss = 0.00161573\n",
      "Iteration 136, loss = 0.00160881\n",
      "Iteration 137, loss = 0.00160036\n",
      "Iteration 138, loss = 0.00159316\n",
      "Iteration 139, loss = 0.00158582\n",
      "Iteration 140, loss = 0.00157948\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
      "The training Accuracy achieved is = 100.000\n",
      "The test Accuracy achieved is = 87.138\n",
      "The number of epochs is = 140\n",
      "The training time achieved is = 54.048\n",
      "--------Training the classifier with following params--------------\n",
      "-------------------------------------------------------------------\n",
      "MLPClassifier(activation='relu', alpha=0.0001, batch_size=100, beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(100, 100), learning_rate='constant',\n",
      "              learning_rate_init=0.1, max_fun=15000, max_iter=400, momentum=0.9,\n",
      "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "              random_state=None, shuffle=True, solver='sgd', tol=1e-05,\n",
      "              validation_fraction=0.1, verbose=True, warm_start=False)\n",
      "-------------------------------------------------------------------\n",
      "Iteration 1, loss = 2.28002440\n",
      "Iteration 2, loss = 0.93314845\n",
      "Iteration 3, loss = 0.72801301\n",
      "Iteration 4, loss = 0.61449528\n",
      "Iteration 5, loss = 0.54836739\n",
      "Iteration 6, loss = 0.48893531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 7, loss = 0.45466208\n",
      "Iteration 8, loss = 0.41740540\n",
      "Iteration 9, loss = 0.40251739\n",
      "Iteration 10, loss = 0.38387278\n",
      "Iteration 11, loss = 0.38546111\n",
      "Iteration 12, loss = 0.31049517\n",
      "Iteration 13, loss = 0.29283614\n",
      "Iteration 14, loss = 0.29064743\n",
      "Iteration 15, loss = 0.28061811\n",
      "Iteration 16, loss = 0.26203566\n",
      "Iteration 17, loss = 0.23036564\n",
      "Iteration 18, loss = 0.25744236\n",
      "Iteration 19, loss = 0.28180248\n",
      "Iteration 20, loss = 0.26243069\n",
      "Iteration 21, loss = 0.23746253\n",
      "Iteration 22, loss = 0.22963996\n",
      "Iteration 23, loss = 0.21726602\n",
      "Iteration 24, loss = 0.29524830\n",
      "Iteration 25, loss = 0.21364457\n",
      "Iteration 26, loss = 0.23123348\n",
      "Iteration 27, loss = 0.22795417\n",
      "Iteration 28, loss = 0.26122991\n",
      "Iteration 29, loss = 0.23128931\n",
      "Iteration 30, loss = 0.23307178\n",
      "Iteration 31, loss = 0.22865244\n",
      "Iteration 32, loss = 0.26366610\n",
      "Iteration 33, loss = 0.22509109\n",
      "Iteration 34, loss = 0.22665693\n",
      "Iteration 35, loss = 0.20683230\n",
      "Iteration 36, loss = 0.21010729\n",
      "Iteration 37, loss = 0.23322607\n",
      "Iteration 38, loss = 0.23006940\n",
      "Iteration 39, loss = 0.27472534\n",
      "Iteration 40, loss = 0.30832348\n",
      "Iteration 41, loss = 0.24362074\n",
      "Iteration 42, loss = 0.24378863\n",
      "Iteration 43, loss = 0.27747537\n",
      "Iteration 44, loss = 0.28507226\n",
      "Iteration 45, loss = 0.26051795\n",
      "Iteration 46, loss = 0.24829271\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
      "The training Accuracy achieved is = 95.100\n",
      "The test Accuracy achieved is = 83.800\n",
      "The number of epochs is = 46\n",
      "The training time achieved is = 17.847\n",
      "--------Training the classifier with following params--------------\n",
      "-------------------------------------------------------------------\n",
      "MLPClassifier(activation='relu', alpha=0.0001, batch_size=100, beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(100, 100), learning_rate='constant',\n",
      "              learning_rate_init=0.1, max_fun=15000, max_iter=400, momentum=0.9,\n",
      "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "              random_state=None, shuffle=True, solver='sgd', tol=1e-05,\n",
      "              validation_fraction=0.1, verbose=True, warm_start=False)\n",
      "-------------------------------------------------------------------\n",
      "Iteration 1, loss = 2.20622490\n",
      "Validation score: 0.721538\n",
      "Iteration 2, loss = 0.82468036\n",
      "Validation score: 0.795385\n",
      "Iteration 3, loss = 0.63752924\n",
      "Validation score: 0.788462\n",
      "Iteration 4, loss = 0.52529713\n",
      "Validation score: 0.835385\n",
      "Iteration 5, loss = 0.45804845\n",
      "Validation score: 0.841538\n",
      "Iteration 6, loss = 0.40087818\n",
      "Validation score: 0.833846\n",
      "Iteration 7, loss = 0.35909792\n",
      "Validation score: 0.843846\n",
      "Iteration 8, loss = 0.32292334\n",
      "Validation score: 0.853846\n",
      "Iteration 9, loss = 0.28772920\n",
      "Validation score: 0.861538\n",
      "Iteration 10, loss = 0.28075918\n",
      "Validation score: 0.852308\n",
      "Iteration 11, loss = 0.25526977\n",
      "Validation score: 0.859231\n",
      "Iteration 12, loss = 0.26589626\n",
      "Validation score: 0.856154\n",
      "Iteration 13, loss = 0.23561895\n",
      "Validation score: 0.855385\n",
      "Iteration 14, loss = 0.23430291\n",
      "Validation score: 0.843846\n",
      "Iteration 15, loss = 0.21951470\n",
      "Validation score: 0.856923\n",
      "Iteration 16, loss = 0.19478360\n",
      "Validation score: 0.870000\n",
      "Iteration 17, loss = 0.16384635\n",
      "Validation score: 0.860000\n",
      "Iteration 18, loss = 0.21333081\n",
      "Validation score: 0.865385\n",
      "Iteration 19, loss = 0.21018309\n",
      "Validation score: 0.859231\n",
      "Iteration 20, loss = 0.19926010\n",
      "Validation score: 0.842308\n",
      "Iteration 21, loss = 0.20578398\n",
      "Validation score: 0.860769\n",
      "Iteration 22, loss = 0.21082835\n",
      "Validation score: 0.866923\n",
      "Iteration 23, loss = 0.21561982\n",
      "Validation score: 0.847692\n",
      "Iteration 24, loss = 0.22850654\n",
      "Validation score: 0.838462\n",
      "Iteration 25, loss = 0.24207469\n",
      "Validation score: 0.867692\n",
      "Iteration 26, loss = 0.21472789\n",
      "Validation score: 0.859231\n",
      "Iteration 27, loss = 0.16972725\n",
      "Validation score: 0.853846\n",
      "Validation score did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
      "The training Accuracy achieved is = 94.485\n",
      "The test Accuracy achieved is = 83.954\n",
      "The number of epochs is = 27\n",
      "The training time achieved is = 9.879\n",
      "--------Training the classifier with following params--------------\n",
      "-------------------------------------------------------------------\n",
      "MLPClassifier(activation='relu', alpha=0.0001, batch_size=100, beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(100, 100), learning_rate='invscaling',\n",
      "              learning_rate_init=0.1, max_fun=15000, max_iter=400, momentum=0.9,\n",
      "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "              random_state=None, shuffle=True, solver='sgd', tol=1e-05,\n",
      "              validation_fraction=0.1, verbose=True, warm_start=False)\n",
      "-------------------------------------------------------------------\n",
      "Iteration 1, loss = 2.73069938\n",
      "Iteration 2, loss = 1.56754722\n",
      "Iteration 3, loss = 1.22990585\n",
      "Iteration 4, loss = 1.16991265\n",
      "Iteration 5, loss = 1.13561008\n",
      "Iteration 6, loss = 1.11233189\n",
      "Iteration 7, loss = 1.09477943\n",
      "Iteration 8, loss = 1.08093597\n",
      "Iteration 9, loss = 1.06987084\n",
      "Iteration 10, loss = 1.06043249\n",
      "Iteration 11, loss = 1.05250075\n",
      "Iteration 12, loss = 1.04554116\n",
      "Iteration 13, loss = 1.03944531\n",
      "Iteration 14, loss = 1.03413500\n",
      "Iteration 15, loss = 1.02942758\n",
      "Iteration 16, loss = 1.02514721\n",
      "Iteration 17, loss = 1.02121338\n",
      "Iteration 18, loss = 1.01770199\n",
      "Iteration 19, loss = 1.01428754\n",
      "Iteration 20, loss = 1.01124867\n",
      "Iteration 21, loss = 1.00827942\n",
      "Iteration 22, loss = 1.00553671\n",
      "Iteration 23, loss = 1.00299230\n",
      "Iteration 24, loss = 1.00054891\n",
      "Iteration 25, loss = 0.99819390\n",
      "Iteration 26, loss = 0.99612769\n",
      "Iteration 27, loss = 0.99403191\n",
      "Iteration 28, loss = 0.99200976\n",
      "Iteration 29, loss = 0.99008519\n",
      "Iteration 30, loss = 0.98819271\n",
      "Iteration 31, loss = 0.98646392\n",
      "Iteration 32, loss = 0.98476812\n",
      "Iteration 33, loss = 0.98317449\n",
      "Iteration 34, loss = 0.98157414\n",
      "Iteration 35, loss = 0.98001716\n",
      "Iteration 36, loss = 0.97860030\n",
      "Iteration 37, loss = 0.97719309\n",
      "Iteration 38, loss = 0.97579893\n",
      "Iteration 39, loss = 0.97449043\n",
      "Iteration 40, loss = 0.97309923\n",
      "Iteration 41, loss = 0.97186980\n",
      "Iteration 42, loss = 0.97061660\n",
      "Iteration 43, loss = 0.96939761\n",
      "Iteration 44, loss = 0.96821220\n",
      "Iteration 45, loss = 0.96707385\n",
      "Iteration 46, loss = 0.96597914\n",
      "Iteration 47, loss = 0.96480615\n",
      "Iteration 48, loss = 0.96371473\n",
      "Iteration 49, loss = 0.96265340\n",
      "Iteration 50, loss = 0.96162773\n",
      "Iteration 51, loss = 0.96061160\n",
      "Iteration 52, loss = 0.95964258\n",
      "Iteration 53, loss = 0.95867207\n",
      "Iteration 54, loss = 0.95768965\n",
      "Iteration 55, loss = 0.95677520\n",
      "Iteration 56, loss = 0.95587059\n",
      "Iteration 57, loss = 0.95495740\n",
      "Iteration 58, loss = 0.95413856\n",
      "Iteration 59, loss = 0.95313865\n",
      "Iteration 60, loss = 0.95230443\n",
      "Iteration 61, loss = 0.95146209\n",
      "Iteration 62, loss = 0.95061197\n",
      "Iteration 63, loss = 0.94980701\n",
      "Iteration 64, loss = 0.94900130\n",
      "Iteration 65, loss = 0.94818868\n",
      "Iteration 66, loss = 0.94735442\n",
      "Iteration 67, loss = 0.94666433\n",
      "Iteration 68, loss = 0.94584680\n",
      "Iteration 69, loss = 0.94513167\n",
      "Iteration 70, loss = 0.94443642\n",
      "Iteration 71, loss = 0.94365084\n",
      "Iteration 72, loss = 0.94294501\n",
      "Iteration 73, loss = 0.94220648\n",
      "Iteration 74, loss = 0.94150838\n",
      "Iteration 75, loss = 0.94078014\n",
      "Iteration 76, loss = 0.94013356\n",
      "Iteration 77, loss = 0.93946519\n",
      "Iteration 78, loss = 0.93877795\n",
      "Iteration 79, loss = 0.93810190\n",
      "Iteration 80, loss = 0.93747966\n",
      "Iteration 81, loss = 0.93685686\n",
      "Iteration 82, loss = 0.93613788\n",
      "Iteration 83, loss = 0.93550625\n",
      "Iteration 84, loss = 0.93493129\n",
      "Iteration 85, loss = 0.93429399\n",
      "Iteration 86, loss = 0.93366307\n",
      "Iteration 87, loss = 0.93306444\n",
      "Iteration 88, loss = 0.93249977\n",
      "Iteration 89, loss = 0.93186505\n",
      "Iteration 90, loss = 0.93124324\n",
      "Iteration 91, loss = 0.93068502\n",
      "Iteration 92, loss = 0.93011523\n",
      "Iteration 93, loss = 0.92953503\n",
      "Iteration 94, loss = 0.92897212\n",
      "Iteration 95, loss = 0.92844283\n",
      "Iteration 96, loss = 0.92785112\n",
      "Iteration 97, loss = 0.92733312\n",
      "Iteration 98, loss = 0.92675049\n",
      "Iteration 99, loss = 0.92619843\n",
      "Iteration 100, loss = 0.92563810\n",
      "Iteration 101, loss = 0.92515549\n",
      "Iteration 102, loss = 0.92458527\n",
      "Iteration 103, loss = 0.92403709\n",
      "Iteration 104, loss = 0.92354200\n",
      "Iteration 105, loss = 0.92303621\n",
      "Iteration 106, loss = 0.92253363\n",
      "Iteration 107, loss = 0.92200707\n",
      "Iteration 108, loss = 0.92146465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 109, loss = 0.92095441\n",
      "Iteration 110, loss = 0.92045281\n",
      "Iteration 111, loss = 0.91997026\n",
      "Iteration 112, loss = 0.91946343\n",
      "Iteration 113, loss = 0.91898389\n",
      "Iteration 114, loss = 0.91854619\n",
      "Iteration 115, loss = 0.91803954\n",
      "Iteration 116, loss = 0.91754126\n",
      "Iteration 117, loss = 0.91708428\n",
      "Iteration 118, loss = 0.91662288\n",
      "Iteration 119, loss = 0.91617878\n",
      "Iteration 120, loss = 0.91568364\n",
      "Iteration 121, loss = 0.91523951\n",
      "Iteration 122, loss = 0.91481068\n",
      "Iteration 123, loss = 0.91431557\n",
      "Iteration 124, loss = 0.91385689\n",
      "Iteration 125, loss = 0.91343110\n",
      "Iteration 126, loss = 0.91296676\n",
      "Iteration 127, loss = 0.91258223\n",
      "Iteration 128, loss = 0.91208481\n",
      "Iteration 129, loss = 0.91164543\n",
      "Iteration 130, loss = 0.91127515\n",
      "Iteration 131, loss = 0.91086186\n",
      "Iteration 132, loss = 0.91038538\n",
      "Iteration 133, loss = 0.90996572\n",
      "Iteration 134, loss = 0.90954440\n",
      "Iteration 135, loss = 0.90909752\n",
      "Iteration 136, loss = 0.90871332\n",
      "Iteration 137, loss = 0.90829956\n",
      "Iteration 138, loss = 0.90791209\n",
      "Iteration 139, loss = 0.90751258\n",
      "Iteration 140, loss = 0.90703604\n",
      "Iteration 141, loss = 0.90666029\n",
      "Iteration 142, loss = 0.90629026\n",
      "Iteration 143, loss = 0.90587952\n",
      "Iteration 144, loss = 0.90548054\n",
      "Iteration 145, loss = 0.90509890\n",
      "Iteration 146, loss = 0.90468873\n",
      "Iteration 147, loss = 0.90428534\n",
      "Iteration 148, loss = 0.90389029\n",
      "Iteration 149, loss = 0.90353455\n",
      "Iteration 150, loss = 0.90310253\n",
      "Iteration 151, loss = 0.90279044\n",
      "Iteration 152, loss = 0.90237694\n",
      "Iteration 153, loss = 0.90199241\n",
      "Iteration 154, loss = 0.90164290\n",
      "Iteration 155, loss = 0.90127558\n",
      "Iteration 156, loss = 0.90087702\n",
      "Iteration 157, loss = 0.90052435\n",
      "Iteration 158, loss = 0.90013694\n",
      "Iteration 159, loss = 0.89977345\n",
      "Iteration 160, loss = 0.89940305\n",
      "Iteration 161, loss = 0.89902009\n",
      "Iteration 162, loss = 0.89870088\n",
      "Iteration 163, loss = 0.89832142\n",
      "Iteration 164, loss = 0.89795020\n",
      "Iteration 165, loss = 0.89759193\n",
      "Iteration 166, loss = 0.89725810\n",
      "Iteration 167, loss = 0.89690390\n",
      "Iteration 168, loss = 0.89656912\n",
      "Iteration 169, loss = 0.89619620\n",
      "Iteration 170, loss = 0.89586150\n",
      "Iteration 171, loss = 0.89547082\n",
      "Iteration 172, loss = 0.89515913\n",
      "Iteration 173, loss = 0.89482783\n",
      "Iteration 174, loss = 0.89447282\n",
      "Iteration 175, loss = 0.89415096\n",
      "Iteration 176, loss = 0.89383148\n",
      "Iteration 177, loss = 0.89348500\n",
      "Iteration 178, loss = 0.89313782\n",
      "Iteration 179, loss = 0.89278040\n",
      "Iteration 180, loss = 0.89245217\n",
      "Iteration 181, loss = 0.89211110\n",
      "Iteration 182, loss = 0.89180730\n",
      "Iteration 183, loss = 0.89147536\n",
      "Iteration 184, loss = 0.89115425\n",
      "Iteration 185, loss = 0.89080590\n",
      "Iteration 186, loss = 0.89053813\n",
      "Iteration 187, loss = 0.89015589\n",
      "Iteration 188, loss = 0.88986530\n",
      "Iteration 189, loss = 0.88955640\n",
      "Iteration 190, loss = 0.88921223\n",
      "Iteration 191, loss = 0.88890387\n",
      "Iteration 192, loss = 0.88860981\n",
      "Iteration 193, loss = 0.88828393\n",
      "Iteration 194, loss = 0.88795991\n",
      "Iteration 195, loss = 0.88765217\n",
      "Iteration 196, loss = 0.88733533\n",
      "Iteration 197, loss = 0.88701514\n",
      "Iteration 198, loss = 0.88675554\n",
      "Iteration 199, loss = 0.88644113\n",
      "Iteration 200, loss = 0.88612434\n",
      "Iteration 201, loss = 0.88584277\n",
      "Iteration 202, loss = 0.88554701\n",
      "Iteration 203, loss = 0.88522004\n",
      "Iteration 204, loss = 0.88494350\n",
      "Iteration 205, loss = 0.88462061\n",
      "Iteration 206, loss = 0.88432347\n",
      "Iteration 207, loss = 0.88402077\n",
      "Iteration 208, loss = 0.88373344\n",
      "Iteration 209, loss = 0.88343249\n",
      "Iteration 210, loss = 0.88316110\n",
      "Iteration 211, loss = 0.88285700\n",
      "Iteration 212, loss = 0.88258207\n",
      "Iteration 213, loss = 0.88226168\n",
      "Iteration 214, loss = 0.88197776\n",
      "Iteration 215, loss = 0.88169938\n",
      "Iteration 216, loss = 0.88142482\n",
      "Iteration 217, loss = 0.88113712\n",
      "Iteration 218, loss = 0.88086989\n",
      "Iteration 219, loss = 0.88054166\n",
      "Iteration 220, loss = 0.88030006\n",
      "Iteration 221, loss = 0.88001308\n",
      "Iteration 222, loss = 0.87972494\n",
      "Iteration 223, loss = 0.87945557\n",
      "Iteration 224, loss = 0.87915715\n",
      "Iteration 225, loss = 0.87889472\n",
      "Iteration 226, loss = 0.87862136\n",
      "Iteration 227, loss = 0.87831993\n",
      "Iteration 228, loss = 0.87806034\n",
      "Iteration 229, loss = 0.87778997\n",
      "Iteration 230, loss = 0.87749902\n",
      "Iteration 231, loss = 0.87722678\n",
      "Iteration 232, loss = 0.87696053\n",
      "Iteration 233, loss = 0.87669165\n",
      "Iteration 234, loss = 0.87641132\n",
      "Iteration 235, loss = 0.87615519\n",
      "Iteration 236, loss = 0.87586667\n",
      "Iteration 237, loss = 0.87561978\n",
      "Iteration 238, loss = 0.87536993\n",
      "Iteration 239, loss = 0.87507459\n",
      "Iteration 240, loss = 0.87482282\n",
      "Iteration 241, loss = 0.87455184\n",
      "Iteration 242, loss = 0.87427412\n",
      "Iteration 243, loss = 0.87403453\n",
      "Iteration 244, loss = 0.87376127\n",
      "Iteration 245, loss = 0.87351324\n",
      "Iteration 246, loss = 0.87322940\n",
      "Iteration 247, loss = 0.87298439\n",
      "Iteration 248, loss = 0.87270840\n",
      "Iteration 249, loss = 0.87249207\n",
      "Iteration 250, loss = 0.87222541\n",
      "Iteration 251, loss = 0.87199293\n",
      "Iteration 252, loss = 0.87169795\n",
      "Iteration 253, loss = 0.87147275\n",
      "Iteration 254, loss = 0.87119471\n",
      "Iteration 255, loss = 0.87094623\n",
      "Iteration 256, loss = 0.87070522\n",
      "Iteration 257, loss = 0.87046585\n",
      "Iteration 258, loss = 0.87024480\n",
      "Iteration 259, loss = 0.86994595\n",
      "Iteration 260, loss = 0.86970187\n",
      "Iteration 261, loss = 0.86944887\n",
      "Iteration 262, loss = 0.86923826\n",
      "Iteration 263, loss = 0.86895764\n",
      "Iteration 264, loss = 0.86873377\n",
      "Iteration 265, loss = 0.86846294\n",
      "Iteration 266, loss = 0.86820217\n",
      "Iteration 267, loss = 0.86797578\n",
      "Iteration 268, loss = 0.86774435\n",
      "Iteration 269, loss = 0.86749780\n",
      "Iteration 270, loss = 0.86728339\n",
      "Iteration 271, loss = 0.86702202\n",
      "Iteration 272, loss = 0.86679662\n",
      "Iteration 273, loss = 0.86650783\n",
      "Iteration 274, loss = 0.86631299\n",
      "Iteration 275, loss = 0.86604680\n",
      "Iteration 276, loss = 0.86581513\n",
      "Iteration 277, loss = 0.86557141\n",
      "Iteration 278, loss = 0.86532365\n",
      "Iteration 279, loss = 0.86511748\n",
      "Iteration 280, loss = 0.86488798\n",
      "Iteration 281, loss = 0.86465772\n",
      "Iteration 282, loss = 0.86441208\n",
      "Iteration 283, loss = 0.86416470\n",
      "Iteration 284, loss = 0.86392620\n",
      "Iteration 285, loss = 0.86368242\n",
      "Iteration 286, loss = 0.86347230\n",
      "Iteration 287, loss = 0.86324716\n",
      "Iteration 288, loss = 0.86302133\n",
      "Iteration 289, loss = 0.86280079\n",
      "Iteration 290, loss = 0.86256099\n",
      "Iteration 291, loss = 0.86231494\n",
      "Iteration 292, loss = 0.86210548\n",
      "Iteration 293, loss = 0.86186383\n",
      "Iteration 294, loss = 0.86164434\n",
      "Iteration 295, loss = 0.86143789\n",
      "Iteration 296, loss = 0.86119656\n",
      "Iteration 297, loss = 0.86094361\n",
      "Iteration 298, loss = 0.86075506\n",
      "Iteration 299, loss = 0.86051705\n",
      "Iteration 300, loss = 0.86031954\n",
      "Iteration 301, loss = 0.86006875\n",
      "Iteration 302, loss = 0.85985305\n",
      "Iteration 303, loss = 0.85964148\n",
      "Iteration 304, loss = 0.85942254\n",
      "Iteration 305, loss = 0.85918128\n",
      "Iteration 306, loss = 0.85898205\n",
      "Iteration 307, loss = 0.85875012\n",
      "Iteration 308, loss = 0.85852913\n",
      "Iteration 309, loss = 0.85829491\n",
      "Iteration 310, loss = 0.85810805\n",
      "Iteration 311, loss = 0.85789204\n",
      "Iteration 312, loss = 0.85769343\n",
      "Iteration 313, loss = 0.85744985\n",
      "Iteration 314, loss = 0.85724223\n",
      "Iteration 315, loss = 0.85701749\n",
      "Iteration 316, loss = 0.85680797\n",
      "Iteration 317, loss = 0.85660332\n",
      "Iteration 318, loss = 0.85637421\n",
      "Iteration 319, loss = 0.85615233\n",
      "Iteration 320, loss = 0.85597940\n",
      "Iteration 321, loss = 0.85574536\n",
      "Iteration 322, loss = 0.85553427\n",
      "Iteration 323, loss = 0.85532914\n",
      "Iteration 324, loss = 0.85510694\n",
      "Iteration 325, loss = 0.85489991\n",
      "Iteration 326, loss = 0.85469504\n",
      "Iteration 327, loss = 0.85450140\n",
      "Iteration 328, loss = 0.85426880\n",
      "Iteration 329, loss = 0.85410838\n",
      "Iteration 330, loss = 0.85385225\n",
      "Iteration 331, loss = 0.85365439\n",
      "Iteration 332, loss = 0.85346910\n",
      "Iteration 333, loss = 0.85330232\n",
      "Iteration 334, loss = 0.85305267\n",
      "Iteration 335, loss = 0.85284099\n",
      "Iteration 336, loss = 0.85261280\n",
      "Iteration 337, loss = 0.85240083\n",
      "Iteration 338, loss = 0.85221966\n",
      "Iteration 339, loss = 0.85202289\n",
      "Iteration 340, loss = 0.85182287\n",
      "Iteration 341, loss = 0.85163129\n",
      "Iteration 342, loss = 0.85139298\n",
      "Iteration 343, loss = 0.85121011\n",
      "Iteration 344, loss = 0.85102597\n",
      "Iteration 345, loss = 0.85081844\n",
      "Iteration 346, loss = 0.85061148\n",
      "Iteration 347, loss = 0.85041033\n",
      "Iteration 348, loss = 0.85023026\n",
      "Iteration 349, loss = 0.85002227\n",
      "Iteration 350, loss = 0.84978844\n",
      "Iteration 351, loss = 0.84963045\n",
      "Iteration 352, loss = 0.84941231\n",
      "Iteration 353, loss = 0.84921127\n",
      "Iteration 354, loss = 0.84901192\n",
      "Iteration 355, loss = 0.84882301\n",
      "Iteration 356, loss = 0.84862512\n",
      "Iteration 357, loss = 0.84846401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 358, loss = 0.84824036\n",
      "Iteration 359, loss = 0.84803610\n",
      "Iteration 360, loss = 0.84786221\n",
      "Iteration 361, loss = 0.84766666\n",
      "Iteration 362, loss = 0.84746268\n",
      "Iteration 363, loss = 0.84727543\n",
      "Iteration 364, loss = 0.84706110\n",
      "Iteration 365, loss = 0.84686935\n",
      "Iteration 366, loss = 0.84667067\n",
      "Iteration 367, loss = 0.84648897\n",
      "Iteration 368, loss = 0.84633199\n",
      "Iteration 369, loss = 0.84610225\n",
      "Iteration 370, loss = 0.84590900\n",
      "Iteration 371, loss = 0.84574617\n",
      "Iteration 372, loss = 0.84552312\n",
      "Iteration 373, loss = 0.84533882\n",
      "Iteration 374, loss = 0.84516721\n",
      "Iteration 375, loss = 0.84497333\n",
      "Iteration 376, loss = 0.84477417\n",
      "Iteration 377, loss = 0.84458420\n",
      "Iteration 378, loss = 0.84443227\n",
      "Iteration 379, loss = 0.84421148\n",
      "Iteration 380, loss = 0.84403171\n",
      "Iteration 381, loss = 0.84380755\n",
      "Iteration 382, loss = 0.84365738\n",
      "Iteration 383, loss = 0.84346041\n",
      "Iteration 384, loss = 0.84325605\n",
      "Iteration 385, loss = 0.84308005\n",
      "Iteration 386, loss = 0.84289855\n",
      "Iteration 387, loss = 0.84270491\n",
      "Iteration 388, loss = 0.84250289\n",
      "Iteration 389, loss = 0.84234348\n",
      "Iteration 390, loss = 0.84212799\n",
      "Iteration 391, loss = 0.84197018\n",
      "Iteration 392, loss = 0.84179111\n",
      "Iteration 393, loss = 0.84161492\n",
      "Iteration 394, loss = 0.84141706\n",
      "Iteration 395, loss = 0.84124264\n",
      "Iteration 396, loss = 0.84106193\n",
      "Iteration 397, loss = 0.84088340\n",
      "Iteration 398, loss = 0.84068984\n",
      "Iteration 399, loss = 0.84051369\n",
      "Iteration 400, loss = 0.84032190\n",
      "The training Accuracy achieved is = 78.238\n",
      "The test Accuracy achieved is = 73.354\n",
      "The number of epochs is = 400\n",
      "The training time achieved is = 145.058\n",
      "--------Training the classifier with following params--------------\n",
      "-------------------------------------------------------------------\n",
      "MLPClassifier(activation='relu', alpha=0.0001, batch_size=100, beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(100, 100), learning_rate='invscaling',\n",
      "              learning_rate_init=0.1, max_fun=15000, max_iter=400, momentum=0.9,\n",
      "              n_iter_no_change=10, nesterovs_momentum=True,\n",
      "              power_t=0.3333333333333333, random_state=None, shuffle=True,\n",
      "              solver='sgd', tol=1e-05, validation_fraction=0.1, verbose=True,\n",
      "              warm_start=False)\n",
      "-------------------------------------------------------------------\n",
      "Iteration 1, loss = 2.16621442\n",
      "Iteration 2, loss = 0.88704192\n",
      "Iteration 3, loss = 0.69863746\n",
      "Iteration 4, loss = 0.65769184\n",
      "Iteration 5, loss = 0.63482190\n",
      "Iteration 6, loss = 0.61623806\n",
      "Iteration 7, loss = 0.60254822\n",
      "Iteration 8, loss = 0.59003410\n",
      "Iteration 9, loss = 0.58026566\n",
      "Iteration 10, loss = 0.56972129\n",
      "Iteration 11, loss = 0.56155086\n",
      "Iteration 12, loss = 0.55297599\n",
      "Iteration 13, loss = 0.54581243\n",
      "Iteration 14, loss = 0.53914658\n",
      "Iteration 15, loss = 0.53221415\n",
      "Iteration 16, loss = 0.52606206\n",
      "Iteration 17, loss = 0.52030167\n",
      "Iteration 18, loss = 0.51445561\n",
      "Iteration 19, loss = 0.50941221\n",
      "Iteration 20, loss = 0.50376875\n",
      "Iteration 21, loss = 0.49884585\n",
      "Iteration 22, loss = 0.49401444\n",
      "Iteration 23, loss = 0.48969436\n",
      "Iteration 24, loss = 0.48495763\n",
      "Iteration 25, loss = 0.48074517\n",
      "Iteration 26, loss = 0.47637311\n",
      "Iteration 27, loss = 0.47230945\n",
      "Iteration 28, loss = 0.46800025\n",
      "Iteration 29, loss = 0.46438749\n",
      "Iteration 30, loss = 0.46034384\n",
      "Iteration 31, loss = 0.45672435\n",
      "Iteration 32, loss = 0.45312908\n",
      "Iteration 33, loss = 0.44991649\n",
      "Iteration 34, loss = 0.44658453\n",
      "Iteration 35, loss = 0.44284812\n",
      "Iteration 36, loss = 0.43968531\n",
      "Iteration 37, loss = 0.43647997\n",
      "Iteration 38, loss = 0.43329739\n",
      "Iteration 39, loss = 0.43009271\n",
      "Iteration 40, loss = 0.42721819\n",
      "Iteration 41, loss = 0.42415931\n",
      "Iteration 42, loss = 0.42136557\n",
      "Iteration 43, loss = 0.41840101\n",
      "Iteration 44, loss = 0.41580193\n",
      "Iteration 45, loss = 0.41293213\n",
      "Iteration 46, loss = 0.41037191\n",
      "Iteration 47, loss = 0.40788199\n",
      "Iteration 48, loss = 0.40489639\n",
      "Iteration 49, loss = 0.40252284\n",
      "Iteration 50, loss = 0.39995187\n",
      "Iteration 51, loss = 0.39762975\n",
      "Iteration 52, loss = 0.39504587\n",
      "Iteration 53, loss = 0.39253267\n",
      "Iteration 54, loss = 0.39031653\n",
      "Iteration 55, loss = 0.38804422\n",
      "Iteration 56, loss = 0.38594870\n",
      "Iteration 57, loss = 0.38331217\n",
      "Iteration 58, loss = 0.38103784\n",
      "Iteration 59, loss = 0.37897563\n",
      "Iteration 60, loss = 0.37615880\n",
      "Iteration 61, loss = 0.37456281\n",
      "Iteration 62, loss = 0.37255830\n",
      "Iteration 63, loss = 0.37060547\n",
      "Iteration 64, loss = 0.36841044\n",
      "Iteration 65, loss = 0.36625983\n",
      "Iteration 66, loss = 0.36422700\n",
      "Iteration 67, loss = 0.36240795\n",
      "Iteration 68, loss = 0.36024725\n",
      "Iteration 69, loss = 0.35849824\n",
      "Iteration 70, loss = 0.35663679\n",
      "Iteration 71, loss = 0.35457984\n",
      "Iteration 72, loss = 0.35287724\n",
      "Iteration 73, loss = 0.35090402\n",
      "Iteration 74, loss = 0.34927141\n",
      "Iteration 75, loss = 0.34710628\n",
      "Iteration 76, loss = 0.34561852\n",
      "Iteration 77, loss = 0.34386433\n",
      "Iteration 78, loss = 0.34210510\n",
      "Iteration 79, loss = 0.34045918\n",
      "Iteration 80, loss = 0.33863091\n",
      "Iteration 81, loss = 0.33707664\n",
      "Iteration 82, loss = 0.33554235\n",
      "Iteration 83, loss = 0.33349366\n",
      "Iteration 84, loss = 0.33233039\n",
      "Iteration 85, loss = 0.33035315\n",
      "Iteration 86, loss = 0.32904203\n",
      "Iteration 87, loss = 0.32759413\n",
      "Iteration 88, loss = 0.32610480\n",
      "Iteration 89, loss = 0.32434816\n",
      "Iteration 90, loss = 0.32295796\n",
      "Iteration 91, loss = 0.32154743\n",
      "Iteration 92, loss = 0.31998755\n",
      "Iteration 93, loss = 0.31855709\n",
      "Iteration 94, loss = 0.31685953\n",
      "Iteration 95, loss = 0.31561629\n",
      "Iteration 96, loss = 0.31381420\n",
      "Iteration 97, loss = 0.31281781\n",
      "Iteration 98, loss = 0.31113966\n",
      "Iteration 99, loss = 0.30976118\n",
      "Iteration 100, loss = 0.30852872\n",
      "Iteration 101, loss = 0.30709462\n",
      "Iteration 102, loss = 0.30573355\n",
      "Iteration 103, loss = 0.30422383\n",
      "Iteration 104, loss = 0.30297511\n",
      "Iteration 105, loss = 0.30160951\n",
      "Iteration 106, loss = 0.30051993\n",
      "Iteration 107, loss = 0.29915113\n",
      "Iteration 108, loss = 0.29784543\n",
      "Iteration 109, loss = 0.29654304\n",
      "Iteration 110, loss = 0.29517652\n",
      "Iteration 111, loss = 0.29427892\n",
      "Iteration 112, loss = 0.29266417\n",
      "Iteration 113, loss = 0.29171915\n",
      "Iteration 114, loss = 0.29034561\n",
      "Iteration 115, loss = 0.28900165\n",
      "Iteration 116, loss = 0.28806185\n",
      "Iteration 117, loss = 0.28664647\n",
      "Iteration 118, loss = 0.28559241\n",
      "Iteration 119, loss = 0.28437959\n",
      "Iteration 120, loss = 0.28318465\n",
      "Iteration 121, loss = 0.28196110\n",
      "Iteration 122, loss = 0.28086865\n",
      "Iteration 123, loss = 0.27978978\n",
      "Iteration 124, loss = 0.27853470\n",
      "Iteration 125, loss = 0.27735090\n",
      "Iteration 126, loss = 0.27623138\n",
      "Iteration 127, loss = 0.27516970\n",
      "Iteration 128, loss = 0.27398319\n",
      "Iteration 129, loss = 0.27294679\n",
      "Iteration 130, loss = 0.27202449\n",
      "Iteration 131, loss = 0.27071451\n",
      "Iteration 132, loss = 0.26967554\n",
      "Iteration 133, loss = 0.26864990\n",
      "Iteration 134, loss = 0.26788568\n",
      "Iteration 135, loss = 0.26667805\n",
      "Iteration 136, loss = 0.26534466\n",
      "Iteration 137, loss = 0.26431130\n",
      "Iteration 138, loss = 0.26341696\n",
      "Iteration 139, loss = 0.26235560\n",
      "Iteration 140, loss = 0.26141435\n",
      "Iteration 141, loss = 0.26045846\n",
      "Iteration 142, loss = 0.25938291\n",
      "Iteration 143, loss = 0.25840776\n",
      "Iteration 144, loss = 0.25747536\n",
      "Iteration 145, loss = 0.25646417\n",
      "Iteration 146, loss = 0.25542842\n",
      "Iteration 147, loss = 0.25441023\n",
      "Iteration 148, loss = 0.25352092\n",
      "Iteration 149, loss = 0.25239377\n",
      "Iteration 150, loss = 0.25155865\n",
      "Iteration 151, loss = 0.25063799\n",
      "Iteration 152, loss = 0.24965514\n",
      "Iteration 153, loss = 0.24880656\n",
      "Iteration 154, loss = 0.24806649\n",
      "Iteration 155, loss = 0.24687617\n",
      "Iteration 156, loss = 0.24620869\n",
      "Iteration 157, loss = 0.24512193\n",
      "Iteration 158, loss = 0.24427775\n",
      "Iteration 159, loss = 0.24355179\n",
      "Iteration 160, loss = 0.24263523\n",
      "Iteration 161, loss = 0.24146533\n",
      "Iteration 162, loss = 0.24078930\n",
      "Iteration 163, loss = 0.23981890\n",
      "Iteration 164, loss = 0.23905795\n",
      "Iteration 165, loss = 0.23822736\n",
      "Iteration 166, loss = 0.23744468\n",
      "Iteration 167, loss = 0.23647825\n",
      "Iteration 168, loss = 0.23579739\n",
      "Iteration 169, loss = 0.23469574\n",
      "Iteration 170, loss = 0.23410345\n",
      "Iteration 171, loss = 0.23316109\n",
      "Iteration 172, loss = 0.23223991\n",
      "Iteration 173, loss = 0.23153481\n",
      "Iteration 174, loss = 0.23063172\n",
      "Iteration 175, loss = 0.22980931\n",
      "Iteration 176, loss = 0.22915923\n",
      "Iteration 177, loss = 0.22831985\n",
      "Iteration 178, loss = 0.22747299\n",
      "Iteration 179, loss = 0.22664620\n",
      "Iteration 180, loss = 0.22581840\n",
      "Iteration 181, loss = 0.22509686\n",
      "Iteration 182, loss = 0.22440516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 183, loss = 0.22378002\n",
      "Iteration 184, loss = 0.22308853\n",
      "Iteration 185, loss = 0.22226440\n",
      "Iteration 186, loss = 0.22129813\n",
      "Iteration 187, loss = 0.22072569\n",
      "Iteration 188, loss = 0.21971548\n",
      "Iteration 189, loss = 0.21924308\n",
      "Iteration 190, loss = 0.21852067\n",
      "Iteration 191, loss = 0.21750360\n",
      "Iteration 192, loss = 0.21688271\n",
      "Iteration 193, loss = 0.21618128\n",
      "Iteration 194, loss = 0.21551231\n",
      "Iteration 195, loss = 0.21462492\n",
      "Iteration 196, loss = 0.21392063\n",
      "Iteration 197, loss = 0.21323670\n",
      "Iteration 198, loss = 0.21264538\n",
      "Iteration 199, loss = 0.21171549\n",
      "Iteration 200, loss = 0.21116058\n",
      "Iteration 201, loss = 0.21031539\n",
      "Iteration 202, loss = 0.20991482\n",
      "Iteration 203, loss = 0.20894214\n",
      "Iteration 204, loss = 0.20833130\n",
      "Iteration 205, loss = 0.20765074\n",
      "Iteration 206, loss = 0.20685114\n",
      "Iteration 207, loss = 0.20632746\n",
      "Iteration 208, loss = 0.20553188\n",
      "Iteration 209, loss = 0.20500502\n",
      "Iteration 210, loss = 0.20418192\n",
      "Iteration 211, loss = 0.20355915\n",
      "Iteration 212, loss = 0.20295277\n",
      "Iteration 213, loss = 0.20220789\n",
      "Iteration 214, loss = 0.20146936\n",
      "Iteration 215, loss = 0.20087229\n",
      "Iteration 216, loss = 0.20039794\n",
      "Iteration 217, loss = 0.19955318\n",
      "Iteration 218, loss = 0.19900304\n",
      "Iteration 219, loss = 0.19826927\n",
      "Iteration 220, loss = 0.19772309\n",
      "Iteration 221, loss = 0.19727340\n",
      "Iteration 222, loss = 0.19637892\n",
      "Iteration 223, loss = 0.19574680\n",
      "Iteration 224, loss = 0.19518091\n",
      "Iteration 225, loss = 0.19445054\n",
      "Iteration 226, loss = 0.19389509\n",
      "Iteration 227, loss = 0.19333123\n",
      "Iteration 228, loss = 0.19253115\n",
      "Iteration 229, loss = 0.19201669\n",
      "Iteration 230, loss = 0.19132176\n",
      "Iteration 231, loss = 0.19083251\n",
      "Iteration 232, loss = 0.19021654\n",
      "Iteration 233, loss = 0.18948003\n",
      "Iteration 234, loss = 0.18899749\n",
      "Iteration 235, loss = 0.18839046\n",
      "Iteration 236, loss = 0.18774985\n",
      "Iteration 237, loss = 0.18721585\n",
      "Iteration 238, loss = 0.18675520\n",
      "Iteration 239, loss = 0.18599985\n",
      "Iteration 240, loss = 0.18533328\n",
      "Iteration 241, loss = 0.18483459\n",
      "Iteration 242, loss = 0.18418501\n",
      "Iteration 243, loss = 0.18368966\n",
      "Iteration 244, loss = 0.18321441\n",
      "Iteration 245, loss = 0.18249226\n",
      "Iteration 246, loss = 0.18200091\n",
      "Iteration 247, loss = 0.18135531\n",
      "Iteration 248, loss = 0.18077577\n",
      "Iteration 249, loss = 0.18020873\n",
      "Iteration 250, loss = 0.17970282\n",
      "Iteration 251, loss = 0.17915154\n",
      "Iteration 252, loss = 0.17871783\n",
      "Iteration 253, loss = 0.17816694\n",
      "Iteration 254, loss = 0.17745628\n",
      "Iteration 255, loss = 0.17680825\n",
      "Iteration 256, loss = 0.17631394\n",
      "Iteration 257, loss = 0.17584205\n",
      "Iteration 258, loss = 0.17523616\n",
      "Iteration 259, loss = 0.17479634\n",
      "Iteration 260, loss = 0.17419545\n",
      "Iteration 261, loss = 0.17372168\n",
      "Iteration 262, loss = 0.17319837\n",
      "Iteration 263, loss = 0.17265528\n",
      "Iteration 264, loss = 0.17217022\n",
      "Iteration 265, loss = 0.17159666\n",
      "Iteration 266, loss = 0.17120212\n",
      "Iteration 267, loss = 0.17046378\n",
      "Iteration 268, loss = 0.16989178\n",
      "Iteration 269, loss = 0.16957824\n",
      "Iteration 270, loss = 0.16903939\n",
      "Iteration 271, loss = 0.16839870\n",
      "Iteration 272, loss = 0.16803681\n",
      "Iteration 273, loss = 0.16745262\n",
      "Iteration 274, loss = 0.16678880\n",
      "Iteration 275, loss = 0.16640722\n",
      "Iteration 276, loss = 0.16594775\n",
      "Iteration 277, loss = 0.16529633\n",
      "Iteration 278, loss = 0.16481308\n",
      "Iteration 279, loss = 0.16442677\n",
      "Iteration 280, loss = 0.16393165\n",
      "Iteration 281, loss = 0.16335899\n",
      "Iteration 282, loss = 0.16294081\n",
      "Iteration 283, loss = 0.16244224\n",
      "Iteration 284, loss = 0.16192736\n",
      "Iteration 285, loss = 0.16158335\n",
      "Iteration 286, loss = 0.16097727\n",
      "Iteration 287, loss = 0.16062477\n",
      "Iteration 288, loss = 0.16005254\n",
      "Iteration 289, loss = 0.15950045\n",
      "Iteration 290, loss = 0.15906012\n",
      "Iteration 291, loss = 0.15857558\n",
      "Iteration 292, loss = 0.15808295\n",
      "Iteration 293, loss = 0.15764326\n",
      "Iteration 294, loss = 0.15721902\n",
      "Iteration 295, loss = 0.15674076\n",
      "Iteration 296, loss = 0.15629738\n",
      "Iteration 297, loss = 0.15583201\n",
      "Iteration 298, loss = 0.15538113\n",
      "Iteration 299, loss = 0.15492748\n",
      "Iteration 300, loss = 0.15447792\n",
      "Iteration 301, loss = 0.15390884\n",
      "Iteration 302, loss = 0.15363422\n",
      "Iteration 303, loss = 0.15319898\n",
      "Iteration 304, loss = 0.15258039\n",
      "Iteration 305, loss = 0.15227217\n",
      "Iteration 306, loss = 0.15175450\n",
      "Iteration 307, loss = 0.15136766\n",
      "Iteration 308, loss = 0.15098336\n",
      "Iteration 309, loss = 0.15042454\n",
      "Iteration 310, loss = 0.15007581\n",
      "Iteration 311, loss = 0.14950508\n",
      "Iteration 312, loss = 0.14931518\n",
      "Iteration 313, loss = 0.14870359\n",
      "Iteration 314, loss = 0.14832228\n",
      "Iteration 315, loss = 0.14795568\n",
      "Iteration 316, loss = 0.14746717\n",
      "Iteration 317, loss = 0.14699970\n",
      "Iteration 318, loss = 0.14658552\n",
      "Iteration 319, loss = 0.14629777\n",
      "Iteration 320, loss = 0.14576639\n",
      "Iteration 321, loss = 0.14537311\n",
      "Iteration 322, loss = 0.14493454\n",
      "Iteration 323, loss = 0.14446464\n",
      "Iteration 324, loss = 0.14419734\n",
      "Iteration 325, loss = 0.14373319\n",
      "Iteration 326, loss = 0.14329289\n",
      "Iteration 327, loss = 0.14283187\n",
      "Iteration 328, loss = 0.14246571\n",
      "Iteration 329, loss = 0.14204361\n",
      "Iteration 330, loss = 0.14167316\n",
      "Iteration 331, loss = 0.14124073\n",
      "Iteration 332, loss = 0.14087039\n",
      "Iteration 333, loss = 0.14048954\n",
      "Iteration 334, loss = 0.14016486\n",
      "Iteration 335, loss = 0.13963883\n",
      "Iteration 336, loss = 0.13930346\n",
      "Iteration 337, loss = 0.13882358\n",
      "Iteration 338, loss = 0.13846336\n",
      "Iteration 339, loss = 0.13810722\n",
      "Iteration 340, loss = 0.13774206\n",
      "Iteration 341, loss = 0.13734074\n",
      "Iteration 342, loss = 0.13691741\n",
      "Iteration 343, loss = 0.13660493\n",
      "Iteration 344, loss = 0.13624601\n",
      "Iteration 345, loss = 0.13585568\n",
      "Iteration 346, loss = 0.13538161\n",
      "Iteration 347, loss = 0.13510180\n",
      "Iteration 348, loss = 0.13465948\n",
      "Iteration 349, loss = 0.13428660\n",
      "Iteration 350, loss = 0.13392197\n",
      "Iteration 351, loss = 0.13356320\n",
      "Iteration 352, loss = 0.13324777\n",
      "Iteration 353, loss = 0.13277116\n",
      "Iteration 354, loss = 0.13247955\n",
      "Iteration 355, loss = 0.13221963\n",
      "Iteration 356, loss = 0.13180486\n",
      "Iteration 357, loss = 0.13128813\n",
      "Iteration 358, loss = 0.13101058\n",
      "Iteration 359, loss = 0.13070951\n",
      "Iteration 360, loss = 0.13031966\n",
      "Iteration 361, loss = 0.12998790\n",
      "Iteration 362, loss = 0.12960962\n",
      "Iteration 363, loss = 0.12933017\n",
      "Iteration 364, loss = 0.12895444\n",
      "Iteration 365, loss = 0.12863041\n",
      "Iteration 366, loss = 0.12814166\n",
      "Iteration 367, loss = 0.12792646\n",
      "Iteration 368, loss = 0.12751815\n",
      "Iteration 369, loss = 0.12710653\n",
      "Iteration 370, loss = 0.12679880\n",
      "Iteration 371, loss = 0.12645486\n",
      "Iteration 372, loss = 0.12614523\n",
      "Iteration 373, loss = 0.12579476\n",
      "Iteration 374, loss = 0.12545125\n",
      "Iteration 375, loss = 0.12510886\n",
      "Iteration 376, loss = 0.12481501\n",
      "Iteration 377, loss = 0.12437036\n",
      "Iteration 378, loss = 0.12413550\n",
      "Iteration 379, loss = 0.12379523\n",
      "Iteration 380, loss = 0.12349231\n",
      "Iteration 381, loss = 0.12318874\n",
      "Iteration 382, loss = 0.12269481\n",
      "Iteration 383, loss = 0.12252617\n",
      "Iteration 384, loss = 0.12220047\n",
      "Iteration 385, loss = 0.12181975\n",
      "Iteration 386, loss = 0.12147061\n",
      "Iteration 387, loss = 0.12117258\n",
      "Iteration 388, loss = 0.12079871\n",
      "Iteration 389, loss = 0.12044840\n",
      "Iteration 390, loss = 0.12024621\n",
      "Iteration 391, loss = 0.11985937\n",
      "Iteration 392, loss = 0.11955165\n",
      "Iteration 393, loss = 0.11932374\n",
      "Iteration 394, loss = 0.11897187\n",
      "Iteration 395, loss = 0.11865840\n",
      "Iteration 396, loss = 0.11840940\n",
      "Iteration 397, loss = 0.11802632\n",
      "Iteration 398, loss = 0.11772557\n",
      "Iteration 399, loss = 0.11739889\n",
      "Iteration 400, loss = 0.11717409\n",
      "The training Accuracy achieved is = 97.908\n",
      "The test Accuracy achieved is = 85.662\n",
      "The number of epochs is = 400\n",
      "The training time achieved is = 152.810\n",
      "--------Training the classifier with following params--------------\n",
      "-------------------------------------------------------------------\n",
      "MLPClassifier(activation='relu', alpha=0.0001, batch_size=100, beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(100, 100), learning_rate='invscaling',\n",
      "              learning_rate_init=0.1, max_fun=15000, max_iter=400, momentum=0.9,\n",
      "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.25,\n",
      "              random_state=None, shuffle=True, solver='sgd', tol=1e-05,\n",
      "              validation_fraction=0.1, verbose=True, warm_start=False)\n",
      "-------------------------------------------------------------------\n",
      "Iteration 1, loss = 2.23735264\n",
      "Iteration 2, loss = 0.82176518\n",
      "Iteration 3, loss = 0.67902448\n",
      "Iteration 4, loss = 0.63485710\n",
      "Iteration 5, loss = 0.60335081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 6, loss = 0.58025195\n",
      "Iteration 7, loss = 0.55987122\n",
      "Iteration 8, loss = 0.53958283\n",
      "Iteration 9, loss = 0.52299793\n",
      "Iteration 10, loss = 0.50883853\n",
      "Iteration 11, loss = 0.49523129\n",
      "Iteration 12, loss = 0.48299034\n",
      "Iteration 13, loss = 0.47009574\n",
      "Iteration 14, loss = 0.45982906\n",
      "Iteration 15, loss = 0.45124358\n",
      "Iteration 16, loss = 0.44011492\n",
      "Iteration 17, loss = 0.43085941\n",
      "Iteration 18, loss = 0.42184192\n",
      "Iteration 19, loss = 0.41269597\n",
      "Iteration 20, loss = 0.40568441\n",
      "Iteration 21, loss = 0.39768676\n",
      "Iteration 22, loss = 0.39032091\n",
      "Iteration 23, loss = 0.38272313\n",
      "Iteration 24, loss = 0.37585606\n",
      "Iteration 25, loss = 0.37127166\n",
      "Iteration 26, loss = 0.36485406\n",
      "Iteration 27, loss = 0.35676028\n",
      "Iteration 28, loss = 0.35136442\n",
      "Iteration 29, loss = 0.34566282\n",
      "Iteration 30, loss = 0.33892398\n",
      "Iteration 31, loss = 0.33487128\n",
      "Iteration 32, loss = 0.33031968\n",
      "Iteration 33, loss = 0.32420922\n",
      "Iteration 34, loss = 0.31845482\n",
      "Iteration 35, loss = 0.31468834\n",
      "Iteration 36, loss = 0.31012624\n",
      "Iteration 37, loss = 0.30438451\n",
      "Iteration 38, loss = 0.30087491\n",
      "Iteration 39, loss = 0.29594258\n",
      "Iteration 40, loss = 0.29150069\n",
      "Iteration 41, loss = 0.28836513\n",
      "Iteration 42, loss = 0.28401236\n",
      "Iteration 43, loss = 0.27969530\n",
      "Iteration 44, loss = 0.27690762\n",
      "Iteration 45, loss = 0.27219675\n",
      "Iteration 46, loss = 0.26855897\n",
      "Iteration 47, loss = 0.26485603\n",
      "Iteration 48, loss = 0.26049112\n",
      "Iteration 49, loss = 0.25768358\n",
      "Iteration 50, loss = 0.25519960\n",
      "Iteration 51, loss = 0.25111019\n",
      "Iteration 52, loss = 0.24851987\n",
      "Iteration 53, loss = 0.24461898\n",
      "Iteration 54, loss = 0.24175530\n",
      "Iteration 55, loss = 0.23909777\n",
      "Iteration 56, loss = 0.23570492\n",
      "Iteration 57, loss = 0.23241994\n",
      "Iteration 58, loss = 0.22951963\n",
      "Iteration 59, loss = 0.22742330\n",
      "Iteration 60, loss = 0.22434626\n",
      "Iteration 61, loss = 0.22123603\n",
      "Iteration 62, loss = 0.21879233\n",
      "Iteration 63, loss = 0.21611665\n",
      "Iteration 64, loss = 0.21275538\n",
      "Iteration 65, loss = 0.21083157\n",
      "Iteration 66, loss = 0.20835560\n",
      "Iteration 67, loss = 0.20623190\n",
      "Iteration 68, loss = 0.20310282\n",
      "Iteration 69, loss = 0.20124868\n",
      "Iteration 70, loss = 0.19824255\n",
      "Iteration 71, loss = 0.19638798\n",
      "Iteration 72, loss = 0.19343089\n",
      "Iteration 73, loss = 0.19149914\n",
      "Iteration 74, loss = 0.18910349\n",
      "Iteration 75, loss = 0.18706564\n",
      "Iteration 76, loss = 0.18514320\n",
      "Iteration 77, loss = 0.18304795\n",
      "Iteration 78, loss = 0.18074024\n",
      "Iteration 79, loss = 0.17848236\n",
      "Iteration 80, loss = 0.17605494\n",
      "Iteration 81, loss = 0.17469780\n",
      "Iteration 82, loss = 0.17275316\n",
      "Iteration 83, loss = 0.17016668\n",
      "Iteration 84, loss = 0.16883838\n",
      "Iteration 85, loss = 0.16701739\n",
      "Iteration 86, loss = 0.16447470\n",
      "Iteration 87, loss = 0.16276089\n",
      "Iteration 88, loss = 0.16099350\n",
      "Iteration 89, loss = 0.15921175\n",
      "Iteration 90, loss = 0.15791468\n",
      "Iteration 91, loss = 0.15589128\n",
      "Iteration 92, loss = 0.15489200\n",
      "Iteration 93, loss = 0.15218721\n",
      "Iteration 94, loss = 0.15039101\n",
      "Iteration 95, loss = 0.14916727\n",
      "Iteration 96, loss = 0.14782335\n",
      "Iteration 97, loss = 0.14576896\n",
      "Iteration 98, loss = 0.14419377\n",
      "Iteration 99, loss = 0.14260636\n",
      "Iteration 100, loss = 0.14113117\n",
      "Iteration 101, loss = 0.13897939\n",
      "Iteration 102, loss = 0.13760362\n",
      "Iteration 103, loss = 0.13615827\n",
      "Iteration 104, loss = 0.13503128\n",
      "Iteration 105, loss = 0.13349371\n",
      "Iteration 106, loss = 0.13192969\n",
      "Iteration 107, loss = 0.13061745\n",
      "Iteration 108, loss = 0.12945206\n",
      "Iteration 109, loss = 0.12774929\n",
      "Iteration 110, loss = 0.12600423\n",
      "Iteration 111, loss = 0.12549182\n",
      "Iteration 112, loss = 0.12362857\n",
      "Iteration 113, loss = 0.12260391\n",
      "Iteration 114, loss = 0.12138187\n",
      "Iteration 115, loss = 0.11954477\n",
      "Iteration 116, loss = 0.11886254\n",
      "Iteration 117, loss = 0.11721325\n",
      "Iteration 118, loss = 0.11602861\n",
      "Iteration 119, loss = 0.11484125\n",
      "Iteration 120, loss = 0.11366366\n",
      "Iteration 121, loss = 0.11260015\n",
      "Iteration 122, loss = 0.11130147\n",
      "Iteration 123, loss = 0.11021454\n",
      "Iteration 124, loss = 0.10882920\n",
      "Iteration 125, loss = 0.10800531\n",
      "Iteration 126, loss = 0.10672241\n",
      "Iteration 127, loss = 0.10572425\n",
      "Iteration 128, loss = 0.10472366\n",
      "Iteration 129, loss = 0.10378301\n",
      "Iteration 130, loss = 0.10250341\n",
      "Iteration 131, loss = 0.10144374\n",
      "Iteration 132, loss = 0.10017135\n",
      "Iteration 133, loss = 0.09906774\n",
      "Iteration 134, loss = 0.09897172\n",
      "Iteration 135, loss = 0.09745201\n",
      "Iteration 136, loss = 0.09643013\n",
      "Iteration 137, loss = 0.09550837\n",
      "Iteration 138, loss = 0.09430345\n",
      "Iteration 139, loss = 0.09339969\n",
      "Iteration 140, loss = 0.09278083\n",
      "Iteration 141, loss = 0.09178110\n",
      "Iteration 142, loss = 0.09100659\n",
      "Iteration 143, loss = 0.08985651\n",
      "Iteration 144, loss = 0.08944951\n",
      "Iteration 145, loss = 0.08823677\n",
      "Iteration 146, loss = 0.08717701\n",
      "Iteration 147, loss = 0.08681679\n",
      "Iteration 148, loss = 0.08572033\n",
      "Iteration 149, loss = 0.08486409\n",
      "Iteration 150, loss = 0.08408857\n",
      "Iteration 151, loss = 0.08337416\n",
      "Iteration 152, loss = 0.08233180\n",
      "Iteration 153, loss = 0.08162917\n",
      "Iteration 154, loss = 0.08065509\n",
      "Iteration 155, loss = 0.08035844\n",
      "Iteration 156, loss = 0.07940185\n",
      "Iteration 157, loss = 0.07870160\n",
      "Iteration 158, loss = 0.07764999\n",
      "Iteration 159, loss = 0.07701743\n",
      "Iteration 160, loss = 0.07626875\n",
      "Iteration 161, loss = 0.07573877\n",
      "Iteration 162, loss = 0.07504270\n",
      "Iteration 163, loss = 0.07429638\n",
      "Iteration 164, loss = 0.07353878\n",
      "Iteration 165, loss = 0.07289745\n",
      "Iteration 166, loss = 0.07229455\n",
      "Iteration 167, loss = 0.07159860\n",
      "Iteration 168, loss = 0.07089093\n",
      "Iteration 169, loss = 0.07051470\n",
      "Iteration 170, loss = 0.06963639\n",
      "Iteration 171, loss = 0.06893353\n",
      "Iteration 172, loss = 0.06815662\n",
      "Iteration 173, loss = 0.06797282\n",
      "Iteration 174, loss = 0.06698478\n",
      "Iteration 175, loss = 0.06657021\n",
      "Iteration 176, loss = 0.06594244\n",
      "Iteration 177, loss = 0.06524500\n",
      "Iteration 178, loss = 0.06468401\n",
      "Iteration 179, loss = 0.06404803\n",
      "Iteration 180, loss = 0.06350234\n",
      "Iteration 181, loss = 0.06317751\n",
      "Iteration 182, loss = 0.06251635\n",
      "Iteration 183, loss = 0.06192627\n",
      "Iteration 184, loss = 0.06111800\n",
      "Iteration 185, loss = 0.06093491\n",
      "Iteration 186, loss = 0.06042957\n",
      "Iteration 187, loss = 0.05976469\n",
      "Iteration 188, loss = 0.05919849\n",
      "Iteration 189, loss = 0.05881629\n",
      "Iteration 190, loss = 0.05820818\n",
      "Iteration 191, loss = 0.05773141\n",
      "Iteration 192, loss = 0.05720023\n",
      "Iteration 193, loss = 0.05669252\n",
      "Iteration 194, loss = 0.05618546\n",
      "Iteration 195, loss = 0.05587628\n",
      "Iteration 196, loss = 0.05534462\n",
      "Iteration 197, loss = 0.05504132\n",
      "Iteration 198, loss = 0.05439211\n",
      "Iteration 199, loss = 0.05398835\n",
      "Iteration 200, loss = 0.05353137\n",
      "Iteration 201, loss = 0.05301748\n",
      "Iteration 202, loss = 0.05265311\n",
      "Iteration 203, loss = 0.05220254\n",
      "Iteration 204, loss = 0.05199113\n",
      "Iteration 205, loss = 0.05134108\n",
      "Iteration 206, loss = 0.05092371\n",
      "Iteration 207, loss = 0.05054053\n",
      "Iteration 208, loss = 0.04999789\n",
      "Iteration 209, loss = 0.04971793\n",
      "Iteration 210, loss = 0.04921654\n",
      "Iteration 211, loss = 0.04885173\n",
      "Iteration 212, loss = 0.04864669\n",
      "Iteration 213, loss = 0.04814701\n",
      "Iteration 214, loss = 0.04763147\n",
      "Iteration 215, loss = 0.04744366\n",
      "Iteration 216, loss = 0.04689301\n",
      "Iteration 217, loss = 0.04657885\n",
      "Iteration 218, loss = 0.04618483\n",
      "Iteration 219, loss = 0.04581477\n",
      "Iteration 220, loss = 0.04547057\n",
      "Iteration 221, loss = 0.04525308\n",
      "Iteration 222, loss = 0.04474653\n",
      "Iteration 223, loss = 0.04452471\n",
      "Iteration 224, loss = 0.04414670\n",
      "Iteration 225, loss = 0.04382289\n",
      "Iteration 226, loss = 0.04338871\n",
      "Iteration 227, loss = 0.04308544\n",
      "Iteration 228, loss = 0.04286919\n",
      "Iteration 229, loss = 0.04236605\n",
      "Iteration 230, loss = 0.04221096\n",
      "Iteration 231, loss = 0.04178849\n",
      "Iteration 232, loss = 0.04132168\n",
      "Iteration 233, loss = 0.04118299\n",
      "Iteration 234, loss = 0.04083337\n",
      "Iteration 235, loss = 0.04049576\n",
      "Iteration 236, loss = 0.04025935\n",
      "Iteration 237, loss = 0.04007945\n",
      "Iteration 238, loss = 0.03962297\n",
      "Iteration 239, loss = 0.03920940\n",
      "Iteration 240, loss = 0.03906700\n",
      "Iteration 241, loss = 0.03876534\n",
      "Iteration 242, loss = 0.03851601\n",
      "Iteration 243, loss = 0.03814277\n",
      "Iteration 244, loss = 0.03802656\n",
      "Iteration 245, loss = 0.03772832\n",
      "Iteration 246, loss = 0.03736073\n",
      "Iteration 247, loss = 0.03716903\n",
      "Iteration 248, loss = 0.03682550\n",
      "Iteration 249, loss = 0.03668703\n",
      "Iteration 250, loss = 0.03640634\n",
      "Iteration 251, loss = 0.03612294\n",
      "Iteration 252, loss = 0.03590384\n",
      "Iteration 253, loss = 0.03561076\n",
      "Iteration 254, loss = 0.03542686\n",
      "Iteration 255, loss = 0.03517487\n",
      "Iteration 256, loss = 0.03487547\n",
      "Iteration 257, loss = 0.03463554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 258, loss = 0.03440347\n",
      "Iteration 259, loss = 0.03418925\n",
      "Iteration 260, loss = 0.03395377\n",
      "Iteration 261, loss = 0.03377979\n",
      "Iteration 262, loss = 0.03339570\n",
      "Iteration 263, loss = 0.03330592\n",
      "Iteration 264, loss = 0.03304871\n",
      "Iteration 265, loss = 0.03283415\n",
      "Iteration 266, loss = 0.03256884\n",
      "Iteration 267, loss = 0.03239052\n",
      "Iteration 268, loss = 0.03220928\n",
      "Iteration 269, loss = 0.03195420\n",
      "Iteration 270, loss = 0.03170700\n",
      "Iteration 271, loss = 0.03154922\n",
      "Iteration 272, loss = 0.03138918\n",
      "Iteration 273, loss = 0.03111388\n",
      "Iteration 274, loss = 0.03095575\n",
      "Iteration 275, loss = 0.03071109\n",
      "Iteration 276, loss = 0.03052132\n",
      "Iteration 277, loss = 0.03027156\n",
      "Iteration 278, loss = 0.03015775\n",
      "Iteration 279, loss = 0.02993633\n",
      "Iteration 280, loss = 0.02971359\n",
      "Iteration 281, loss = 0.02963429\n",
      "Iteration 282, loss = 0.02938193\n",
      "Iteration 283, loss = 0.02918376\n",
      "Iteration 284, loss = 0.02903303\n",
      "Iteration 285, loss = 0.02876871\n",
      "Iteration 286, loss = 0.02863484\n",
      "Iteration 287, loss = 0.02848699\n",
      "Iteration 288, loss = 0.02828884\n",
      "Iteration 289, loss = 0.02815826\n",
      "Iteration 290, loss = 0.02798871\n",
      "Iteration 291, loss = 0.02781374\n",
      "Iteration 292, loss = 0.02763354\n",
      "Iteration 293, loss = 0.02746253\n",
      "Iteration 294, loss = 0.02735359\n",
      "Iteration 295, loss = 0.02713460\n",
      "Iteration 296, loss = 0.02694920\n",
      "Iteration 297, loss = 0.02684026\n",
      "Iteration 298, loss = 0.02661431\n",
      "Iteration 299, loss = 0.02653839\n",
      "Iteration 300, loss = 0.02635206\n",
      "Iteration 301, loss = 0.02611864\n",
      "Iteration 302, loss = 0.02600146\n",
      "Iteration 303, loss = 0.02589846\n",
      "Iteration 304, loss = 0.02574692\n",
      "Iteration 305, loss = 0.02555325\n",
      "Iteration 306, loss = 0.02535839\n",
      "Iteration 307, loss = 0.02531604\n",
      "Iteration 308, loss = 0.02514834\n",
      "Iteration 309, loss = 0.02500843\n",
      "Iteration 310, loss = 0.02485650\n",
      "Iteration 311, loss = 0.02468677\n",
      "Iteration 312, loss = 0.02453319\n",
      "Iteration 313, loss = 0.02442512\n",
      "Iteration 314, loss = 0.02431630\n",
      "Iteration 315, loss = 0.02418512\n",
      "Iteration 316, loss = 0.02401995\n",
      "Iteration 317, loss = 0.02388897\n",
      "Iteration 318, loss = 0.02377652\n",
      "Iteration 319, loss = 0.02364680\n",
      "Iteration 320, loss = 0.02356555\n",
      "Iteration 321, loss = 0.02340517\n",
      "Iteration 322, loss = 0.02326632\n",
      "Iteration 323, loss = 0.02312747\n",
      "Iteration 324, loss = 0.02303955\n",
      "Iteration 325, loss = 0.02287991\n",
      "Iteration 326, loss = 0.02273506\n",
      "Iteration 327, loss = 0.02263243\n",
      "Iteration 328, loss = 0.02246089\n",
      "Iteration 329, loss = 0.02242083\n",
      "Iteration 330, loss = 0.02230644\n",
      "Iteration 331, loss = 0.02214775\n",
      "Iteration 332, loss = 0.02206382\n",
      "Iteration 333, loss = 0.02196501\n",
      "Iteration 334, loss = 0.02180980\n",
      "Iteration 335, loss = 0.02174458\n",
      "Iteration 336, loss = 0.02161197\n",
      "Iteration 337, loss = 0.02148432\n",
      "Iteration 338, loss = 0.02142251\n",
      "Iteration 339, loss = 0.02126514\n",
      "Iteration 340, loss = 0.02113256\n",
      "Iteration 341, loss = 0.02107131\n",
      "Iteration 342, loss = 0.02094088\n",
      "Iteration 343, loss = 0.02087184\n",
      "Iteration 344, loss = 0.02073931\n",
      "Iteration 345, loss = 0.02062713\n",
      "Iteration 346, loss = 0.02055525\n",
      "Iteration 347, loss = 0.02044512\n",
      "Iteration 348, loss = 0.02034063\n",
      "Iteration 349, loss = 0.02019979\n",
      "Iteration 350, loss = 0.02014366\n",
      "Iteration 351, loss = 0.02006035\n",
      "Iteration 352, loss = 0.01998231\n",
      "Iteration 353, loss = 0.01984100\n",
      "Iteration 354, loss = 0.01975869\n",
      "Iteration 355, loss = 0.01964482\n",
      "Iteration 356, loss = 0.01953419\n",
      "Iteration 357, loss = 0.01948353\n",
      "Iteration 358, loss = 0.01939677\n",
      "Iteration 359, loss = 0.01933973\n",
      "Iteration 360, loss = 0.01917514\n",
      "Iteration 361, loss = 0.01910346\n",
      "Iteration 362, loss = 0.01901688\n",
      "Iteration 363, loss = 0.01892993\n",
      "Iteration 364, loss = 0.01884551\n",
      "Iteration 365, loss = 0.01871134\n",
      "Iteration 366, loss = 0.01869746\n",
      "Iteration 367, loss = 0.01858956\n",
      "Iteration 368, loss = 0.01846485\n",
      "Iteration 369, loss = 0.01839792\n",
      "Iteration 370, loss = 0.01830673\n",
      "Iteration 371, loss = 0.01823231\n",
      "Iteration 372, loss = 0.01817263\n",
      "Iteration 373, loss = 0.01807407\n",
      "Iteration 374, loss = 0.01800664\n",
      "Iteration 375, loss = 0.01791347\n",
      "Iteration 376, loss = 0.01784185\n",
      "Iteration 377, loss = 0.01777260\n",
      "Iteration 378, loss = 0.01768257\n",
      "Iteration 379, loss = 0.01760636\n",
      "Iteration 380, loss = 0.01754211\n",
      "Iteration 381, loss = 0.01747731\n",
      "Iteration 382, loss = 0.01741102\n",
      "Iteration 383, loss = 0.01727455\n",
      "Iteration 384, loss = 0.01723826\n",
      "Iteration 385, loss = 0.01717040\n",
      "Iteration 386, loss = 0.01707221\n",
      "Iteration 387, loss = 0.01699677\n",
      "Iteration 388, loss = 0.01695810\n",
      "Iteration 389, loss = 0.01685334\n",
      "Iteration 390, loss = 0.01680441\n",
      "Iteration 391, loss = 0.01673251\n",
      "Iteration 392, loss = 0.01664871\n",
      "Iteration 393, loss = 0.01659589\n",
      "Iteration 394, loss = 0.01652268\n",
      "Iteration 395, loss = 0.01643381\n",
      "Iteration 396, loss = 0.01638480\n",
      "Iteration 397, loss = 0.01633336\n",
      "Iteration 398, loss = 0.01624556\n",
      "Iteration 399, loss = 0.01617537\n",
      "Iteration 400, loss = 0.01609103\n",
      "The training Accuracy achieved is = 99.977\n",
      "The test Accuracy achieved is = 85.185\n",
      "The number of epochs is = 400\n",
      "The training time achieved is = 161.150\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(clf)):\n",
    "    print(\"--------Training the classifier with following params--------------\")\n",
    "    print(\"-------------------------------------------------------------------\")\n",
    "    print(clf[i])\n",
    "    print(\"-------------------------------------------------------------------\")\n",
    "    start =time.time()\n",
    "    clf[i].fit(X_train, train_class_enc)\n",
    "    end = time.time()\n",
    "    epochs.append(clf[i].n_iter_)\n",
    "    train_accuracy.append(clf[i].score(X_train, train_class_enc)*100)\n",
    "    test_accuracy.append(clf[i].score(X_test, test_actual_class_enc)*100)\n",
    "    train_time.append(end-start)\n",
    "    print(\"The training Accuracy achieved is = {:2.3f}\".format(train_accuracy[-1]))\n",
    "    print(\"The test Accuracy achieved is = {:2.3f}\".format(test_accuracy[-1]))    \n",
    "    print(\"The number of epochs is = {}\".format(epochs[-1]))    \n",
    "    print(\"The training time achieved is = {:2.3f}\".format(train_time[-1]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(6,5))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_title(\"Accuracy with different MLP classifiers with\\n ReLU and Sigmoid Activation\")\n",
    "data=['S_lr=0.1', 'S_earlystop','S_0.3/ep^(1/5)','S_0.3/ep^(1/4)','R_lr=0.03', 'R_lr=0.1', 'R_EarlyStop', 'R_0.1/ep^(1/2)', 'R_0.1/ep^(1/3)', 'R_0.1/ep^(1/4)']\n",
    "ax.plot(x, train_accuracy, marker='o', label='Train Accuracy')\n",
    "ax.plot(x, test_accuracy, marker='o', label='Test Accuracy')\n",
    "ax.set_xlabel(\"Classifier Type\")\n",
    "ax.set_xticklabels(data, rotation=90)\n",
    "ax.set_ylabel(\"Accuracy (%)\")\n",
    "ax.legend(framealpha=0.5)\n",
    "ax.set_xticks(range(0,len(clf)))\n",
    "ax1=ax.twinx()\n",
    "ax1.set_ylabel(\"Number of Epochs\")\n",
    "ax1.plot(x, epochs, marker='*', color='m',label='Epochs')\n",
    "ax1.tick_params(axis='y', labelcolor='m', labelsize=12)\n",
    "plt.legend(loc=4,framealpha=0.5)\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "#plt.savefig(\"plots/parte/accuracy_final.png\", dpi=1000, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
