{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------Reading the Data-------------------------\n",
      "----------------Data Reading completed-------------------\n",
      "The total number of training samples = 13000\n",
      "The number of features = 784\n"
     ]
    }
   ],
   "source": [
    "print(\"----------------Reading the Data-------------------------\")\n",
    "PATH = os.getcwd()\n",
    "os.chdir('Alphabets/')\n",
    "\n",
    "X_train = pd.read_csv('train.csv', sep=',', header=None, index_col=False)\n",
    "X_test = pd.read_csv('test.csv', sep=',', header=None, index_col=False)\n",
    "\n",
    "train_class = X_train[X_train.columns[-1]]\n",
    "test_actual_class = X_test[X_test.columns[-1]]\n",
    "\n",
    "X_train = X_train.drop(X_train.columns[-1], axis=1)\n",
    "X_test = X_test.drop(X_test.columns[-1], axis=1)\n",
    "\n",
    "print(\"----------------Data Reading completed-------------------\")\n",
    "\n",
    "os.chdir('../')\n",
    "\n",
    "X_train = X_train/255\n",
    "X_test = X_test/255\n",
    "m = X_train.shape[0] # Number of Training Samples\n",
    "n = X_train.shape[1] # Number of input features\n",
    "\n",
    "print(\"The total number of training samples = {}\".format(m))\n",
    "print(\"The number of features = {}\".format(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Perform 1-hot encoding of class labels------------\n"
     ]
    }
   ],
   "source": [
    "#To get the one hot encoding of each label\n",
    "print(\"--------Perform 1-hot encoding of class labels------------\")\n",
    "\n",
    "train_class_enc = pd.get_dummies(train_class).to_numpy()\n",
    "test_actual_class_enc = pd.get_dummies(test_actual_class).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add the intercept term to the data samples both in training and test dataset\n",
    "X_train = np.hstack((np.ones((m,1)),X_train.to_numpy()))\n",
    "X_test = np.hstack((np.ones((X_test.shape[0],1)),X_test.to_numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(data, theta):\n",
    "    l0 = data\n",
    "    l1 = activation(np.dot(l0, theta[0]))\n",
    "    l2 = activation(np.dot(l1, theta[1]))\n",
    "    return l1,l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_total(X, theta, Y, m):\n",
    "    l1, l2 = forward_prop(X, theta)\n",
    "    cost = (1/(2*m))*np.sum((Y-l2)**2)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13000, 26)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_class_enc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.5"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost_total(X_train, [theta1, theta2], train_class_enc, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mini-Batch formation\n",
    "mini_batch = [(X_train[i:i+batch_size,:], train_class_enc[i:i+batch_size]) for i in range(0, m, batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1300"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mini_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "arch = [1000] #means one hidden layer with 2 perceptrons \n",
    "batch_size = 10 # Mini-Batch Size\n",
    "r = np.max(train_class) + 1 # Default value of the number of classes = 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta1 = np.random.random((n+1,arch[0]))\n",
    "theta2 = np.random.random((arch[0],r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Cost = 12.5\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Cost after 0 iterations is = 12.5\n",
      "Initial Cost = 12.5\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Initial Cost = 12.5\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Initial Cost = 12.5\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Initial Cost = 12.5\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Initial Cost = 12.5\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Initial Cost = 12.5\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Initial Cost = 12.5\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Initial Cost = 12.5\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Initial Cost = 12.5\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Initial Cost = 12.5\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Cost after 10 iterations is = 12.5\n",
      "Initial Cost = 12.5\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Initial Cost = 12.5\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Initial Cost = 12.5\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n",
      "Error = 0.9615384615384616\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-135-efe84b862862>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mY_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0ml0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0ml1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtheta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error = \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_b\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0ml2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-85-f4e384e557c1>\u001b[0m in \u001b[0;36mforward_prop\u001b[0;34m(data, theta)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mforward_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0ml0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0ml1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0ml2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ml1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for it in range(100):\n",
    "    count = 0\n",
    "\n",
    "    cost_init = cost_total(X_train, [theta1, theta2], train_class_enc, m)\n",
    "    print(\"Initial Cost = {}\".format(cost_init))\n",
    "    \n",
    "    for b in mini_batch:\n",
    "        X_b = b[0]\n",
    "        Y_b = b[1]\n",
    "        l0 = X_b\n",
    "        l1, l2 = forward_prop(X_b, [theta1, theta2])\n",
    "        if (count % 100 == 0):\n",
    "            print(\"Error = \"+str(np.mean(np.abs(Y_b - l2))))\n",
    "        #Backward Propagation\n",
    "        delta_l2 = (1/batch_size)*(Y_b - l2)*l2*(1-l2)\n",
    "\n",
    "        delta_l1 = np.dot(delta_l2, theta2.T)*l1*(1-l1)\n",
    "\n",
    "        theta1 += lr*np.dot(l0.T, delta_l1)\n",
    "        theta2 += lr*np.dot(l1.T, delta_l2)\n",
    "        count+=1\n",
    "    if(it%10 == 0):\n",
    "        cost_final = cost_total(X_train, [theta1, theta2], train_class_enc, m)\n",
    "        print(\"Cost after {} iterations is = {}\".format(it, cost_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-50-b068ff788a8b>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-50-b068ff788a8b>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    test_acc++\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "test_pred_class = forward_prop(X_test, [theta1, theta2])\n",
    "test_acc = 0\n",
    "for i in range(len(test_actual_class_enc)):\n",
    "    if (np.array_equal(test_pred_class[i], test_actual_class_enc[i])):\n",
    "        test_acc+=1\n",
    "        test_acc /= X_test.shape[0]\n",
    "\n",
    "print(\"The Test Accuracy of the model = {}%\".format(test_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
