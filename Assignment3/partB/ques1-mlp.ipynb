{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier as mlp_classifier\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------Reading the Data-------------------------\n",
      "----------------Data Reading completed-------------------\n",
      "The total number of training samples = 13000\n",
      "The number of features = 784\n"
     ]
    }
   ],
   "source": [
    "print(\"----------------Reading the Data-------------------------\")\n",
    "PATH = os.getcwd()\n",
    "os.chdir('Alphabets/')\n",
    "\n",
    "X_train = pd.read_csv('train.csv', sep=',', header=None, index_col=False)\n",
    "X_test = pd.read_csv('test.csv', sep=',', header=None, index_col=False)\n",
    "np.random.shuffle(X_train.to_numpy())\n",
    "train_class = X_train[X_train.columns[-1]]\n",
    "test_actual_class = X_test[X_test.columns[-1]]\n",
    "\n",
    "X_train = X_train.drop(X_train.columns[-1], axis=1)\n",
    "X_test = X_test.drop(X_test.columns[-1], axis=1)\n",
    "\n",
    "print(\"----------------Data Reading completed-------------------\")\n",
    "\n",
    "os.chdir('../')\n",
    "\n",
    "X_train = X_train/255\n",
    "X_test = X_test/255\n",
    "\n",
    "m = X_train.shape[0] # Number of Training Samples\n",
    "n = X_train.shape[1] # Number of input features\n",
    "\n",
    "print(\"The total number of training samples = {}\".format(m))\n",
    "print(\"The number of features = {}\".format(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Perform 1-hot encoding of class labels------------\n"
     ]
    }
   ],
   "source": [
    "#To get the one hot encoding of each label\n",
    "print(\"--------Perform 1-hot encoding of class labels------------\")\n",
    "\n",
    "train_class_enc = pd.get_dummies(train_class).to_numpy()\n",
    "test_actual_class_enc = pd.get_dummies(test_actual_class).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = []\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "valid_accuracy = []\n",
    "train_time = []\n",
    "clf=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classifier - logistic with constant LR\n",
    "clf.append(mlp_classifier(hidden_layer_sizes=(100, 100), activation='logistic', solver='sgd', \n",
    "                     batch_size=100, learning_rate_init=0.1, learning_rate='constant', max_iter=400,\n",
    "                     tol=1e-4, verbose=True))\n",
    "#Classifier - logistic with early_stopping =True\n",
    "clf.append(mlp_classifier(hidden_layer_sizes=(100, 100), activation='logistic', solver='sgd', \n",
    "                     batch_size=100, learning_rate_init=0.1, learning_rate='constant', max_iter=400,\n",
    "                     early_stopping=True, tol=1e-4, verbose=True))\n",
    "#Classifier - logistic with invscaling with sqrt\n",
    "clf.append(mlp_classifier(hidden_layer_sizes=(100, 100), activation='logistic', solver='sgd', \n",
    "                     batch_size=100, learning_rate_init=0.1, learning_rate='invscaling', max_iter=400,\n",
    "                     tol=1e-4, verbose=True))\n",
    "#Classifier - logistic with invscaling with pow(1/3)\n",
    "clf.append(mlp_classifier(hidden_layer_sizes=(100, 100), activation='logistic', solver='sgd', \n",
    "                     batch_size=100, learning_rate_init=0.1, learning_rate='invscaling', max_iter=400,\n",
    "                     power_t=(1/3), tol=1e-4, verbose=True))\n",
    "\n",
    "\n",
    "#Classifier ReLU with constant LR (1e-4)\n",
    "clf.append(mlp_classifier(hidden_layer_sizes=(100, 100), activation='relu', solver='sgd', \n",
    "                     batch_size=100, learning_rate_init=0.1, learning_rate='constant', max_iter=400,\n",
    "                     tol=1e-4, verbose=True))\n",
    "#Classifier ReLU with constant LR (1e-6)\n",
    "clf.append(mlp_classifier(hidden_layer_sizes=(100, 100), activation='relu', solver='sgd', \n",
    "                     batch_size=100, learning_rate_init=0.1, learning_rate='constant', max_iter=400,\n",
    "                     tol=1e-6, verbose=True))\n",
    "#Classifier ReLU with constant LR (1e-6) with early stopping\n",
    "clf.append(mlp_classifier(hidden_layer_sizes=(100, 100), activation='relu', solver='sgd', \n",
    "                     batch_size=100, learning_rate_init=0.1, learning_rate='constant', max_iter=400,\n",
    "                     early_stopping=True, tol=1e-6, verbose=True))\n",
    "#Classifier ReLU with constant LR (1e-6) with invscaling sqrt\n",
    "clf.append(mlp_classifier(hidden_layer_sizes=(100, 100), activation='relu', solver='sgd', \n",
    "                     batch_size=100, learning_rate_init=0.1, learning_rate='invscaling', max_iter=400,\n",
    "                     tol=1e-6, verbose=True))\n",
    "#Classifier ReLU with constant LR (1e-6) with invscaling pow(1/3)\n",
    "clf.append(mlp_classifier(hidden_layer_sizes=(100, 100), activation='relu', solver='sgd', \n",
    "                     batch_size=100, learning_rate_init=0.1, learning_rate='invscaling', max_iter=400,\n",
    "                     power_t=(1/3), tol=1e-6, verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 4.39657590\n",
      "Iteration 2, loss = 3.71217380\n",
      "Iteration 3, loss = 2.55477885\n",
      "Iteration 4, loss = 1.82249887\n",
      "Iteration 5, loss = 1.42661593\n",
      "Iteration 6, loss = 1.18147019\n",
      "Iteration 7, loss = 1.01453049\n",
      "Iteration 8, loss = 0.88970916\n",
      "Iteration 9, loss = 0.79782116\n",
      "Iteration 10, loss = 0.71824045\n",
      "Iteration 11, loss = 0.65070040\n",
      "Iteration 12, loss = 0.59544737\n",
      "Iteration 13, loss = 0.54298830\n",
      "Iteration 14, loss = 0.50012259\n",
      "Iteration 15, loss = 0.45860232\n",
      "Iteration 16, loss = 0.42214258\n",
      "Iteration 17, loss = 0.39042106\n",
      "Iteration 18, loss = 0.36076863\n",
      "Iteration 19, loss = 0.33359145\n",
      "Iteration 20, loss = 0.31101551\n",
      "Iteration 21, loss = 0.28558857\n",
      "Iteration 22, loss = 0.26732505\n",
      "Iteration 23, loss = 0.24695319\n",
      "Iteration 24, loss = 0.22988228\n",
      "Iteration 25, loss = 0.21498075\n",
      "Iteration 26, loss = 0.19882505\n",
      "Iteration 27, loss = 0.18647809\n",
      "Iteration 28, loss = 0.17370369\n",
      "Iteration 29, loss = 0.16128112\n",
      "Iteration 30, loss = 0.15129465\n",
      "Iteration 31, loss = 0.14131756\n",
      "Iteration 32, loss = 0.13271587\n",
      "Iteration 33, loss = 0.12366871\n",
      "Iteration 34, loss = 0.11546812\n",
      "Iteration 35, loss = 0.10946931\n",
      "Iteration 36, loss = 0.10114273\n",
      "Iteration 37, loss = 0.09492371\n",
      "Iteration 38, loss = 0.08985511\n",
      "Iteration 39, loss = 0.08407509\n",
      "Iteration 40, loss = 0.08005410\n",
      "Iteration 41, loss = 0.07513532\n",
      "Iteration 42, loss = 0.07057614\n",
      "Iteration 43, loss = 0.06748307\n",
      "Iteration 44, loss = 0.06403540\n",
      "Iteration 45, loss = 0.06023049\n",
      "Iteration 46, loss = 0.05728098\n",
      "Iteration 47, loss = 0.05442417\n",
      "Iteration 48, loss = 0.05192093\n",
      "Iteration 49, loss = 0.04965679\n",
      "Iteration 50, loss = 0.04712682\n",
      "Iteration 51, loss = 0.04549742\n",
      "Iteration 52, loss = 0.04367464\n",
      "Iteration 53, loss = 0.04173919\n",
      "Iteration 54, loss = 0.03999262\n",
      "Iteration 55, loss = 0.03860462\n",
      "Iteration 56, loss = 0.03747013\n",
      "Iteration 57, loss = 0.03607986\n",
      "Iteration 58, loss = 0.03487785\n",
      "Iteration 59, loss = 0.03391011\n",
      "Iteration 60, loss = 0.03272133\n",
      "Iteration 61, loss = 0.03153632\n",
      "Iteration 62, loss = 0.03075858\n",
      "Iteration 63, loss = 0.02998596\n",
      "Iteration 64, loss = 0.02900805\n",
      "Iteration 65, loss = 0.02835707\n",
      "Iteration 66, loss = 0.02759312\n",
      "Iteration 67, loss = 0.02677486\n",
      "Iteration 68, loss = 0.02610884\n",
      "Iteration 69, loss = 0.02545986\n",
      "Iteration 70, loss = 0.02491958\n",
      "Iteration 71, loss = 0.02434415\n",
      "Iteration 72, loss = 0.02379884\n",
      "Iteration 73, loss = 0.02330589\n",
      "Iteration 74, loss = 0.02277206\n",
      "Iteration 75, loss = 0.02230741\n",
      "Iteration 76, loss = 0.02185561\n",
      "Iteration 77, loss = 0.02146677\n",
      "Iteration 78, loss = 0.02107239\n",
      "Iteration 79, loss = 0.02068572\n",
      "Iteration 80, loss = 0.02030977\n",
      "Iteration 81, loss = 0.01991134\n",
      "Iteration 82, loss = 0.01956065\n",
      "Iteration 83, loss = 0.01923119\n",
      "Iteration 84, loss = 0.01893119\n",
      "Iteration 85, loss = 0.01862787\n",
      "Iteration 86, loss = 0.01831376\n",
      "Iteration 87, loss = 0.01803268\n",
      "Iteration 88, loss = 0.01774912\n",
      "Iteration 89, loss = 0.01751259\n",
      "Iteration 90, loss = 0.01730748\n",
      "Iteration 91, loss = 0.01701820\n",
      "Iteration 92, loss = 0.01677888\n",
      "Iteration 93, loss = 0.01656336\n",
      "Iteration 94, loss = 0.01635810\n",
      "Iteration 95, loss = 0.01612163\n",
      "Iteration 96, loss = 0.01593499\n",
      "Iteration 97, loss = 0.01574682\n",
      "Iteration 98, loss = 0.01553299\n",
      "Iteration 99, loss = 0.01536167\n",
      "Iteration 100, loss = 0.01522419\n",
      "Iteration 101, loss = 0.01501852\n",
      "Iteration 102, loss = 0.01488032\n",
      "Iteration 103, loss = 0.01470732\n",
      "Iteration 104, loss = 0.01454592\n",
      "Iteration 105, loss = 0.01441311\n",
      "Iteration 106, loss = 0.01425229\n",
      "Iteration 107, loss = 0.01412957\n",
      "Iteration 108, loss = 0.01397388\n",
      "Iteration 109, loss = 0.01385923\n",
      "Iteration 110, loss = 0.01372761\n",
      "Iteration 111, loss = 0.01359299\n",
      "Iteration 112, loss = 0.01348799\n",
      "Iteration 113, loss = 0.01335948\n",
      "Iteration 114, loss = 0.01324433\n",
      "Iteration 115, loss = 0.01314230\n",
      "Iteration 116, loss = 0.01301585\n",
      "Iteration 117, loss = 0.01290979\n",
      "Iteration 118, loss = 0.01281404\n",
      "Iteration 119, loss = 0.01271700\n",
      "Iteration 120, loss = 0.01262508\n",
      "Iteration 121, loss = 0.01252736\n",
      "Iteration 122, loss = 0.01243883\n",
      "Iteration 123, loss = 0.01234956\n",
      "Iteration 124, loss = 0.01225903\n",
      "Iteration 125, loss = 0.01217237\n",
      "Iteration 126, loss = 0.01209464\n",
      "Iteration 127, loss = 0.01201382\n",
      "Iteration 128, loss = 0.01191794\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.42891540\n",
      "Validation score: 0.000000\n",
      "Iteration 2, loss = 3.83392467\n",
      "Validation score: 0.000000\n",
      "Iteration 3, loss = 2.92167663\n",
      "Validation score: 0.080769\n",
      "Iteration 4, loss = 2.22639753\n",
      "Validation score: 0.350000\n",
      "Iteration 5, loss = 1.64739015\n",
      "Validation score: 0.560769\n",
      "Iteration 6, loss = 1.33295703\n",
      "Validation score: 0.643077\n",
      "Iteration 7, loss = 1.13217105\n",
      "Validation score: 0.703077\n",
      "Iteration 8, loss = 0.99406660\n",
      "Validation score: 0.736923\n",
      "Iteration 9, loss = 0.88980824\n",
      "Validation score: 0.754615\n",
      "Iteration 10, loss = 0.80177490\n",
      "Validation score: 0.774615\n",
      "Iteration 11, loss = 0.73207713\n",
      "Validation score: 0.783846\n",
      "Iteration 12, loss = 0.66991550\n",
      "Validation score: 0.786923\n",
      "Iteration 13, loss = 0.61756297\n",
      "Validation score: 0.803846\n",
      "Iteration 14, loss = 0.56905220\n",
      "Validation score: 0.814615\n",
      "Iteration 15, loss = 0.52764432\n",
      "Validation score: 0.819231\n",
      "Iteration 16, loss = 0.48897491\n",
      "Validation score: 0.815385\n",
      "Iteration 17, loss = 0.45023059\n",
      "Validation score: 0.830769\n",
      "Iteration 18, loss = 0.41836393\n",
      "Validation score: 0.826154\n",
      "Iteration 19, loss = 0.38921059\n",
      "Validation score: 0.842308\n",
      "Iteration 20, loss = 0.36262521\n",
      "Validation score: 0.842308\n",
      "Iteration 21, loss = 0.33830013\n",
      "Validation score: 0.842308\n",
      "Iteration 22, loss = 0.31394957\n",
      "Validation score: 0.841538\n",
      "Iteration 23, loss = 0.29055721\n",
      "Validation score: 0.838462\n",
      "Iteration 24, loss = 0.27341894\n",
      "Validation score: 0.846154\n",
      "Iteration 25, loss = 0.25363555\n",
      "Validation score: 0.858462\n",
      "Iteration 26, loss = 0.23642363\n",
      "Validation score: 0.850000\n",
      "Iteration 27, loss = 0.22117187\n",
      "Validation score: 0.856154\n",
      "Iteration 28, loss = 0.20544479\n",
      "Validation score: 0.860769\n",
      "Iteration 29, loss = 0.19119258\n",
      "Validation score: 0.866154\n",
      "Iteration 30, loss = 0.18041276\n",
      "Validation score: 0.861538\n",
      "Iteration 31, loss = 0.16611274\n",
      "Validation score: 0.857692\n",
      "Iteration 32, loss = 0.15773181\n",
      "Validation score: 0.860769\n",
      "Iteration 33, loss = 0.14502910\n",
      "Validation score: 0.865385\n",
      "Iteration 34, loss = 0.13549779\n",
      "Validation score: 0.867692\n",
      "Iteration 35, loss = 0.12745978\n",
      "Validation score: 0.861538\n",
      "Iteration 36, loss = 0.12096948\n",
      "Validation score: 0.866923\n",
      "Iteration 37, loss = 0.11186006\n",
      "Validation score: 0.870000\n",
      "Iteration 38, loss = 0.10495400\n",
      "Validation score: 0.869231\n",
      "Iteration 39, loss = 0.09878714\n",
      "Validation score: 0.871538\n",
      "Iteration 40, loss = 0.09268140\n",
      "Validation score: 0.877692\n",
      "Iteration 41, loss = 0.08827904\n",
      "Validation score: 0.870769\n",
      "Iteration 42, loss = 0.08283058\n",
      "Validation score: 0.869231\n",
      "Iteration 43, loss = 0.07786877\n",
      "Validation score: 0.874615\n",
      "Iteration 44, loss = 0.07373301\n",
      "Validation score: 0.874615\n",
      "Iteration 45, loss = 0.07047199\n",
      "Validation score: 0.873077\n",
      "Iteration 46, loss = 0.06677480\n",
      "Validation score: 0.874615\n",
      "Iteration 47, loss = 0.06350604\n",
      "Validation score: 0.876154\n",
      "Iteration 48, loss = 0.06068200\n",
      "Validation score: 0.880769\n",
      "Iteration 49, loss = 0.05744424\n",
      "Validation score: 0.873846\n",
      "Iteration 50, loss = 0.05505549\n",
      "Validation score: 0.879231\n",
      "Iteration 51, loss = 0.05254055\n",
      "Validation score: 0.879231\n",
      "Iteration 52, loss = 0.05054573\n",
      "Validation score: 0.872308\n",
      "Iteration 53, loss = 0.04806280\n",
      "Validation score: 0.873846\n",
      "Iteration 54, loss = 0.04621108\n",
      "Validation score: 0.881538\n",
      "Iteration 55, loss = 0.04423386\n",
      "Validation score: 0.876923\n",
      "Iteration 56, loss = 0.04252483\n",
      "Validation score: 0.876923\n",
      "Iteration 57, loss = 0.04080482\n",
      "Validation score: 0.873846\n",
      "Iteration 58, loss = 0.03942414\n",
      "Validation score: 0.879231\n",
      "Iteration 59, loss = 0.03793561\n",
      "Validation score: 0.878462\n",
      "Iteration 60, loss = 0.03689140\n",
      "Validation score: 0.875385\n",
      "Iteration 61, loss = 0.03562528\n",
      "Validation score: 0.877692\n",
      "Iteration 62, loss = 0.03441402\n",
      "Validation score: 0.877692\n",
      "Iteration 63, loss = 0.03333519\n",
      "Validation score: 0.880000\n",
      "Iteration 64, loss = 0.03238906\n",
      "Validation score: 0.874615\n",
      "Iteration 65, loss = 0.03157833\n",
      "Validation score: 0.877692\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 4.40809620\n",
      "Iteration 2, loss = 4.19598114\n",
      "Iteration 3, loss = 4.18883807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4, loss = 4.18438605\n",
      "Iteration 5, loss = 4.18107421\n",
      "Iteration 6, loss = 4.17840663\n",
      "Iteration 7, loss = 4.17613849\n",
      "Iteration 8, loss = 4.17415907\n",
      "Iteration 9, loss = 4.17237027\n",
      "Iteration 10, loss = 4.17073630\n",
      "Iteration 11, loss = 4.16923612\n",
      "Iteration 12, loss = 4.16782614\n",
      "Iteration 13, loss = 4.16649870\n",
      "Iteration 14, loss = 4.16521938\n",
      "Iteration 15, loss = 4.16401307\n",
      "Iteration 16, loss = 4.16284685\n",
      "Iteration 17, loss = 4.16172032\n",
      "Iteration 18, loss = 4.16063626\n",
      "Iteration 19, loss = 4.15957294\n",
      "Iteration 20, loss = 4.15853989\n",
      "Iteration 21, loss = 4.15753405\n",
      "Iteration 22, loss = 4.15654766\n",
      "Iteration 23, loss = 4.15557807\n",
      "Iteration 24, loss = 4.15462467\n",
      "Iteration 25, loss = 4.15369194\n",
      "Iteration 26, loss = 4.15276776\n",
      "Iteration 27, loss = 4.15185880\n",
      "Iteration 28, loss = 4.15095383\n",
      "Iteration 29, loss = 4.15006565\n",
      "Iteration 30, loss = 4.14918646\n",
      "Iteration 31, loss = 4.14831216\n",
      "Iteration 32, loss = 4.14745183\n",
      "Iteration 33, loss = 4.14658793\n",
      "Iteration 34, loss = 4.14574079\n",
      "Iteration 35, loss = 4.14489428\n",
      "Iteration 36, loss = 4.14405084\n",
      "Iteration 37, loss = 4.14321983\n",
      "Iteration 38, loss = 4.14238702\n",
      "Iteration 39, loss = 4.14156536\n",
      "Iteration 40, loss = 4.14073972\n",
      "Iteration 41, loss = 4.13991716\n",
      "Iteration 42, loss = 4.13910119\n",
      "Iteration 43, loss = 4.13829515\n",
      "Iteration 44, loss = 4.13748816\n",
      "Iteration 45, loss = 4.13667539\n",
      "Iteration 46, loss = 4.13587182\n",
      "Iteration 47, loss = 4.13507167\n",
      "Iteration 48, loss = 4.13426531\n",
      "Iteration 49, loss = 4.13346607\n",
      "Iteration 50, loss = 4.13266872\n",
      "Iteration 51, loss = 4.13187750\n",
      "Iteration 52, loss = 4.13108297\n",
      "Iteration 53, loss = 4.13028756\n",
      "Iteration 54, loss = 4.12949942\n",
      "Iteration 55, loss = 4.12870707\n",
      "Iteration 56, loss = 4.12791941\n",
      "Iteration 57, loss = 4.12712748\n",
      "Iteration 58, loss = 4.12633919\n",
      "Iteration 59, loss = 4.12555451\n",
      "Iteration 60, loss = 4.12476701\n",
      "Iteration 61, loss = 4.12398015\n",
      "Iteration 62, loss = 4.12319373\n",
      "Iteration 63, loss = 4.12240915\n",
      "Iteration 64, loss = 4.12162586\n",
      "Iteration 65, loss = 4.12083580\n",
      "Iteration 66, loss = 4.12005454\n",
      "Iteration 67, loss = 4.11927133\n",
      "Iteration 68, loss = 4.11848495\n",
      "Iteration 69, loss = 4.11770286\n",
      "Iteration 70, loss = 4.11691457\n",
      "Iteration 71, loss = 4.11612929\n",
      "Iteration 72, loss = 4.11534658\n",
      "Iteration 73, loss = 4.11456220\n",
      "Iteration 74, loss = 4.11377851\n",
      "Iteration 75, loss = 4.11298881\n",
      "Iteration 76, loss = 4.11220516\n",
      "Iteration 77, loss = 4.11141986\n",
      "Iteration 78, loss = 4.11063628\n",
      "Iteration 79, loss = 4.10984717\n",
      "Iteration 80, loss = 4.10905817\n",
      "Iteration 81, loss = 4.10827535\n",
      "Iteration 82, loss = 4.10748426\n",
      "Iteration 83, loss = 4.10669685\n",
      "Iteration 84, loss = 4.10590914\n",
      "Iteration 85, loss = 4.10511506\n",
      "Iteration 86, loss = 4.10432579\n",
      "Iteration 87, loss = 4.10353599\n",
      "Iteration 88, loss = 4.10274496\n",
      "Iteration 89, loss = 4.10195291\n",
      "Iteration 90, loss = 4.10116048\n",
      "Iteration 91, loss = 4.10036858\n",
      "Iteration 92, loss = 4.09957173\n",
      "Iteration 93, loss = 4.09878020\n",
      "Iteration 94, loss = 4.09798327\n",
      "Iteration 95, loss = 4.09718843\n",
      "Iteration 96, loss = 4.09639232\n",
      "Iteration 97, loss = 4.09559386\n",
      "Iteration 98, loss = 4.09479373\n",
      "Iteration 99, loss = 4.09399509\n",
      "Iteration 100, loss = 4.09319781\n",
      "Iteration 101, loss = 4.09239468\n",
      "Iteration 102, loss = 4.09159485\n",
      "Iteration 103, loss = 4.09079208\n",
      "Iteration 104, loss = 4.08999042\n",
      "Iteration 105, loss = 4.08918766\n",
      "Iteration 106, loss = 4.08838070\n",
      "Iteration 107, loss = 4.08757571\n",
      "Iteration 108, loss = 4.08677092\n",
      "Iteration 109, loss = 4.08596145\n",
      "Iteration 110, loss = 4.08515347\n",
      "Iteration 111, loss = 4.08434625\n",
      "Iteration 112, loss = 4.08353775\n",
      "Iteration 113, loss = 4.08272420\n",
      "Iteration 114, loss = 4.08191278\n",
      "Iteration 115, loss = 4.08109901\n",
      "Iteration 116, loss = 4.08028562\n",
      "Iteration 117, loss = 4.07947022\n",
      "Iteration 118, loss = 4.07865621\n",
      "Iteration 119, loss = 4.07784140\n",
      "Iteration 120, loss = 4.07702066\n",
      "Iteration 121, loss = 4.07620521\n",
      "Iteration 122, loss = 4.07538501\n",
      "Iteration 123, loss = 4.07456519\n",
      "Iteration 124, loss = 4.07374346\n",
      "Iteration 125, loss = 4.07292033\n",
      "Iteration 126, loss = 4.07209784\n",
      "Iteration 127, loss = 4.07127719\n",
      "Iteration 128, loss = 4.07044995\n",
      "Iteration 129, loss = 4.06962415\n",
      "Iteration 130, loss = 4.06879821\n",
      "Iteration 131, loss = 4.06796951\n",
      "Iteration 132, loss = 4.06714189\n",
      "Iteration 133, loss = 4.06631126\n",
      "Iteration 134, loss = 4.06548088\n",
      "Iteration 135, loss = 4.06464989\n",
      "Iteration 136, loss = 4.06381734\n",
      "Iteration 137, loss = 4.06298396\n",
      "Iteration 138, loss = 4.06215174\n",
      "Iteration 139, loss = 4.06131444\n",
      "Iteration 140, loss = 4.06048069\n",
      "Iteration 141, loss = 4.05964332\n",
      "Iteration 142, loss = 4.05880325\n",
      "Iteration 143, loss = 4.05796432\n",
      "Iteration 144, loss = 4.05712557\n",
      "Iteration 145, loss = 4.05628553\n",
      "Iteration 146, loss = 4.05544261\n",
      "Iteration 147, loss = 4.05459892\n",
      "Iteration 148, loss = 4.05375429\n",
      "Iteration 149, loss = 4.05291063\n",
      "Iteration 150, loss = 4.05206525\n",
      "Iteration 151, loss = 4.05122119\n",
      "Iteration 152, loss = 4.05037014\n",
      "Iteration 153, loss = 4.04952346\n",
      "Iteration 154, loss = 4.04867475\n",
      "Iteration 155, loss = 4.04782080\n",
      "Iteration 156, loss = 4.04697129\n",
      "Iteration 157, loss = 4.04612035\n",
      "Iteration 158, loss = 4.04526498\n",
      "Iteration 159, loss = 4.04441247\n",
      "Iteration 160, loss = 4.04355863\n",
      "Iteration 161, loss = 4.04270246\n",
      "Iteration 162, loss = 4.04184505\n",
      "Iteration 163, loss = 4.04098798\n",
      "Iteration 164, loss = 4.04012908\n",
      "Iteration 165, loss = 4.03926935\n",
      "Iteration 166, loss = 4.03840951\n",
      "Iteration 167, loss = 4.03754710\n",
      "Iteration 168, loss = 4.03668571\n",
      "Iteration 169, loss = 4.03582303\n",
      "Iteration 170, loss = 4.03495977\n",
      "Iteration 171, loss = 4.03409615\n",
      "Iteration 172, loss = 4.03323138\n",
      "Iteration 173, loss = 4.03236499\n",
      "Iteration 174, loss = 4.03149850\n",
      "Iteration 175, loss = 4.03062957\n",
      "Iteration 176, loss = 4.02975895\n",
      "Iteration 177, loss = 4.02888932\n",
      "Iteration 178, loss = 4.02801819\n",
      "Iteration 179, loss = 4.02714724\n",
      "Iteration 180, loss = 4.02627800\n",
      "Iteration 181, loss = 4.02540222\n",
      "Iteration 182, loss = 4.02452869\n",
      "Iteration 183, loss = 4.02365346\n",
      "Iteration 184, loss = 4.02277891\n",
      "Iteration 185, loss = 4.02190328\n",
      "Iteration 186, loss = 4.02102678\n",
      "Iteration 187, loss = 4.02014778\n",
      "Iteration 188, loss = 4.01927209\n",
      "Iteration 189, loss = 4.01839338\n",
      "Iteration 190, loss = 4.01751197\n",
      "Iteration 191, loss = 4.01662981\n",
      "Iteration 192, loss = 4.01574803\n",
      "Iteration 193, loss = 4.01486610\n",
      "Iteration 194, loss = 4.01398427\n",
      "Iteration 195, loss = 4.01310131\n",
      "Iteration 196, loss = 4.01221519\n",
      "Iteration 197, loss = 4.01133077\n",
      "Iteration 198, loss = 4.01044392\n",
      "Iteration 199, loss = 4.00955586\n",
      "Iteration 200, loss = 4.00866962\n",
      "Iteration 201, loss = 4.00778192\n",
      "Iteration 202, loss = 4.00689247\n",
      "Iteration 203, loss = 4.00600415\n",
      "Iteration 204, loss = 4.00511414\n",
      "Iteration 205, loss = 4.00422475\n",
      "Iteration 206, loss = 4.00333332\n",
      "Iteration 207, loss = 4.00244252\n",
      "Iteration 208, loss = 4.00154692\n",
      "Iteration 209, loss = 4.00065514\n",
      "Iteration 210, loss = 3.99975869\n",
      "Iteration 211, loss = 3.99886497\n",
      "Iteration 212, loss = 3.99797107\n",
      "Iteration 213, loss = 3.99707275\n",
      "Iteration 214, loss = 3.99617931\n",
      "Iteration 215, loss = 3.99527976\n",
      "Iteration 216, loss = 3.99438453\n",
      "Iteration 217, loss = 3.99348530\n",
      "Iteration 218, loss = 3.99258674\n",
      "Iteration 219, loss = 3.99168966\n",
      "Iteration 220, loss = 3.99078550\n",
      "Iteration 221, loss = 3.98988700\n",
      "Iteration 222, loss = 3.98898625\n",
      "Iteration 223, loss = 3.98808452\n",
      "Iteration 224, loss = 3.98718271\n",
      "Iteration 225, loss = 3.98628062\n",
      "Iteration 226, loss = 3.98537596\n",
      "Iteration 227, loss = 3.98447306\n",
      "Iteration 228, loss = 3.98356792\n",
      "Iteration 229, loss = 3.98266317\n",
      "Iteration 230, loss = 3.98175718\n",
      "Iteration 231, loss = 3.98085182\n",
      "Iteration 232, loss = 3.97994533\n",
      "Iteration 233, loss = 3.97903932\n",
      "Iteration 234, loss = 3.97813155\n",
      "Iteration 235, loss = 3.97722355\n",
      "Iteration 236, loss = 3.97631694\n",
      "Iteration 237, loss = 3.97540969\n",
      "Iteration 238, loss = 3.97449852\n",
      "Iteration 239, loss = 3.97358926\n",
      "Iteration 240, loss = 3.97267934\n",
      "Iteration 241, loss = 3.97176820\n",
      "Iteration 242, loss = 3.97085748\n",
      "Iteration 243, loss = 3.96994822\n",
      "Iteration 244, loss = 3.96903610\n",
      "Iteration 245, loss = 3.96812478\n",
      "Iteration 246, loss = 3.96721283\n",
      "Iteration 247, loss = 3.96629966\n",
      "Iteration 248, loss = 3.96538732\n",
      "Iteration 249, loss = 3.96447300\n",
      "Iteration 250, loss = 3.96355862\n",
      "Iteration 251, loss = 3.96264646\n",
      "Iteration 252, loss = 3.96173172\n",
      "Iteration 253, loss = 3.96081672\n",
      "Iteration 254, loss = 3.95990106\n",
      "Iteration 255, loss = 3.95898545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 256, loss = 3.95807039\n",
      "Iteration 257, loss = 3.95715441\n",
      "Iteration 258, loss = 3.95623944\n",
      "Iteration 259, loss = 3.95532244\n",
      "Iteration 260, loss = 3.95440588\n",
      "Iteration 261, loss = 3.95348798\n",
      "Iteration 262, loss = 3.95257004\n",
      "Iteration 263, loss = 3.95165328\n",
      "Iteration 264, loss = 3.95073581\n",
      "Iteration 265, loss = 3.94981854\n",
      "Iteration 266, loss = 3.94889842\n",
      "Iteration 267, loss = 3.94798079\n",
      "Iteration 268, loss = 3.94706332\n",
      "Iteration 269, loss = 3.94614314\n",
      "Iteration 270, loss = 3.94522305\n",
      "Iteration 271, loss = 3.94430494\n",
      "Iteration 272, loss = 3.94338391\n",
      "Iteration 273, loss = 3.94246527\n",
      "Iteration 274, loss = 3.94154474\n",
      "Iteration 275, loss = 3.94062464\n",
      "Iteration 276, loss = 3.93970438\n",
      "Iteration 277, loss = 3.93878534\n",
      "Iteration 278, loss = 3.93786529\n",
      "Iteration 279, loss = 3.93694289\n",
      "Iteration 280, loss = 3.93602124\n",
      "Iteration 281, loss = 3.93510103\n",
      "Iteration 282, loss = 3.93418089\n",
      "Iteration 283, loss = 3.93325909\n",
      "Iteration 284, loss = 3.93233764\n",
      "Iteration 285, loss = 3.93141796\n",
      "Iteration 286, loss = 3.93049612\n",
      "Iteration 287, loss = 3.92957344\n",
      "Iteration 288, loss = 3.92865192\n",
      "Iteration 289, loss = 3.92773009\n",
      "Iteration 290, loss = 3.92680821\n",
      "Iteration 291, loss = 3.92588716\n",
      "Iteration 292, loss = 3.92496474\n",
      "Iteration 293, loss = 3.92404130\n",
      "Iteration 294, loss = 3.92312008\n",
      "Iteration 295, loss = 3.92219979\n",
      "Iteration 296, loss = 3.92127854\n",
      "Iteration 297, loss = 3.92035596\n",
      "Iteration 298, loss = 3.91943433\n",
      "Iteration 299, loss = 3.91851186\n",
      "Iteration 300, loss = 3.91758970\n",
      "Iteration 301, loss = 3.91666875\n",
      "Iteration 302, loss = 3.91574586\n",
      "Iteration 303, loss = 3.91482192\n",
      "Iteration 304, loss = 3.91390206\n",
      "Iteration 305, loss = 3.91297946\n",
      "Iteration 306, loss = 3.91205782\n",
      "Iteration 307, loss = 3.91113623\n",
      "Iteration 308, loss = 3.91021384\n",
      "Iteration 309, loss = 3.90929141\n",
      "Iteration 310, loss = 3.90837228\n",
      "Iteration 311, loss = 3.90744852\n",
      "Iteration 312, loss = 3.90652615\n",
      "Iteration 313, loss = 3.90560664\n",
      "Iteration 314, loss = 3.90468415\n",
      "Iteration 315, loss = 3.90376382\n",
      "Iteration 316, loss = 3.90284617\n",
      "Iteration 317, loss = 3.90192066\n",
      "Iteration 318, loss = 3.90100073\n",
      "Iteration 319, loss = 3.90008118\n",
      "Iteration 320, loss = 3.89916076\n",
      "Iteration 321, loss = 3.89824091\n",
      "Iteration 322, loss = 3.89732045\n",
      "Iteration 323, loss = 3.89639977\n",
      "Iteration 324, loss = 3.89548070\n",
      "Iteration 325, loss = 3.89456146\n",
      "Iteration 326, loss = 3.89363952\n",
      "Iteration 327, loss = 3.89272109\n",
      "Iteration 328, loss = 3.89180306\n",
      "Iteration 329, loss = 3.89088347\n",
      "Iteration 330, loss = 3.88996375\n",
      "Iteration 331, loss = 3.88904637\n",
      "Iteration 332, loss = 3.88812762\n",
      "Iteration 333, loss = 3.88720962\n",
      "Iteration 334, loss = 3.88629136\n",
      "Iteration 335, loss = 3.88537436\n",
      "Iteration 336, loss = 3.88445504\n",
      "Iteration 337, loss = 3.88353953\n",
      "Iteration 338, loss = 3.88262175\n",
      "Iteration 339, loss = 3.88170505\n",
      "Iteration 340, loss = 3.88078874\n",
      "Iteration 341, loss = 3.87987258\n",
      "Iteration 342, loss = 3.87895730\n",
      "Iteration 343, loss = 3.87803952\n",
      "Iteration 344, loss = 3.87712650\n",
      "Iteration 345, loss = 3.87621257\n",
      "Iteration 346, loss = 3.87529686\n",
      "Iteration 347, loss = 3.87438269\n",
      "Iteration 348, loss = 3.87346981\n",
      "Iteration 349, loss = 3.87255359\n",
      "Iteration 350, loss = 3.87164077\n",
      "Iteration 351, loss = 3.87072643\n",
      "Iteration 352, loss = 3.86981643\n",
      "Iteration 353, loss = 3.86890192\n",
      "Iteration 354, loss = 3.86798974\n",
      "Iteration 355, loss = 3.86707795\n",
      "Iteration 356, loss = 3.86616718\n",
      "Iteration 357, loss = 3.86525513\n",
      "Iteration 358, loss = 3.86434488\n",
      "Iteration 359, loss = 3.86343450\n",
      "Iteration 360, loss = 3.86252402\n",
      "Iteration 361, loss = 3.86161388\n",
      "Iteration 362, loss = 3.86070491\n",
      "Iteration 363, loss = 3.85979620\n",
      "Iteration 364, loss = 3.85888733\n",
      "Iteration 365, loss = 3.85797970\n",
      "Iteration 366, loss = 3.85707284\n",
      "Iteration 367, loss = 3.85616572\n",
      "Iteration 368, loss = 3.85525680\n",
      "Iteration 369, loss = 3.85435276\n",
      "Iteration 370, loss = 3.85344641\n",
      "Iteration 371, loss = 3.85253916\n",
      "Iteration 372, loss = 3.85163518\n",
      "Iteration 373, loss = 3.85072905\n",
      "Iteration 374, loss = 3.84982533\n",
      "Iteration 375, loss = 3.84892130\n",
      "Iteration 376, loss = 3.84801904\n",
      "Iteration 377, loss = 3.84711652\n",
      "Iteration 378, loss = 3.84621251\n",
      "Iteration 379, loss = 3.84531342\n",
      "Iteration 380, loss = 3.84441098\n",
      "Iteration 381, loss = 3.84350879\n",
      "Iteration 382, loss = 3.84260762\n",
      "Iteration 383, loss = 3.84170668\n",
      "Iteration 384, loss = 3.84080932\n",
      "Iteration 385, loss = 3.83990880\n",
      "Iteration 386, loss = 3.83901048\n",
      "Iteration 387, loss = 3.83811318\n",
      "Iteration 388, loss = 3.83721362\n",
      "Iteration 389, loss = 3.83631658\n",
      "Iteration 390, loss = 3.83542116\n",
      "Iteration 391, loss = 3.83452538\n",
      "Iteration 392, loss = 3.83363014\n",
      "Iteration 393, loss = 3.83273625\n",
      "Iteration 394, loss = 3.83184194\n",
      "Iteration 395, loss = 3.83094813\n",
      "Iteration 396, loss = 3.83005426\n",
      "Iteration 397, loss = 3.82916117\n",
      "Iteration 398, loss = 3.82826909\n",
      "Iteration 399, loss = 3.82737577\n",
      "Iteration 400, loss = 3.82648599\n",
      "Iteration 1, loss = 4.40331974\n",
      "Iteration 2, loss = 4.17482433\n",
      "Iteration 3, loss = 4.15041043\n",
      "Iteration 4, loss = 4.13186172\n",
      "Iteration 5, loss = 4.11218364\n",
      "Iteration 6, loss = 4.09015389\n",
      "Iteration 7, loss = 4.06543021\n",
      "Iteration 8, loss = 4.03780816\n",
      "Iteration 9, loss = 4.00730951\n",
      "Iteration 10, loss = 3.97401808\n",
      "Iteration 11, loss = 3.93808432\n",
      "Iteration 12, loss = 3.90023225\n",
      "Iteration 13, loss = 3.86088017\n",
      "Iteration 14, loss = 3.82082597\n",
      "Iteration 15, loss = 3.78057140\n",
      "Iteration 16, loss = 3.74084417\n",
      "Iteration 17, loss = 3.70211773\n",
      "Iteration 18, loss = 3.66470070\n",
      "Iteration 19, loss = 3.62893635\n",
      "Iteration 20, loss = 3.59487514\n",
      "Iteration 21, loss = 3.56268086\n",
      "Iteration 22, loss = 3.53224104\n",
      "Iteration 23, loss = 3.50363692\n",
      "Iteration 24, loss = 3.47646668\n",
      "Iteration 25, loss = 3.45096320\n",
      "Iteration 26, loss = 3.42690517\n",
      "Iteration 27, loss = 3.40409102\n",
      "Iteration 28, loss = 3.38240824\n",
      "Iteration 29, loss = 3.36169917\n",
      "Iteration 30, loss = 3.34194347\n",
      "Iteration 31, loss = 3.32297411\n",
      "Iteration 32, loss = 3.30474082\n",
      "Iteration 33, loss = 3.28709772\n",
      "Iteration 34, loss = 3.27014023\n",
      "Iteration 35, loss = 3.25358407\n",
      "Iteration 36, loss = 3.23730238\n",
      "Iteration 37, loss = 3.22147676\n",
      "Iteration 38, loss = 3.20593464\n",
      "Iteration 39, loss = 3.19061243\n",
      "Iteration 40, loss = 3.17558001\n",
      "Iteration 41, loss = 3.16079910\n",
      "Iteration 42, loss = 3.14611174\n",
      "Iteration 43, loss = 3.13164655\n",
      "Iteration 44, loss = 3.11733884\n",
      "Iteration 45, loss = 3.10319461\n",
      "Iteration 46, loss = 3.08919692\n",
      "Iteration 47, loss = 3.07538460\n",
      "Iteration 48, loss = 3.06162984\n",
      "Iteration 49, loss = 3.04812931\n",
      "Iteration 50, loss = 3.03478876\n",
      "Iteration 51, loss = 3.02158251\n",
      "Iteration 52, loss = 3.00866146\n",
      "Iteration 53, loss = 2.99583195\n",
      "Iteration 54, loss = 2.98328818\n",
      "Iteration 55, loss = 2.97083486\n",
      "Iteration 56, loss = 2.95878329\n",
      "Iteration 57, loss = 2.94681191\n",
      "Iteration 58, loss = 2.93501976\n",
      "Iteration 59, loss = 2.92348397\n",
      "Iteration 60, loss = 2.91217153\n",
      "Iteration 61, loss = 2.90105189\n",
      "Iteration 62, loss = 2.89021213\n",
      "Iteration 63, loss = 2.87949072\n",
      "Iteration 64, loss = 2.86900828\n",
      "Iteration 65, loss = 2.85864264\n",
      "Iteration 66, loss = 2.84855919\n",
      "Iteration 67, loss = 2.83854280\n",
      "Iteration 68, loss = 2.82870720\n",
      "Iteration 69, loss = 2.81910656\n",
      "Iteration 70, loss = 2.80951796\n",
      "Iteration 71, loss = 2.80014592\n",
      "Iteration 72, loss = 2.79088249\n",
      "Iteration 73, loss = 2.78168425\n",
      "Iteration 74, loss = 2.77269950\n",
      "Iteration 75, loss = 2.76381261\n",
      "Iteration 76, loss = 2.75502569\n",
      "Iteration 77, loss = 2.74630902\n",
      "Iteration 78, loss = 2.73769038\n",
      "Iteration 79, loss = 2.72904308\n",
      "Iteration 80, loss = 2.72067438\n",
      "Iteration 81, loss = 2.71229731\n",
      "Iteration 82, loss = 2.70395969\n",
      "Iteration 83, loss = 2.69574919\n",
      "Iteration 84, loss = 2.68758653\n",
      "Iteration 85, loss = 2.67948133\n",
      "Iteration 86, loss = 2.67142269\n",
      "Iteration 87, loss = 2.66340770\n",
      "Iteration 88, loss = 2.65546714\n",
      "Iteration 89, loss = 2.64755224\n",
      "Iteration 90, loss = 2.63972192\n",
      "Iteration 91, loss = 2.63193060\n",
      "Iteration 92, loss = 2.62421941\n",
      "Iteration 93, loss = 2.61655039\n",
      "Iteration 94, loss = 2.60897525\n",
      "Iteration 95, loss = 2.60127736\n",
      "Iteration 96, loss = 2.59371473\n",
      "Iteration 97, loss = 2.58615632\n",
      "Iteration 98, loss = 2.57878175\n",
      "Iteration 99, loss = 2.57137864\n",
      "Iteration 100, loss = 2.56388718\n",
      "Iteration 101, loss = 2.55651666\n",
      "Iteration 102, loss = 2.54919344\n",
      "Iteration 103, loss = 2.54187103\n",
      "Iteration 104, loss = 2.53472289\n",
      "Iteration 105, loss = 2.52744658\n",
      "Iteration 106, loss = 2.52024370\n",
      "Iteration 107, loss = 2.51311581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 108, loss = 2.50599272\n",
      "Iteration 109, loss = 2.49884147\n",
      "Iteration 110, loss = 2.49178515\n",
      "Iteration 111, loss = 2.48483455\n",
      "Iteration 112, loss = 2.47780173\n",
      "Iteration 113, loss = 2.47085404\n",
      "Iteration 114, loss = 2.46394333\n",
      "Iteration 115, loss = 2.45699478\n",
      "Iteration 116, loss = 2.45013902\n",
      "Iteration 117, loss = 2.44326913\n",
      "Iteration 118, loss = 2.43648600\n",
      "Iteration 119, loss = 2.42968859\n",
      "Iteration 120, loss = 2.42291198\n",
      "Iteration 121, loss = 2.41615148\n",
      "Iteration 122, loss = 2.40934012\n",
      "Iteration 123, loss = 2.40267632\n",
      "Iteration 124, loss = 2.39597161\n",
      "Iteration 125, loss = 2.38929682\n",
      "Iteration 126, loss = 2.38267524\n",
      "Iteration 127, loss = 2.37602131\n",
      "Iteration 128, loss = 2.36936198\n",
      "Iteration 129, loss = 2.36284175\n",
      "Iteration 130, loss = 2.35619963\n",
      "Iteration 131, loss = 2.34970430\n",
      "Iteration 132, loss = 2.34314239\n",
      "Iteration 133, loss = 2.33657702\n",
      "Iteration 134, loss = 2.33006618\n",
      "Iteration 135, loss = 2.32359667\n",
      "Iteration 136, loss = 2.31711320\n",
      "Iteration 137, loss = 2.31063117\n",
      "Iteration 138, loss = 2.30411361\n",
      "Iteration 139, loss = 2.29758394\n",
      "Iteration 140, loss = 2.29119625\n",
      "Iteration 141, loss = 2.28477925\n",
      "Iteration 142, loss = 2.27829830\n",
      "Iteration 143, loss = 2.27186200\n",
      "Iteration 144, loss = 2.26544971\n",
      "Iteration 145, loss = 2.25899839\n",
      "Iteration 146, loss = 2.25259742\n",
      "Iteration 147, loss = 2.24619483\n",
      "Iteration 148, loss = 2.23980326\n",
      "Iteration 149, loss = 2.23342982\n",
      "Iteration 150, loss = 2.22705485\n",
      "Iteration 151, loss = 2.22072162\n",
      "Iteration 152, loss = 2.21431278\n",
      "Iteration 153, loss = 2.20798365\n",
      "Iteration 154, loss = 2.20162402\n",
      "Iteration 155, loss = 2.19533204\n",
      "Iteration 156, loss = 2.18894439\n",
      "Iteration 157, loss = 2.18268954\n",
      "Iteration 158, loss = 2.17639296\n",
      "Iteration 159, loss = 2.17009588\n",
      "Iteration 160, loss = 2.16379186\n",
      "Iteration 161, loss = 2.15759807\n",
      "Iteration 162, loss = 2.15132124\n",
      "Iteration 163, loss = 2.14509286\n",
      "Iteration 164, loss = 2.13887526\n",
      "Iteration 165, loss = 2.13268110\n",
      "Iteration 166, loss = 2.12647729\n",
      "Iteration 167, loss = 2.12041001\n",
      "Iteration 168, loss = 2.11421521\n",
      "Iteration 169, loss = 2.10814242\n",
      "Iteration 170, loss = 2.10208268\n",
      "Iteration 171, loss = 2.09603397\n",
      "Iteration 172, loss = 2.09002942\n",
      "Iteration 173, loss = 2.08397910\n",
      "Iteration 174, loss = 2.07805915\n",
      "Iteration 175, loss = 2.07209710\n",
      "Iteration 176, loss = 2.06617117\n",
      "Iteration 177, loss = 2.06035657\n",
      "Iteration 178, loss = 2.05450015\n",
      "Iteration 179, loss = 2.04867679\n",
      "Iteration 180, loss = 2.04294623\n",
      "Iteration 181, loss = 2.03719793\n",
      "Iteration 182, loss = 2.03154458\n",
      "Iteration 183, loss = 2.02591564\n",
      "Iteration 184, loss = 2.02033070\n",
      "Iteration 185, loss = 2.01472298\n",
      "Iteration 186, loss = 2.00930509\n",
      "Iteration 187, loss = 2.00371746\n",
      "Iteration 188, loss = 1.99833555\n",
      "Iteration 189, loss = 1.99294390\n",
      "Iteration 190, loss = 1.98764361\n",
      "Iteration 191, loss = 1.98236238\n",
      "Iteration 192, loss = 1.97711027\n",
      "Iteration 193, loss = 1.97190406\n",
      "Iteration 194, loss = 1.96672668\n",
      "Iteration 195, loss = 1.96158982\n",
      "Iteration 196, loss = 1.95654066\n",
      "Iteration 197, loss = 1.95150964\n",
      "Iteration 198, loss = 1.94649710\n",
      "Iteration 199, loss = 1.94159685\n",
      "Iteration 200, loss = 1.93669627\n",
      "Iteration 201, loss = 1.93180876\n",
      "Iteration 202, loss = 1.92701624\n",
      "Iteration 203, loss = 1.92226561\n",
      "Iteration 204, loss = 1.91752841\n",
      "Iteration 205, loss = 1.91287516\n",
      "Iteration 206, loss = 1.90824120\n",
      "Iteration 207, loss = 1.90365121\n",
      "Iteration 208, loss = 1.89908869\n",
      "Iteration 209, loss = 1.89459144\n",
      "Iteration 210, loss = 1.89016737\n",
      "Iteration 211, loss = 1.88568236\n",
      "Iteration 212, loss = 1.88131587\n",
      "Iteration 213, loss = 1.87692747\n",
      "Iteration 214, loss = 1.87262549\n",
      "Iteration 215, loss = 1.86836955\n",
      "Iteration 216, loss = 1.86413180\n",
      "Iteration 217, loss = 1.85986587\n",
      "Iteration 218, loss = 1.85571455\n",
      "Iteration 219, loss = 1.85158692\n",
      "Iteration 220, loss = 1.84751199\n",
      "Iteration 221, loss = 1.84338069\n",
      "Iteration 222, loss = 1.83934788\n",
      "Iteration 223, loss = 1.83534340\n",
      "Iteration 224, loss = 1.83136358\n",
      "Iteration 225, loss = 1.82739878\n",
      "Iteration 226, loss = 1.82348876\n",
      "Iteration 227, loss = 1.81961728\n",
      "Iteration 228, loss = 1.81575122\n",
      "Iteration 229, loss = 1.81191680\n",
      "Iteration 230, loss = 1.80814515\n",
      "Iteration 231, loss = 1.80437934\n",
      "Iteration 232, loss = 1.80059875\n",
      "Iteration 233, loss = 1.79686417\n",
      "Iteration 234, loss = 1.79315360\n",
      "Iteration 235, loss = 1.78944897\n",
      "Iteration 236, loss = 1.78585319\n",
      "Iteration 237, loss = 1.78220534\n",
      "Iteration 238, loss = 1.77861219\n",
      "Iteration 239, loss = 1.77503263\n",
      "Iteration 240, loss = 1.77148593\n",
      "Iteration 241, loss = 1.76791160\n",
      "Iteration 242, loss = 1.76446511\n",
      "Iteration 243, loss = 1.76092744\n",
      "Iteration 244, loss = 1.75747145\n",
      "Iteration 245, loss = 1.75406039\n",
      "Iteration 246, loss = 1.75058071\n",
      "Iteration 247, loss = 1.74721809\n",
      "Iteration 248, loss = 1.74377931\n",
      "Iteration 249, loss = 1.74042800\n",
      "Iteration 250, loss = 1.73709356\n",
      "Iteration 251, loss = 1.73372005\n",
      "Iteration 252, loss = 1.73041159\n",
      "Iteration 253, loss = 1.72718161\n",
      "Iteration 254, loss = 1.72380669\n",
      "Iteration 255, loss = 1.72058927\n",
      "Iteration 256, loss = 1.71735698\n",
      "Iteration 257, loss = 1.71415610\n",
      "Iteration 258, loss = 1.71090896\n",
      "Iteration 259, loss = 1.70772134\n",
      "Iteration 260, loss = 1.70455259\n",
      "Iteration 261, loss = 1.70138889\n",
      "Iteration 262, loss = 1.69822474\n",
      "Iteration 263, loss = 1.69507320\n",
      "Iteration 264, loss = 1.69195006\n",
      "Iteration 265, loss = 1.68886443\n",
      "Iteration 266, loss = 1.68577487\n",
      "Iteration 267, loss = 1.68262675\n",
      "Iteration 268, loss = 1.67968746\n",
      "Iteration 269, loss = 1.67657493\n",
      "Iteration 270, loss = 1.67356378\n",
      "Iteration 271, loss = 1.67052204\n",
      "Iteration 272, loss = 1.66753467\n",
      "Iteration 273, loss = 1.66457572\n",
      "Iteration 274, loss = 1.66151900\n",
      "Iteration 275, loss = 1.65857727\n",
      "Iteration 276, loss = 1.65565157\n",
      "Iteration 277, loss = 1.65264816\n",
      "Iteration 278, loss = 1.64969813\n",
      "Iteration 279, loss = 1.64682983\n",
      "Iteration 280, loss = 1.64390729\n",
      "Iteration 281, loss = 1.64100143\n",
      "Iteration 282, loss = 1.63809462\n",
      "Iteration 283, loss = 1.63528781\n",
      "Iteration 284, loss = 1.63242742\n",
      "Iteration 285, loss = 1.62956756\n",
      "Iteration 286, loss = 1.62666613\n",
      "Iteration 287, loss = 1.62389446\n",
      "Iteration 288, loss = 1.62107808\n",
      "Iteration 289, loss = 1.61825990\n",
      "Iteration 290, loss = 1.61546158\n",
      "Iteration 291, loss = 1.61265846\n",
      "Iteration 292, loss = 1.60999637\n",
      "Iteration 293, loss = 1.60716546\n",
      "Iteration 294, loss = 1.60438614\n",
      "Iteration 295, loss = 1.60160838\n",
      "Iteration 296, loss = 1.59890795\n",
      "Iteration 297, loss = 1.59617907\n",
      "Iteration 298, loss = 1.59343376\n",
      "Iteration 299, loss = 1.59069683\n",
      "Iteration 300, loss = 1.58807936\n",
      "Iteration 301, loss = 1.58540258\n",
      "Iteration 302, loss = 1.58267003\n",
      "Iteration 303, loss = 1.58005328\n",
      "Iteration 304, loss = 1.57737523\n",
      "Iteration 305, loss = 1.57472549\n",
      "Iteration 306, loss = 1.57208561\n",
      "Iteration 307, loss = 1.56945871\n",
      "Iteration 308, loss = 1.56679949\n",
      "Iteration 309, loss = 1.56421651\n",
      "Iteration 310, loss = 1.56158407\n",
      "Iteration 311, loss = 1.55903028\n",
      "Iteration 312, loss = 1.55642972\n",
      "Iteration 313, loss = 1.55388201\n",
      "Iteration 314, loss = 1.55136106\n",
      "Iteration 315, loss = 1.54878707\n",
      "Iteration 316, loss = 1.54619090\n",
      "Iteration 317, loss = 1.54361702\n",
      "Iteration 318, loss = 1.54112777\n",
      "Iteration 319, loss = 1.53861764\n",
      "Iteration 320, loss = 1.53607677\n",
      "Iteration 321, loss = 1.53361409\n",
      "Iteration 322, loss = 1.53109568\n",
      "Iteration 323, loss = 1.52857025\n",
      "Iteration 324, loss = 1.52611918\n",
      "Iteration 325, loss = 1.52363667\n",
      "Iteration 326, loss = 1.52118461\n",
      "Iteration 327, loss = 1.51867627\n",
      "Iteration 328, loss = 1.51628524\n",
      "Iteration 329, loss = 1.51377806\n",
      "Iteration 330, loss = 1.51141006\n",
      "Iteration 331, loss = 1.50898600\n",
      "Iteration 332, loss = 1.50652571\n",
      "Iteration 333, loss = 1.50407014\n",
      "Iteration 334, loss = 1.50171332\n",
      "Iteration 335, loss = 1.49931748\n",
      "Iteration 336, loss = 1.49693748\n",
      "Iteration 337, loss = 1.49454950\n",
      "Iteration 338, loss = 1.49220862\n",
      "Iteration 339, loss = 1.48986435\n",
      "Iteration 340, loss = 1.48747441\n",
      "Iteration 341, loss = 1.48510370\n",
      "Iteration 342, loss = 1.48280093\n",
      "Iteration 343, loss = 1.48045344\n",
      "Iteration 344, loss = 1.47810571\n",
      "Iteration 345, loss = 1.47581089\n",
      "Iteration 346, loss = 1.47347251\n",
      "Iteration 347, loss = 1.47126006\n",
      "Iteration 348, loss = 1.46891657\n",
      "Iteration 349, loss = 1.46662354\n",
      "Iteration 350, loss = 1.46432921\n",
      "Iteration 351, loss = 1.46210359\n",
      "Iteration 352, loss = 1.45979607\n",
      "Iteration 353, loss = 1.45752458\n",
      "Iteration 354, loss = 1.45530780\n",
      "Iteration 355, loss = 1.45305327\n",
      "Iteration 356, loss = 1.45080241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 357, loss = 1.44859964\n",
      "Iteration 358, loss = 1.44633511\n",
      "Iteration 359, loss = 1.44416446\n",
      "Iteration 360, loss = 1.44197888\n",
      "Iteration 361, loss = 1.43979706\n",
      "Iteration 362, loss = 1.43752859\n",
      "Iteration 363, loss = 1.43536198\n",
      "Iteration 364, loss = 1.43316319\n",
      "Iteration 365, loss = 1.43104277\n",
      "Iteration 366, loss = 1.42881290\n",
      "Iteration 367, loss = 1.42667629\n",
      "Iteration 368, loss = 1.42455664\n",
      "Iteration 369, loss = 1.42239869\n",
      "Iteration 370, loss = 1.42026365\n",
      "Iteration 371, loss = 1.41816598\n",
      "Iteration 372, loss = 1.41602118\n",
      "Iteration 373, loss = 1.41392596\n",
      "Iteration 374, loss = 1.41177423\n",
      "Iteration 375, loss = 1.40967604\n",
      "Iteration 376, loss = 1.40759647\n",
      "Iteration 377, loss = 1.40554628\n",
      "Iteration 378, loss = 1.40346016\n",
      "Iteration 379, loss = 1.40133793\n",
      "Iteration 380, loss = 1.39931798\n",
      "Iteration 381, loss = 1.39725402\n",
      "Iteration 382, loss = 1.39521875\n",
      "Iteration 383, loss = 1.39318338\n",
      "Iteration 384, loss = 1.39108893\n",
      "Iteration 385, loss = 1.38905098\n",
      "Iteration 386, loss = 1.38708769\n",
      "Iteration 387, loss = 1.38500997\n",
      "Iteration 388, loss = 1.38300843\n",
      "Iteration 389, loss = 1.38103855\n",
      "Iteration 390, loss = 1.37904728\n",
      "Iteration 391, loss = 1.37703867\n",
      "Iteration 392, loss = 1.37508539\n",
      "Iteration 393, loss = 1.37313599\n",
      "Iteration 394, loss = 1.37110423\n",
      "Iteration 395, loss = 1.36915715\n",
      "Iteration 396, loss = 1.36719789\n",
      "Iteration 397, loss = 1.36527541\n",
      "Iteration 398, loss = 1.36331065\n",
      "Iteration 399, loss = 1.36142442\n",
      "Iteration 400, loss = 1.35947252\n",
      "Iteration 1, loss = 2.23042584\n",
      "Iteration 2, loss = 0.88372608\n",
      "Iteration 3, loss = 0.66204851\n",
      "Iteration 4, loss = 0.57402244\n",
      "Iteration 5, loss = 0.51638319\n",
      "Iteration 6, loss = 0.45554398\n",
      "Iteration 7, loss = 0.40514828\n",
      "Iteration 8, loss = 0.38255465\n",
      "Iteration 9, loss = 0.34070416\n",
      "Iteration 10, loss = 0.30918189\n",
      "Iteration 11, loss = 0.29556710\n",
      "Iteration 12, loss = 0.30330014\n",
      "Iteration 13, loss = 0.26419980\n",
      "Iteration 14, loss = 0.26053636\n",
      "Iteration 15, loss = 0.22162149\n",
      "Iteration 16, loss = 0.22560174\n",
      "Iteration 17, loss = 0.24825369\n",
      "Iteration 18, loss = 0.23305076\n",
      "Iteration 19, loss = 0.22247073\n",
      "Iteration 20, loss = 0.23074050\n",
      "Iteration 21, loss = 0.22828078\n",
      "Iteration 22, loss = 0.26470800\n",
      "Iteration 23, loss = 0.25270836\n",
      "Iteration 24, loss = 0.21533199\n",
      "Iteration 25, loss = 0.22200625\n",
      "Iteration 26, loss = 0.24873441\n",
      "Iteration 27, loss = 0.23272024\n",
      "Iteration 28, loss = 0.18906819\n",
      "Iteration 29, loss = 0.23395834\n",
      "Iteration 30, loss = 0.23179438\n",
      "Iteration 31, loss = 0.22541420\n",
      "Iteration 32, loss = 0.22769749\n",
      "Iteration 33, loss = 0.26170067\n",
      "Iteration 34, loss = 0.25720157\n",
      "Iteration 35, loss = 0.34977278\n",
      "Iteration 36, loss = 0.30138950\n",
      "Iteration 37, loss = 0.32140292\n",
      "Iteration 38, loss = 0.32347175\n",
      "Iteration 39, loss = 0.31242108\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.30577716\n",
      "Iteration 2, loss = 0.88493523\n",
      "Iteration 3, loss = 0.68230034\n",
      "Iteration 4, loss = 0.57104345\n",
      "Iteration 5, loss = 0.51034515\n",
      "Iteration 6, loss = 0.47759888\n",
      "Iteration 7, loss = 0.43701493\n",
      "Iteration 8, loss = 0.42624694\n",
      "Iteration 9, loss = 0.37407677\n",
      "Iteration 10, loss = 0.36522651\n",
      "Iteration 11, loss = 0.33387197\n",
      "Iteration 12, loss = 0.31042735\n",
      "Iteration 13, loss = 0.32410102\n",
      "Iteration 14, loss = 0.30844951\n",
      "Iteration 15, loss = 0.30385134\n",
      "Iteration 16, loss = 0.27507434\n",
      "Iteration 17, loss = 0.31235949\n",
      "Iteration 18, loss = 0.27223380\n",
      "Iteration 19, loss = 0.24205960\n",
      "Iteration 20, loss = 0.25557246\n",
      "Iteration 21, loss = 0.24260725\n",
      "Iteration 22, loss = 0.27431866\n",
      "Iteration 23, loss = 0.28832529\n",
      "Iteration 24, loss = 0.31287534\n",
      "Iteration 25, loss = 0.27441986\n",
      "Iteration 26, loss = 0.26073866\n",
      "Iteration 27, loss = 0.25251219\n",
      "Iteration 28, loss = 0.25325157\n",
      "Iteration 29, loss = 0.21460875\n",
      "Iteration 30, loss = 0.30020734\n",
      "Iteration 31, loss = 0.30165979\n",
      "Iteration 32, loss = 0.25685064\n",
      "Iteration 33, loss = 0.32740676\n",
      "Iteration 34, loss = 0.29886423\n",
      "Iteration 35, loss = 0.27861760\n",
      "Iteration 36, loss = 0.28762199\n",
      "Iteration 37, loss = 0.25664496\n",
      "Iteration 38, loss = 0.23351322\n",
      "Iteration 39, loss = 0.32054303\n",
      "Iteration 40, loss = 0.24429866\n",
      "Training loss did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.60660736\n",
      "Validation score: 0.675385\n",
      "Iteration 2, loss = 0.97035042\n",
      "Validation score: 0.763846\n",
      "Iteration 3, loss = 0.75322812\n",
      "Validation score: 0.783077\n",
      "Iteration 4, loss = 0.63240819\n",
      "Validation score: 0.823077\n",
      "Iteration 5, loss = 0.56196412\n",
      "Validation score: 0.826923\n",
      "Iteration 6, loss = 0.50380731\n",
      "Validation score: 0.837692\n",
      "Iteration 7, loss = 0.46327021\n",
      "Validation score: 0.830000\n",
      "Iteration 8, loss = 0.44726225\n",
      "Validation score: 0.846154\n",
      "Iteration 9, loss = 0.41427685\n",
      "Validation score: 0.856154\n",
      "Iteration 10, loss = 0.37607332\n",
      "Validation score: 0.839231\n",
      "Iteration 11, loss = 0.35838220\n",
      "Validation score: 0.848462\n",
      "Iteration 12, loss = 0.32285798\n",
      "Validation score: 0.843077\n",
      "Iteration 13, loss = 0.31090743\n",
      "Validation score: 0.848462\n",
      "Iteration 14, loss = 0.30037075\n",
      "Validation score: 0.863077\n",
      "Iteration 15, loss = 0.27898856\n",
      "Validation score: 0.853846\n",
      "Iteration 16, loss = 0.24784369\n",
      "Validation score: 0.853846\n",
      "Iteration 17, loss = 0.27667840\n",
      "Validation score: 0.856154\n",
      "Iteration 18, loss = 0.26514338\n",
      "Validation score: 0.862308\n",
      "Iteration 19, loss = 0.26906000\n",
      "Validation score: 0.865385\n",
      "Iteration 20, loss = 0.24100134\n",
      "Validation score: 0.853846\n",
      "Iteration 21, loss = 0.21657095\n",
      "Validation score: 0.856154\n",
      "Iteration 22, loss = 0.21291636\n",
      "Validation score: 0.864615\n",
      "Iteration 23, loss = 0.22860944\n",
      "Validation score: 0.847692\n",
      "Iteration 24, loss = 0.23060646\n",
      "Validation score: 0.840769\n",
      "Iteration 25, loss = 0.22436762\n",
      "Validation score: 0.857692\n",
      "Iteration 26, loss = 0.19417502\n",
      "Validation score: 0.842308\n",
      "Iteration 27, loss = 0.26129266\n",
      "Validation score: 0.859231\n",
      "Iteration 28, loss = 0.26727856\n",
      "Validation score: 0.860769\n",
      "Iteration 29, loss = 0.26105713\n",
      "Validation score: 0.867692\n",
      "Iteration 30, loss = 0.24693619\n",
      "Validation score: 0.846923\n",
      "Iteration 31, loss = 0.24515844\n",
      "Validation score: 0.843077\n",
      "Iteration 32, loss = 0.24829510\n",
      "Validation score: 0.866923\n",
      "Iteration 33, loss = 0.24539474\n",
      "Validation score: 0.861538\n",
      "Iteration 34, loss = 0.22446669\n",
      "Validation score: 0.858462\n",
      "Iteration 35, loss = 0.22647922\n",
      "Validation score: 0.843077\n",
      "Iteration 36, loss = 0.23079137\n",
      "Validation score: 0.844615\n",
      "Iteration 37, loss = 0.24055539\n",
      "Validation score: 0.860769\n",
      "Iteration 38, loss = 0.22907580\n",
      "Validation score: 0.856923\n",
      "Iteration 39, loss = 0.27410627\n",
      "Validation score: 0.840000\n",
      "Iteration 40, loss = 0.27257941\n",
      "Validation score: 0.852308\n",
      "Validation score did not improve more than tol=0.000001 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.33485872\n",
      "Iteration 2, loss = 1.38581151\n",
      "Iteration 3, loss = 0.92661670\n",
      "Iteration 4, loss = 0.87044315\n",
      "Iteration 5, loss = 0.84219337\n",
      "Iteration 6, loss = 0.82354338\n",
      "Iteration 7, loss = 0.81002740\n",
      "Iteration 8, loss = 0.79950649\n",
      "Iteration 9, loss = 0.79092297\n",
      "Iteration 10, loss = 0.78366636\n",
      "Iteration 11, loss = 0.77753547\n",
      "Iteration 12, loss = 0.77216710\n",
      "Iteration 13, loss = 0.76729352\n",
      "Iteration 14, loss = 0.76288439\n",
      "Iteration 15, loss = 0.75902245\n",
      "Iteration 16, loss = 0.75534855\n",
      "Iteration 17, loss = 0.75194505\n",
      "Iteration 18, loss = 0.74892971\n",
      "Iteration 19, loss = 0.74590031\n",
      "Iteration 20, loss = 0.74316623\n",
      "Iteration 21, loss = 0.74064539\n",
      "Iteration 22, loss = 0.73823135\n",
      "Iteration 23, loss = 0.73589687\n",
      "Iteration 24, loss = 0.73373323\n",
      "Iteration 25, loss = 0.73162291\n",
      "Iteration 26, loss = 0.72966653\n",
      "Iteration 27, loss = 0.72777784\n",
      "Iteration 28, loss = 0.72597999\n",
      "Iteration 29, loss = 0.72423655\n",
      "Iteration 30, loss = 0.72257568\n",
      "Iteration 31, loss = 0.72099497\n",
      "Iteration 32, loss = 0.71942468\n",
      "Iteration 33, loss = 0.71790397\n",
      "Iteration 34, loss = 0.71647811\n",
      "Iteration 35, loss = 0.71506528\n",
      "Iteration 36, loss = 0.71371316\n",
      "Iteration 37, loss = 0.71232863\n",
      "Iteration 38, loss = 0.71105297\n",
      "Iteration 39, loss = 0.70979928\n",
      "Iteration 40, loss = 0.70863660\n",
      "Iteration 41, loss = 0.70745458\n",
      "Iteration 42, loss = 0.70627093\n",
      "Iteration 43, loss = 0.70517387\n",
      "Iteration 44, loss = 0.70402370\n",
      "Iteration 45, loss = 0.70297768\n",
      "Iteration 46, loss = 0.70192635\n",
      "Iteration 47, loss = 0.70087189\n",
      "Iteration 48, loss = 0.69987490\n",
      "Iteration 49, loss = 0.69886684\n",
      "Iteration 50, loss = 0.69788660\n",
      "Iteration 51, loss = 0.69694028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 52, loss = 0.69602286\n",
      "Iteration 53, loss = 0.69511840\n",
      "Iteration 54, loss = 0.69422098\n",
      "Iteration 55, loss = 0.69333600\n",
      "Iteration 56, loss = 0.69250567\n",
      "Iteration 57, loss = 0.69160454\n",
      "Iteration 58, loss = 0.69080745\n",
      "Iteration 59, loss = 0.68998943\n",
      "Iteration 60, loss = 0.68914975\n",
      "Iteration 61, loss = 0.68837447\n",
      "Iteration 62, loss = 0.68764855\n",
      "Iteration 63, loss = 0.68681230\n",
      "Iteration 64, loss = 0.68602941\n",
      "Iteration 65, loss = 0.68528494\n",
      "Iteration 66, loss = 0.68457145\n",
      "Iteration 67, loss = 0.68382579\n",
      "Iteration 68, loss = 0.68313389\n",
      "Iteration 69, loss = 0.68239635\n",
      "Iteration 70, loss = 0.68172685\n",
      "Iteration 71, loss = 0.68100016\n",
      "Iteration 72, loss = 0.68035316\n",
      "Iteration 73, loss = 0.67965857\n",
      "Iteration 74, loss = 0.67901687\n",
      "Iteration 75, loss = 0.67836356\n",
      "Iteration 76, loss = 0.67771402\n",
      "Iteration 77, loss = 0.67704859\n",
      "Iteration 78, loss = 0.67645571\n",
      "Iteration 79, loss = 0.67581666\n",
      "Iteration 80, loss = 0.67522065\n",
      "Iteration 81, loss = 0.67458090\n",
      "Iteration 82, loss = 0.67396804\n",
      "Iteration 83, loss = 0.67338367\n",
      "Iteration 84, loss = 0.67277694\n",
      "Iteration 85, loss = 0.67221759\n",
      "Iteration 86, loss = 0.67162840\n",
      "Iteration 87, loss = 0.67106556\n",
      "Iteration 88, loss = 0.67048350\n",
      "Iteration 89, loss = 0.66992588\n",
      "Iteration 90, loss = 0.66935974\n",
      "Iteration 91, loss = 0.66878414\n",
      "Iteration 92, loss = 0.66827082\n",
      "Iteration 93, loss = 0.66771027\n",
      "Iteration 94, loss = 0.66719084\n",
      "Iteration 95, loss = 0.66664182\n",
      "Iteration 96, loss = 0.66611762\n",
      "Iteration 97, loss = 0.66563464\n",
      "Iteration 98, loss = 0.66507551\n",
      "Iteration 99, loss = 0.66456668\n",
      "Iteration 100, loss = 0.66403488\n",
      "Iteration 101, loss = 0.66352799\n",
      "Iteration 102, loss = 0.66306366\n",
      "Iteration 103, loss = 0.66256108\n",
      "Iteration 104, loss = 0.66204945\n",
      "Iteration 105, loss = 0.66156977\n",
      "Iteration 106, loss = 0.66107769\n",
      "Iteration 107, loss = 0.66059277\n",
      "Iteration 108, loss = 0.66010648\n",
      "Iteration 109, loss = 0.65961229\n",
      "Iteration 110, loss = 0.65913660\n",
      "Iteration 111, loss = 0.65870150\n",
      "Iteration 112, loss = 0.65824964\n",
      "Iteration 113, loss = 0.65778731\n",
      "Iteration 114, loss = 0.65733050\n",
      "Iteration 115, loss = 0.65687031\n",
      "Iteration 116, loss = 0.65641588\n",
      "Iteration 117, loss = 0.65597714\n",
      "Iteration 118, loss = 0.65552679\n",
      "Iteration 119, loss = 0.65509857\n",
      "Iteration 120, loss = 0.65464986\n",
      "Iteration 121, loss = 0.65421641\n",
      "Iteration 122, loss = 0.65377180\n",
      "Iteration 123, loss = 0.65336420\n",
      "Iteration 124, loss = 0.65294244\n",
      "Iteration 125, loss = 0.65250092\n",
      "Iteration 126, loss = 0.65209813\n",
      "Iteration 127, loss = 0.65169941\n",
      "Iteration 128, loss = 0.65127000\n",
      "Iteration 129, loss = 0.65086846\n",
      "Iteration 130, loss = 0.65047690\n",
      "Iteration 131, loss = 0.65005106\n",
      "Iteration 132, loss = 0.64967064\n",
      "Iteration 133, loss = 0.64927161\n",
      "Iteration 134, loss = 0.64887242\n",
      "Iteration 135, loss = 0.64846794\n",
      "Iteration 136, loss = 0.64809325\n",
      "Iteration 137, loss = 0.64769522\n",
      "Iteration 138, loss = 0.64730436\n",
      "Iteration 139, loss = 0.64692020\n",
      "Iteration 140, loss = 0.64657130\n",
      "Iteration 141, loss = 0.64620160\n",
      "Iteration 142, loss = 0.64581618\n",
      "Iteration 143, loss = 0.64544577\n",
      "Iteration 144, loss = 0.64505121\n",
      "Iteration 145, loss = 0.64473766\n",
      "Iteration 146, loss = 0.64432882\n",
      "Iteration 147, loss = 0.64398691\n",
      "Iteration 148, loss = 0.64362382\n",
      "Iteration 149, loss = 0.64327161\n",
      "Iteration 150, loss = 0.64292136\n",
      "Iteration 151, loss = 0.64255214\n",
      "Iteration 152, loss = 0.64222055\n",
      "Iteration 153, loss = 0.64185027\n",
      "Iteration 154, loss = 0.64147214\n",
      "Iteration 155, loss = 0.64114508\n",
      "Iteration 156, loss = 0.64080231\n",
      "Iteration 157, loss = 0.64046962\n",
      "Iteration 158, loss = 0.64012352\n",
      "Iteration 159, loss = 0.63978152\n",
      "Iteration 160, loss = 0.63944130\n",
      "Iteration 161, loss = 0.63912046\n",
      "Iteration 162, loss = 0.63878022\n",
      "Iteration 163, loss = 0.63846166\n",
      "Iteration 164, loss = 0.63811173\n",
      "Iteration 165, loss = 0.63781560\n",
      "Iteration 166, loss = 0.63746870\n",
      "Iteration 167, loss = 0.63712404\n",
      "Iteration 168, loss = 0.63680477\n",
      "Iteration 169, loss = 0.63651233\n",
      "Iteration 170, loss = 0.63616167\n",
      "Iteration 171, loss = 0.63584388\n",
      "Iteration 172, loss = 0.63554465\n",
      "Iteration 173, loss = 0.63524300\n",
      "Iteration 174, loss = 0.63490933\n",
      "Iteration 175, loss = 0.63459440\n",
      "Iteration 176, loss = 0.63430064\n",
      "Iteration 177, loss = 0.63397754\n",
      "Iteration 178, loss = 0.63366217\n",
      "Iteration 179, loss = 0.63341022\n",
      "Iteration 180, loss = 0.63307882\n",
      "Iteration 181, loss = 0.63274800\n",
      "Iteration 182, loss = 0.63243909\n",
      "Iteration 183, loss = 0.63214562\n",
      "Iteration 184, loss = 0.63182926\n",
      "Iteration 185, loss = 0.63157834\n",
      "Iteration 186, loss = 0.63126056\n",
      "Iteration 187, loss = 0.63095560\n",
      "Iteration 188, loss = 0.63068560\n",
      "Iteration 189, loss = 0.63036797\n",
      "Iteration 190, loss = 0.63006497\n",
      "Iteration 191, loss = 0.62979189\n",
      "Iteration 192, loss = 0.62948448\n",
      "Iteration 193, loss = 0.62919892\n",
      "Iteration 194, loss = 0.62891639\n",
      "Iteration 195, loss = 0.62864000\n",
      "Iteration 196, loss = 0.62833513\n",
      "Iteration 197, loss = 0.62806090\n",
      "Iteration 198, loss = 0.62777234\n",
      "Iteration 199, loss = 0.62751159\n",
      "Iteration 200, loss = 0.62723774\n",
      "Iteration 201, loss = 0.62694730\n",
      "Iteration 202, loss = 0.62668447\n",
      "Iteration 203, loss = 0.62641393\n",
      "Iteration 204, loss = 0.62612380\n",
      "Iteration 205, loss = 0.62584955\n",
      "Iteration 206, loss = 0.62556504\n",
      "Iteration 207, loss = 0.62529295\n",
      "Iteration 208, loss = 0.62503191\n",
      "Iteration 209, loss = 0.62477726\n",
      "Iteration 210, loss = 0.62450500\n",
      "Iteration 211, loss = 0.62424998\n",
      "Iteration 212, loss = 0.62397968\n",
      "Iteration 213, loss = 0.62373037\n",
      "Iteration 214, loss = 0.62344193\n",
      "Iteration 215, loss = 0.62316504\n",
      "Iteration 216, loss = 0.62293580\n",
      "Iteration 217, loss = 0.62266231\n",
      "Iteration 218, loss = 0.62242350\n",
      "Iteration 219, loss = 0.62212927\n",
      "Iteration 220, loss = 0.62188441\n",
      "Iteration 221, loss = 0.62165537\n",
      "Iteration 222, loss = 0.62137196\n",
      "Iteration 223, loss = 0.62113908\n",
      "Iteration 224, loss = 0.62088167\n",
      "Iteration 225, loss = 0.62060707\n",
      "Iteration 226, loss = 0.62036529\n",
      "Iteration 227, loss = 0.62011870\n",
      "Iteration 228, loss = 0.61986460\n",
      "Iteration 229, loss = 0.61962284\n",
      "Iteration 230, loss = 0.61935831\n",
      "Iteration 231, loss = 0.61911105\n",
      "Iteration 232, loss = 0.61886943\n",
      "Iteration 233, loss = 0.61861601\n",
      "Iteration 234, loss = 0.61840177\n",
      "Iteration 235, loss = 0.61813793\n",
      "Iteration 236, loss = 0.61789837\n",
      "Iteration 237, loss = 0.61765836\n",
      "Iteration 238, loss = 0.61741885\n",
      "Iteration 239, loss = 0.61716879\n",
      "Iteration 240, loss = 0.61693957\n",
      "Iteration 241, loss = 0.61669844\n",
      "Iteration 242, loss = 0.61647038\n",
      "Iteration 243, loss = 0.61625207\n",
      "Iteration 244, loss = 0.61600073\n",
      "Iteration 245, loss = 0.61575288\n",
      "Iteration 246, loss = 0.61553721\n",
      "Iteration 247, loss = 0.61528182\n",
      "Iteration 248, loss = 0.61505870\n",
      "Iteration 249, loss = 0.61482639\n",
      "Iteration 250, loss = 0.61459031\n",
      "Iteration 251, loss = 0.61436476\n",
      "Iteration 252, loss = 0.61413543\n",
      "Iteration 253, loss = 0.61391219\n",
      "Iteration 254, loss = 0.61367780\n",
      "Iteration 255, loss = 0.61345230\n",
      "Iteration 256, loss = 0.61322693\n",
      "Iteration 257, loss = 0.61298394\n",
      "Iteration 258, loss = 0.61279006\n",
      "Iteration 259, loss = 0.61254469\n",
      "Iteration 260, loss = 0.61232178\n",
      "Iteration 261, loss = 0.61209384\n",
      "Iteration 262, loss = 0.61187953\n",
      "Iteration 263, loss = 0.61164624\n",
      "Iteration 264, loss = 0.61143014\n",
      "Iteration 265, loss = 0.61120325\n",
      "Iteration 266, loss = 0.61097218\n",
      "Iteration 267, loss = 0.61076345\n",
      "Iteration 268, loss = 0.61054490\n",
      "Iteration 269, loss = 0.61034618\n",
      "Iteration 270, loss = 0.61009675\n",
      "Iteration 271, loss = 0.60989987\n",
      "Iteration 272, loss = 0.60968962\n",
      "Iteration 273, loss = 0.60945064\n",
      "Iteration 274, loss = 0.60925398\n",
      "Iteration 275, loss = 0.60901951\n",
      "Iteration 276, loss = 0.60882181\n",
      "Iteration 277, loss = 0.60861612\n",
      "Iteration 278, loss = 0.60840324\n",
      "Iteration 279, loss = 0.60817046\n",
      "Iteration 280, loss = 0.60797490\n",
      "Iteration 281, loss = 0.60775678\n",
      "Iteration 282, loss = 0.60754670\n",
      "Iteration 283, loss = 0.60734458\n",
      "Iteration 284, loss = 0.60713114\n",
      "Iteration 285, loss = 0.60692561\n",
      "Iteration 286, loss = 0.60670866\n",
      "Iteration 287, loss = 0.60648553\n",
      "Iteration 288, loss = 0.60628921\n",
      "Iteration 289, loss = 0.60610127\n",
      "Iteration 290, loss = 0.60586531\n",
      "Iteration 291, loss = 0.60567577\n",
      "Iteration 292, loss = 0.60546150\n",
      "Iteration 293, loss = 0.60528494\n",
      "Iteration 294, loss = 0.60505927\n",
      "Iteration 295, loss = 0.60486330\n",
      "Iteration 296, loss = 0.60466440\n",
      "Iteration 297, loss = 0.60446273\n",
      "Iteration 298, loss = 0.60425319\n",
      "Iteration 299, loss = 0.60405431\n",
      "Iteration 300, loss = 0.60383700\n",
      "Iteration 301, loss = 0.60366040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 302, loss = 0.60344632\n",
      "Iteration 303, loss = 0.60325882\n",
      "Iteration 304, loss = 0.60306040\n",
      "Iteration 305, loss = 0.60285005\n",
      "Iteration 306, loss = 0.60268034\n",
      "Iteration 307, loss = 0.60247664\n",
      "Iteration 308, loss = 0.60227089\n",
      "Iteration 309, loss = 0.60206295\n",
      "Iteration 310, loss = 0.60188321\n",
      "Iteration 311, loss = 0.60168727\n",
      "Iteration 312, loss = 0.60148887\n",
      "Iteration 313, loss = 0.60129474\n",
      "Iteration 314, loss = 0.60111119\n",
      "Iteration 315, loss = 0.60091668\n",
      "Iteration 316, loss = 0.60071898\n",
      "Iteration 317, loss = 0.60051741\n",
      "Iteration 318, loss = 0.60034132\n",
      "Iteration 319, loss = 0.60013144\n",
      "Iteration 320, loss = 0.59995265\n",
      "Iteration 321, loss = 0.59976942\n",
      "Iteration 322, loss = 0.59957115\n",
      "Iteration 323, loss = 0.59939048\n",
      "Iteration 324, loss = 0.59919450\n",
      "Iteration 325, loss = 0.59901060\n",
      "Iteration 326, loss = 0.59881813\n",
      "Iteration 327, loss = 0.59863157\n",
      "Iteration 328, loss = 0.59846395\n",
      "Iteration 329, loss = 0.59826171\n",
      "Iteration 330, loss = 0.59807759\n",
      "Iteration 331, loss = 0.59787329\n",
      "Iteration 332, loss = 0.59769745\n",
      "Iteration 333, loss = 0.59751874\n",
      "Iteration 334, loss = 0.59734165\n",
      "Iteration 335, loss = 0.59714900\n",
      "Iteration 336, loss = 0.59697231\n",
      "Iteration 337, loss = 0.59678855\n",
      "Iteration 338, loss = 0.59659332\n",
      "Iteration 339, loss = 0.59643777\n",
      "Iteration 340, loss = 0.59625365\n",
      "Iteration 341, loss = 0.59605836\n",
      "Iteration 342, loss = 0.59588702\n",
      "Iteration 343, loss = 0.59571052\n",
      "Iteration 344, loss = 0.59551029\n",
      "Iteration 345, loss = 0.59534121\n",
      "Iteration 346, loss = 0.59517221\n",
      "Iteration 347, loss = 0.59498255\n",
      "Iteration 348, loss = 0.59481768\n",
      "Iteration 349, loss = 0.59462769\n",
      "Iteration 350, loss = 0.59444984\n",
      "Iteration 351, loss = 0.59428237\n",
      "Iteration 352, loss = 0.59409547\n",
      "Iteration 353, loss = 0.59392688\n",
      "Iteration 354, loss = 0.59374236\n",
      "Iteration 355, loss = 0.59357851\n",
      "Iteration 356, loss = 0.59339642\n",
      "Iteration 357, loss = 0.59322087\n",
      "Iteration 358, loss = 0.59305635\n",
      "Iteration 359, loss = 0.59286819\n",
      "Iteration 360, loss = 0.59270694\n",
      "Iteration 361, loss = 0.59253347\n",
      "Iteration 362, loss = 0.59236576\n",
      "Iteration 363, loss = 0.59218566\n",
      "Iteration 364, loss = 0.59202241\n",
      "Iteration 365, loss = 0.59183867\n",
      "Iteration 366, loss = 0.59167436\n",
      "Iteration 367, loss = 0.59150594\n",
      "Iteration 368, loss = 0.59133940\n",
      "Iteration 369, loss = 0.59116398\n",
      "Iteration 370, loss = 0.59098620\n",
      "Iteration 371, loss = 0.59081994\n",
      "Iteration 372, loss = 0.59065707\n",
      "Iteration 373, loss = 0.59048633\n",
      "Iteration 374, loss = 0.59031221\n",
      "Iteration 375, loss = 0.59015122\n",
      "Iteration 376, loss = 0.58999746\n",
      "Iteration 377, loss = 0.58982173\n",
      "Iteration 378, loss = 0.58966207\n",
      "Iteration 379, loss = 0.58948543\n",
      "Iteration 380, loss = 0.58933379\n",
      "Iteration 381, loss = 0.58916500\n",
      "Iteration 382, loss = 0.58899589\n",
      "Iteration 383, loss = 0.58882747\n",
      "Iteration 384, loss = 0.58867839\n",
      "Iteration 385, loss = 0.58851379\n",
      "Iteration 386, loss = 0.58833932\n",
      "Iteration 387, loss = 0.58816629\n",
      "Iteration 388, loss = 0.58800921\n",
      "Iteration 389, loss = 0.58785107\n",
      "Iteration 390, loss = 0.58766816\n",
      "Iteration 391, loss = 0.58752103\n",
      "Iteration 392, loss = 0.58735592\n",
      "Iteration 393, loss = 0.58720143\n",
      "Iteration 394, loss = 0.58705180\n",
      "Iteration 395, loss = 0.58687475\n",
      "Iteration 396, loss = 0.58670662\n",
      "Iteration 397, loss = 0.58654562\n",
      "Iteration 398, loss = 0.58639490\n",
      "Iteration 399, loss = 0.58623487\n",
      "Iteration 400, loss = 0.58608920\n",
      "Iteration 1, loss = 2.30039779\n",
      "Iteration 2, loss = 0.92562844\n",
      "Iteration 3, loss = 0.75282928\n",
      "Iteration 4, loss = 0.71386216\n",
      "Iteration 5, loss = 0.68971877\n",
      "Iteration 6, loss = 0.67082343\n",
      "Iteration 7, loss = 0.65652685\n",
      "Iteration 8, loss = 0.64387711\n",
      "Iteration 9, loss = 0.63246044\n",
      "Iteration 10, loss = 0.62213931\n",
      "Iteration 11, loss = 0.61278533\n",
      "Iteration 12, loss = 0.60442292\n",
      "Iteration 13, loss = 0.59603662\n",
      "Iteration 14, loss = 0.58861279\n",
      "Iteration 15, loss = 0.58190098\n",
      "Iteration 16, loss = 0.57545457\n",
      "Iteration 17, loss = 0.56909056\n",
      "Iteration 18, loss = 0.56280157\n",
      "Iteration 19, loss = 0.55738946\n",
      "Iteration 20, loss = 0.55170104\n",
      "Iteration 21, loss = 0.54643041\n",
      "Iteration 22, loss = 0.54158426\n",
      "Iteration 23, loss = 0.53660389\n",
      "Iteration 24, loss = 0.53172759\n",
      "Iteration 25, loss = 0.52715405\n",
      "Iteration 26, loss = 0.52280838\n",
      "Iteration 27, loss = 0.51816783\n",
      "Iteration 28, loss = 0.51445464\n",
      "Iteration 29, loss = 0.51005880\n",
      "Iteration 30, loss = 0.50644978\n",
      "Iteration 31, loss = 0.50302468\n",
      "Iteration 32, loss = 0.49870208\n",
      "Iteration 33, loss = 0.49505891\n",
      "Iteration 34, loss = 0.49153267\n",
      "Iteration 35, loss = 0.48870509\n",
      "Iteration 36, loss = 0.48482583\n",
      "Iteration 37, loss = 0.48141219\n",
      "Iteration 38, loss = 0.47834593\n",
      "Iteration 39, loss = 0.47501400\n",
      "Iteration 40, loss = 0.47187029\n",
      "Iteration 41, loss = 0.46882990\n",
      "Iteration 42, loss = 0.46580140\n",
      "Iteration 43, loss = 0.46300795\n",
      "Iteration 44, loss = 0.46030578\n",
      "Iteration 45, loss = 0.45733671\n",
      "Iteration 46, loss = 0.45452578\n",
      "Iteration 47, loss = 0.45230567\n",
      "Iteration 48, loss = 0.44958584\n",
      "Iteration 49, loss = 0.44642765\n",
      "Iteration 50, loss = 0.44429179\n",
      "Iteration 51, loss = 0.44158654\n",
      "Iteration 52, loss = 0.43890602\n",
      "Iteration 53, loss = 0.43675385\n",
      "Iteration 54, loss = 0.43437664\n",
      "Iteration 55, loss = 0.43184471\n",
      "Iteration 56, loss = 0.42966033\n",
      "Iteration 57, loss = 0.42728882\n",
      "Iteration 58, loss = 0.42516860\n",
      "Iteration 59, loss = 0.42260830\n",
      "Iteration 60, loss = 0.42068775\n",
      "Iteration 61, loss = 0.41854112\n",
      "Iteration 62, loss = 0.41617587\n",
      "Iteration 63, loss = 0.41419334\n",
      "Iteration 64, loss = 0.41230877\n",
      "Iteration 65, loss = 0.41019781\n",
      "Iteration 66, loss = 0.40784434\n",
      "Iteration 67, loss = 0.40594569\n",
      "Iteration 68, loss = 0.40418638\n",
      "Iteration 69, loss = 0.40225471\n",
      "Iteration 70, loss = 0.40026452\n",
      "Iteration 71, loss = 0.39822153\n",
      "Iteration 72, loss = 0.39640573\n",
      "Iteration 73, loss = 0.39443047\n",
      "Iteration 74, loss = 0.39267040\n",
      "Iteration 75, loss = 0.39077794\n",
      "Iteration 76, loss = 0.38884170\n",
      "Iteration 77, loss = 0.38730018\n",
      "Iteration 78, loss = 0.38547469\n",
      "Iteration 79, loss = 0.38369519\n",
      "Iteration 80, loss = 0.38201785\n",
      "Iteration 81, loss = 0.38045438\n",
      "Iteration 82, loss = 0.37884658\n",
      "Iteration 83, loss = 0.37682936\n",
      "Iteration 84, loss = 0.37525082\n",
      "Iteration 85, loss = 0.37398679\n",
      "Iteration 86, loss = 0.37189262\n",
      "Iteration 87, loss = 0.37035655\n",
      "Iteration 88, loss = 0.36899489\n",
      "Iteration 89, loss = 0.36731187\n",
      "Iteration 90, loss = 0.36572606\n",
      "Iteration 91, loss = 0.36387949\n",
      "Iteration 92, loss = 0.36294345\n",
      "Iteration 93, loss = 0.36127935\n",
      "Iteration 94, loss = 0.35952318\n",
      "Iteration 95, loss = 0.35797930\n",
      "Iteration 96, loss = 0.35661003\n",
      "Iteration 97, loss = 0.35523727\n",
      "Iteration 98, loss = 0.35385345\n",
      "Iteration 99, loss = 0.35206329\n",
      "Iteration 100, loss = 0.35089305\n",
      "Iteration 101, loss = 0.34943040\n",
      "Iteration 102, loss = 0.34794930\n",
      "Iteration 103, loss = 0.34684286\n",
      "Iteration 104, loss = 0.34524509\n",
      "Iteration 105, loss = 0.34405187\n",
      "Iteration 106, loss = 0.34263020\n",
      "Iteration 107, loss = 0.34133557\n",
      "Iteration 108, loss = 0.34012205\n",
      "Iteration 109, loss = 0.33866354\n",
      "Iteration 110, loss = 0.33715363\n",
      "Iteration 111, loss = 0.33598185\n",
      "Iteration 112, loss = 0.33472514\n",
      "Iteration 113, loss = 0.33340542\n",
      "Iteration 114, loss = 0.33207198\n",
      "Iteration 115, loss = 0.33087990\n",
      "Iteration 116, loss = 0.32960682\n",
      "Iteration 117, loss = 0.32852293\n",
      "Iteration 118, loss = 0.32713538\n",
      "Iteration 119, loss = 0.32619486\n",
      "Iteration 120, loss = 0.32488521\n",
      "Iteration 121, loss = 0.32363279\n",
      "Iteration 122, loss = 0.32230469\n",
      "Iteration 123, loss = 0.32133249\n",
      "Iteration 124, loss = 0.32035592\n",
      "Iteration 125, loss = 0.31881440\n",
      "Iteration 126, loss = 0.31782618\n",
      "Iteration 127, loss = 0.31681909\n",
      "Iteration 128, loss = 0.31546362\n",
      "Iteration 129, loss = 0.31436678\n",
      "Iteration 130, loss = 0.31323778\n",
      "Iteration 131, loss = 0.31220570\n",
      "Iteration 132, loss = 0.31096752\n",
      "Iteration 133, loss = 0.30996937\n",
      "Iteration 134, loss = 0.30890884\n",
      "Iteration 135, loss = 0.30783495\n",
      "Iteration 136, loss = 0.30680916\n",
      "Iteration 137, loss = 0.30553117\n",
      "Iteration 138, loss = 0.30454906\n",
      "Iteration 139, loss = 0.30370410\n",
      "Iteration 140, loss = 0.30244156\n",
      "Iteration 141, loss = 0.30132546\n",
      "Iteration 142, loss = 0.30051576\n",
      "Iteration 143, loss = 0.29938276\n",
      "Iteration 144, loss = 0.29839509\n",
      "Iteration 145, loss = 0.29764650\n",
      "Iteration 146, loss = 0.29630815\n",
      "Iteration 147, loss = 0.29547126\n",
      "Iteration 148, loss = 0.29435844\n",
      "Iteration 149, loss = 0.29337865\n",
      "Iteration 150, loss = 0.29251581\n",
      "Iteration 151, loss = 0.29167401\n",
      "Iteration 152, loss = 0.29093935\n",
      "Iteration 153, loss = 0.28983907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 154, loss = 0.28866736\n",
      "Iteration 155, loss = 0.28797929\n",
      "Iteration 156, loss = 0.28701111\n",
      "Iteration 157, loss = 0.28583846\n",
      "Iteration 158, loss = 0.28508300\n",
      "Iteration 159, loss = 0.28414550\n",
      "Iteration 160, loss = 0.28330775\n",
      "Iteration 161, loss = 0.28227194\n",
      "Iteration 162, loss = 0.28139125\n",
      "Iteration 163, loss = 0.28059035\n",
      "Iteration 164, loss = 0.27966558\n",
      "Iteration 165, loss = 0.27891245\n",
      "Iteration 166, loss = 0.27792956\n",
      "Iteration 167, loss = 0.27722313\n",
      "Iteration 168, loss = 0.27626333\n",
      "Iteration 169, loss = 0.27531938\n",
      "Iteration 170, loss = 0.27436832\n",
      "Iteration 171, loss = 0.27356637\n",
      "Iteration 172, loss = 0.27286344\n",
      "Iteration 173, loss = 0.27187830\n",
      "Iteration 174, loss = 0.27130448\n",
      "Iteration 175, loss = 0.27026242\n",
      "Iteration 176, loss = 0.26935983\n",
      "Iteration 177, loss = 0.26864111\n",
      "Iteration 178, loss = 0.26799920\n",
      "Iteration 179, loss = 0.26705425\n",
      "Iteration 180, loss = 0.26629009\n",
      "Iteration 181, loss = 0.26561516\n",
      "Iteration 182, loss = 0.26476857\n",
      "Iteration 183, loss = 0.26404102\n",
      "Iteration 184, loss = 0.26321425\n",
      "Iteration 185, loss = 0.26243604\n",
      "Iteration 186, loss = 0.26158407\n",
      "Iteration 187, loss = 0.26101244\n",
      "Iteration 188, loss = 0.26007638\n",
      "Iteration 189, loss = 0.25944286\n",
      "Iteration 190, loss = 0.25888280\n",
      "Iteration 191, loss = 0.25794765\n",
      "Iteration 192, loss = 0.25684277\n",
      "Iteration 193, loss = 0.25666278\n",
      "Iteration 194, loss = 0.25583281\n",
      "Iteration 195, loss = 0.25491693\n",
      "Iteration 196, loss = 0.25412417\n",
      "Iteration 197, loss = 0.25373265\n",
      "Iteration 198, loss = 0.25287781\n",
      "Iteration 199, loss = 0.25208660\n",
      "Iteration 200, loss = 0.25150806\n",
      "Iteration 201, loss = 0.25076992\n",
      "Iteration 202, loss = 0.25009100\n",
      "Iteration 203, loss = 0.24914205\n",
      "Iteration 204, loss = 0.24841246\n",
      "Iteration 205, loss = 0.24793965\n",
      "Iteration 206, loss = 0.24706661\n",
      "Iteration 207, loss = 0.24661140\n",
      "Iteration 208, loss = 0.24581935\n",
      "Iteration 209, loss = 0.24509984\n",
      "Iteration 210, loss = 0.24439193\n",
      "Iteration 211, loss = 0.24401073\n",
      "Iteration 212, loss = 0.24309756\n",
      "Iteration 213, loss = 0.24259153\n",
      "Iteration 214, loss = 0.24190935\n",
      "Iteration 215, loss = 0.24130248\n",
      "Iteration 216, loss = 0.24052651\n",
      "Iteration 217, loss = 0.23991033\n",
      "Iteration 218, loss = 0.23918485\n",
      "Iteration 219, loss = 0.23839289\n",
      "Iteration 220, loss = 0.23782850\n",
      "Iteration 221, loss = 0.23724156\n",
      "Iteration 222, loss = 0.23660837\n",
      "Iteration 223, loss = 0.23591337\n",
      "Iteration 224, loss = 0.23539955\n",
      "Iteration 225, loss = 0.23461018\n",
      "Iteration 226, loss = 0.23429449\n",
      "Iteration 227, loss = 0.23359523\n",
      "Iteration 228, loss = 0.23261309\n",
      "Iteration 229, loss = 0.23217295\n",
      "Iteration 230, loss = 0.23179756\n",
      "Iteration 231, loss = 0.23103568\n",
      "Iteration 232, loss = 0.23033861\n",
      "Iteration 233, loss = 0.22987632\n",
      "Iteration 234, loss = 0.22926388\n",
      "Iteration 235, loss = 0.22852981\n",
      "Iteration 236, loss = 0.22803855\n",
      "Iteration 237, loss = 0.22744162\n",
      "Iteration 238, loss = 0.22681363\n",
      "Iteration 239, loss = 0.22630919\n",
      "Iteration 240, loss = 0.22562619\n",
      "Iteration 241, loss = 0.22503328\n",
      "Iteration 242, loss = 0.22436272\n",
      "Iteration 243, loss = 0.22389348\n",
      "Iteration 244, loss = 0.22326708\n",
      "Iteration 245, loss = 0.22253667\n",
      "Iteration 246, loss = 0.22226128\n",
      "Iteration 247, loss = 0.22161503\n",
      "Iteration 248, loss = 0.22112808\n",
      "Iteration 249, loss = 0.22060086\n",
      "Iteration 250, loss = 0.21980599\n",
      "Iteration 251, loss = 0.21943113\n",
      "Iteration 252, loss = 0.21867492\n",
      "Iteration 253, loss = 0.21817426\n",
      "Iteration 254, loss = 0.21766380\n",
      "Iteration 255, loss = 0.21727593\n",
      "Iteration 256, loss = 0.21637930\n",
      "Iteration 257, loss = 0.21600288\n",
      "Iteration 258, loss = 0.21555088\n",
      "Iteration 259, loss = 0.21501785\n",
      "Iteration 260, loss = 0.21433853\n",
      "Iteration 261, loss = 0.21379662\n",
      "Iteration 262, loss = 0.21311843\n",
      "Iteration 263, loss = 0.21298123\n",
      "Iteration 264, loss = 0.21234499\n",
      "Iteration 265, loss = 0.21161239\n",
      "Iteration 266, loss = 0.21117216\n",
      "Iteration 267, loss = 0.21047974\n",
      "Iteration 268, loss = 0.21001940\n",
      "Iteration 269, loss = 0.20977429\n",
      "Iteration 270, loss = 0.20910555\n",
      "Iteration 271, loss = 0.20871756\n",
      "Iteration 272, loss = 0.20803918\n",
      "Iteration 273, loss = 0.20752224\n",
      "Iteration 274, loss = 0.20693973\n",
      "Iteration 275, loss = 0.20652135\n",
      "Iteration 276, loss = 0.20581012\n",
      "Iteration 277, loss = 0.20544937\n",
      "Iteration 278, loss = 0.20503533\n",
      "Iteration 279, loss = 0.20465919\n",
      "Iteration 280, loss = 0.20403469\n",
      "Iteration 281, loss = 0.20349410\n",
      "Iteration 282, loss = 0.20297984\n",
      "Iteration 283, loss = 0.20253008\n",
      "Iteration 284, loss = 0.20210805\n",
      "Iteration 285, loss = 0.20159359\n",
      "Iteration 286, loss = 0.20105560\n",
      "Iteration 287, loss = 0.20044312\n",
      "Iteration 288, loss = 0.19999275\n",
      "Iteration 289, loss = 0.19960376\n",
      "Iteration 290, loss = 0.19912603\n",
      "Iteration 291, loss = 0.19856599\n",
      "Iteration 292, loss = 0.19801112\n",
      "Iteration 293, loss = 0.19772090\n",
      "Iteration 294, loss = 0.19718809\n",
      "Iteration 295, loss = 0.19675945\n",
      "Iteration 296, loss = 0.19618824\n",
      "Iteration 297, loss = 0.19572135\n",
      "Iteration 298, loss = 0.19528374\n",
      "Iteration 299, loss = 0.19476857\n",
      "Iteration 300, loss = 0.19445999\n",
      "Iteration 301, loss = 0.19389068\n",
      "Iteration 302, loss = 0.19337712\n",
      "Iteration 303, loss = 0.19285967\n",
      "Iteration 304, loss = 0.19258410\n",
      "Iteration 305, loss = 0.19200539\n",
      "Iteration 306, loss = 0.19164776\n",
      "Iteration 307, loss = 0.19128678\n",
      "Iteration 308, loss = 0.19058531\n",
      "Iteration 309, loss = 0.19017348\n",
      "Iteration 310, loss = 0.18975632\n",
      "Iteration 311, loss = 0.18934739\n",
      "Iteration 312, loss = 0.18890205\n",
      "Iteration 313, loss = 0.18847559\n",
      "Iteration 314, loss = 0.18801190\n",
      "Iteration 315, loss = 0.18746831\n",
      "Iteration 316, loss = 0.18713336\n",
      "Iteration 317, loss = 0.18665707\n",
      "Iteration 318, loss = 0.18646016\n",
      "Iteration 319, loss = 0.18583541\n",
      "Iteration 320, loss = 0.18547235\n",
      "Iteration 321, loss = 0.18502309\n",
      "Iteration 322, loss = 0.18466970\n",
      "Iteration 323, loss = 0.18406344\n",
      "Iteration 324, loss = 0.18390999\n",
      "Iteration 325, loss = 0.18330003\n",
      "Iteration 326, loss = 0.18285596\n",
      "Iteration 327, loss = 0.18250859\n",
      "Iteration 328, loss = 0.18195525\n",
      "Iteration 329, loss = 0.18150881\n",
      "Iteration 330, loss = 0.18130710\n",
      "Iteration 331, loss = 0.18092503\n",
      "Iteration 332, loss = 0.18021074\n",
      "Iteration 333, loss = 0.17997468\n",
      "Iteration 334, loss = 0.17953507\n",
      "Iteration 335, loss = 0.17911109\n",
      "Iteration 336, loss = 0.17875615\n",
      "Iteration 337, loss = 0.17833008\n",
      "Iteration 338, loss = 0.17794336\n",
      "Iteration 339, loss = 0.17759423\n",
      "Iteration 340, loss = 0.17708650\n",
      "Iteration 341, loss = 0.17666349\n",
      "Iteration 342, loss = 0.17630617\n",
      "Iteration 343, loss = 0.17594425\n",
      "Iteration 344, loss = 0.17546930\n",
      "Iteration 345, loss = 0.17516175\n",
      "Iteration 346, loss = 0.17478763\n",
      "Iteration 347, loss = 0.17423802\n",
      "Iteration 348, loss = 0.17404156\n",
      "Iteration 349, loss = 0.17345453\n",
      "Iteration 350, loss = 0.17321841\n",
      "Iteration 351, loss = 0.17295886\n",
      "Iteration 352, loss = 0.17243988\n",
      "Iteration 353, loss = 0.17192761\n",
      "Iteration 354, loss = 0.17159652\n",
      "Iteration 355, loss = 0.17126820\n",
      "Iteration 356, loss = 0.17090805\n",
      "Iteration 357, loss = 0.17044318\n",
      "Iteration 358, loss = 0.17011715\n",
      "Iteration 359, loss = 0.16968301\n",
      "Iteration 360, loss = 0.16929838\n",
      "Iteration 361, loss = 0.16894080\n",
      "Iteration 362, loss = 0.16870505\n",
      "Iteration 363, loss = 0.16828030\n",
      "Iteration 364, loss = 0.16788267\n",
      "Iteration 365, loss = 0.16746787\n",
      "Iteration 366, loss = 0.16712139\n",
      "Iteration 367, loss = 0.16672175\n",
      "Iteration 368, loss = 0.16629385\n",
      "Iteration 369, loss = 0.16600873\n",
      "Iteration 370, loss = 0.16566674\n",
      "Iteration 371, loss = 0.16529241\n",
      "Iteration 372, loss = 0.16501457\n",
      "Iteration 373, loss = 0.16449906\n",
      "Iteration 374, loss = 0.16430768\n",
      "Iteration 375, loss = 0.16385586\n",
      "Iteration 376, loss = 0.16357889\n",
      "Iteration 377, loss = 0.16316485\n",
      "Iteration 378, loss = 0.16283964\n",
      "Iteration 379, loss = 0.16237953\n",
      "Iteration 380, loss = 0.16207977\n",
      "Iteration 381, loss = 0.16192244\n",
      "Iteration 382, loss = 0.16147448\n",
      "Iteration 383, loss = 0.16095034\n",
      "Iteration 384, loss = 0.16076924\n",
      "Iteration 385, loss = 0.16032748\n",
      "Iteration 386, loss = 0.16001335\n",
      "Iteration 387, loss = 0.15977798\n",
      "Iteration 388, loss = 0.15923406\n",
      "Iteration 389, loss = 0.15901230\n",
      "Iteration 390, loss = 0.15866437\n",
      "Iteration 391, loss = 0.15829985\n",
      "Iteration 392, loss = 0.15788069\n",
      "Iteration 393, loss = 0.15764022\n",
      "Iteration 394, loss = 0.15727612\n",
      "Iteration 395, loss = 0.15698881\n",
      "Iteration 396, loss = 0.15663221\n",
      "Iteration 397, loss = 0.15613042\n",
      "Iteration 398, loss = 0.15602203\n",
      "Iteration 399, loss = 0.15561995\n",
      "Iteration 400, loss = 0.15528357\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(clf)):\n",
    "    start =time.time()\n",
    "    clf[i].fit(X_train, train_class_enc)\n",
    "    end = time.time()\n",
    "    epochs.append(clf[i].n_iter_)\n",
    "    train_accuracy.append(clf[i].score(X_train, train_class_enc)*100)\n",
    "    test_accuracy.append(clf[i].score(X_test, test_actual_class_enc)*100)\n",
    "    train_time.append(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = mlp_classifier(hidden_layer_sizes=(100, 100), activation='logistic', solver='sgd', \n",
    "                     batch_size=100, learning_rate_init=0.3, learning_rate='invscaling', max_iter=400,\n",
    "                     power_t=(1/4), tol=1e-4, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 4.43050756\n",
      "Iteration 2, loss = 4.10995572\n",
      "Iteration 3, loss = 3.91585847\n",
      "Iteration 4, loss = 3.76880427\n",
      "Iteration 5, loss = 3.61733016\n",
      "Iteration 6, loss = 3.47240127\n",
      "Iteration 7, loss = 3.35420871\n",
      "Iteration 8, loss = 3.25560397\n",
      "Iteration 9, loss = 3.16826830\n",
      "Iteration 10, loss = 3.08959850\n",
      "Iteration 11, loss = 3.01977670\n",
      "Iteration 12, loss = 2.95769707\n",
      "Iteration 13, loss = 2.90314175\n",
      "Iteration 14, loss = 2.85330171\n",
      "Iteration 15, loss = 2.80677889\n",
      "Iteration 16, loss = 2.76217178\n",
      "Iteration 17, loss = 2.71924428\n",
      "Iteration 18, loss = 2.67564112\n",
      "Iteration 19, loss = 2.63233309\n",
      "Iteration 20, loss = 2.58808626\n",
      "Iteration 21, loss = 2.54382897\n",
      "Iteration 22, loss = 2.50082509\n",
      "Iteration 23, loss = 2.45838136\n",
      "Iteration 24, loss = 2.41766267\n",
      "Iteration 25, loss = 2.37825881\n",
      "Iteration 26, loss = 2.33876119\n",
      "Iteration 27, loss = 2.29948132\n",
      "Iteration 28, loss = 2.26084208\n",
      "Iteration 29, loss = 2.22140945\n",
      "Iteration 30, loss = 2.18211375\n",
      "Iteration 31, loss = 2.14281381\n",
      "Iteration 32, loss = 2.10296770\n",
      "Iteration 33, loss = 2.06316212\n",
      "Iteration 34, loss = 2.02343369\n",
      "Iteration 35, loss = 1.98200153\n",
      "Iteration 36, loss = 1.94156740\n",
      "Iteration 37, loss = 1.89956478\n",
      "Iteration 38, loss = 1.85819488\n",
      "Iteration 39, loss = 1.81859570\n",
      "Iteration 40, loss = 1.77943382\n",
      "Iteration 41, loss = 1.74238440\n",
      "Iteration 42, loss = 1.70747000\n",
      "Iteration 43, loss = 1.67369888\n",
      "Iteration 44, loss = 1.64209407\n",
      "Iteration 45, loss = 1.61153298\n",
      "Iteration 46, loss = 1.58247205\n",
      "Iteration 47, loss = 1.55452076\n",
      "Iteration 48, loss = 1.52753927\n",
      "Iteration 49, loss = 1.50086902\n",
      "Iteration 50, loss = 1.47499217\n",
      "Iteration 51, loss = 1.44999528\n",
      "Iteration 52, loss = 1.42597548\n",
      "Iteration 53, loss = 1.40173255\n",
      "Iteration 54, loss = 1.37875086\n",
      "Iteration 55, loss = 1.35660026\n",
      "Iteration 56, loss = 1.33449060\n",
      "Iteration 57, loss = 1.31244702\n",
      "Iteration 58, loss = 1.29162131\n",
      "Iteration 59, loss = 1.27172120\n",
      "Iteration 60, loss = 1.25159827\n",
      "Iteration 61, loss = 1.23242044\n",
      "Iteration 62, loss = 1.21374775\n",
      "Iteration 63, loss = 1.19529716\n",
      "Iteration 64, loss = 1.17680149\n",
      "Iteration 65, loss = 1.15966533\n",
      "Iteration 66, loss = 1.14307831\n",
      "Iteration 67, loss = 1.12573281\n",
      "Iteration 68, loss = 1.10931028\n",
      "Iteration 69, loss = 1.09378417\n",
      "Iteration 70, loss = 1.07850832\n",
      "Iteration 71, loss = 1.06362358\n",
      "Iteration 72, loss = 1.04871599\n",
      "Iteration 73, loss = 1.03449589\n",
      "Iteration 74, loss = 1.02061177\n",
      "Iteration 75, loss = 1.00744448\n",
      "Iteration 76, loss = 0.99348829\n",
      "Iteration 77, loss = 0.98044749\n",
      "Iteration 78, loss = 0.96729912\n",
      "Iteration 79, loss = 0.95462688\n",
      "Iteration 80, loss = 0.94246717\n",
      "Iteration 81, loss = 0.93049466\n",
      "Iteration 82, loss = 0.91862591\n",
      "Iteration 83, loss = 0.90679505\n",
      "Iteration 84, loss = 0.89564204\n",
      "Iteration 85, loss = 0.88388277\n",
      "Iteration 86, loss = 0.87343010\n",
      "Iteration 87, loss = 0.86236625\n",
      "Iteration 88, loss = 0.85226639\n",
      "Iteration 89, loss = 0.84166232\n",
      "Iteration 90, loss = 0.83211996\n",
      "Iteration 91, loss = 0.82198692\n",
      "Iteration 92, loss = 0.81261480\n",
      "Iteration 93, loss = 0.80307467\n",
      "Iteration 94, loss = 0.79343362\n",
      "Iteration 95, loss = 0.78476744\n",
      "Iteration 96, loss = 0.77565006\n",
      "Iteration 97, loss = 0.76778170\n",
      "Iteration 98, loss = 0.75890151\n",
      "Iteration 99, loss = 0.75030556\n",
      "Iteration 100, loss = 0.74258880\n",
      "Iteration 101, loss = 0.73369104\n",
      "Iteration 102, loss = 0.72648633\n",
      "Iteration 103, loss = 0.71879162\n",
      "Iteration 104, loss = 0.71102881\n",
      "Iteration 105, loss = 0.70401590\n",
      "Iteration 106, loss = 0.69650374\n",
      "Iteration 107, loss = 0.68974177\n",
      "Iteration 108, loss = 0.68255390\n",
      "Iteration 109, loss = 0.67605813\n",
      "Iteration 110, loss = 0.66903730\n",
      "Iteration 111, loss = 0.66308862\n",
      "Iteration 112, loss = 0.65621807\n",
      "Iteration 113, loss = 0.64911056\n",
      "Iteration 114, loss = 0.64338289\n",
      "Iteration 115, loss = 0.63774618\n",
      "Iteration 116, loss = 0.63173978\n",
      "Iteration 117, loss = 0.62534638\n",
      "Iteration 118, loss = 0.61982163\n",
      "Iteration 119, loss = 0.61403508\n",
      "Iteration 120, loss = 0.60923249\n",
      "Iteration 121, loss = 0.60260629\n",
      "Iteration 122, loss = 0.59768769\n",
      "Iteration 123, loss = 0.59240749\n",
      "Iteration 124, loss = 0.58748103\n",
      "Iteration 125, loss = 0.58198430\n",
      "Iteration 126, loss = 0.57679511\n",
      "Iteration 127, loss = 0.57192124\n",
      "Iteration 128, loss = 0.56731765\n",
      "Iteration 129, loss = 0.56220664\n",
      "Iteration 130, loss = 0.55776409\n",
      "Iteration 131, loss = 0.55331917\n",
      "Iteration 132, loss = 0.54806505\n",
      "Iteration 133, loss = 0.54353898\n",
      "Iteration 134, loss = 0.53886551\n",
      "Iteration 135, loss = 0.53473812\n",
      "Iteration 136, loss = 0.53010580\n",
      "Iteration 137, loss = 0.52612660\n",
      "Iteration 138, loss = 0.52147176\n",
      "Iteration 139, loss = 0.51732949\n",
      "Iteration 140, loss = 0.51340617\n",
      "Iteration 141, loss = 0.50916513\n",
      "Iteration 142, loss = 0.50577895\n",
      "Iteration 143, loss = 0.50159666\n",
      "Iteration 144, loss = 0.49740043\n",
      "Iteration 145, loss = 0.49410965\n",
      "Iteration 146, loss = 0.48968063\n",
      "Iteration 147, loss = 0.48575162\n",
      "Iteration 148, loss = 0.48261324\n",
      "Iteration 149, loss = 0.47893951\n",
      "Iteration 150, loss = 0.47514845\n",
      "Iteration 151, loss = 0.47139656\n",
      "Iteration 152, loss = 0.46796443\n",
      "Iteration 153, loss = 0.46498598\n",
      "Iteration 154, loss = 0.46072164\n",
      "Iteration 155, loss = 0.45727487\n",
      "Iteration 156, loss = 0.45437051\n",
      "Iteration 157, loss = 0.45074906\n",
      "Iteration 158, loss = 0.44744971\n",
      "Iteration 159, loss = 0.44386859\n",
      "Iteration 160, loss = 0.44087333\n",
      "Iteration 161, loss = 0.43805700\n",
      "Iteration 162, loss = 0.43467764\n",
      "Iteration 163, loss = 0.43168293\n",
      "Iteration 164, loss = 0.42856537\n",
      "Iteration 165, loss = 0.42541786\n",
      "Iteration 166, loss = 0.42249626\n",
      "Iteration 167, loss = 0.41956518\n",
      "Iteration 168, loss = 0.41618980\n",
      "Iteration 169, loss = 0.41407797\n",
      "Iteration 170, loss = 0.41063974\n",
      "Iteration 171, loss = 0.40786169\n",
      "Iteration 172, loss = 0.40526761\n",
      "Iteration 173, loss = 0.40231250\n",
      "Iteration 174, loss = 0.39943522\n",
      "Iteration 175, loss = 0.39679534\n",
      "Iteration 176, loss = 0.39394043\n",
      "Iteration 177, loss = 0.39170706\n",
      "Iteration 178, loss = 0.38918524\n",
      "Iteration 179, loss = 0.38623304\n",
      "Iteration 180, loss = 0.38358932\n",
      "Iteration 181, loss = 0.38093629\n",
      "Iteration 182, loss = 0.37861402\n",
      "Iteration 183, loss = 0.37594171\n",
      "Iteration 184, loss = 0.37329421\n",
      "Iteration 185, loss = 0.37116333\n",
      "Iteration 186, loss = 0.36814273\n",
      "Iteration 187, loss = 0.36651174\n",
      "Iteration 188, loss = 0.36413287\n",
      "Iteration 189, loss = 0.36149287\n",
      "Iteration 190, loss = 0.35901240\n",
      "Iteration 191, loss = 0.35660931\n",
      "Iteration 192, loss = 0.35467301\n",
      "Iteration 193, loss = 0.35220099\n",
      "Iteration 194, loss = 0.35044937\n",
      "Iteration 195, loss = 0.34786613\n",
      "Iteration 196, loss = 0.34591648\n",
      "Iteration 197, loss = 0.34349922\n",
      "Iteration 198, loss = 0.34121704\n",
      "Iteration 199, loss = 0.33916888\n",
      "Iteration 200, loss = 0.33673124\n",
      "Iteration 201, loss = 0.33484994\n",
      "Iteration 202, loss = 0.33297646\n",
      "Iteration 203, loss = 0.33033332\n",
      "Iteration 204, loss = 0.32888242\n",
      "Iteration 205, loss = 0.32673905\n",
      "Iteration 206, loss = 0.32499240\n",
      "Iteration 207, loss = 0.32282472\n",
      "Iteration 208, loss = 0.32103377\n",
      "Iteration 209, loss = 0.31908808\n",
      "Iteration 210, loss = 0.31711975\n",
      "Iteration 211, loss = 0.31512345\n",
      "Iteration 212, loss = 0.31336824\n",
      "Iteration 213, loss = 0.31126088\n",
      "Iteration 214, loss = 0.30916954\n",
      "Iteration 215, loss = 0.30768456\n",
      "Iteration 216, loss = 0.30573243\n",
      "Iteration 217, loss = 0.30410587\n",
      "Iteration 218, loss = 0.30233143\n",
      "Iteration 219, loss = 0.30061090\n",
      "Iteration 220, loss = 0.29895122\n",
      "Iteration 221, loss = 0.29687851\n",
      "Iteration 222, loss = 0.29511932\n",
      "Iteration 223, loss = 0.29341434\n",
      "Iteration 224, loss = 0.29210012\n",
      "Iteration 225, loss = 0.29029472\n",
      "Iteration 226, loss = 0.28853072\n",
      "Iteration 227, loss = 0.28686576\n",
      "Iteration 228, loss = 0.28487674\n",
      "Iteration 229, loss = 0.28367679\n",
      "Iteration 230, loss = 0.28167262\n",
      "Iteration 231, loss = 0.28051755\n",
      "Iteration 232, loss = 0.27912584\n",
      "Iteration 233, loss = 0.27718694\n",
      "Iteration 234, loss = 0.27596050\n",
      "Iteration 235, loss = 0.27434667\n",
      "Iteration 236, loss = 0.27260763\n",
      "Iteration 237, loss = 0.27100769\n",
      "Iteration 238, loss = 0.26961639\n",
      "Iteration 239, loss = 0.26790794\n",
      "Iteration 240, loss = 0.26689790\n",
      "Iteration 241, loss = 0.26525565\n",
      "Iteration 242, loss = 0.26341841\n",
      "Iteration 243, loss = 0.26235667\n",
      "Iteration 244, loss = 0.26067745\n",
      "Iteration 245, loss = 0.25933146\n",
      "Iteration 246, loss = 0.25807220\n",
      "Iteration 247, loss = 0.25663613\n",
      "Iteration 248, loss = 0.25537091\n",
      "Iteration 249, loss = 0.25380255\n",
      "Iteration 250, loss = 0.25239855\n",
      "Iteration 251, loss = 0.25096455\n",
      "Iteration 252, loss = 0.24961010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 253, loss = 0.24812404\n",
      "Iteration 254, loss = 0.24702899\n",
      "Iteration 255, loss = 0.24542399\n",
      "Iteration 256, loss = 0.24432685\n",
      "Iteration 257, loss = 0.24297936\n",
      "Iteration 258, loss = 0.24188515\n",
      "Iteration 259, loss = 0.24065773\n",
      "Iteration 260, loss = 0.23925629\n",
      "Iteration 261, loss = 0.23800089\n",
      "Iteration 262, loss = 0.23675926\n",
      "Iteration 263, loss = 0.23577028\n",
      "Iteration 264, loss = 0.23444924\n",
      "Iteration 265, loss = 0.23294139\n",
      "Iteration 266, loss = 0.23178358\n",
      "Iteration 267, loss = 0.23068842\n",
      "Iteration 268, loss = 0.22959484\n",
      "Iteration 269, loss = 0.22820432\n",
      "Iteration 270, loss = 0.22738234\n",
      "Iteration 271, loss = 0.22599333\n",
      "Iteration 272, loss = 0.22484229\n",
      "Iteration 273, loss = 0.22397379\n",
      "Iteration 274, loss = 0.22236155\n",
      "Iteration 275, loss = 0.22139183\n",
      "Iteration 276, loss = 0.22067848\n",
      "Iteration 277, loss = 0.21913161\n",
      "Iteration 278, loss = 0.21804810\n",
      "Iteration 279, loss = 0.21676596\n",
      "Iteration 280, loss = 0.21585632\n",
      "Iteration 281, loss = 0.21485148\n",
      "Iteration 282, loss = 0.21344923\n",
      "Iteration 283, loss = 0.21295839\n",
      "Iteration 284, loss = 0.21163461\n",
      "Iteration 285, loss = 0.21048829\n",
      "Iteration 286, loss = 0.20973775\n",
      "Iteration 287, loss = 0.20827666\n",
      "Iteration 288, loss = 0.20730173\n",
      "Iteration 289, loss = 0.20628211\n",
      "Iteration 290, loss = 0.20526575\n",
      "Iteration 291, loss = 0.20430551\n",
      "Iteration 292, loss = 0.20329219\n",
      "Iteration 293, loss = 0.20230682\n",
      "Iteration 294, loss = 0.20121269\n",
      "Iteration 295, loss = 0.20046829\n",
      "Iteration 296, loss = 0.19905816\n",
      "Iteration 297, loss = 0.19865406\n",
      "Iteration 298, loss = 0.19752060\n",
      "Iteration 299, loss = 0.19628668\n",
      "Iteration 300, loss = 0.19567189\n",
      "Iteration 301, loss = 0.19450076\n",
      "Iteration 302, loss = 0.19365148\n",
      "Iteration 303, loss = 0.19268137\n",
      "Iteration 304, loss = 0.19186831\n",
      "Iteration 305, loss = 0.19101537\n",
      "Iteration 306, loss = 0.18989667\n",
      "Iteration 307, loss = 0.18908124\n",
      "Iteration 308, loss = 0.18815896\n",
      "Iteration 309, loss = 0.18723075\n",
      "Iteration 310, loss = 0.18646544\n",
      "Iteration 311, loss = 0.18560060\n",
      "Iteration 312, loss = 0.18486878\n",
      "Iteration 313, loss = 0.18375957\n",
      "Iteration 314, loss = 0.18293364\n",
      "Iteration 315, loss = 0.18208733\n",
      "Iteration 316, loss = 0.18143093\n",
      "Iteration 317, loss = 0.18060340\n",
      "Iteration 318, loss = 0.17957792\n",
      "Iteration 319, loss = 0.17901427\n",
      "Iteration 320, loss = 0.17792081\n",
      "Iteration 321, loss = 0.17718806\n",
      "Iteration 322, loss = 0.17647022\n",
      "Iteration 323, loss = 0.17548193\n",
      "Iteration 324, loss = 0.17483606\n",
      "Iteration 325, loss = 0.17413612\n",
      "Iteration 326, loss = 0.17312962\n",
      "Iteration 327, loss = 0.17251840\n",
      "Iteration 328, loss = 0.17188723\n",
      "Iteration 329, loss = 0.17102226\n",
      "Iteration 330, loss = 0.17001264\n",
      "Iteration 331, loss = 0.16936716\n",
      "Iteration 332, loss = 0.16853182\n",
      "Iteration 333, loss = 0.16801368\n",
      "Iteration 334, loss = 0.16725917\n",
      "Iteration 335, loss = 0.16654389\n",
      "Iteration 336, loss = 0.16578748\n",
      "Iteration 337, loss = 0.16511470\n",
      "Iteration 338, loss = 0.16413377\n",
      "Iteration 339, loss = 0.16359955\n",
      "Iteration 340, loss = 0.16288207\n",
      "Iteration 341, loss = 0.16224786\n",
      "Iteration 342, loss = 0.16141067\n",
      "Iteration 343, loss = 0.16093328\n",
      "Iteration 344, loss = 0.16013837\n",
      "Iteration 345, loss = 0.15968208\n",
      "Iteration 346, loss = 0.15863953\n",
      "Iteration 347, loss = 0.15821579\n",
      "Iteration 348, loss = 0.15742156\n",
      "Iteration 349, loss = 0.15688656\n",
      "Iteration 350, loss = 0.15613139\n",
      "Iteration 351, loss = 0.15541911\n",
      "Iteration 352, loss = 0.15491148\n",
      "Iteration 353, loss = 0.15415313\n",
      "Iteration 354, loss = 0.15359779\n",
      "Iteration 355, loss = 0.15293980\n",
      "Iteration 356, loss = 0.15235607\n",
      "Iteration 357, loss = 0.15165777\n",
      "Iteration 358, loss = 0.15104596\n",
      "Iteration 359, loss = 0.15044298\n",
      "Iteration 360, loss = 0.14976115\n",
      "Iteration 361, loss = 0.14928787\n",
      "Iteration 362, loss = 0.14854851\n",
      "Iteration 363, loss = 0.14798409\n",
      "Iteration 364, loss = 0.14744341\n",
      "Iteration 365, loss = 0.14695448\n",
      "Iteration 366, loss = 0.14628441\n",
      "Iteration 367, loss = 0.14565829\n",
      "Iteration 368, loss = 0.14499807\n",
      "Iteration 369, loss = 0.14457479\n",
      "Iteration 370, loss = 0.14400880\n",
      "Iteration 371, loss = 0.14329837\n",
      "Iteration 372, loss = 0.14286030\n",
      "Iteration 373, loss = 0.14226876\n",
      "Iteration 374, loss = 0.14171550\n",
      "Iteration 375, loss = 0.14111135\n",
      "Iteration 376, loss = 0.14066254\n",
      "Iteration 377, loss = 0.14012328\n",
      "Iteration 378, loss = 0.13954763\n",
      "Iteration 379, loss = 0.13892197\n",
      "Iteration 380, loss = 0.13848820\n",
      "Iteration 381, loss = 0.13789414\n",
      "Iteration 382, loss = 0.13740549\n",
      "Iteration 383, loss = 0.13677250\n",
      "Iteration 384, loss = 0.13639179\n",
      "Iteration 385, loss = 0.13582674\n",
      "Iteration 386, loss = 0.13530976\n",
      "Iteration 387, loss = 0.13477908\n",
      "Iteration 388, loss = 0.13438241\n",
      "Iteration 389, loss = 0.13378207\n",
      "Iteration 390, loss = 0.13327740\n",
      "Iteration 391, loss = 0.13283233\n",
      "Iteration 392, loss = 0.13224131\n",
      "Iteration 393, loss = 0.13176059\n",
      "Iteration 394, loss = 0.13131279\n",
      "Iteration 395, loss = 0.13080744\n",
      "Iteration 396, loss = 0.13031368\n",
      "Iteration 397, loss = 0.12992857\n",
      "Iteration 398, loss = 0.12937037\n",
      "Iteration 399, loss = 0.12886202\n",
      "Iteration 400, loss = 0.12860902\n"
     ]
    }
   ],
   "source": [
    "start =time.time()\n",
    "clf.fit(X_train, train_class_enc)\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time taken to train the model using MLP classifier is = 162.838sec\n",
      "The test accuracy of the model =86.077%\n"
     ]
    }
   ],
   "source": [
    "print(\"The time taken to train the model using MLP classifier is = {:2.3f}sec\".format(end-start))\n",
    "print(\"The test accuracy of the model ={:2.3f}%\".format(clf.score(X_test, test_actual_class_enc)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f824aba1b00>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAe4UlEQVR4nO3de5RU1Zn38e9jd3PxAqShFARMAzIKEgVsjUp0EMeJogPRkCUqibwxw2vUpb6aSSROjDLRUTPxFieJRPBONPEWNF6iAVd0EoFGAUEkIN5FaAEVzcj1ef/Yp+yiqO6u7q6qU5ffZ62zqk6dbdXDAX99etc+e5u7IyIipW+3uAsQEZHcUKCLiJQJBbqISJlQoIuIlAkFuohImaiO64N79erldXV1cX28iEhJWrhw4Qfunsh0LLZAr6uro6GhIa6PFxEpSWb2ZnPH1OUiIlImsg50M6sys5fM7LEMxyabWaOZLYq27+S2TBERaU1bulwuBJYD3Zo5fr+7n9/xkkREpD2yukI3s37AScBt+S1HRETaK9sulxuB7wM7WmjzdTNbYmYPmFn/TA3MbIqZNZhZQ2NjY1trFRGRFrQa6GZ2MrDO3Re20OxRoM7dDwaeBu7M1Mjdp7t7vbvXJxIZR92IiEg7ZXOFPgoYZ2ZvAPcBY8zsntQG7r7e3TdHu7cBh+a0ShERaVWrge7uU929n7vXAROBOe4+KbWNmfVJ2R1H+PI0P5YuhX//d1i/Pm8fISJSito9Dt3MppnZuGj3AjNbZmaLgQuAybkoLqOVK+Gqq+Ctt/L2ESIipahNd4q6+7PAs9Hzy1NenwpMzWVhzaqtDY8bNhTk40RESkXp3Snas2d4VJeLiMhOSjfQdYUuIrKT0gv0ZJeLrtBFRHZSeoHeuTPssYcCXUQkTekFOoRuF3W5iIjspDQDvbZWV+giImlKM9B79lSgi4ikKc1Ar61Vl4uISJrSDPREAt57D/7+97grEREpGqUZ6BMnwqZNcN11cVciIlI0SjPQjz4aTjsNrr0W3mx2vVQRkYpSmoEO4ep8xw646aa4KxERKQqlG+j77Qdjx8J998H27XFXIyISu9INdIAzzoA1a+Cvf427EhGR2JV2oI8ZEx7//Od46xARKQKlHeg9e8LQofDcc3FXIiISu6wD3cyqzOwlM3ssw7HOZna/ma0ys3lmVpfLIlt09NHwl7+EL0hFRCpYW67QL6T5tULPBja6+/7ADcC1HS0sayNHwscfa0k6Eal4WQW6mfUDTgJua6bJeODO6PkDwHFmZh0vLwvDhoXHpUsL8nEiIsUq2yv0G4HvA831a/QF3gZw923AR0DP9EZmNsXMGsysobGxsR3lZnDQQeFRgS4iFa7VQDezk4F17r6wox/m7tPdvd7d6xOJREffLujeHfr3V6CLSMXL5gp9FDDOzN4A7gPGmNk9aW3eBfoDmFk10B0o3Py2Q4bAq68W7ONERIpRq4Hu7lPdvZ+71wETgTnuPimt2WzgrOj5hKiN57TSlgwaBKtXF+zjRESKUbvHoZvZNDMbF+3OAHqa2SrgYuDSXBSXtYEDYePGsImIVKjqtjR292eBZ6Pnl6e8/hnwjVwW1iYDB4bH1avh0ENjK0NEJE6lfado0qBB4VHdLiJSwcoj0AcMCI8KdBGpYOUR6N26Qa9eCnQRqWjlEegQul1eey3uKkREYlM+gT5woK7QRaSilVegv/UWbN0adyUiIrEon0AfNCgsRadZF0WkQpVPoCfHoqsfXUQqVPkF+uuvx1uHiEhMyifQ990XOnXSF6MiUrHKJ9CrqqCuToEuIhWrfAIdNHRRRCqaAl1EpEyUX6B/+KGm0RWRilR+gQ66SheRiqRAFxEpE9ksEt3FzOab2WIzW2ZmV2ZoM9nMGs1sUbR9Jz/ltiI5ja7GootIBcpmxaLNwBh3/8TMaoDnzewJd38hrd397n5+7ktsA02jKyIVrNVAjxZ7/iTarYm2wi0A3VYDB+r2fxGpSFn1oZtZlZktAtYBT7v7vAzNvm5mS8zsATPr38z7TDGzBjNraGxs7EDZLdh/f1i5Mj/vLSJSxLIKdHff7u7DgX7A4WY2LK3Jo0Cdux8MPA3c2cz7THf3enevTyQSHam7eUOGwJtvwqef5uf9RUSKVJtGubj7h8Bc4IS019e7++Zo9zbg0NyU1w4HHhgeV6yIrQQRkThkM8olYWY9ouddgeOBV9Pa9EnZHQcsz2WRbTJkSHh89dWW24mIlJlsRrn0Ae40syrCD4DfuvtjZjYNaHD32cAFZjYO2AZsACbnq+BWDR4cJupaHt/PFBGROGQzymUJMCLD65enPJ8KTM1tae3UqVNYvUiBLiIVprzuFE068EB1uYhIxSnPQB8yBP72N9i2Le5KREQKpnwDfetW3TEqIhWlfAMd4JVX4q1DRKSAyjPQhw0LI10WLoy7EhGRginPQN99d/jSl2D+/LgrEREpmPIMdIDDDoMFC8CLdx4xEZFcKu9A37hRMy+KSMUo70CHcJUuIlIByjfQDzoIunZVP7qIVIzyDfSaGhgxQoEuIhWjfAMdYNQoaGjQ3OgiUhHKO9D/6Z9gyxZ4/vm4KxERybvyDvSvfCXMvvjMM3FXIiKSd+Ud6LvvDkcdpUAXkYpQ3oEOodtl0SLI16LUIiJFojICHXSVLiJlL5s1RbuY2XwzW2xmy8zsygxtOpvZ/Wa2yszmmVldPoptl/p6SCRg9uy4KxERyatsrtA3A2Pc/RBgOHCCmR2R1uZsYKO77w/cAFyb2zI7oKoKxo+HP/wBNm+OuxoRkbxpNdA9+CTarYm29BmvxgN3Rs8fAI4zM8tZlR11yimwaRPMmRN3JSIieZNVH7qZVZnZImAd8LS7z0tr0hd4G8DdtwEfAT0zvM8UM2sws4bGQn5JOWYM7LknPPxw4T5TRKTAsgp0d9/u7sOBfsDhZjasPR/m7tPdvd7d6xOJRHveon26dIGTToJHHtE6oyJStto0ysXdPwTmAiekHXoX6A9gZtVAd2B9LgrMmdNOC0MX//SnuCsREcmLbEa5JMysR/S8K3A88Gpas9nAWdHzCcAc9yJbWWLsWOjeHe69N+5KRETyojqLNn2AO82sivAD4Lfu/piZTQMa3H02MAO428xWARuAiXmruL06d4YJE+D+++Hvfw93kYqIlJFWA93dlwAjMrx+ecrzz4Bv5La0PDjzTJgxAx59NHTBiIiUkfK/UzTVMcfAvvvCrFlxVyIiknOVFehVVXD66fDEE7BhQ9zViIjkVGUFOoRul61b4Xe/i7sSEZGcqrxAHz4cDjxQo11EpOxUXqCbhav0556Dt96KuxoRkZypvEAHOOOM8KgvR0WkjFRmoA8cGBaQvvtuKLL7n0RE2qsyAx1g0iR45RVYvDjuSkREcqJyA/0b34CaGrjnnrgrERHJicoN9J49w/wus2bB9u1xVyMi0mGVG+gQul3WrIG5c+OuRESkwyo70E8+Gbp1U7eLiJSFyg70Ll1CX/qDD4YZGEVESlhlBzqEbpdPPoHZs+OuRESkQxToxxwD/fqp20VESp4CfbfdwlQATz4ZlqgTESlR2SxB19/M5prZK2a2zMwuzNBmtJl9ZGaLou3yTO9VtCZNCkMX778/7kpERNotmyv0bcAl7j4UOAI4z8yGZmj3nLsPj7ZpOa0y34YNg0MOUbeLiJS0VgPd3de4+4vR803AcqBvvgsruEmTYN48WLky7kpERNqlTX3oZlZHWF90XobDR5rZYjN7wswOaua/n2JmDWbW0Fhs/dWnnx6m1tU86SJSorIOdDPbE3gQuMjdP047/CLwRXc/BPg58Eim93D36e5e7+71iUSivTXnR9++MGZM6HbRDIwiUoKyCnQzqyGE+b3u/lD6cXf/2N0/iZ4/DtSYWa+cVloIkybBa6+FrhcRkRKTzSgXA2YAy939+mba9I7aYWaHR++7PpeFFsSpp4a7R/XlqIiUoGyu0EcB3wTGpAxLHGtm55jZOVGbCcBSM1sM3AxMdC/Bfotu3WD8eLjvvrCQtIhICalurYG7Pw9YK21uAW7JVVGxmjQpjEd/6qkweZeISInQnaLpvvpVSCTgttvirkREpE0U6OlqamDKlDBZ1+rVcVcjIpI1BXom554LVVXw85/HXYmISNYU6Jnsuy+cdhrMmAEfpw+5FxEpTgr05lx0EWzaBDNnxl2JiEhWFOjNqa+HUaPgxhs1hFFESoICvSVTp8Kbb8Jdd8VdiYhIqxToLRk7Nlyp/+QnukoXkaKnQG+JGVxxBbzxhq7SRaToKdBbo6t0ESkRCvTWmMGVV4ar9FtvjbsaEZFmKdCzceKJcOyxofvlww/jrkZEJCMFejbM4Gc/gw0b4Oqr465GRCQjBXq2RoyAb30LbroJXn897mpERHahQG+Lq66C6upwF6mISJFRoLdF376hH332bHgk47KpIiKxyWYJuv5mNtfMXjGzZWZ2YYY2ZmY3m9kqM1tiZiPzU24RuOgiOPhgOP/8MNeLiEiRyOYKfRtwibsPBY4AzjOzoWltTgQGR9sU4Jc5rbKY1NSE4YvvvQeXXx53NSIin2s10N19jbu/GD3fBCwH+qY1Gw/c5cELQA8z65PzaovFEUfAOefAzTfDwoVxVyMiArSxD93M6oARwLy0Q32Bt1P232HX0MfMpphZg5k1NDY2tq3SYnP11bDPPnDWWfC//xt3NSIi2Qe6me0JPAhc5O7tWvXB3ae7e7271ycSifa8RfHo0QNuvx2WLQuzMoqIxCyrQDezGkKY3+vuD2Vo8i7QP2W/X/RaefvqV+GCC8LY9D/+Me5qRKTCZTPKxYAZwHJ3v76ZZrOBb0WjXY4APnL3NTmss3hdcw0MHQqTJ8P69XFXIyIVLJsr9FHAN4ExZrYo2saa2Tlmdk7U5nFgNbAK+DVwbn7KLUJdu8K998IHH4T+9B074q5IRCpUdWsN3P15wFpp48B5uSqq5AwfDjfcEMam/+QnGs4oIrHQnaK5cu658M1vhjtJH3887mpEpAIp0HPFDH71KzjkEDjzTHjttbgrEpEKo0DPpd13h4ceCuH+L/8CGzfGXZGIVBAFeq4NGBBCfdUqOPVU2LIl7opEpEIo0PNh9GiYOROefRa+8x1wj7siEakArY5ykXaaNCmsQ/qjH0G/flrpSETyToGeT5ddBm+/Df/5n2G8+o9+FHdFIlLGFOj5ZAa//CVs3hzGptfUwKWXxl2ViJQpBXq+7bYbzJgBW7eGSbw6dYKLL467KhEpQwr0QqiqgjvvDKF+ySVheoDvfS/uqkSkzCjQC6W6Osz5Ygb/9m9hIq+rrw77IiI5oEAvpJoamDULamvDLI0ffBD62Kv11yAiHackKbSqKvjFLyCRgP/4j3ClPmsWdOkSd2UiUuJ0Y1EczGDatLAwxsMPwz/+I6ypjOnjRSR/FOhxuuCCEOjLlkF9PTQ0xF2RiJQwBXrcvvY1+MtfQv/60UfDfffFXZGIlCgFejE4+GBYsAAOOwxOPx1++EPYvj3uqkSkxGSzpuhMM1tnZkubOT7azD5KWZ5Oy/W0RyIBzzwDU6aEqQKOPx7efz/uqkSkhGRzhX4HcEIrbZ5z9+HRNq3jZVWoTp3g1lvhjjvghRfC0nZz5sRdlYiUiFYD3d3/DGwoQC2SdNZZoQumtjZcqU+bpi4YEWlVrvrQjzSzxWb2hJkd1FwjM5tiZg1m1tDY2Jijjy5TBx0E8+fDGWfAj38Mxx0Hb74Zd1UiUsRyEegvAl9090OAnwOPNNfQ3ae7e7271ycSiRx8dJnbc0+46y64/XZ48UX40pfCvhbMEJEMOhzo7v6xu38SPX8cqDGzXh2uTAIzmDwZliwJfepnnQUTJoRpA0REUnQ40M2st1mYYcrMDo/ec31H31fS1NXB3Llw3XXw2GMwbBg8/njcVYlIEclm2OJvgL8CB5jZO2Z2tpmdY2bnRE0mAEvNbDFwMzDRXX0CeVFVFWZqXLAA9t4bTjoprFm6cWPclYlIEbC4sre+vt4bdKt7+23eDFdcAT/9aRjDfsst8PWvx12ViOSZmS109/pMx3SnaKnq3DncgDR/PvTpE/rVTz0V3nsv7spEJCYK9FI3cmQI9WuvhSeegKFD4de/1kgYkQqkQC8H1dXw/e+HkTAjRoTpA445Jgx1FJGKoUAvJ4MHh6kCZsyAFSvClLz/+q+wbl3clYlIASjQy40ZfPvbsHIlXHxxmBdm8GC4/nrYsiXu6kQkjxTo5ap7d/iv/4KlS2HUKLjkknCn6aOPqn9dpEwp0MvdAQeEG5D+8IewP24cHHWUZnEUKUMK9Eoxdmy4Wr/1VnjnnTDZ13HHwXPP6YpdpEwo0CtJTU0YAbNyJdxwA7z8chgNM2oU/P73sGNH3BWKSAco0CtRly5w0UXwxhvhDtM1a8LapsOGwcyZ8NlncVcoIu2gQK9ku+8O550XrthnzQorJp19NvTrF8a1r14dd4Ui0gYKdAk3Jp1+Orz0EvzpT3DssWGY4/77h773Rx7RkEeREqBAlyZmMGYM/O53YXWkyy+HRYvglFOgd2845xx4/nn1tYsUKQW6ZNa3b5jN8a23whwxY8fC3XfD0UfDoEFw2WWwfHncVYpICgW6tKy6Gk44Ae65B9auDaF+wAFwzTVhIrCRI8Osj8uXa/ijSMwU6JK9PfeESZPgySfh3XfhxhvDUMgf/jCE+4EHwg9+AH/9q7plRGKQzYpFM81snZktbea4mdnNZrbKzJaY2cjclylFp3dvuPBCmDcv3Kj0i1+EZfKuvz7cidq3b5hT5re/hQ0b4q5WpCJkc4V+B3BCC8dPBAZH2xTglx0vS0pK377w3e/CU09BYyPce2+4Yenhh+G008KKSkcdBVdeGX4AbN8ed8UiZSmrJejMrA54zN2HZTh2K/Csu/8m2l8BjHb3NS29p5agqwDbtoX1T598MmwLFoR+9tpaGD06bMceG7prdlPvn0g2WlqCrjoH798XeDtl/53otV0C3cymEK7i2W+//XLw0VLUqqvhyCPDduWV8MEH8Mwz4Up+7lx46KHQrlevpnAfPRqGDAlDKEWkTXIR6Flz9+nAdAhX6IX8bCkCvXrBxIlhgzD1wLPPhnCfOxceeCC8vvfeocvmqKPCPDMjRoQvX0WkRbkI9HeB/in7/aLXRFpWVweTJ4fNHV5/vSngn3++KeC7doXDDw/hPmoUfPnL0LNnfHWLFKlcBPps4Hwzuw/4MvBRa/3nIrswg4EDw/btb4fX3nsP/ud/mrZrr236QnXgwBDyhx0WHkeMgD32iK9+kSLQ6peiZvYbYDTQC1gL/BioAXD3X5mZAbcQRsL8Hfg/7t7qt536UlTa7NNPwxer8+eHbcGCcCcrhC9Vhw1rCvjDDgv76qqRMtPSl6JZjXLJBwW65MTatU0hn3xMjnvv3BkOOggOPnjnLZGIt2aRDlCgS+VI9sXPnw8NDbBkSdjWrm1q07t3WF81NeSHDAk/AESKnAJdZN26sEJTMuCXLIFly2Dz5nC8qipMXZAe9P36aQilFBUFukgm27bBqlU7h/ySJWHq4KQePUKwpwb9sGFhXhuRGCjQRdrio4/CgtqpIf/yy7BpU1Ob/v3DrJPpW//+uutV8irfd4qKlJfu3ZvGvCft2BGu3JPdNitWhO2uu3YO+q5dYfDgzGHfrVvh/yxSURToItnYbTcYMCBs48Y1ve4O77/fFPDJ7cUX4cEHd55GuHfvEOz/8A9Nj4MGhffs2rXwfyYpOwp0kY4wgz59wjZ69M7HNm+G117bNewffHDXKYX79Ak3Sw0a1HSDVXJ/n330xaxkRYEuki+dO4eZJIcO3fXYBx/AypVhiOXq1SH4V6+GOXPCqlCp32117bpryCef19Xp6l4+p0AXiUOvXmE78shdj332WeivTw365DZnTrhjNtW++4Zum/79M2+JhK7wK4QCXaTYdOnS9EVqOvewiEh60L/+erhT9uGHm8bWJ3XqFMbTZwr75Ou1tQr9MqBAFyklZmF64b33znx1nwz8t9/eeXvnnfD43HNhPdht23b+73bffdfQT9/v3r0wf0ZpNwW6SDlJDfxDD83cZvv2MBVCetgnt6efhjVrdl3oe6+9mg/7/v1D189ee+lKP0YKdJFKU1UVwnfffcPc8pls2xamL04P++S2aNHO8+MkdekSRuVks/XoofDPMQW6iOyquhr22y9szdmyJXTfJEP+vfdCyK9dG+bOeeut0K+/bt2uV/sQ+vb33nvXoM/0Ws+eugM3Cwp0EWmfTp2abrZqyY4dsH59U9hn2t5/HxYvDuG/deuu71FVFUbrZHPl36tX+IFUgSrzTy0ihbPbbiGME4kwsVlL3GHjxp2v9DP9AFixIjx+9tmu72EWQr214E9+19CpU37+3DHIKtDN7ATgJqAKuM3dr0k7Phn4KU1rid7i7rflsE4RqQRmYQhlbW2Yo74l7mEenZau/NeuhRdeCI/p4/eTvvCF8AOgZ8+w1dY2Pc+01daGUUFF2P/faqCbWRXw38DxwDvAAjOb7e6vpDW9393Pz0ONIiK7MgsTnnXrFiZEa82nnzZ/xb9+fdjWrAkzba5fD5980vx7de7ceuinv/aFL+S9Kyibdz8cWOXuqwGixaDHA+mBLiJSvPbYI7s+/6QtW8KcO8mwb2l75ZWmtulj/FP16BHC/dxz4eKLc/PnSpFNoPcF3k7ZfwfINNbp62Z2DPA34P+5+9vpDcxsCjAFYL+Wvj0XEYlbp05hhszevbP/b5LdQK39AGjLe7ZBrq7/HwV+4+6bzez/AncCY9Ibuft0YDqEBS5y9NkiIsUhtRso298EciibgZ3vAv1T9vvR9OUnAO6+3t2TE0jcBjRzi5qIiORLNoG+ABhsZgPMrBMwEZid2sDM+qTsjgOW565EERHJRqtdLu6+zczOB54iDFuc6e7LzGwa0ODus4ELzGwcsA3YAEzOY80iIpKBFokWESkhLS0SrckRRETKhAJdRKRMKNBFRMqEAl1EpEzE9qWomTUCb7bzP+8FfJDDcnKpWGtTXW2jutpGdbVde2v7orsnMh2ILdA7wswamvuWN27FWpvqahvV1Taqq+3yUZu6XEREyoQCXUSkTJRqoE+Pu4AWFGttqqttVFfbqK62y3ltJdmHLiIiuyrVK3QREUmjQBcRKRMlF+hmdoKZrTCzVWZ2acy1vGFmL5vZIjNriF6rNbOnzWxl9PiFAtQx08zWmdnSlNcy1mHBzdH5W2JmIwtc1xVm9m50zhaZ2diUY1OjulaY2VfzWFd/M5trZq+Y2TIzuzB6PdZz1kJdxXDOupjZfDNbHNV2ZfT6ADObF9VwfzTFNmbWOdpfFR2vK3Bdd5jZ6ynnbHj0esH+/UefV2VmL5nZY9F+fs+Xu5fMRpi+9zVgINAJWAwMjbGeN4Beaa9dB1waPb8UuLYAdRwDjASWtlYHMBZ4AjDgCGBegeu6AvhehrZDo7/PzsCA6O+5Kk919QFGRs/3IiybODTuc9ZCXcVwzgzYM3peA8yLzsVvgYnR678Cvhs9Pxf4VfR8ImER+ULWdQcwIUP7gv37jz7vYmAW8Fi0n9fzVWpX6J8vWO3uW4DkgtXFZDxhCT6ix6/l+wPd/c+EeeizqWM8cJcHLwA9bOcFSvJdV3PGA/e5+2Z3fx1YRfj7zkdda9z9xej5JsKCLH2J+Zy1UFdzCnnO3N0/iXZros0JS00+EL2efs6S5/IB4DgzswLW1ZyC/fs3s37ASYRV3Ij+/Hk9X6UW6JkWrG7pH3y+OfBHM1toYQFsgH3cfU30/H1gn3hKa7aOYjiH50e/7s5M6ZKKpa7oV9sRhCu7ojlnaXVBEZyzqPtgEbAOeJrwG8GH7p5c5j718z+vLTr+EdCzEHW5e/KcXRWdsxvMrHN6XRlqzrUbge8DO6L9nuT5fJVaoBebr7j7SOBE4DwzOyb1oIffn2IfF1osdUR+CQwChgNrgJ/FVYiZ7Qk8CFzk7h+nHovznGWoqyjOmbtvd/fhhHWFDwcOjKOOdOl1mdkwYCqhvsOAWuAHhazJzE4G1rn7wkJ+bqkFeqsLVheSu78bPa4DHib8I1+b/BUuelwXU3nN1RHrOXT3tdH/gDuAX9PURVDQusyshhCa97r7Q9HLsZ+zTHUVyzlLcvcPgbnAkYQui+RSlqmf/3lt0fHuwPoC1XVC1H3lHhavv53Cn7NRwDgze4PQNTwGuIk8n69SC/RWF6wuFDPbw8z2Sj4H/hlYGtVzVtTsLOD3cdTXQh2zgW9F3/YfAXyU0s2Qd2n9lacQzlmyronRt/0DgMHA/DzVYMAMYLm7X59yKNZz1lxdRXLOEmbWI3reFTie0Mc/F5gQNUs/Z8lzOQGYE/3WU4i6Xk35wWyEfurUc5b3v0t3n+ru/dy9jpBTc9z9TPJ9vnL5jW4hNsK31H8j9N9dFmMdAwkjDBYDy5K1EPq9/gSsBJ4BagtQy28Iv4pvJfTLnd1cHYRv9/87On8vA/UFruvu6HOXRP+I+6S0vyyqawVwYh7r+gqhO2UJsCjaxsZ9zlqoqxjO2cHAS1ENS4HLU/4/mE/4QvZ3QOfo9S7R/qro+MAC1zUnOmdLgXtoGglTsH//KTWOpmmUS17Pl279FxEpE6XW5SIiIs1QoIuIlAkFuohImVCgi4iUCQW6iEiZUKCLiJQJBbqISJn4/3O46c3r2A9wAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(clf.loss_curve_, c='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'MLPClassifier' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-e9b9d5a64175>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Plot for {} \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m111\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_curve_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'o'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'MLPClassifier' has no len()"
     ]
    }
   ],
   "source": [
    "for i in range(len(clf)):\n",
    "    print(\"Plot for {} \".format(clf[i]))\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(clf[i].loss_curve_, marker='o', label='Loss')\n",
    "    ax.set_xlabel(\"Epochs\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_title(\"Loss over Epochs with MLPClassifier\")\n",
    "\n",
    "    ax.legend()\n",
    "\n",
    "    #plt.savefig(\"plots/parte/relu_e-8.png\", dpi=1000, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(clf.loss_)\n",
    "#print(clf.intercepts_)\n",
    "#print(clf.coefs_)\n",
    "# print(clf.out_activation_)\n",
    "# print(clf.classes_)\n",
    "# print(clf.n_layers_)\n",
    "# prit(clf.n_outputs_)\n",
    "#clf.loss_curve_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'MLPClassifier' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-a879266afaf5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m111\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy with different MLP classifiers with\\n ReLU Activation\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'o'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Train Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'o'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Test Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'MLPClassifier' has no len()"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_title(\"Accuracy with different MLP classifiers with\\n ReLU Activation\")\n",
    "x=np.arange(len(clf))\n",
    "ax.plot(x, train_accuracy, marker='o', label='Train Accuracy')\n",
    "ax.plot(x, test_accuracy, marker='o', label='Test Accuracy')\n",
    "ax.set_xlabel(\"Classifier Type\")\n",
    "ax.set_ylabel(\"Accuracy (%)\")\n",
    "\n",
    "plt.legend()\n",
    "#plt.savefig(\"plots/parte/accuracy_mlp_relu.png\", dpi=1000, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(211)\n",
    "plt.title(\"Epochs/Time with MLP Classifier \\n ReLU Activation\")\n",
    "ax.plot(x[4:], epochs[4:], c='b', marker='o', label='#epochs')\n",
    "ax.set_xlabel(\"Classifier Type\")\n",
    "ax.set_ylabel(\"Epochs\")\n",
    "ax.legend()\n",
    "\n",
    "ax1 = fig.add_subplot(212)\n",
    "ax1.plot(x[4:], train_time[4:], c='b', marker='o', label='train time')\n",
    "ax1.set_xlabel(\"Classifier type\")\n",
    "ax1.set_ylabel(\"train time(sec)\")\n",
    "plt.legend()\n",
    "#plt.savefig(\"plots/parte/epochs_time_relu.png\", dpi=1000, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
