{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------Reading the Data-------------------------\n",
      "----------------Data Reading completed-------------------\n",
      "The total number of training samples = 13000\n",
      "The number of features = 784\n"
     ]
    }
   ],
   "source": [
    "print(\"----------------Reading the Data-------------------------\")\n",
    "PATH = os.getcwd()\n",
    "os.chdir('Alphabets/')\n",
    "\n",
    "X_train = pd.read_csv('train.csv', sep=',', header=None, index_col=False)\n",
    "X_test = pd.read_csv('test.csv', sep=',', header=None, index_col=False)\n",
    "\n",
    "train_class = X_train[X_train.columns[-1]]\n",
    "test_actual_class = X_test[X_test.columns[-1]]\n",
    "\n",
    "X_train = X_train.drop(X_train.columns[-1], axis=1)\n",
    "X_test = X_test.drop(X_test.columns[-1], axis=1)\n",
    "\n",
    "print(\"----------------Data Reading completed-------------------\")\n",
    "\n",
    "os.chdir('../')\n",
    "\n",
    "X_train = X_train/255\n",
    "X_test = X_test/255\n",
    "m = X_train.shape[0] # Number of Training Samples\n",
    "n = X_train.shape[1] # Number of input features\n",
    "\n",
    "print(\"The total number of training samples = {}\".format(m))\n",
    "print(\"The number of features = {}\".format(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Perform 1-hot encoding of class labels------------\n"
     ]
    }
   ],
   "source": [
    "#To get the one hot encoding of each label\n",
    "print(\"--------Perform 1-hot encoding of class labels------------\")\n",
    "\n",
    "train_class_enc = pd.get_dummies(train_class).to_numpy()\n",
    "test_actual_class_enc = pd.get_dummies(test_actual_class).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add the intercept term to the data samples both in training and test dataset\n",
    "X_train = np.hstack((np.ones((m,1)),X_train.to_numpy()))\n",
    "X_test = np.hstack((np.ones((X_test.shape[0],1)),X_test.to_numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "arch = [2,2,3] #means one hidden layer with 2 perceptrons \n",
    "# Run with 1000 units in one hidden layer arch shows no change in error with 0.1 lr and full dataset\n",
    "\n",
    "M = 100 # Mini-Batch Size\n",
    "r = np.max(train_class) + 1 # Default value of the number of classes = 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Theta Initialization \n",
    "theta = []\n",
    "\n",
    "for i in range(len(arch)+1):\n",
    "    if i == 0:\n",
    "        dim0=n+1\n",
    "        dim1=arch[i]\n",
    "    elif (i == len(arch)):\n",
    "        dim0=arch[i-1]\n",
    "        dim1 = r\n",
    "    else:\n",
    "        dim0=arch[i-1]\n",
    "        dim1= arch[i]\n",
    "        \n",
    "    #theta.append(2*np.random.random((dim0, dim1))-1)\n",
    "    theta.append(np.zeros((dim0, dim1)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "theta1 = np.random.random((n+1,arch[0]))\n",
    "theta2 = np.random.random((arch[0], arch[1]))\n",
    "theta3 = np.random.random((arch[1],r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(data, theta):\n",
    "    fm = []\n",
    "    fm.append(data)\n",
    "    for l in range(len(theta)):\n",
    "        fm.append(activation(np.dot(fm[l], theta[l])))\n",
    "    #l1 = activation(np.dot(l0, theta[0]))\n",
    "    #l2 = activation(np.dot(l1, theta[1]))\n",
    "    return fm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm = forward_prop(X_train, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_total(X, theta, Y, m):\n",
    "    fm = forward_prop(X, theta)\n",
    "    cost = (1/(2*m))*np.sum((Y-fm[-1])**2)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.25"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost_total(X_train, theta, train_class_enc, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       ...,\n",
      "       [0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.]]), array([[0., 0.],\n",
      "       [0., 0.]]), array([[0., 0., 0.],\n",
      "       [0., 0., 0.]]), array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])]\n"
     ]
    }
   ],
   "source": [
    "print(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error = 3.25\n",
      "Error = 3.0004761152782558\n",
      "Error = 2.766758010558254\n",
      "Error = 2.5433632964699298\n",
      "Error = 2.3280665121291917\n",
      "Error = 2.121096100285749\n",
      "Error = 1.9242912456150978\n",
      "Error = 1.7402108005022272\n",
      "Error = 1.57127234374041\n",
      "Error = 1.4190818562162617\n",
      "Error = 1.2841404424262488\n",
      "Error = 1.1660103366874506\n",
      "Error = 1.0637858797189979\n",
      "Error = 0.9764754003172456\n",
      "Error = 0.902998570668764\n",
      "Error = 0.8419927560810644\n",
      "Error = 0.7917948954895361\n",
      "Error = 0.7506299580534488\n",
      "Error = 0.7168224829740912\n",
      "Error = 0.688920549219516\n",
      "Error = 0.6657309383797322\n",
      "Error = 0.6463022049903381\n",
      "Error = 0.6298871438238185\n",
      "Error = 0.6159022849197531\n",
      "Error = 0.6038919200004105\n",
      "Error = 0.5934987725450422\n",
      "Error = 0.5844410781676542\n",
      "Error = 0.5764950491038525\n",
      "Error = 0.5694815837638428\n",
      "Error = 0.5632562194692143\n",
      "Error = 0.5577015250862665\n",
      "Error = 0.5527213175803376\n",
      "Error = 0.5482362406403849\n",
      "Error = 0.5441803627628532\n",
      "Error = 0.540498541620986\n",
      "Error = 0.5371443675883979\n",
      "Error = 0.5340785477103381\n",
      "Error = 0.5312676268539933\n",
      "Error = 0.5286829687354987\n",
      "Error = 0.5262999386096928\n",
      "Error = 0.5240972435047508\n",
      "Error = 0.5220563963491346\n",
      "Error = 0.520161278153428\n",
      "Error = 0.5183977782814614\n",
      "Error = 0.5167534972844149\n",
      "Error = 0.5152175001488797\n",
      "Error = 0.5137801103951743\n",
      "Error = 0.5124327374533808\n",
      "Error = 0.5111677312873119\n",
      "Error = 0.5099782594389297\n",
      "Error = 0.5088582026079811\n",
      "Error = 0.5078020656241943\n",
      "Error = 0.50680490125762\n",
      "Error = 0.5058622447811545\n",
      "Error = 0.5049700575741307\n",
      "Error = 0.5041246783572941\n",
      "Error = 0.5033227808929569\n",
      "Error = 0.502561337181689\n",
      "Error = 0.5018375853478968\n",
      "Error = 0.5011490015383634\n",
      "Error = 0.5004932752660589\n",
      "Error = 0.49986828772076447\n",
      "Error = 0.49927209264194566\n",
      "Error = 0.4987028994106771\n",
      "Error = 0.49815905806859184\n",
      "Error = 0.4976390460146188\n",
      "Error = 0.4971414561661827\n",
      "Error = 0.49666498640177875\n",
      "Error = 0.4962084301273413\n",
      "Error = 0.4957706678304546\n",
      "Error = 0.4953506595047973\n",
      "Error = 0.49494743784284007\n",
      "Error = 0.49456010210816465\n",
      "Error = 0.49418781261019085\n",
      "Error = 0.49382978571389535\n",
      "Error = 0.4934852893255447\n",
      "Error = 0.4931536388027269\n",
      "Error = 0.49283419324325406\n",
      "Error = 0.49252635211294066\n",
      "Error = 0.49222955217699743\n",
      "Error = 0.49194326470387156\n",
      "Error = 0.4916669929139607\n",
      "Error = 0.49140026964874023\n",
      "Error = 0.49114265523858336\n",
      "Error = 0.49089373554995586\n",
      "Error = 0.4906531201947627\n",
      "Error = 0.490420440886482\n",
      "Error = 0.4901953499293525\n",
      "Error = 0.4899775188283211\n",
      "Error = 0.48976663700872725\n",
      "Error = 0.48956241063583383\n",
      "Error = 0.48936456152531344\n",
      "Error = 0.4891728261366777\n",
      "Error = 0.4889869546424474\n",
      "Error = 0.48880671006654175\n",
      "Error = 0.48863186748601223\n",
      "Error = 0.48846221329079414\n",
      "Error = 0.48829754449666096\n",
      "Error = 0.4881376681070085\n",
      "Error = 0.4879824005195002\n",
      "Error = 0.4878315669739739\n",
      "Error = 0.4876850010383211\n",
      "Error = 0.48754254412936315\n",
      "Error = 0.48740404506598833\n",
      "Error = 0.487269359652076\n",
      "Error = 0.4871383502869294\n",
      "Error = 0.4870108856011466\n",
      "Error = 0.48688684011602723\n",
      "Error = 0.48676609392477355\n",
      "Error = 0.48664853239389433\n",
      "Error = 0.4865340458833436\n",
      "Error = 0.48642252948404846\n",
      "Error = 0.4863138827715939\n",
      "Error = 0.4862080095749213\n",
      "Error = 0.4861048177589978\n",
      "Error = 0.486004219020487\n",
      "Error = 0.48590612869553473\n",
      "Error = 0.4858104655788441\n",
      "Error = 0.4857171517532826\n",
      "Error = 0.48562611242931997\n",
      "Error = 0.4855372757936473\n",
      "Error = 0.48545057286637344\n",
      "Error = 0.4853659373662488\n",
      "Error = 0.48528330558339305\n",
      "Error = 0.4852026162590521\n",
      "Error = 0.48512381047194014\n",
      "Error = 0.485046831530752\n",
      "Error = 0.4849716248724628\n",
      "Error = 0.4848981379660599\n",
      "Error = 0.48482632022137157\n",
      "Error = 0.484756122902688\n",
      "Error = 0.4846874990468795\n",
      "Error = 0.4846204033857523\n",
      "Error = 0.4845547922723823\n",
      "Error = 0.4844906236112009\n",
      "Error = 0.48442785679160744\n",
      "Error = 0.48436645262490946\n",
      "Error = 0.4843063732843972\n",
      "Error = 0.48424758224837566\n",
      "Error = 0.48419004424598383\n",
      "Error = 0.4841337252056499\n",
      "Error = 0.48407859220603033\n",
      "Error = 0.4840246134293002\n",
      "Error = 0.48397175811666165\n",
      "Error = 0.4839199965259543\n",
      "Error = 0.48386929989124966\n",
      "Error = 0.4838196403843244\n",
      "Error = 0.48377099107791677\n",
      "Error = 0.4837233259106624\n",
      "Error = 0.48367661965363234\n",
      "Error = 0.48363084787838023\n",
      "Error = 0.4835859869264269\n",
      "Error = 0.48354201388010615\n",
      "Error = 0.4834989065346997\n",
      "Error = 0.4834566433718015\n",
      "Error = 0.483415203533845\n",
      "Error = 0.4833745667997348\n",
      "Error = 0.4833347135615322\n",
      "Error = 0.4832956248021377\n",
      "Error = 0.4832572820739247\n",
      "Error = 0.483219667478277\n",
      "Error = 0.48318276364598517\n",
      "Error = 0.48314655371846216\n",
      "Error = 0.4831110213297387\n",
      "Error = 0.4830761505892013\n",
      "Error = 0.483041926065036\n",
      "Error = 0.48300833276834787\n",
      "Error = 0.48297535613792236\n",
      "Error = 0.4829429820256009\n",
      "Error = 0.4829111966822391\n",
      "Error = 0.482879986744224\n",
      "Error = 0.4828493392205245\n",
      "Error = 0.48281924148024885\n",
      "Error = 0.48278968124068916\n",
      "Error = 0.4827606465558283\n",
      "Error = 0.48273212580529135\n",
      "Error = 0.4827041076837203\n",
      "Error = 0.4826765811905537\n",
      "Error = 0.48264953562019347\n",
      "Error = 0.48262296055254306\n",
      "Error = 0.48259684584390034\n",
      "Error = 0.4825711816181883\n",
      "Error = 0.4825459582585124\n",
      "Error = 0.4825211663990285\n",
      "Error = 0.4824967969171085\n",
      "Error = 0.48247284092579157\n",
      "Error = 0.48244928976650914\n",
      "Error = 0.4824261350020724\n",
      "Error = 0.48240336840990994\n",
      "Error = 0.4823809819755496\n",
      "Error = 0.4823589678863264\n",
      "Error = 0.4823373185253178\n",
      "Error = 0.4823160264654881\n",
      "Error = 0.4822950844640394\n",
      "Error = 0.4822744854569576\n",
      "Error = 0.482254222553747\n",
      "Error = 0.4822342890323452\n",
      "Error = 0.48221467833421455\n",
      "Error = 0.4821953840595969\n",
      "Error = 0.4821763999629309\n"
     ]
    }
   ],
   "source": [
    "for it in range(2000):\n",
    "    delta = [None]*len(fm)\n",
    "    # Forward Propagation generic\n",
    "    fm = forward_prop(X_train, theta)\n",
    "    if (it % 10 == 0):\n",
    "        print(\"Error = \"+str(cost_total(X_train, theta, train_class_enc, m)))\n",
    "        #print(\"Error = \"+str(np.mean(np.abs(train_class_enc-l2))))\n",
    "\n",
    "    #Backward Propagation generic\n",
    "    \n",
    "    #delta_output = (1/m)*(train_class_enc - fm[-1])*fm[-1]*(1-fm[-1])\n",
    "\n",
    "    for l in range(len(fm)-1, 0, -1):\n",
    "        if (l == len(fm)-1):\n",
    "            delta[l] = ((1/m)*(train_class_enc - fm[l])*fm[l]*(1-fm[l]))\n",
    "        else:\n",
    "            delta[l]=(np.dot(delta[l+1], theta[l].T)*fm[l]*(1-fm[l]))\n",
    "        \n",
    "    #delta_l1 = np.dot(delta_l2, theta2.T)*l1*(1-l1)\n",
    "\n",
    "    for t in range(len(theta)):\n",
    "        theta[t] += lr*np.dot(fm[t].T, delta[t+1])\n",
    "#     theta1 += lr*np.dot(l0.T, delta_l1)\n",
    "#     theta2 += lr*np.dot(l1.T, delta_l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for it in range(20000):\n",
    "    # Forward Propagation\n",
    "    l0 = X_train\n",
    "    l1 = activation(np.dot(l0, theta1))\n",
    "    l2 = activation(np.dot(l1, theta2))\n",
    "\n",
    "    if (it % 1000 == 0):\n",
    "        print(\"Error = \"+str(cost_total(X_train, [theta1, theta2], train_class_enc, m)))\n",
    "        #print(\"Error = \"+str(np.mean(np.abs(train_class_enc-l2))))\n",
    "\n",
    "    #Backward Propagation\n",
    "    delta_l2 = (1/m)*(train_class_enc - l2)*l2*(1-l2)\n",
    "\n",
    "    delta_l1 = np.dot(delta_l2, theta2.T)*l1*(1-l1)\n",
    "\n",
    "    theta1 += lr*np.dot(l0.T, delta_l1)\n",
    "    theta2 += lr*np.dot(l1.T, delta_l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Test Accuracy of the model = 3.8461538461538463%\n"
     ]
    }
   ],
   "source": [
    "pred_class = forward_prop(X_test, theta)\n",
    "test_pred_class = pred_class[-1]\n",
    "for i in range(len(test_pred_class)):\n",
    "    test_pred_class[i][test_pred_class[i] == np.max(test_pred_class[i])] = 1\n",
    "    test_pred_class[i][test_pred_class[i] != np.max(test_pred_class[i])] = 0\n",
    "    \n",
    "    \n",
    "test_acc = 0\n",
    "for i in range(len(test_actual_class_enc)):\n",
    "    if (np.array_equal(test_pred_class[i], test_actual_class_enc[i])):\n",
    "        test_acc+=1\n",
    "test_acc /= X_test.shape[0]\n",
    "\n",
    "print(\"The Test Accuracy of the model = {}%\".format(test_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
