{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the training set is =  1600000\n",
      "The number of classes (label=4) = 800000\n",
      "The number of classes (label=0) = 800000\n",
      "The length of the test set is =  359\n"
     ]
    }
   ],
   "source": [
    "#Importing Data from the CSV file\n",
    "%matplotlib inline\n",
    "import string\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "f_train = open(\"training.1600000.processed.noemoticon.csv\", \"r\", encoding=\"ISO-8859-1\")\n",
    "X_train = f_train.readlines()\n",
    "f_test = open(\"testdata.manual.2009.06.14.csv\", \"r\")\n",
    "X_test = f_test.readlines()\n",
    "\n",
    "#Creating the List with just the tweets and finding the number of positive and negative classes. (TRAINING) \n",
    "#class_0 = number of classes with label = 0\n",
    "#class_4 = number of classes with label = 4\n",
    "\n",
    "class_0=class_4=0\n",
    "tweets_train =[]\n",
    "for x in X_train:\n",
    "    a = x.split('\",\"')\n",
    "    if (a[0] == '\"0'): class_0+=1\n",
    "    else: class_4+=1\n",
    "    tweets_train.append('%s\"|\"%s' % (a[0],a[-1]))\n",
    "\n",
    "#Creating the List with just the tweets (TEST DATA)\n",
    "tweets_test =[]\n",
    "for x in X_test:\n",
    "    a = x.split('\",\"')\n",
    "    tweets_test.append('%s\"|\"%s' % (a[0], a[-1]))\n",
    "    \n",
    "print(\"The length of the training set is = \", len(X_train))\n",
    "print(\"The number of classes (label=4) =\", class_4)\n",
    "print(\"The number of classes (label=0) =\", class_0)\n",
    "print(\"The length of the test set is = \", len(X_test))\n",
    "\n",
    "f_test.close()\n",
    "f_train.close()\n",
    "\n",
    "m= len(X_train)\n",
    "phi_0 = class_0/m\n",
    "phi_4 = class_4/m"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Part(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "from joblib import parallel_backend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets_train_pd = pd.read_pickle('tweet_train_lemma.pkl')\n",
    "#tweets_test_pd = pd.read_pickle('tweet_test_lemma.pkl')\n",
    "train_actual_classes = pd.read_pickle('train_class.pkl')\n",
    "test_actual_classes = pd.read_pickle('test_class.pkl')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TF-IDF - Multinomial Naive Bayes using Library - 1-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_train_pd = pd.Series(tweets_train)\n",
    "tweets_train_pd = tweets_train_pd.apply(lambda x: x.split(\"|\")[1])\n",
    "tweets_test_pd = pd.Series(tweets_test)\n",
    "tweets_test_pd = tweets_test_pd.apply(lambda x: x.split(\"|\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classes = train_actual_classes.apply(lambda x: 0 if x=='\"0\"' else 1)\n",
    "test_classes = test_actual_classes.apply(lambda x: 0 if x=='\"0\"' else 1)\n",
    "all_classes = train_classes.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(analyzer=\"word\", stop_words='english',sublinear_tf=True)\n",
    "tfidf_train = vec.fit_transform(tweets_train_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=[0.5, 0.5], fit_prior=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = MultinomialNB(class_prior=[phi_0,phi_4])\n",
    "nb.fit(tfidf_train, list(train_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_test = vec.transform(tweets_test_pd)\n",
    "test_class_pred = nb.predict(tfidf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accu(test_actual_classes, test_class_pred):\n",
    "    test_acc=0\n",
    "    for i in range(len(test_actual_classes)):\n",
    "        if (list(test_class_pred)[i]==list(test_actual_classes)[i]):\n",
    "            test_acc+=1\n",
    "    return(test_acc/len(test_actual_classes))*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Test Accuracy of the MultinomialNB model trained with TF-IDF is =  78.55153203342618\n"
     ]
    }
   ],
   "source": [
    "print(\"The Test Accuracy of the MultinomialNB model trained with TF-IDF is = \", test_accu(test_classes, test_class_pred))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TF-IDF - Multinomial Naive Bayes using Library - bigrams and unigrams together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(analyzer=\"word\", stop_words='english',sublinear_tf=True, ngram_range=(1,2))\n",
    "tfidf_train = vec.fit_transform(tweets_train_pd)ravind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=[0.5, 0.5], fit_prior=True)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = MultinomialNB(class_prior=[phi_0,phi_4])\n",
    "nb.fit(tfidf_train, list(train_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_test = vec.transform(tweets_test_pd)\n",
    "test_class_pred = nb.predict(tfidf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Test Accuracy of the MultinomialNB model trained with TF-IDF and bigrams is=  82.72980501392759\n"
     ]
    }
   ],
   "source": [
    "print(\"The Test Accuracy of the MultinomialNB model trained with TF-IDF and bigrams is= \", nb.score(tfidf_test, test_classes)*100)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TF-IDF - Multinomial Naive Bayes using Library - trigrams and unigrams together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(analyzer=\"word\", stop_words='english',sublinear_tf=True, ngram_range=(1,3))\n",
    "tfidf_train = vec.fit_transform(tweets_train_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=[0.5, 0.5], fit_prior=True)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = MultinomialNB(class_prior=[phi_0,phi_4])\n",
    "nb.fit(tfidf_train, list(train_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_test = vec.transform(tweets_test_pd)\n",
    "test_class_pred = nb.predict(tfidf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Test Accuracy of the MultinomialNB model trained with TF-IDF with trigrams is =  82.45125348189416\n"
     ]
    }
   ],
   "source": [
    "print(\"The Test Accuracy of the MultinomialNB model trained with TF-IDF with trigrams is = \", nb.score(tfidf_test, test_classes)*100)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TF-IDF - Multinomial Naive Bayes using Library - using max features and cut features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(analyzer=\"word\", stop_words='english',sublinear_tf=True, min_df=0.001)\n",
    "\n",
    "tfidf_train = vec.fit_transform(tweets_train_pd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1600000x937 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 6503097 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=[0.5, 0.5], fit_prior=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = MultinomialNB(class_prior=[phi_0,phi_4])\n",
    "nb.fit(tfidf_train, list(train_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_test = vec.transform(tweets_test_pd)\n",
    "test_class_pred = nb.predict(tfidf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Test Accuracy of the MultinomialNB model trained with TF-IDF with min_df is =  77.99442896935933\n"
     ]
    }
   ],
   "source": [
    "print(\"The Test Accuracy of the MultinomialNB model trained with TF-IDF with min_df is = \", nb.score(tfidf_test, test_classes)*100)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TFIDF - Using Gaussian Naive Bayes - And using minibatch training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_minibatch(tweets, classes, batch_size):\n",
    "    l = len(tweets)\n",
    "    for i in range(0, l, batch_size):\n",
    "        X_batch=tweets[i:min(i + batch_size, l)]\n",
    "        Y_batch=classes[i:min(i + batch_size, l)]\n",
    "        yield X_batch,Y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gauss = GaussianNB()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Only using Hashing Vectorizer that will create a sparse matrix of token occurrences and store the counts. The ocunts will be normalized with euclidean distance or l2 norm here. \n",
    "#Training takes 4 hours for batch size = 1000\n",
    "Hashing Vectorizer used only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create minibatches\n",
    "vec = HashingVectorizer(analyzer=\"word\", stop_words='english',norm='l2')\n",
    "X = vec.transform(tweets_train_pd)\n",
    "\n",
    "for (X_batch, Y_batch) in create_minibatch(tweets_train_pd, train_classes, 1000):\n",
    "    tfidf_train = vec.transform(X_batch)\n",
    "    model_gauss.partial_fit(tfidf_train.toarray(), Y_batch, classes=all_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = vec.transform(tweets_test_pd)\n",
    "test_accuracy = model_gauss.score(X_test.toarray(), test_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Test Accuracy of the GaussianNB model trained with Hashing Vectorizer and batch size=1000 is = 55.98885793871866%\n"
     ]
    }
   ],
   "source": [
    "print(\"The Test Accuracy of the GaussianNB model trained with Hashing Vectorizer and batch size=1000 is = {}%\".format(test_accuracy*100))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Only using Hashing Vectorizer that will create a sparse matrix of token occurrences and store the counts. The ocunts will be normalized with euclidean distance or l2 norm here. \n",
    "#Training takes 4 hours for batch size = 10000 (memory error on using default features)\n",
    "So tried with 2**18 features - 32 GB RAM system hangs\n",
    "Hashing Vectorizer used only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = HashingVectorizer(analyzer=\"word\", stop_words='english',norm='l2')\n",
    "X = vec.transform(tweets_train_pd)\n",
    "print(X.shape)\n",
    "for (X_batch, Y_batch) in create_minibatch(tweets_train_pd, train_classes, 1000):\n",
    "    tfidf_train = vec.transform(X_batch)\n",
    "    print(tfidf_train.shape)\n",
    "    model_gauss.partial_fit(tfidf_train.toarray(), Y_batch, classes=all_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create minibatches\n",
    "import time\n",
    "start = time.time()\n",
    "vec = HashingVectorizer(analyzer=\"word\", stop_words='english',norm='l2', n_features=2**18)\n",
    "#X = vec.transform(tweets_train_pd)\n",
    "\n",
    "for (X_batch, Y_batch) in create_minibatch(tweets_train_pd, train_classes, 1000):\n",
    "    tfidf_train = vec.transform(X_batch)\n",
    "    #print(tfidf_train.shape)\n",
    "    model_gauss.partial_fit(tfidf_train.toarray(), Y_batch, classes=all_classes)\n",
    "\n",
    "print(\"The time taken for training on batch size=10000 using hashing vect only=\", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ayushi/.local/lib/python3.6/site-packages/sklearn/naive_bayes.py:450: RuntimeWarning: invalid value encountered in log\n",
      "  n_ij = - 0.5 * np.sum(np.log(2. * np.pi * self.sigma_[i, :]))\n"
     ]
    }
   ],
   "source": [
    "X_test = vec.transform(tweets_test_pd)\n",
    "test_accuracy = model_gauss.score(X_test.toarray(), test_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The Test Accuracy of the GaussianNB model trained with Hashing Vectorizer and batch size=10000 is = {}%\".format(test_accuracy*100))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Using Count Vectorizer with TFIDF transform\n",
    "that will create a sparse matrix of token  counts. \n",
    "Then we apply TFIDF transform on the "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_count = tfidf_vec.fit(count_vec.fit_transform(tweets_train_pd[1000:2000]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_count.idf_.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ravindcv = count_vec.transform(tweets_train_pd)\n",
    "tf_idf_vec= X_count.transform(cv)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tf_idf_vec.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "feature_names=count_vec.get_feature_names()\n",
    "first_document_vector=tf_idf_vec[1]\n",
    " \n",
    "#print the scores\n",
    "df = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=[\"tfidf\"])\n",
    "#df.sort_values(by=[\"tfidf\"],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time taken for training on batch size=100 using tfidf= 8480.386967897415\n"
     ]
    }
   ],
   "source": [
    "#Create minibatches\n",
    "\n",
    "count_vec = CountVectorizer(analyzer=\"word\", stop_words='english')\n",
    "tfidf_vec = TfidfTransformer(use_idf=True, smooth_idf=True, sublinear_tf=True)\n",
    "#X_count = tfidf_vec.fit_transform(count_vec.fit_transform(tweets_train_pd))\n",
    "tfidf_transform = (tfidf_vec.fit(count_vec.fit_transform(tweets_train_pd)))\n",
    "\n",
    "start=time.time()\n",
    "for (X_batch, Y_batch) in create_minibatch(tweets_train_pd, train_classes, 1000):\n",
    "\n",
    "    count_batch=count_vec.transform(X_batch)\n",
    "    tfidf_train_g=tfidf_transform.transform(count_batch)\n",
    "    model_gauss.partial_fit(tfidf_train_g.toarray(), Y_batch, classes=all_classes)\n",
    "\n",
    "print(\"The time taken for training on batch size=100 using tfidf=\", time.time() - start)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = tfidf_transform.transform(count_vec.transform(tweets_test_pd))\n",
    "test_accuracy = model_gauss.score(X_test.toarray(), test_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Test Accuracy of the GaussianNB model trained with TFIDF and batch size=100 is = 51.25348189415042%\n"
     ]
    }
   ],
   "source": [
    "print(\"The Test Accuracy of the GaussianNB model trained with TFIDF and batch size=100 is = {}%\".format(test_accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Test Accuracy of the GaussianNB model trained with TFIDF and batch size=1000 is = 51.25348189415042%\n"
     ]
    }
   ],
   "source": [
    "print(\"The Test Accuracy of the GaussianNB model trained with TFIDF and batch size=1000 is = {}%\".format(test_accuracy*100))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Randomize data before using minibatches to try to improve Accuracy\n",
    "Accuracy achieved with Count Vectorizer + TFIDF Transform = 51.25% with batch sizes=1000/100\n",
    "Accuract with shuffled batches = 50.97%\n",
    "\n",
    "Now try min_df=0.001 with TFIDF_vectorizer - Accuracy = 77.71% (features=937\n",
    "Gives memory error at min-df=0.0001, features=5972 - try with batches size=1000 - Accuracy = 74.65%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_shuffled = pd.concat([tweets_train_pd, train_classes], axis=1).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_train = list(tweets_shuffled[0])\n",
    "train_classes = list(tweets_shuffled[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vec = TfidfVectorizer(analyzer=\"word\", stop_words='english',sublinear_tf=True, min_df=0.001)\n",
    "tfidf_train = tfidf_vec.fit_transform(tweets_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gauss.fit(tfidf_train.toarray(), train_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7771587743732591\n"
     ]
    }
   ],
   "source": [
    "X_test = tfidf_vec.transform(tweets_test_pd)\n",
    "test_accuracy = model_gauss.score(X_test.toarray(), test_classes)\n",
    "print(test_accuracy) #min-df=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vec = CountVectorizer(analyzer=\"word\", stop_words='english', min_df=0.001)\n",
    "tfidf_vec = TfidfTransformer(use_idf=True, smooth_idf=True, sublinear_tf=True)\n",
    "X_count = tfidf_vec.fit_transform(count_vec.fit_transform(tweets_train))\n",
    "#tfidf_transform = (tfidf_vec.fit(count_vec.fit_transform(tweets_train)))\n",
    "model_gauss.fit(X_count.toarray(), train_classes)\n",
    "#Also gives 77.71% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Create minibatches\n",
    "\n",
    "count_vec = CountVectorizer(analyzer=\"word\", stop_words='english', min_df=0.00001)\n",
    "tfidf_vec = TfidfTransformer(use_idf=True, smooth_idf=True, sublinear_tf=True)\n",
    "#X_count = tfidf_vec.fit_transform(count_vec.fit_transform(tweets_train_pd))\n",
    "tfidf_transform = (tfidf_vec.fit(count_vec.fit_transform(tweets_train)))\n",
    "\n",
    "start=time.time()\n",
    "for (X_batch, Y_batch) in create_minibatch(tweets_train, train_classes, 1000):\n",
    "\n",
    "    count_batch=count_vec.transform(X_batch)\n",
    "    tfidf_train_g=tfidf_transform.transform(count_batch)\n",
    "    model_gauss.partial_fit(tfidf_train_g.toarray(), Y_batch, classes=all_classes)\n",
    "\n",
    "print(\"The time taken for training on batch size=10000 using tfidf=\", time.time() - start)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = tfidf_transform.transform(count_vec.transform(tweets_test_pd))\n",
    "test_accuracy = model_gauss.score(X_test.toarray(), test_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Test Accuracy of the GaussianNB model trained with TFIDF and batch size=10000 is = 74.65181058495823%\n"
     ]
    }
   ],
   "source": [
    "print(\"The Test Accuracy of the GaussianNB model trained with TFIDF and batch size=10000 is = {}%\".format(test_accuracy*100))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "b) Using SelectPercentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(analyzer=\"word\", stop_words='english',sublinear_tf=True, ngram_range=(1,1))\n",
    "tfidf_train = vec.fit_transform(tweets_train_pd)\n",
    "tfidf_train_new = SelectPercentile(chi2, percentile=50).fit_transform(tfidf_train, train_classes)\n",
    "tfidf_train_new"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Parallelize code for minibatch training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-11-e3be00bc68e6>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-e3be00bc68e6>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    # Your scikit-learn code here\u001b[0m\n\u001b[0m                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with parallel_backend('threading', n_jobs=2):\n",
    "    # Your scikit-learn code here"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tfidf_vec= TfidfTransformer(sublinear_tf=True)\n",
    "tfidf_vec.transform(X)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "pipe = Pipeline([('hash', HashingVectorizer(analyzer=\"word\", stop_words='english',norm='l2')),\n",
    "            ('tfidf', TfidfTransformer())]).fit(tweets_train_pd)\n",
    "#model.fit(tfidf_train, list(train_classes))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pipe['hash'].transform(tweets_test_pd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
