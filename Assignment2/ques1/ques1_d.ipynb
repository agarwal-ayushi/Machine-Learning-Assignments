{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This code contains solutions for Question 1 Part D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ayushi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ayushi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the training set is =  1600000\n",
      "The number of classes (label=4) = 800000\n",
      "The number of classes (label=0) = 800000\n",
      "The length of the test set is =  359\n"
     ]
    }
   ],
   "source": [
    "#Importing Data from the CSV file\n",
    "%matplotlib inline\n",
    "\n",
    "# All imports\n",
    "import string\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "\n",
    "f_train = open(\"training.1600000.processed.noemoticon.csv\", \"r\", encoding=\"ISO-8859-1\")\n",
    "X_train = f_train.readlines()\n",
    "f_test = open(\"testdata.manual.2009.06.14.csv\", \"r\")\n",
    "X_test = f_test.readlines()\n",
    "\n",
    "#Creating the List with just the tweets and finding the number of positive and negative classes. (TRAINING) \n",
    "#class_0 = number of classes with label = 0\n",
    "#class_4 = number of classes with label = 4\n",
    "\n",
    "class_0=class_4=0\n",
    "tweets_train =[]\n",
    "for x in X_train:\n",
    "    a = x.split('\",\"')\n",
    "    if (a[0] == '\"0'): class_0+=1\n",
    "    else: class_4+=1\n",
    "    tweets_train.append('%s\"|\"%s' % (a[0],a[-1]))\n",
    "\n",
    "#Creating the List with just the tweets (TEST DATA)\n",
    "tweets_test =[]\n",
    "for x in X_test:\n",
    "    a = x.split('\",\"')\n",
    "    tweets_test.append('%s\"|\"%s' % (a[0], a[-1]))\n",
    "    \n",
    "print(\"The length of the training set is = \", len(X_train))\n",
    "print(\"The number of classes (label=4) =\", class_4)\n",
    "print(\"The number of classes (label=0) =\", class_0)\n",
    "print(\"The length of the test set is = \", len(X_test))\n",
    "\n",
    "f_test.close()\n",
    "f_train.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating pandas series for the training data and the test data and create series for actual classes\n",
    "\n",
    "tweets_train_pd = pd.Series(tweets_train)\n",
    "tweets_test_pd = pd.Series(tweets_test)\n",
    "\n",
    "train_classes = tweets_train_pd.apply(lambda x: (x.split(\"|\")[0]))\n",
    "test_actual_classes = tweets_test_pd.apply(lambda x: (x.split(\"|\")[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------Processing Raw Data for stopwords removal, stemming and lemmatization-------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"------------------Processing Raw Data for stopwords removal, stemming and lemmatization-------------\\n\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "#Tokenize the Training data to remove twitter handles\n",
    "tweets_train_pd['tokenize_data'] = tweets_train_pd.apply(lambda x: '\\n'.join([' '.join(token) for token in ([tknzr.tokenize(x.split(\"|\")[1])])]))\n",
    "tweets_test_pd['tokenize_data'] = tweets_test_pd.apply(lambda x: '\\n'.join([' '.join(token) for token in ([tknzr.tokenize(x.split(\"|\")[1])])]))\n",
    "\n",
    "#Import regex library in python to search for regular expressions of a certain type to replace hem\n",
    "tweets_train_pd['noURL'] = tweets_train_pd['tokenize_data'].replace(r'http\\S+', '', regex=True).replace(r'www\\.\\S+', '', regex=True)\n",
    "tweets_test_pd['noURL'] = tweets_test_pd['tokenize_data'].replace(r'http\\S+', '', regex=True).replace(r'www\\.\\S+', '', regex=True)\n",
    "#create Train dataset Clean\n",
    "\n",
    "tweets_train_pd['noEnter']=tweets_train_pd['noURL'].str.rstrip(\"\\n\\r\")\n",
    "tweets_train_pd['clean_data'] = tweets_train_pd['noEnter'].str.translate(str.maketrans('','',string.punctuation)).str.lower().str.split()\n",
    "#Use lambda function to apply the same expression on all the strings together. \n",
    "#function takes one string at a time and returns a string wihtout stop words\n",
    "stop_words=set(stopwords.words('english'))\n",
    "tweets_train_pd['no_stop_words'] = tweets_train_pd['clean_data'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "tweets_train_pd['lemma1'] = tweets_train_pd['no_stop_words'].apply(lambda x: [wordnet_lemmatizer.lemmatize(word, pos =\"n\") for word in x])\n",
    "tweets_train_pd['lemma2'] = tweets_train_pd['lemma1'].apply(lambda x: [wordnet_lemmatizer.lemmatize(word, pos =\"v\") for word in x])\n",
    "tweets_train_pd['lemma'] = tweets_train_pd['lemma2'].apply(lambda x: [wordnet_lemmatizer.lemmatize(word, pos =\"a\") for word in x])\n",
    "\n",
    "print(\"\\n Time taken to clean the data using lemmatization, stemming and stop words removal = {:2.3f} sec\".format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create clean test data\n",
    "\n",
    "tweets_test_pd['noEnter']=tweets_test_pd['noURL'].str.rstrip(\"\\n\\r\")\n",
    "tweets_test_pd['clean_data'] = tweets_test_pd['noEnter'].str.translate(str.maketrans('','',string.punctuation)).str.lower().str.split()\n",
    "#Use lambda function to apply the same expression on all the strings together. \n",
    "#function takes one string at a time and returns a string wihtout stop words\n",
    "stop_words=set(stopwords.words('english'))\n",
    "tweets_test_pd['no_stop_words'] = tweets_test_pd['clean_data'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "tweets_test_pd['lemma1'] = tweets_test_pd['no_stop_words'].apply(lambda x: [wordnet_lemmatizer.lemmatize(word, pos =\"n\") for word in x])\n",
    "tweets_test_pd['lemma2'] = tweets_test_pd['lemma1'].apply(lambda x: [wordnet_lemmatizer.lemmatize(word, pos =\"v\") for word in x])\n",
    "tweets_test_pd['lemma'] = tweets_test_pd['lemma2'].apply(lambda x: [wordnet_lemmatizer.lemmatize(word, pos =\"a\") for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Create Dictionary\n",
    "def create_dict(tweets, train_classes):\n",
    "    print(\"----------------------------Starting Dictionary Creation---------------------\\n\")\n",
    "    start = time.time()\n",
    "    dictionary = {}\n",
    "    dict_0 = {}\n",
    "    dict_4 = {}\n",
    "    n = []\n",
    "    n_0 =[]\n",
    "    n_4 = []\n",
    "    for i in range(len(tweets)):\n",
    "        x = tweets.get(i)\n",
    "        n.append(len(x))\n",
    "        if (train_classes.get(i) == '\"0\"'): n_0.append(len(x)) \n",
    "        else: n_4.append(len(x))\n",
    "            \n",
    "        for w in x:\n",
    "    #Global Vocabulary\n",
    "            if w in dictionary:\n",
    "                dictionary[w]+=1\n",
    "            else:\n",
    "                dictionary[w]=1\n",
    "    #build Vocabulary for class 0 (-ve class)\n",
    "            if (train_classes.get(i) == '\"0\"'):\n",
    "                if w in dict_0:\n",
    "                    dict_0[w]+=1\n",
    "                else:\n",
    "                    dict_0[w]=1\n",
    "    #build Vocabulary for class 4 (+ve class)\n",
    "            else:\n",
    "                if w in dict_4:\n",
    "                    dict_4[w]+=1\n",
    "                else:\n",
    "                    dict_4[w]=1\n",
    "\n",
    "    end = time.time()                \n",
    "    print(\"Time taken to Create Dictionary on proccessed tweets =\", end-start)\n",
    "    return dictionary, dict_0, dict_4, n, n_0, n_4;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary, dict_0, dict_4, n, n_0, n_4 = create_dict(tweets_train_pd['lemma'], train_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m= len(X_train)\n",
    "phi_0 = class_0/m\n",
    "phi_4 = class_4/m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nb_classifier(dictionary, dict_0, dict_4, len_tweets, n_0, n_4, class_0, class_4):\n",
    "    v = len(dictionary)\n",
    "    theta_0 = {}\n",
    "    theta_4 = {}\n",
    "    n_0 = sum(n_0) # Sum of all the length of the tweets in class 0\n",
    "    n_4 = sum(n_4) # Sum of all the length of the tweets in class 0\n",
    "    c=1\n",
    "    for word in dictionary.keys():\n",
    "            if word in dict_0:\n",
    "                theta_0[word] = ((dict_0[word]+c)/(n_0 + v*c))\n",
    "            else:\n",
    "                theta_0[word] = ((c) / (n_0 + v*c))\n",
    "            if word in dict_4:\n",
    "                theta_4[word] = ((dict_4[word]+c)/(n_4 + v*c))\n",
    "            else:\n",
    "                theta_4[word] = ((c)/(n_4 + v*c))\n",
    "    return theta_0,theta_4;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----------------Training a NB model on processed Data---------------------------\\n\")\n",
    "\n",
    "theta_0, theta_4 = train_nb_classifier(dictionary, dict_0, dict_4, n, n_0, n_4, class_0, class_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data(tweets, theta_0, theta_4, actual_classes):\n",
    "    pred_class=[]\n",
    "    t0=[];t4=[]\n",
    "    #actual_class=[]\n",
    "    for x in tweets:\n",
    "        test_class0=test_class4=0\n",
    "        \n",
    "        #Finding probability of tweet being in a class         \n",
    "        for w in x:\n",
    "            if w in theta_0: test_class0 += math.log(theta_0[w])\n",
    "            else: test_class0 += math.log(1)\n",
    "            if w in theta_4: test_class4 += math.log(theta_4[w])\n",
    "            else: test_class4 += math.log(1)\n",
    "        test_class0 += math.log(phi_0)\n",
    "        test_class4 += math.log(phi_4)\n",
    "        \n",
    "        t0.append(test_class0)\n",
    "        t4.append(test_class4)\n",
    "        #Classifying the probability into classes\n",
    "        if (test_class0 > test_class4): pred_class.append(0)\n",
    "        else: pred_class.append(1)\n",
    "\n",
    "    range0= max(t4)-min(t4)\n",
    "    range1=max(t0)-min(t0)\n",
    "    t0 =[x/range1 for x in t0]\n",
    "    t4 =[x/range0 for x in t4]\n",
    "    for i in range(len(t0)):\n",
    "        t4[i]=t4[i]/(t0[i]+t4[i])\n",
    "        t0[i]=t0[i]/(t0[i]+t4[i])\n",
    "        \n",
    "    actual_class = actual_classes.apply(lambda x: 0 if x=='\"0\"' else 1).tolist()\n",
    "    test_error = sum(np.bitwise_xor(actual_class, pred_class))\n",
    "    accuracy = ((len(tweets) - test_error)/len(tweets))*100\n",
    "    return accuracy, actual_class, pred_class, t0, t4;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy,actual_class_train,pred_class_train , prob0, prob4= test_data(tweets_train_pd['lemma'], theta_0, theta_4, train_classes)\n",
    "print(\"Result (d) : The train accuracy of the model on processed data is = {:2.3f}%\".format(train_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy, actual_class,pred_class, prob0, prob4 = test_data(tweets_test_pd['lemma'], theta_0, theta_4, test_actual_classes)\n",
    "print(\"Result (d) : The test accuracy of the model on processed data is = {:2.3f}%\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n-----------------------Creating the Confusion Matrix for this Model---------------------------\\n\")\n",
    "\n",
    "from ques1_a_b_c import create_confusion_matrix as create_confusion_matrix\n",
    "from ques1_a_b_c import plot_confusion_matrix as plot_confusion_matrix\n",
    "\n",
    "conf_matrix = create_confusion_matrix(actual_class, pred_class)\n",
    "print(\"The confusion Matrix of the model is = \\n\", conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print confusion matrix (Code help from stackoverflow)\n",
    "plot_confusion_matrix(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc(prob4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
