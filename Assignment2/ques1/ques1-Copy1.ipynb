{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the training set is =  1600000\n",
      "The number of classes (label=4) = 800000\n",
      "The number of classes (label=0) = 800000\n",
      "The length of the test set is =  359\n"
     ]
    }
   ],
   "source": [
    "#Importing Data from the CSV file\n",
    "%matplotlib inline\n",
    "import string\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "f_train = open(\"training.1600000.processed.noemoticon.csv\", \"r\", encoding=\"ISO-8859-1\")\n",
    "X_train = f_train.readlines()\n",
    "f_test = open(\"testdata.manual.2009.06.14.csv\", \"r\")\n",
    "X_test = f_test.readlines()\n",
    "\n",
    "#Creating the List with just the tweets and finding the number of positive and negative classes. (TRAINING) \n",
    "#class_0 = number of classes with label = 0\n",
    "#class_4 = number of classes with label = 4\n",
    "\n",
    "class_0=class_4=0\n",
    "tweets_train =[]\n",
    "for x in X_train:\n",
    "    a = x.split('\",\"')\n",
    "    if (a[0] == '\"0'): class_0+=1\n",
    "    else: class_4+=1\n",
    "    tweets_train.append('%s\"|\"%s' % (a[0],a[-1]))\n",
    "\n",
    "#Creating the List with just the tweets (TEST DATA)\n",
    "tweets_test =[]\n",
    "for x in X_test:\n",
    "    a = x.split('\",\"')\n",
    "    tweets_test.append('%s\"|\"%s' % (a[0], a[-1]))\n",
    "    \n",
    "print(\"The length of the training set is = \", len(X_train))\n",
    "print(\"The number of classes (label=4) =\", class_4)\n",
    "print(\"The number of classes (label=0) =\", class_0)\n",
    "print(\"The length of the test set is = \", len(X_test))\n",
    "\n",
    "f_test.close()\n",
    "f_train.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#For verification of parsing and creating tweets\n",
    "print(tweets_test[1].split(\"|\")[1])\n",
    "print(tweets_test[1])\n",
    "print(tweets_train[159903],\"\\n\", tweets_train[10])\n",
    "print(len(tweets_train))\n",
    "print(len(tweets_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing the tweets to a file (not necessary - REMOVE if not used anywhere later)\n",
    "!rm -rf tweet_test.txt\n",
    "!rm -rf tweet_train.txt\n",
    "f = open(\"tweet_train.txt\", \"w\")\n",
    "for x in tweets_train:\n",
    "    f.write(x)\n",
    "f.close()\n",
    "\n",
    "f = open(\"tweet_test.txt\", \"w\")\n",
    "for x in tweets_test:\n",
    "    f.write(x)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(tweets_train[0].translate(str.maketrans('','',string.punctuation))) \n",
    "#So this is the python translate function that translates the original words by mapping <first string> to <second string> after removal of <third string>\n",
    "#Here first and second string are empty so the word will remain as it is but lal punctuations will be gone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dict(tweets_train):\n",
    "    dictionary = {}\n",
    "    dict_0 = {}\n",
    "    dict_4 = {}\n",
    "    n = []\n",
    "    for x in tweets_train:\n",
    "        x = x.rstrip(\"\\n\\r\")\n",
    "        x = x.split(\"|\")\n",
    "        twt = x[1].translate(str.maketrans('','',string.punctuation)).lower()\n",
    "        words = twt.split() # if I use split(\" \"), it takes more than one white space as a word\n",
    "        n.append(len(words))\n",
    "        for w in words:\n",
    "    #Global Vocabulary\n",
    "            if w in dictionary:\n",
    "                dictionary[w]+=1\n",
    "            else:\n",
    "                dictionary[w]=1\n",
    "    #build Vocabulary for class 0 (-ve class)\n",
    "            if (x[0] == '\"0\"'):\n",
    "                if w in dict_0:\n",
    "                    dict_0[w]+=1\n",
    "                else:\n",
    "                    dict_0[w]=1\n",
    "    #build Vocabulary for class 4 (+ve class)\n",
    "            else:\n",
    "                if w in dict_4:\n",
    "                    dict_4[w]+=1\n",
    "                else:\n",
    "                    dict_4[w]=1\n",
    "    return dictionary,dict_0,dict_4,n;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary, dict_0, dict_4, len_tweets = create_dict(tweets_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "m= len(X_train)\n",
    "phi_0 = class_0/m\n",
    "phi_4 = class_4/m\n",
    "v = len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nb_classifier(dictionary, dict_0, dict_4,len_tweets, class_0, class_4):\n",
    "    theta_0 = {}\n",
    "    theta_4 = {}\n",
    "    n_0 = sum(len_tweets[0:class_0])\n",
    "    n_4 = sum(len_tweets[class_0:len(len_tweets)])\n",
    "    c=1\n",
    "    for word in dictionary.keys():\n",
    "            if word in dict_0:\n",
    "                theta_0[word] = ((dict_0[word]+c)/(n_0 + v*c))\n",
    "            else:\n",
    "                theta_0[word] = ((c) / (n_0 + v*c))\n",
    "            if word in dict_4:\n",
    "                theta_4[word] = ((dict_4[word]+c)/(n_4 + v*c))\n",
    "            else:\n",
    "                theta_4[word] = ((c)/(n_4 + v*c))\n",
    "    return theta_0,theta_4;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_0, theta_4 = train_nb_classifier(dictionary, dict_0, dict_4, len_tweets, class_0, class_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data(tweets):\n",
    "    pred_class=[]\n",
    "    actual_class=[]\n",
    "    for x in tweets:\n",
    "        test_class0=test_class4=0\n",
    "        x = x.rstrip(\"\\n\\r\")\n",
    "        x = x.split(\"|\")\n",
    "        twt = x[1].translate(str.maketrans('','',string.punctuation)).lower()\n",
    "        words = twt.split() # if I use split(\" \"), it takes more than one white space as a word\n",
    "        \n",
    "        #Store actual class number in Test Data for forming Confusion matrix\n",
    "        actual_class.append(x[0])\n",
    "        \n",
    "        #Finding probability of tweet being in a class         \n",
    "        for w in words:\n",
    "            if w in theta_0: test_class0 += math.log(theta_0[w])\n",
    "            else: test_class0 += math.log(1)\n",
    "            if w in theta_4: test_class4 += math.log(theta_4[w])\n",
    "            else: test_class4 += math.log(1)\n",
    "        test_class0 += math.log(phi_0)\n",
    "        test_class4 += math.log(phi_4)\n",
    "        #Classifying the probability into classes\n",
    "        if (test_class0 > test_class4): pred_class.append('\"0\"')\n",
    "        else: pred_class.append('\"4\"')\n",
    "\n",
    "            \n",
    "    actual_class=[0 if x=='\"0\"' else 1 for x in actual_class]\n",
    "    pred_class=[0 if x=='\"0\"' else 1 for x in pred_class]\n",
    "    test_error = sum(np.bitwise_xor(actual_class, pred_class))\n",
    "    accuracy = ((len(tweets) - test_error)/len(tweets))*100\n",
    "    return accuracy, actual_class, pred_class;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result (a) : The train accuracy of the model on raw data is = 84.210875%\n"
     ]
    }
   ],
   "source": [
    "train_accuracy,actual_class_train,pred_class_train = test_data(tweets_train)\n",
    "print(\"Result (a) : The train accuracy of the model on raw data is = {}%\".format(train_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result (a) : The test accuracy of the model on raw data is = 80.77994428969359%\n"
     ]
    }
   ],
   "source": [
    "test_accuracy, actual_class,pred_class = test_data(tweets_test)\n",
    "print(\"Result (a) : The test accuracy of the model on raw data is = {}%\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Part(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ayushi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ayushi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import pandas as pd\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "tokens=tknzr.tokenize(tweets_train[0].split(\"|\")[1])\n",
    "x= ' '.join(tokens)\n",
    "print(type(x))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Removing stopwords\n",
    "stop_words=set(stopwords.words('english'))\n",
    "x = x.rstrip(\"\\n\\r\")\n",
    "twt = x.translate(str.maketrans('','',string.punctuation)).lower()\n",
    "print(twt)\n",
    "words = twt.split()\n",
    "new_words=[]\n",
    "for w in words:\n",
    "    if w not in stop_words:\n",
    "        new_words.append(w)\n",
    "print(new_words)\n",
    "lemma_word=[]\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "for w in new_words:\n",
    "    word1 = wordnet_lemmatizer.lemmatize(w, pos = \"n\") #Noun\n",
    "    word2 = wordnet_lemmatizer.lemmatize(word1, pos = \"v\") #Verb\n",
    "    word3 = wordnet_lemmatizer.lemmatize(word2, pos = (\"a\")) #Adjective\n",
    "    lemma_word.append(word3)\n",
    "print(lemma_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_train_pd = pd.Series(tweets_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "tokens= [tknzr.tokenize(sentence.split(\"|\")[1]) for sentence in tweets_train_pd['train_data']]\n",
    "tweets_train_pd['tokenize1'] = pd.Series([\" \".join(token) for token in tokens])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          \"0\"|\"@switchfoot http://twitpic.com/2y1zl - Aw...\n",
      "1          \"0\"|\"is upset that he can't update his Faceboo...\n",
      "2          \"0\"|\"@Kenichan I dived many times for the ball...\n",
      "3          \"0\"|\"my whole body feels itchy and like its on...\n",
      "4          \"0\"|\"@nationwideclass no, it's not behaving at...\n",
      "                                 ...                        \n",
      "1599995    \"4\"|\"Just woke up. Having no school is the bes...\n",
      "1599996    \"4\"|\"TheWDB.com - Very cool to hear old Walt i...\n",
      "1599997    \"4\"|\"Are you ready for your MoJo Makeover? Ask...\n",
      "1599998    \"4\"|\"Happy 38th Birthday to my boo of alll tim...\n",
      "1599999    \"4\"|\"happy #charitytuesday @theNSPCC @SparksCh...\n",
      "Length: 1600000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(tweets_train_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "tweets_train_pd['tokenize_data'] = tweets_train_pd.apply(lambda x: '\\n'.join([' '.join(token) for token in ([tknzr.tokenize(x.split(\"|\")[1])])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          [httptwitpiccom2y1zl, awww, thats, a, bummer, ...\n",
      "1          [is, upset, that, he, cant, update, his, faceb...\n",
      "2          [i, dived, many, times, for, the, ball, manage...\n",
      "3          [my, whole, body, feels, itchy, and, like, its...\n",
      "4          [no, its, not, behaving, at, all, im, mad, why...\n",
      "                                 ...                        \n",
      "1599995    [just, woke, up, having, no, school, is, the, ...\n",
      "1599996    [thewdbcom, very, cool, to, hear, old, walt, i...\n",
      "1599997    [are, you, ready, for, your, mojo, makeover, a...\n",
      "1599998    [happy, 38th, birthday, to, my, boo, of, alll,...\n",
      "1599999                              [happy, charitytuesday]\n",
      "Length: 1600000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(tweets_train_pd['clean_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_train_pd['tokenize_data']=tweets_train_pd['tokenize_data'].str.rstrip(\"\\n\\r\")\n",
    "tweets_train_pd['clean_data'] = tweets_train_pd['tokenize_data'].str.translate(str.maketrans('','',string.punctuation)).str.lower().str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use lambda function to apply the same expression on all the strings together. \n",
    "#function takes one string at a time and returns a string wihtout stop words\n",
    "stop_words=set(stopwords.words('english'))\n",
    "tweets_train_pd['no_stop_words'] = tweets_train_pd['clean_data'].apply(lambda x: [word for word in x if word not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          [httptwitpiccom2y1zl, awww, thats, bummer, sho...\n",
      "1          [upset, cant, update, facebook, texting, might...\n",
      "2          [dived, many, times, ball, managed, save, 50, ...\n",
      "3                    [whole, body, feels, itchy, like, fire]\n",
      "4                             [behaving, im, mad, cant, see]\n",
      "                                 ...                        \n",
      "1599995                  [woke, school, best, feeling, ever]\n",
      "1599996    [thewdbcom, cool, hear, old, walt, interviews,...\n",
      "1599997                [ready, mojo, makeover, ask, details]\n",
      "1599998    [happy, 38th, birthday, boo, alll, time, tupac...\n",
      "1599999                              [happy, charitytuesday]\n",
      "Length: 1600000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(tweets_train_pd['no_stop_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "tweets_train_pd['lemma1'] = tweets_train_pd['no_stop_words'].apply(lambda x: [wordnet_lemmatizer.lemmatize(word, pos =\"n\") for word in x])\n",
    "tweets_train_pd['lemma2'] = tweets_train_pd['lemma1'].apply(lambda x: [wordnet_lemmatizer.lemmatize(word, pos =\"v\") for word in x])\n",
    "tweets_train_pd['lemma'] = tweets_train_pd['lemma2'].apply(lambda x: [wordnet_lemmatizer.lemmatize(word, pos =\"a\") for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          [httptwitpiccom2y1zl, awww, thats, bummer, sho...\n",
      "1          [upset, cant, update, facebook, texting, might...\n",
      "2          [dive, many, time, ball, manage, save, 50, res...\n",
      "3                     [whole, body, feel, itchy, like, fire]\n",
      "4                               [behave, im, mad, cant, see]\n",
      "                                 ...                        \n",
      "1599995                     [wake, school, best, feel, ever]\n",
      "1599996    [thewdbcom, cool, hear, old, walt, interview, ...\n",
      "1599997                 [ready, mojo, makeover, ask, detail]\n",
      "1599998    [happy, 38th, birthday, boo, alll, time, tupac...\n",
      "1599999                              [happy, charitytuesday]\n",
      "Length: 1600000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(tweets_train_pd['lemma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
