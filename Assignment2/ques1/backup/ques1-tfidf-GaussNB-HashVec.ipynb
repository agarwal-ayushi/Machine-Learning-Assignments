{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the training set is =  1600000\n",
      "The number of classes (label=4) = 800000\n",
      "The number of classes (label=0) = 800000\n",
      "The length of the test set is =  359\n"
     ]
    }
   ],
   "source": [
    "#Importing Data from the CSV file\n",
    "%matplotlib inline\n",
    "import string\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "f_train = open(\"training.1600000.processed.noemoticon.csv\", \"r\", encoding=\"ISO-8859-1\")\n",
    "X_train = f_train.readlines()\n",
    "f_test = open(\"testdata.manual.2009.06.14.csv\", \"r\")\n",
    "X_test = f_test.readlines()\n",
    "\n",
    "#Creating the List with just the tweets and finding the number of positive and negative classes. (TRAINING) \n",
    "#class_0 = number of classes with label = 0\n",
    "#class_4 = number of classes with label = 4\n",
    "\n",
    "class_0=class_4=0\n",
    "tweets_train =[]\n",
    "for x in X_train:\n",
    "    a = x.split('\",\"')\n",
    "    if (a[0] == '\"0'): class_0+=1\n",
    "    else: class_4+=1\n",
    "    tweets_train.append('%s\"|\"%s' % (a[0],a[-1]))\n",
    "\n",
    "#Creating the List with just the tweets (TEST DATA)\n",
    "tweets_test =[]\n",
    "for x in X_test:\n",
    "    a = x.split('\",\"')\n",
    "    tweets_test.append('%s\"|\"%s' % (a[0], a[-1]))\n",
    "    \n",
    "print(\"The length of the training set is = \", len(X_train))\n",
    "print(\"The number of classes (label=4) =\", class_4)\n",
    "print(\"The number of classes (label=0) =\", class_0)\n",
    "print(\"The length of the test set is = \", len(X_test))\n",
    "\n",
    "f_test.close()\n",
    "f_train.close()\n",
    "\n",
    "m= len(X_train)\n",
    "phi_0 = class_0/m\n",
    "phi_4 = class_4/m"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Part(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.feature_selection import SelectPercentile, chi2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets_train_pd = pd.read_pickle('tweet_train_lemma.pkl')\n",
    "#tweets_test_pd = pd.read_pickle('tweet_test_lemma.pkl')\n",
    "train_actual_classes = pd.read_pickle('train_class.pkl')\n",
    "test_actual_classes = pd.read_pickle('test_class.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_train_pd = pd.Series(tweets_train)\n",
    "tweets_train_pd = tweets_train_pd.apply(lambda x: x.split(\"|\")[1])\n",
    "tweets_test_pd = pd.Series(tweets_test)\n",
    "tweets_test_pd = tweets_test_pd.apply(lambda x: x.split(\"|\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classes = train_actual_classes.apply(lambda x: 0 if x=='\"0\"' else 1)\n",
    "test_classes = test_actual_classes.apply(lambda x: 0 if x=='\"0\"' else 1)\n",
    "all_classes = train_classes.unique()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TFIDF - Using Gaussian Naive Bayes - And using minibatch training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_minibatch(tweets, classes, batch_size):\n",
    "    l = len(tweets)\n",
    "    for i in range(0, l, batch_size):\n",
    "        X_batch=tweets[i:min(i + batch_size, l)]\n",
    "        Y_batch=classes[i:min(i + batch_size, l)]\n",
    "        yield X_batch,Y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gauss = GaussianNB()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Only using Hashing Vectorizer that will create a sparse matrix of token occurrences and store the counts. The ocunts will be normalized with euclidean distance or l2 norm here. \n",
    "#Training takes 4 hours for batch size = 1000\n",
    "Hashing Vectorizer used only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create minibatches\n",
    "vec = HashingVectorizer(analyzer=\"word\", stop_words='english',norm='l2')\n",
    "X = vec.transform(tweets_train_pd)\n",
    "\n",
    "for (X_batch, Y_batch) in create_minibatch(tweets_train_pd, train_classes, 1000):\n",
    "    tfidf_train = vec.transform(X_batch)\n",
    "    model_gauss.partial_fit(tfidf_train.toarray(), Y_batch, classes=all_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = vec.transform(tweets_test_pd)\n",
    "test_accuracy = model_gauss.score(X_test.toarray(), test_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Test Accuracy of the GaussianNB model trained with Hashing Vectorizer and batch size=1000 is = 55.98885793871866%\n"
     ]
    }
   ],
   "source": [
    "print(\"The Test Accuracy of the GaussianNB model trained with Hashing Vectorizer and batch size=1000 is = {}%\".format(test_accuracy*100))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Only using Hashing Vectorizer that will create a sparse matrix of token occurrences and store the counts. The ocunts will be normalized with euclidean distance or l2 norm here. \n",
    "#Training takes 4 hours for batch size = 10000 (memory error on using default features)\n",
    "So tried with 2**18 features - 32 GB RAM system hangs\n",
    "Hashing Vectorizer used only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create minibatches\n",
    "import time\n",
    "start = time.time()\n",
    "vec = HashingVectorizer(analyzer=\"word\", stop_words='english',norm='l2', n_features=2**18)\n",
    "#X = vec.transform(tweets_train_pd)\n",
    "\n",
    "for (X_batch, Y_batch) in create_minibatch(tweets_train_pd, train_classes, 1000):\n",
    "    tfidf_train = vec.transform(X_batch)\n",
    "    #print(tfidf_train.shape)\n",
    "    model_gauss.partial_fit(tfidf_train.toarray(), Y_batch, classes=all_classes)\n",
    "\n",
    "print(\"The time taken for training on batch size=10000 using hashing vect only=\", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ayushi/.local/lib/python3.6/site-packages/sklearn/naive_bayes.py:450: RuntimeWarning: invalid value encountered in log\n",
      "  n_ij = - 0.5 * np.sum(np.log(2. * np.pi * self.sigma_[i, :]))\n"
     ]
    }
   ],
   "source": [
    "X_test = vec.transform(tweets_test_pd)\n",
    "test_accuracy = model_gauss.score(X_test.toarray(), test_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The Test Accuracy of the GaussianNB model trained with Hashing Vectorizer and batch size=10000 is = {}%\".format(test_accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_shuffled = pd.concat([tweets_train_pd, train_classes], axis=1).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_train = list(tweets_shuffled[0])\n",
    "train_classes = list(tweets_shuffled[1])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "b) Using SelectPercentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(analyzer=\"word\", stop_words='english',sublinear_tf=True, ngram_range=(1,1))\n",
    "tfidf_train = vec.fit_transform(tweets_train_pd)\n",
    "tfidf_train_new = SelectPercentile(chi2, percentile=50).fit_transform(tfidf_train, train_classes)\n",
    "tfidf_train_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
