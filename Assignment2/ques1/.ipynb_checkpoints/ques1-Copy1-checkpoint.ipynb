{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Data from the CSV file\n",
    "%matplotlib inline\n",
    "import string\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "f_train = open(\"training.1600000.processed.noemoticon.csv\", \"r\", encoding=\"ISO-8859-1\")\n",
    "X_train = f_train.readlines()\n",
    "f_test = open(\"testdata.manual.2009.06.14.csv\", \"r\")\n",
    "X_test = f_test.readlines()\n",
    "\n",
    "#Creating the List with just the tweets and finding the number of positive and negative classes. (TRAINING) \n",
    "#class_0 = number of classes with label = 0\n",
    "#class_4 = number of classes with label = 4\n",
    "\n",
    "class_0=class_4=0\n",
    "tweets_train =[]\n",
    "for x in X_train:\n",
    "    a = x.split('\",\"')\n",
    "    if (a[0] == '\"0'): class_0+=1\n",
    "    else: class_4+=1\n",
    "    tweets_train.append('%s\"|\"%s' % (a[0],a[-1]))\n",
    "\n",
    "#Creating the List with just the tweets (TEST DATA)\n",
    "tweets_test =[]\n",
    "for x in X_test:\n",
    "    a = x.split('\",\"')\n",
    "    tweets_test.append('%s\"|\"%s' % (a[0], a[-1]))\n",
    "    \n",
    "print(\"The length of the training set is = \", len(X_train))\n",
    "print(\"The number of classes (label=4) =\", class_4)\n",
    "print(\"The number of classes (label=0) =\", class_0)\n",
    "print(\"The length of the test set is = \", len(X_test))\n",
    "\n",
    "f_test.close()\n",
    "f_train.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#For verification of parsing and creating tweets\n",
    "print(tweets_test[1].split(\"|\")[1])\n",
    "print(tweets_test[1])\n",
    "print(tweets_train[159903],\"\\n\", tweets_train[10])\n",
    "print(len(tweets_train))\n",
    "print(len(tweets_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing the tweets to a file (not necessary - REMOVE if not used anywhere later)\n",
    "!rm -rf tweet_test.txt\n",
    "!rm -rf tweet_train.txt\n",
    "f = open(\"tweet_train.txt\", \"w\")\n",
    "for x in tweets_train:\n",
    "    f.write(x)\n",
    "f.close()\n",
    "\n",
    "f = open(\"tweet_test.txt\", \"w\")\n",
    "for x in tweets_test:\n",
    "    f.write(x)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m= len(X_train)\n",
    "phi_0 = class_0/m\n",
    "phi_4 = class_4/m\n",
    "v = len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nb_classifier(dictionary, dict_0, dict_4,len_tweets, class_0, class_4):\n",
    "    theta_0 = {}\n",
    "    theta_4 = {}\n",
    "    n_0 = sum(len_tweets[0:class_0])\n",
    "    n_4 = sum(len_tweets[class_0:len(len_tweets)])\n",
    "    c=1\n",
    "    for word in dictionary.keys():\n",
    "            if word in dict_0:\n",
    "                theta_0[word] = ((dict_0[word]+c)/(n_0 + v*c))\n",
    "            else:\n",
    "                theta_0[word] = ((c) / (n_0 + v*c))\n",
    "            if word in dict_4:\n",
    "                theta_4[word] = ((dict_4[word]+c)/(n_4 + v*c))\n",
    "            else:\n",
    "                theta_4[word] = ((c)/(n_4 + v*c))\n",
    "    return theta_0,theta_4;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_0, theta_4 = train_nb_classifier(dictionary, dict_0, dict_4, len_tweets, class_0, class_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data(tweets):\n",
    "    pred_class=[]\n",
    "    actual_class=[]\n",
    "    for x in tweets:\n",
    "        test_class0=test_class4=0\n",
    "        x = x.rstrip(\"\\n\\r\")\n",
    "        x = x.split(\"|\")\n",
    "        twt = x[1].translate(str.maketrans('','',string.punctuation)).lower()\n",
    "        words = twt.split() # if I use split(\" \"), it takes more than one white space as a word\n",
    "        \n",
    "        #Store actual class number in Test Data for forming Confusion matrix\n",
    "        actual_class.append(x[0])\n",
    "        \n",
    "        #Finding probability of tweet being in a class         \n",
    "        for w in words:\n",
    "            if w in theta_0: test_class0 += math.log(theta_0[w])\n",
    "            else: test_class0 += math.log(1)\n",
    "            if w in theta_4: test_class4 += math.log(theta_4[w])\n",
    "            else: test_class4 += math.log(1)\n",
    "        test_class0 += math.log(phi_0)\n",
    "        test_class4 += math.log(phi_4)\n",
    "        #Classifying the probability into classes\n",
    "        if (test_class0 > test_class4): pred_class.append('\"0\"')\n",
    "        else: pred_class.append('\"4\"')\n",
    "\n",
    "            \n",
    "    actual_class=[0 if x=='\"0\"' else 1 for x in actual_class]\n",
    "    pred_class=[0 if x=='\"0\"' else 1 for x in pred_class]\n",
    "    test_error = sum(np.bitwise_xor(actual_class, pred_class))\n",
    "    accuracy = ((len(tweets) - test_error)/len(tweets))*100\n",
    "    return accuracy, actual_class, pred_class;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy,actual_class_train,pred_class_train = test_data(tweets_train)\n",
    "print(\"Result (a) : The train accuracy of the model on raw data is = {}%\".format(train_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy, actual_class,pred_class = test_data(tweets_test)\n",
    "print(\"Result (a) : The test accuracy of the model on raw data is = {}%\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Part(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import pandas as pd\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_train_pd = pd.Series(tweets_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classes = tweets_train_pd.apply(lambda x: (x.split(\"|\")[0]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "tokens= [tknzr.tokenize(sentence.split(\"|\")[1]) for sentence in tweets_train_pd['train_data']]\n",
    "tweets_train_pd['tokenize1'] = pd.Series([\" \".join(token) for token in tokens])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "tweets_train_pd['tokenize_data'] = tweets_train_pd.apply(lambda x: '\\n'.join([' '.join(token) for token in ([tknzr.tokenize(x.split(\"|\")[1])])]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_train_pd['count'].mask(train_classes == '\"4\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_train_pd['tokenize_data']=tweets_train_pd['tokenize_data'].str.rstrip(\"\\n\\r\")\n",
    "tweets_train_pd['clean_data'] = tweets_train_pd['tokenize_data'].str.translate(str.maketrans('','',string.punctuation)).str.lower().str.split()\n",
    "#Use lambda function to apply the same expression on all the strings together. \n",
    "#function takes one string at a time and returns a string wihtout stop words\n",
    "stop_words=set(stopwords.words('english'))\n",
    "tweets_train_pd['no_stop_words'] = tweets_train_pd['clean_data'].apply(lambda x: [word for word in x if word not in stop_words and word.isalpha()])\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "tweets_train_pd['lemma1'] = tweets_train_pd['no_stop_words'].apply(lambda x: [wordnet_lemmatizer.lemmatize(word, pos =\"n\") for word in x])\n",
    "tweets_train_pd['lemma2'] = tweets_train_pd['lemma1'].apply(lambda x: [wordnet_lemmatizer.lemmatize(word, pos =\"v\") for word in x])\n",
    "tweets_train_pd['lemma'] = tweets_train_pd['lemma2'].apply(lambda x: [wordnet_lemmatizer.lemmatize(word, pos =\"a\") for word in x])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(tweets_train_pd['no_stop_words'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(tweets_train_pd['lemma'][::100000])\n",
    "print(tweets_train_pd['lemma'][1])\n",
    "#for w in tweets_train_pd['lemma']:\n",
    "    #print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "tweets_train_pd['count'] = tweets_train_pd['lemma'].apply(lambda x: Counter(x)) \n",
    "dictionary=tweets_train_pd['count'].sum()\n",
    "dictionary = dict(dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_train_pd['count'][::10].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "tokens=tknzr.tokenize(tweets_train[0].split(\"|\")[1])\n",
    "x= ' '.join(tokens)\n",
    "print(type(x))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Removing stopwords\n",
    "stop_words=set(stopwords.words('english'))\n",
    "x = x.rstrip(\"\\n\\r\")\n",
    "twt = x.translate(str.maketrans('','',string.punctuation)).lower()\n",
    "print(twt)\n",
    "words = twt.split()\n",
    "new_words=[]\n",
    "for w in words:\n",
    "    if w not in stop_words:\n",
    "        new_words.append(w)\n",
    "print(new_words)\n",
    "lemma_word=[]\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "for w in new_words:\n",
    "    word1 = wordnet_lemmatizer.lemmatize(w, pos = \"n\") #Noun\n",
    "    word2 = wordnet_lemmatizer.lemmatize(word1, pos = \"v\") #Verb\n",
    "    word3 = wordnet_lemmatizer.lemmatize(word2, pos = (\"a\")) #Adjective\n",
    "    lemma_word.append(word3)\n",
    "print(lemma_word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
