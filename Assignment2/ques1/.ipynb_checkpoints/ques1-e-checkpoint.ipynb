{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the training set is =  1600000\n",
      "The number of classes (label=4) = 800000\n",
      "The number of classes (label=0) = 800000\n",
      "The length of the test set is =  359\n"
     ]
    }
   ],
   "source": [
    "#Importing Data from the CSV file\n",
    "%matplotlib inline\n",
    "import string\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import pandas as pd\n",
    "\n",
    "f_train = open(\"training.1600000.processed.noemoticon.csv\", \"r\", encoding=\"ISO-8859-1\")\n",
    "X_train = f_train.readlines()\n",
    "f_test = open(\"testdata.manual.2009.06.14.csv\", \"r\")\n",
    "X_test = f_test.readlines()\n",
    "\n",
    "#Creating the List with just the tweets and finding the number of positive and negative classes. (TRAINING) \n",
    "#class_0 = number of classes with label = 0\n",
    "#class_4 = number of classes with label = 4\n",
    "\n",
    "class_0=class_4=0\n",
    "tweets_train =[]\n",
    "for x in X_train:\n",
    "    a = x.split('\",\"')\n",
    "    if (a[0] == '\"0'): class_0+=1\n",
    "    else: class_4+=1\n",
    "    tweets_train.append('%s\"|\"%s' % (a[0],a[-1]))\n",
    "\n",
    "#Creating the List with just the tweets (TEST DATA)\n",
    "tweets_test =[]\n",
    "for x in X_test:\n",
    "    a = x.split('\",\"')\n",
    "    tweets_test.append('%s\"|\"%s' % (a[0], a[-1]))\n",
    "    \n",
    "print(\"The length of the training set is = \", len(X_train))\n",
    "print(\"The number of classes (label=4) =\", class_4)\n",
    "print(\"The number of classes (label=0) =\", class_0)\n",
    "print(\"The length of the test set is = \", len(X_test))\n",
    "\n",
    "f_test.close()\n",
    "f_train.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Dictionary\n",
    "import time\n",
    "\n",
    "def create_dict(tweets, train_classes):\n",
    "    start = time.time()\n",
    "    dictionary = {}\n",
    "    dict_0 = {}\n",
    "    dict_4 = {}\n",
    "    n = []\n",
    "    n_0 =[]\n",
    "    n_4 = []\n",
    "    for i in range(len(tweets)):\n",
    "        x = tweets.get(i)\n",
    "        n.append(len(x))\n",
    "        if (train_classes.get(i) == '\"0\"'): n_0.append(len(x)) \n",
    "        else: n_4.append(len(x))\n",
    "            \n",
    "        for w in x:\n",
    "    #Global Vocabulary\n",
    "            if w in dictionary:\n",
    "                dictionary[w]+=1\n",
    "            else:\n",
    "                dictionary[w]=1\n",
    "    #build Vocabulary for class 0 (-ve class)\n",
    "            if (train_classes.get(i) == '\"0\"'):\n",
    "                if w in dict_0:\n",
    "                    dict_0[w]+=1\n",
    "                else:\n",
    "                    dict_0[w]=1\n",
    "    #build Vocabulary for class 4 (+ve class)\n",
    "            else:\n",
    "                if w in dict_4:\n",
    "                    dict_4[w]+=1\n",
    "                else:\n",
    "                    dict_4[w]=1\n",
    "\n",
    "    end = time.time()                \n",
    "    print(\"Time to Create Dictionary =\", end-start)\n",
    "    return dictionary, dict_0, dict_4, n, n_0, n_4;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nb_classifier(dictionary, dict_0, dict_4, len_tweets, n_0, n_4, class_0, class_4):\n",
    "    v = len(dictionary)\n",
    "    theta_0 = {}\n",
    "    theta_4 = {}\n",
    "    n_0 = sum(n_0) # Sum of all the length of the tweets in class 0\n",
    "    n_4 = sum(n_4) # Sum of all the length of the tweets in class 0\n",
    "    c=1\n",
    "    for word in dictionary.keys():\n",
    "            if word in dict_0:\n",
    "                theta_0[word] = ((dict_0[word]+c)/(n_0 + v*c))\n",
    "            else:\n",
    "                theta_0[word] = ((c) / (n_0 + v*c))\n",
    "            if word in dict_4:\n",
    "                theta_4[word] = ((dict_4[word]+c)/(n_4 + v*c))\n",
    "            else:\n",
    "                theta_4[word] = ((c)/(n_4 + v*c))\n",
    "    return theta_0,theta_4;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data(tweets, theta_0, theta_4, actual_classes):\n",
    "    pred_class=[]\n",
    "    #actual_class=[]\n",
    "    for x in tweets:\n",
    "        test_class0=test_class4=0\n",
    "        \n",
    "        #Finding probability of tweet being in a class         \n",
    "        for w in x:\n",
    "            if w in theta_0: test_class0 += math.log(theta_0[w])\n",
    "            else: test_class0 += math.log(1)\n",
    "            if w in theta_4: test_class4 += math.log(theta_4[w])\n",
    "            else: test_class4 += math.log(1)\n",
    "        test_class0 += math.log(phi_0)\n",
    "        test_class4 += math.log(phi_4)\n",
    "        #Classifying the probability into classes\n",
    "        if (test_class0 > test_class4): pred_class.append(0)\n",
    "        else: pred_class.append(1)\n",
    "\n",
    "    actual_class = actual_classes.apply(lambda x: 0 if x=='\"0\"' else 1).tolist()\n",
    "    test_error = sum(np.bitwise_xor(actual_class, pred_class))\n",
    "    accuracy = ((len(tweets) - test_error)/len(tweets))*100\n",
    "    return accuracy, actual_class, pred_class;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_train_pd = pd.Series(tweets_train)\n",
    "tweets_test_pd = pd.Series(tweets_test\n",
    "                           \n",
    "train_classes = tweets_train_pd.apply(lambda x: (x.split(\"|\")[0]))\n",
    "test_actual_classes = tweets_test_pd.apply(lambda x: (x.split(\"|\")[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "#Tokenize the Training data\n",
    "tweets_train_pd['tokenize_data'] = tweets_train_pd.apply(lambda x: '\\n'.join([' '.join(token) for token in ([tknzr.tokenize(x.split(\"|\")[1])])]))\n",
    "tweets_test_pd['tokenize_data'] = tweets_test_pd.apply(lambda x: '\\n'.join([' '.join(token) for token in ([tknzr.tokenize(x.split(\"|\")[1])])]))\n",
    "\n",
    "#Import regex library in python to earch for regular expressions of a certain type to replace hem\n",
    "import re\n",
    "tweets_train_pd['noURL'] = tweets_train_pd['tokenize_data'].replace(r'http\\S+', '', regex=True).replace(r'www\\.\\S+', '', regex=True)\n",
    "tweets_test_pd['noURL'] = tweets_test_pd['tokenize_data'].replace(r'http\\S+', '', regex=True).replace(r'www\\.\\S+', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create Train dataset Clean\n",
    "\n",
    "tweets_train_pd['noEnter']=tweets_train_pd['noURL'].str.rstrip(\"\\n\\r\")\n",
    "tweets_train_pd['clean_data'] = tweets_train_pd['noEnter'].str.translate(str.maketrans('','',string.punctuation)).str.lower().str.split()\n",
    "#Use lambda function to apply the same expression on all the strings together. \n",
    "#function takes one string at a time and returns a string wihtout stop words\n",
    "stop_words=set(stopwords.words('english'))\n",
    "tweets_train_pd['no_stop_words'] = tweets_train_pd['clean_data'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "tweets_train_pd['lemma1'] = tweets_train_pd['no_stop_words'].apply(lambda x: [wordnet_lemmatizer.lemmatize(word, pos =\"n\") for word in x])\n",
    "tweets_train_pd['lemma2'] = tweets_train_pd['lemma1'].apply(lambda x: [wordnet_lemmatizer.lemmatize(word, pos =\"v\") for word in x])\n",
    "tweets_train_pd['lemma'] = tweets_train_pd['lemma2'].apply(lambda x: [wordnet_lemmatizer.lemmatize(word, pos =\"a\") for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create clean test data\n",
    "\n",
    "tweets_test_pd['noEnter']=tweets_test_pd['noURL'].str.rstrip(\"\\n\\r\")\n",
    "tweets_test_pd['clean_data'] = tweets_test_pd['noEnter'].str.translate(str.maketrans('','',string.punctuation)).str.lower().str.split()\n",
    "#Use lambda function to apply the same expression on all the strings together. \n",
    "#function takes one string at a time and returns a string wihtout stop words\n",
    "stop_words=set(stopwords.words('english'))\n",
    "tweets_test_pd['no_stop_words'] = tweets_test_pd['clean_data'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "tweets_test_pd['lemma1'] = tweets_test_pd['no_stop_words'].apply(lambda x: [wordnet_lemmatizer.lemmatize(word, pos =\"n\") for word in x])\n",
    "tweets_test_pd['lemma2'] = tweets_test_pd['lemma1'].apply(lambda x: [wordnet_lemmatizer.lemmatize(word, pos =\"v\") for word in x])\n",
    "tweets_test_pd['lemma'] = tweets_test_pd['lemma2'].apply(lambda x: [wordnet_lemmatizer.lemmatize(word, pos =\"a\") for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_raw, dict_0_raw, dict_4_raw, len_tweets, len_tweets_0, len_tweets_4 = create_dict(tweets_train_pd['lemma'], train_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nw = [0 for i in range(max(dictionary_raw.values())+1)] \n",
    "f = [i for i in range(max(dictionary_raw.values())+1)] \n",
    "for key in dictionary_raw:\n",
    "    nw[dictionary_raw[key]]+=1\n",
    "plt.plot(f,nw)\n",
    "plt.axis([40, 80, 0, 2000])\n",
    "plt.xlabel(\"Frequency\")\n",
    "plt.ylabel(\"No of words\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature 1: Remove the number of words with very less frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----------------@@@@@@@@@@FEATURE - 1 Dictionary Refinement-----------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Feature - We see that the number of words with very less frequency is very high. So we can actually \n",
    "# Discard those words since they will not play much role in deciding the class of a tweet\n",
    "# This would be for words like numbers, or language specific words like muera, wrong spelling words, etc. \n",
    "# We don't need to modify dict_0 and dict_4 since they will only be accessed for words found in the global\n",
    "# dictionary \n",
    "\n",
    "cutoff_freq = 40 #40 is the optimum for 83% accuracy\n",
    "n_cutoff = len(dictionary_raw)-sum(nw[0:cutoff_freq]) \n",
    "print(\"Number of words with frequency higher than cutoff frequency({}) :\".format(cutoff_freq),n_cutoff)\n",
    "\n",
    "dictionary_raw_new = {}\n",
    "for key in dictionary_raw:\n",
    "    if dictionary_raw[key] >=cutoff_freq:\n",
    "        dictionary_raw_new[key]=dictionary_raw[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "m= len(X_train)\n",
    "phi_0 = class_0/m\n",
    "phi_4 = class_4/m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_0_raw, theta_4_raw = train_nb_classifier(dictionary_raw_new, dict_0_raw, dict_4_raw, len_tweets, len_tweets_0, len_tweets_4, class_0, class_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result (a) : The train accuracy of the model on raw data is = 79.256625%\n"
     ]
    }
   ],
   "source": [
    "train_accuracy,actual_class_train,pred_class_train = test_data(tweets_train_pd['lemma'], theta_0_raw, theta_4_raw, train_classes, phi_0, phi_4)\n",
    "print(\"Result (a) : The train accuracy of the model on raw data is = {}%\".format(train_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result (a) : The test accuracy of the model on raw data is = 80.50139275766016%\n"
     ]
    }
   ],
   "source": [
    "test_accuracy, actual_class,pred_class = test_data(tweets_test_pd['lemma'], theta_0_raw, theta_4_raw, test_actual_classes, phi_0, phi_4)\n",
    "print(\"Result (a) : The test accuracy of the model on raw data is = {}%\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Effect of cutoff-frequency on Train and Test Accuracy without bigrams\n",
    "cutoff = [0, 20, 35, 40, 50, 55, 60, 75, 100]\n",
    "train_acc = [79.18, 76.92, 76.71,76.78,76.59,76.55,76.51,76.42,76.30]\n",
    "test_acc = [80.50, 81.89, 82.45, 82.72, 82.45, 81.615, 81.615, 81.615, 81.615]\n",
    "plt.plot(cutoff, train_acc, marker='o', markersize=4, c='m', label='Train Accuracy')\n",
    "plt.plot(cutoff, test_acc, marker='*', markersize=4, c='b', label ='Test Accurcay')\n",
    "plt.axvline(x=40, c='k', linestyle='--', linewidth=0.9)\n",
    "plt.scatter(0,79.07, marker='^', s=30, c='c', label='Train Acc without digits')\n",
    "plt.scatter(0,79.97, marker='^', s=30, c='g', label='Test Acc without digits')\n",
    "plt.scatter(0,79.25, marker='^', s=30, c='k', label='Train Acc with digits')\n",
    "plt.scatter(0,80.50, marker='^', s=30, c='r', label='Test Acc with digits')\n",
    "/\n",
    "plt.xlabel(\"cutoff frequency\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(fontsize=8)\n",
    "plt.title(\"Affect of Cut-off Frequency and \\non Test and Train Accuracy\")\n",
    "plt.savefig(\"cutoff_accuracy_feature1.png\",dpi=1000, bbox_inches='tight')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURE  - BI-GRAMS only consecutive words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----------------@@@@@@@@@@FEATURE - 2 BIGRAMS-----------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools \n",
    "\n",
    "tweets_test =tweets_test_pd['lemma'].apply(lambda x: list(map(list, zip(x, x[1:]))))\n",
    "tweets_test['bigrams_str']=tweets_test.apply(lambda x: [' '.join(x[i]) for i in range(len(x))])\n",
    "\n",
    "tweets_train=tweets_train_pd['lemma'].apply(lambda x: list(map(list, zip(x, x[1:]))))\n",
    "tweets_train['bigrams_str']=tweets_train.apply(lambda x: [' '.join(x[i]) for i in range(len(x))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-----------------Create Dictionary with bigrams---------------------------\")\n",
    "\n",
    "dictionary_bigrams_1, dict_0_bigrams, dict_4_bigrams, len_bigrams, len_bigrams_0, len_bigrams_4 = create_dict(tweets_train['bigrams_str'], train_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nw = [0 for i in range(max(dictionary_bigrams_1.values())+1)] \n",
    "f = [i for i in range(max(dictionary_bigrams_1.values())+1)] \n",
    "for key in dictionary_bigrams_1:\n",
    "    nw[dictionary_bigrams_1[key]]+=1\n",
    "plt.plot(f,nw)\n",
    "plt.axis([40, 80, 0, 2000])\n",
    "plt.xlabel(\"Frequency\")\n",
    "plt.ylabel(\"No of words\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) Using BIGRAMS - only consecutive words\n",
    "# We don't need to modify dict_0 and dict_4 since they will only be accessed for words found in the global\n",
    "# dictionary \n",
    "\n",
    "cutoff_freq = 50\n",
    "n_cutoff = len(dictionary_bigrams_1)-sum(nw[0:cutoff_freq]) \n",
    "print(\"Number of words with frequency higher than cutoff frequency({}) :\".format(cutoff_freq),n_cutoff)\n",
    "\n",
    "dictionary_bigrams_new = {}\n",
    "for key in dictionary_bigrams_1:\n",
    "    if dictionary_bigrams_1[key] >=cutoff_freq:\n",
    "        dictionary_bigrams_new[key]=dictionary_bigrams_1[key]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add the bigrams dictionary to raw dictionary of unigrams\n",
    "dictionary_bigrams_new.update(dictionary_raw_new)\n",
    "dict_0_bigrams.update(dict_0_raw)\n",
    "dict_4_bigrams.update(dict_4_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Effect of cutoff-frequency on Train and Test Accuracy - Raw Dictionary+Bigrams\n",
    "cutoff = [0, 10, 30, 40, 50]\n",
    "train_acc = [79.25, 80.94, 80.112, 79.97, 79.89]\n",
    "test_acc =  [80.50, 81.61, 81.61, 81.62, 81.05]\n",
    "plt.plot(cutoff, train_acc, marker='o', markersize=4, c='m', label='Train Accuracy')\n",
    "plt.plot(cutoff, test_acc, marker='*', markersize=4, c='b', label ='Test Accurcay')\n",
    "plt.axvline(x=10, c='k', linestyle='--', linewidth=0.9, label='Max Test Accuracy')\n",
    "plt.axvline(x=0, c='r', linestyle='--', linewidth=0.9, label='Overfitting')\n",
    "plt.scatter(0,91.468, marker='^', s=30, c='c', label='Train Acc with only bigrams')\n",
    "plt.scatter(0,76.02, marker='^', s=30, c='c', label='Test Acc with only bigrams')\n",
    "plt.scatter(0,75.36, marker='^', s=30, c='r', label='Train Acc with only bigrams (cutoff=10)')\n",
    "plt.scatter(0,77.15, marker='^', s=30, c='r', label='Test Acc with only bigrams(cutoff=10)')\n",
    "\n",
    "plt.xlabel(\"cutoff frequency\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(fontsize=8)\n",
    "plt.title(\"Affect of Cut-off Frequency on Test and \\nTrain Accuracy with bigrams plus raw dictionary\")\n",
    "plt.savefig(\"Accuracy_with_Bigrams_and_Raw+cutoff.png\",dpi=1000, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n------------------Learning NB on Unigrams+Bigrams-------------------------------\\n\")\n",
    "\n",
    "theta_0_bigrams, theta_4_bigrams = train_nb_classifier(dictionary_bigrams_new, dict_0_bigrams, dict_4_bigrams, len_tweets, len_tweets_0, len_tweets_4, class_0, class_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_accuracy,actual_class_train,pred_class_train = test_data(tweets_train_pd['lemma']+tweets_train['bigrams_str'], theta_0_bigrams, theta_4_bigrams, train_classes, phi_0, phi_4)\n",
    "print(\"Result (e) : The train accuracy of the model on bigrams+unigrams data is = {}%\".format(train_accuracy))\n",
    "\n",
    "test_accuracy, actual_class,pred_class = test_data((tweets_test_pd['lemma']+tweets_test['bigrams_str']), theta_0_bigrams, theta_4_bigrams, test_actual_classes, phi_0, phi_4)\n",
    "print(\"Result (e) : The test accuracy of the model on bigrams+unigrams data is = {}%\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURE  - TRI-GRAMS only consecutive words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----------------@@@@@@@@@@FEATURE - 3 TRIGRAMS-----------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_test =tweets_test_pd['lemma'].apply(lambda x: list(map(list, zip(x, x[1:],x[2:]))))\n",
    "tweets_test['trigrams_str']=tweets_test.apply(lambda x: [' '.join(x[i]) for i in range(len(x))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_train=tweets_train_pd['lemma'].apply(lambda x: list(map(list, zip(x, x[1:], x[2:]))))\n",
    "tweets_train['trigrams_str']=tweets_train.apply(lambda x: [' '.join(x[i]) for i in range(len(x))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n-----------------Create Dictionary with trigrams---------------------------\")\n",
    "\n",
    "\n",
    "dictionary_trigrams_1, dict_0_trigrams, dict_4_trigrams, len_trigrams, len_trigrams_0, len_trigrams_4 = create_dict(tweets_train['trigrams_str'], train_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) Using TRIGRAMS - only consecutive words\n",
    "# We don't need to modify dict_0 and dict_4 since they will only be accessed for words found in the global\n",
    "# dictionary \n",
    "\n",
    "cutoff_freq = 10\n",
    "n_cutoff = len(dictionary_trigrams_1)-sum(nw[0:cutoff_freq]) \n",
    "print(\"Number of words with frequency higher than cutoff frequency({}) :\".format(cutoff_freq),n_cutoff)\n",
    "\n",
    "dictionary_trigrams_new = {}\n",
    "for key in dictionary_trigrams_1:\n",
    "    if dictionary_trigrams_1[key] >=cutoff_freq:\n",
    "        dictionary_trigrams_new[key]=dictionary_trigrams_1[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_trigrams_new.update(dictionary_raw_new)\n",
    "dict_0_trigrams.update(dict_0_raw)\n",
    "dict_4_trigrams.update(dict_4_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n------------------Learning NB on Unigrams+Trigrams-------------------------------\\n\")\n",
    "\n",
    "theta_0_trigrams, theta_4_trigrams = train_nb_classifier(dictionary_trigrams_new, dict_0_trigrams, dict_4_trigrams, len_tweets, len_tweets_0, len_tweets_4, class_0, class_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy,actual_class_train,pred_class_train = test_data(tweets_train_pd['lemma']+tweets_train['trigrams_str'], theta_0_trigrams, theta_4_trigrams, train_classes, phi_0, phi_4)\n",
    "print(\"Result (e) : The train accuracy of the model on Unigrams+Trigrams data is = {}%\".format(train_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy, actual_class,pred_class = test_data((tweets_test_pd['lemma']+tweets_test['trigrams_str']), theta_0_trigrams, theta_4_trigrams, test_actual_classes, phi_0, phi_4)\n",
    "print(\"Result (e) : The test accuracy of the model on Unigrams+Trigrams data is = {}%\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Effect of cutoff-frequency on Train and Test Accuracy - Raw Dictionary(with cutoff=40)+Trigrams(with cutoff)\n",
    "cutoff = [0, 10, 15, 20, 30, 40, 60, 100]\n",
    "train_acc = [94.80, 77.50, 77.31, 77.20, 77.09, 77.03, 76.97, 76.92]\n",
    "test_acc =  [82.17, 83.28, 82.72, 83.00, 82.73, 82.45, 82.45, 82.72]\n",
    "plt.plot(cutoff, train_acc, marker='o', markersize=4, c='m', label='Train Accuracy')\n",
    "plt.plot(cutoff, test_acc, marker='*', markersize=4, c='b', label ='Test Accurcay')\n",
    "plt.axvline(x=10, c='k', linestyle='--', linewidth=0.9, label='Max Test Accuracy')\n",
    "plt.axvline(x=0, c='r', linestyle='--', linewidth=0.9, label='Overfitting')\n",
    "plt.scatter(0,76.78, marker='^', s=30, c='k', label='Train Acc with raw data')\n",
    "plt.scatter(0,82.72, marker='^', s=30, c='r', label='Test Acc with raw data')\n",
    "\n",
    "plt.xlabel(\"cutoff frequency\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(fontsize=8)\n",
    "plt.title(\"Affect of Cut-off Frequency on Test and \\nTrain Accuracy with trigrams \\nplus raw dictionary(cutoff=40)\")\n",
    "#plt.savefig(\"Accuracy_with_trigrams_and_Raw_with_cutoff.png\",dpi=1000, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
