{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the training set is =  1600000\n",
      "The number of classes (label=4) = 800000\n",
      "The number of classes (label=0) = 800000\n",
      "The length of the test set is =  359\n"
     ]
    }
   ],
   "source": [
    "#Importing Data from the CSV file\n",
    "%matplotlib inline\n",
    "import string\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f_train = open(\"training.1600000.processed.noemoticon.csv\", \"r\", encoding=\"ISO-8859-1\")\n",
    "X_train = f_train.readlines()\n",
    "f_test = open(\"testdata.manual.2009.06.14.csv\", \"r\")\n",
    "X_test = f_test.readlines()\n",
    "\n",
    "#Creating the List with just the tweets and finding the number of positive and negative classes. (TRAINING) \n",
    "#class_0 = number of classes with label = 0\n",
    "#class_4 = number of classes with label = 4\n",
    "\n",
    "class_0=class_4=0\n",
    "tweets_train =[]\n",
    "for x in X_train:\n",
    "    a = x.split('\",\"')\n",
    "    if (a[0] == '\"0'): class_0+=1\n",
    "    else: class_4+=1\n",
    "    tweets_train.append('%s\"|\"%s' % (a[0],a[-1]))\n",
    "\n",
    "#Creating the List with just the tweets (TEST DATA)\n",
    "tweets_test =[]\n",
    "for x in X_test:\n",
    "    a = x.split('\",\"')\n",
    "    tweets_test.append('%s\"|\"%s' % (a[0], a[-1]))\n",
    "    \n",
    "print(\"The length of the training set is = \", len(X_train))\n",
    "print(\"The number of classes (label=4) =\", class_4)\n",
    "print(\"The number of classes (label=0) =\", class_0)\n",
    "print(\"The length of the test set is = \", len(X_test))\n",
    "\n",
    "f_test.close()\n",
    "f_train.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Part(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ayushi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ayushi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_train_pd = pd.Series(tweets_train)\n",
    "tweets_test_pd = pd.Series(tweets_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classes = tweets_train_pd.apply(lambda x: (x.split(\"|\")[0]))\n",
    "test_actual_classes = tweets_test_pd.apply(lambda x: (x.split(\"|\")[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "#Tokenize the Training data\n",
    "tweets_train_pd['tokenize_data'] = tweets_train_pd.apply(lambda x: '\\n'.join([' '.join(token) for token in ([tknzr.tokenize(x.split(\"|\")[1])])]))\n",
    "tweets_test_pd['tokenize_data'] = tweets_test_pd.apply(lambda x: '\\n'.join([' '.join(token) for token in ([tknzr.tokenize(x.split(\"|\")[1])])]))\n",
    "\n",
    "#Import regex library in python to earch for regular expressions of a certain type to replace hem\n",
    "import re\n",
    "tweets_train_pd['noURL'] = tweets_train_pd['tokenize_data'].replace(r'http\\S+', '', regex=True).replace(r'www\\.\\S+', '', regex=True)\n",
    "tweets_test_pd['noURL'] = tweets_test_pd['tokenize_data'].replace(r'http\\S+', '', regex=True).replace(r'www\\.\\S+', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create Train dataset Clean\n",
    "\n",
    "tweets_train_pd['noEnter']=tweets_train_pd['noURL'].str.rstrip(\"\\n\\r\")\n",
    "tweets_train_pd['clean_data'] = tweets_train_pd['noEnter'].str.translate(str.maketrans('','',string.punctuation)).str.lower().str.split()\n",
    "#Use lambda function to apply the same expression on all the strings together. \n",
    "#function takes one string at a time and returns a string wihtout stop words\n",
    "stop_words=set(stopwords.words('english'))\n",
    "tweets_train_pd['no_stop_words'] = tweets_train_pd['clean_data'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "tweets_train_pd['lemma1'] = tweets_train_pd['no_stop_words'].apply(lambda x: [wordnet_lemmatizer.lemmatize(word, pos =\"n\") for word in x])\n",
    "tweets_train_pd['lemma2'] = tweets_train_pd['lemma1'].apply(lambda x: [wordnet_lemmatizer.lemmatize(word, pos =\"v\") for word in x])\n",
    "tweets_train_pd['lemma'] = tweets_train_pd['lemma2'].apply(lambda x: [wordnet_lemmatizer.lemmatize(word, pos =\"a\") for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create clean test data\n",
    "\n",
    "tweets_test_pd['noEnter']=tweets_test_pd['noURL'].str.rstrip(\"\\n\\r\")\n",
    "tweets_test_pd['clean_data'] = tweets_test_pd['noEnter'].str.translate(str.maketrans('','',string.punctuation)).str.lower().str.split()\n",
    "#Use lambda function to apply the same expression on all the strings together. \n",
    "#function takes one string at a time and returns a string wihtout stop words\n",
    "stop_words=set(stopwords.words('english'))\n",
    "tweets_test_pd['no_stop_words'] = tweets_test_pd['clean_data'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "tweets_test_pd['lemma1'] = tweets_test_pd['no_stop_words'].apply(lambda x: [wordnet_lemmatizer.lemmatize(word, pos =\"n\") for word in x])\n",
    "tweets_test_pd['lemma2'] = tweets_test_pd['lemma1'].apply(lambda x: [wordnet_lemmatizer.lemmatize(word, pos =\"v\") for word in x])\n",
    "tweets_test_pd['lemma'] = tweets_test_pd['lemma2'].apply(lambda x: [wordnet_lemmatizer.lemmatize(word, pos =\"a\") for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing the tweets to a file (not necessary - REMOVE if not used anywhere later)\n",
    "!rm -rf tweet_test_lemma.pkl\n",
    "!rm -rf tweet_train_lemma.pkl\n",
    "\n",
    "# f = open(\"tweet_train_lemma.txt\", \"w\")\n",
    "# for x in tweets_train_pd['lemma']:\n",
    "#     f.write(\"%s\\n\" %x)\n",
    "# f.close()\n",
    "\n",
    "# f = open(\"tweet_test_lemma.txt\", \"w\")\n",
    "# for x in tweets_test_pd['lemma']:\n",
    "#     f.write(\"%s\\n\" %x)\n",
    "# f.close()\n",
    "\n",
    "tweets_train_pd['lemma'].to_pickle('tweet_train_lemma.pkl')\n",
    "tweets_test_pd['lemma'].to_pickle('tweet_test_lemma.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf train_class.pkl\n",
    "!rm -rf test_class.pkl\n",
    "# f = open(\"train_class.txt\", \"w\")\n",
    "# f.writelines(\"%s\\n\" %x for x in train_classes)\n",
    "# f.close()\n",
    "# f = open(\"test_class.txt\", \"w\")\n",
    "# f.writelines(\"%s\\n\" %x for x in test_actual_classes)\n",
    "# f.close()\n",
    "\n",
    "train_classes.to_pickle('train_class.pkl')\n",
    "test_actual_classes.to_pickle('test_class.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Create Dictionary\n",
    "import time\n",
    "\n",
    "def create_dict(tweets, train_classes):\n",
    "    start = time.time()\n",
    "    dictionary = {}\n",
    "    dict_0 = {}\n",
    "    dict_4 = {}\n",
    "    n = []\n",
    "    n_0 =[]\n",
    "    n_4 = []\n",
    "    for i in range(len(tweets)):\n",
    "        x = tweets.get(i)\n",
    "        n.append(len(x))\n",
    "        if (train_classes.get(i) == '\"0\"'): n_0.append(len(x)) \n",
    "        else: n_4.append(len(x))\n",
    "            \n",
    "        for w in x:\n",
    "    #Global Vocabulary\n",
    "            if w in dictionary:\n",
    "                dictionary[w]+=1\n",
    "            else:\n",
    "                dictionary[w]=1\n",
    "    #build Vocabulary for class 0 (-ve class)\n",
    "            if (train_classes.get(i) == '\"0\"'):\n",
    "                if w in dict_0:\n",
    "                    dict_0[w]+=1\n",
    "                else:\n",
    "                    dict_0[w]=1\n",
    "    #build Vocabulary for class 4 (+ve class)\n",
    "            else:\n",
    "                if w in dict_4:\n",
    "                    dict_4[w]+=1\n",
    "                else:\n",
    "                    dict_4[w]=1\n",
    "\n",
    "    end = time.time()                \n",
    "    print(\"Time to Create Dictionary =\", end-start)\n",
    "    return dictionary, dict_0, dict_4, n, n_0, n_4;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to Create Dictionary = 251.93174076080322\n"
     ]
    }
   ],
   "source": [
    "dictionary, dict_0, dict_4, n, n_0, n_4 = create_dict(tweets_train_pd['lemma'], train_classes)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import dill\n",
    "dill.dump_session('ques1_d_dump.db')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import dill\n",
    "dill.load_session('ques1_d_dump.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "m= len(X_train)\n",
    "phi_0 = class_0/m\n",
    "phi_4 = class_4/m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nb_classifier(dictionary, dict_0, dict_4, len_tweets, n_0, n_4, class_0, class_4):\n",
    "    v = len(dictionary)\n",
    "    theta_0 = {}\n",
    "    theta_4 = {}\n",
    "    n_0 = sum(n_0) # Sum of all the length of the tweets in class 0\n",
    "    n_4 = sum(n_4) # Sum of all the length of the tweets in class 0\n",
    "    c=1\n",
    "    for word in dictionary.keys():\n",
    "            if word in dict_0:\n",
    "                theta_0[word] = ((dict_0[word]+c)/(n_0 + v*c))\n",
    "            else:\n",
    "                theta_0[word] = ((c) / (n_0 + v*c))\n",
    "            if word in dict_4:\n",
    "                theta_4[word] = ((dict_4[word]+c)/(n_4 + v*c))\n",
    "            else:\n",
    "                theta_4[word] = ((c)/(n_4 + v*c))\n",
    "    return theta_0,theta_4;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_0, theta_4 = train_nb_classifier(dictionary, dict_0, dict_4, n, n_0, n_4, class_0, class_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data(tweets, theta_0, theta_4, actual_classes):\n",
    "    pred_class=[]\n",
    "    #actual_class=[]\n",
    "    for x in tweets:\n",
    "        test_class0=test_class4=0\n",
    "        \n",
    "        #Finding probability of tweet being in a class         \n",
    "        for w in x:\n",
    "            if w in theta_0: test_class0 += math.log(theta_0[w])\n",
    "            else: test_class0 += math.log(1)\n",
    "            if w in theta_4: test_class4 += math.log(theta_4[w])\n",
    "            else: test_class4 += math.log(1)\n",
    "        test_class0 += math.log(phi_0)\n",
    "        test_class4 += math.log(phi_4)\n",
    "        #Classifying the probability into classes\n",
    "        if (test_class0 > test_class4): pred_class.append(0)\n",
    "        else: pred_class.append(1)\n",
    "\n",
    "    actual_class = actual_classes.apply(lambda x: 0 if x=='\"0\"' else 1).tolist()\n",
    "    test_error = sum(np.bitwise_xor(actual_class, pred_class))\n",
    "    accuracy = ((len(tweets) - test_error)/len(tweets))*100\n",
    "    return accuracy, actual_class, pred_class;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result (a) : The train accuracy of the model on raw data is = 79.256625%\n"
     ]
    }
   ],
   "source": [
    "train_accuracy,actual_class_train,pred_class_train = test_data(tweets_train_pd['lemma'], theta_0, theta_4, train_classes)\n",
    "print(\"Result (a) : The train accuracy of the model on raw data is = {}%\".format(train_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result (a) : The test accuracy of the model on raw data is = 80.50139275766016%\n"
     ]
    }
   ],
   "source": [
    "test_accuracy, actual_class,pred_class = test_data(tweets_test_pd['lemma'], theta_0, theta_4, test_actual_classes)\n",
    "print(\"Result (a) : The test accuracy of the model on raw data is = {}%\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Method of using counters to create dictionary. - but this is taking a lot of time for the creation of dictionary\n",
    "from collections import Counter\n",
    "tweets_train_pd['count'] = tweets_train_pd['lemma'].apply(lambda x: Counter(x)) \n",
    "print(tweets_train_pd['count'])\n",
    "print(train_classes)\n",
    "#tweets_test_pd['count_0']=tweets_train_pd['count'].mask(train_classes == '\"4\"')\n",
    "tweets_test_pd['class-4']=tweets_train_pd['count'][train_classes == '\"4\"']\n",
    "tweets_test_pd['class-0']=tweets_train_pd['count'][train_classes == '\"0\"']\n",
    "import time\n",
    "start = time.time()\n",
    "print(tweets_test_pd['class-0'][::10].sum())\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "tokens=tknzr.tokenize(tweets_train[0].split(\"|\")[1])\n",
    "x= ' '.join(tokens)\n",
    "print(type(x))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Removing stopwords\n",
    "stop_words=set(stopwords.words('english'))\n",
    "x = x.rstrip(\"\\n\\r\")\n",
    "twt = x.translate(str.maketrans('','',string.punctuation)).lower()\n",
    "print(twt)\n",
    "words = twt.split()\n",
    "new_words=[]\n",
    "for w in words:\n",
    "    if w not in stop_words:\n",
    "        new_words.append(w)\n",
    "print(new_words)\n",
    "lemma_word=[]\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "for w in new_words:\n",
    "    word1 = wordnet_lemmatizer.lemmatize(w, pos = \"n\") #Noun\n",
    "    word2 = wordnet_lemmatizer.lemmatize(word1, pos = \"v\") #Verb\n",
    "    word3 = wordnet_lemmatizer.lemmatize(word2, pos = (\"a\")) #Adjective\n",
    "    lemma_word.append(word3)\n",
    "print(lemma_word)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "tokens= [tknzr.tokenize(sentence.split(\"|\")[1]) for sentence in tweets_train_pd['train_data']]\n",
    "tweets_train_pd['tokenize1'] = pd.Series([\" \".join(token) for token in tokens])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
