{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the training set is =  1600000\n",
      "The number of classes (label=4) = 800000\n",
      "The number of classes (label=0) = 800000\n",
      "The length of the test set is =  359\n"
     ]
    }
   ],
   "source": [
    "#Importing Data from the CSV file\n",
    "%matplotlib inline\n",
    "import string\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "f_train = open(\"training.1600000.processed.noemoticon.csv\", \"r\", encoding=\"ISO-8859-1\")\n",
    "X_train = f_train.readlines()\n",
    "f_test = open(\"testdata.manual.2009.06.14.csv\", \"r\")\n",
    "X_test = f_test.readlines()\n",
    "\n",
    "#Creating the List with just the tweets and finding the number of positive and negative classes. (TRAINING) \n",
    "#class_0 = number of classes with label = 0\n",
    "#class_4 = number of classes with label = 4\n",
    "\n",
    "class_0=class_4=0\n",
    "tweets_train =[]\n",
    "for x in X_train:\n",
    "    a = x.split('\",\"')\n",
    "    if (a[0] == '\"0'): class_0+=1\n",
    "    else: class_4+=1\n",
    "    tweets_train.append('%s\"|\"%s' % (a[0],a[-1]))\n",
    "\n",
    "#Creating the List with just the tweets (TEST DATA)\n",
    "tweets_test =[]\n",
    "for x in X_test:\n",
    "    a = x.split('\",\"')\n",
    "    tweets_test.append('%s\"|\"%s' % (a[0], a[-1]))\n",
    "    \n",
    "print(\"The length of the training set is = \", len(X_train))\n",
    "print(\"The number of classes (label=4) =\", class_4)\n",
    "print(\"The number of classes (label=0) =\", class_0)\n",
    "print(\"The length of the test set is = \", len(X_test))\n",
    "\n",
    "f_test.close()\n",
    "f_train.close()\n",
    "\n",
    "m= len(X_train)\n",
    "phi_0 = class_0/m\n",
    "phi_4 = class_4/m"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Part(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets_train_pd = pd.read_pickle('tweet_train_lemma.pkl')\n",
    "#tweets_test_pd = pd.read_pickle('tweet_test_lemma.pkl')\n",
    "train_actual_classes = pd.read_pickle('train_class.pkl')\n",
    "test_actual_classes = pd.read_pickle('test_class.pkl')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TF-IDF - Multinomial Naive Bayes using Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_train_pd = pd.Series(tweets_train)\n",
    "tweets_train_pd = tweets_train_pd.apply(lambda x: x.split(\"|\")[1])\n",
    "tweets_test_pd = pd.Series(tweets_test)\n",
    "tweets_test_pd = tweets_test_pd.apply(lambda x: x.split(\"|\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(analyzer=\"word\", stop_words='english',sublinear_tf=True)\n",
    "tfidf_train = vec.fit_transform(tweets_train_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=[0.5, 0.5], fit_prior=True)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB(class_prior=[phi_0,phi_4])\n",
    "nb.fit(tfidf_train, list(train_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_test = vec.transform(tweets_test_pd)\n",
    "test_class_pred = nb.predict(tfidf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7855153203342619\n"
     ]
    }
   ],
   "source": [
    "test_acc=0\n",
    "for i in range(len(test_actual_classes)):\n",
    "    if (list(test_class_pred)[i]==list(test_actual_classes)[i]):\n",
    "        test_acc+=1\n",
    "print(test_acc/len(test_actual_classes))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TFIDF - Using Gaussian Naive Bayes - And using minibatch training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_minibatch(tweets, classes, batch_size):\n",
    "    l = len(tweets)\n",
    "    for i in range(0, l, batch_size):\n",
    "        X_batch=tweets[i:min(i + batch_size, l)]\n",
    "        Y_batch=classes[i:min(i + batch_size, l)]\n",
    "        yield X_batch,Y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB  \n",
    "\n",
    "model = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classes = train_actual_classes.apply(lambda x: 0 if x=='\"0\"' else 1)\n",
    "test_classes = test_actual_classes.apply(lambda x: 0 if x=='\"0\"' else 1)\n",
    "all_classes = train_classes.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alternate_sign': True, 'analyzer': 'word', 'binary': False, 'decode_error': 'strict', 'dtype': <class 'numpy.float64'>, 'encoding': 'utf-8', 'input': 'content', 'lowercase': True, 'n_features': 1048576, 'ngram_range': (1, 1), 'norm': 'l2', 'preprocessor': None, 'stop_words': 'english', 'strip_accents': None, 'token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'tokenizer': None}\n"
     ]
    }
   ],
   "source": [
    "#Create minibatches\n",
    "vec = HashingVectorizer(analyzer=\"word\", stop_words='english',norm='l2')\n",
    "X = vec.fit_transform(tweets_train_pd)\n",
    "print(vec.get_params())\n",
    "# for (X_batch, Y_batch) in create_minibatch(tweets_train_pd, train_classes, 1000):\n",
    "#     tfidf_train = vec.transform(X_batch)\n",
    "#     model.partial_fit(tfidf_train.toarray(), Y_batch, classes=all_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 112243)\t-0.2886751345948129\n",
      "  (0, 112415)\t-0.2886751345948129\n",
      "  (0, 317092)\t-0.2886751345948129\n",
      "  (0, 522004)\t0.2886751345948129\n",
      "  (0, 571465)\t-0.2886751345948129\n",
      "  (0, 585009)\t-0.2886751345948129\n",
      "  (0, 608512)\t-0.2886751345948129\n",
      "  (0, 736301)\t-0.2886751345948129\n",
      "  (0, 787517)\t0.2886751345948129\n",
      "  (0, 806262)\t0.2886751345948129\n",
      "  (0, 984006)\t0.2886751345948129\n",
      "  (0, 1019106)\t-0.2886751345948129\n",
      "  (1, 19963)\t0.35355339059327373\n",
      "  (1, 93641)\t-0.35355339059327373\n",
      "  (1, 506094)\t-0.35355339059327373\n",
      "  (1, 614194)\t0.35355339059327373\n",
      "  (1, 733099)\t0.35355339059327373\n",
      "  (1, 751916)\t-0.35355339059327373\n",
      "  (1, 775584)\t0.35355339059327373\n",
      "  (1, 840974)\t-0.35355339059327373\n",
      "  (2, 85697)\t-0.3333333333333333\n",
      "  (2, 116157)\t-0.3333333333333333\n",
      "  (2, 215994)\t0.3333333333333333\n",
      "  (2, 284243)\t-0.3333333333333333\n",
      "  (2, 387955)\t-0.3333333333333333\n",
      "  :\t:\n",
      "  (1599996, 511013)\t0.30151134457776363\n",
      "  (1599996, 522004)\t0.30151134457776363\n",
      "  (1599996, 635142)\t-0.30151134457776363\n",
      "  (1599996, 663010)\t0.30151134457776363\n",
      "  (1599996, 806262)\t0.30151134457776363\n",
      "  (1599996, 1019611)\t-0.30151134457776363\n",
      "  (1599997, 222306)\t-0.4472135954999579\n",
      "  (1599997, 300673)\t0.4472135954999579\n",
      "  (1599997, 709087)\t0.4472135954999579\n",
      "  (1599997, 1034625)\t0.4472135954999579\n",
      "  (1599997, 1036250)\t0.4472135954999579\n",
      "  (1599998, 158803)\t-0.3333333333333333\n",
      "  (1599998, 258474)\t-0.3333333333333333\n",
      "  (1599998, 267245)\t-0.3333333333333333\n",
      "  (1599998, 314205)\t0.3333333333333333\n",
      "  (1599998, 493879)\t0.3333333333333333\n",
      "  (1599998, 544753)\t-0.3333333333333333\n",
      "  (1599998, 583732)\t-0.3333333333333333\n",
      "  (1599998, 665327)\t-0.3333333333333333\n",
      "  (1599998, 699305)\t0.3333333333333333\n",
      "  (1599999, 37412)\t0.4472135954999579\n",
      "  (1599999, 258474)\t-0.4472135954999579\n",
      "  (1599999, 421796)\t0.4472135954999579\n",
      "  (1599999, 814867)\t-0.4472135954999579\n",
      "  (1599999, 823936)\t-0.4472135954999579\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "#model.fit(tfidf_train, list(train_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
