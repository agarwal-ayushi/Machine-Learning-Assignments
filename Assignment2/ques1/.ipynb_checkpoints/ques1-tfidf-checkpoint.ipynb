{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the training set is =  1600000\n",
      "The number of classes (label=4) = 800000\n",
      "The number of classes (label=0) = 800000\n",
      "The length of the test set is =  359\n"
     ]
    }
   ],
   "source": [
    "#Importing Data from the CSV file\n",
    "%matplotlib inline\n",
    "import string\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "f_train = open(\"training.1600000.processed.noemoticon.csv\", \"r\", encoding=\"ISO-8859-1\")\n",
    "X_train = f_train.readlines()\n",
    "f_test = open(\"testdata.manual.2009.06.14.csv\", \"r\")\n",
    "X_test = f_test.readlines()\n",
    "\n",
    "#Creating the List with just the tweets and finding the number of positive and negative classes. (TRAINING) \n",
    "#class_0 = number of classes with label = 0\n",
    "#class_4 = number of classes with label = 4\n",
    "\n",
    "class_0=class_4=0\n",
    "tweets_train =[]\n",
    "for x in X_train:\n",
    "    a = x.split('\",\"')\n",
    "    if (a[0] == '\"0'): class_0+=1\n",
    "    else: class_4+=1\n",
    "    tweets_train.append('%s\"|\"%s' % (a[0],a[-1]))\n",
    "\n",
    "#Creating the List with just the tweets (TEST DATA)\n",
    "tweets_test =[]\n",
    "for x in X_test:\n",
    "    a = x.split('\",\"')\n",
    "    tweets_test.append('%s\"|\"%s' % (a[0], a[-1]))\n",
    "    \n",
    "print(\"The length of the training set is = \", len(X_train))\n",
    "print(\"The number of classes (label=4) =\", class_4)\n",
    "print(\"The number of classes (label=0) =\", class_0)\n",
    "print(\"The length of the test set is = \", len(X_test))\n",
    "\n",
    "f_test.close()\n",
    "f_train.close()\n",
    "\n",
    "m= len(X_train)\n",
    "phi_0 = class_0/m\n",
    "phi_4 = class_4/m"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Part(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.feature_selection import SelectPercentile, chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets_train_pd = pd.read_pickle('tweet_train_lemma.pkl')\n",
    "#tweets_test_pd = pd.read_pickle('tweet_test_lemma.pkl')\n",
    "train_actual_classes = pd.read_pickle('train_class.pkl')\n",
    "test_actual_classes = pd.read_pickle('test_class.pkl')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TF-IDF - Multinomial Naive Bayes using Library - 1-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_train_pd = pd.Series(tweets_train)\n",
    "tweets_train_pd = tweets_train_pd.apply(lambda x: x.split(\"|\")[1])\n",
    "tweets_test_pd = pd.Series(tweets_test)\n",
    "tweets_test_pd = tweets_test_pd.apply(lambda x: x.split(\"|\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(analyzer=\"word\", stop_words='english',sublinear_tf=True)\n",
    "tfidf_train = vec.fit_transform(tweets_train_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=[0.5, 0.5], fit_prior=True)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = MultinomialNB(class_prior=[phi_0,phi_4])\n",
    "nb.fit(tfidf_train, list(train_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_test = vec.transform(tweets_test_pd)\n",
    "test_class_pred = nb.predict(tfidf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7855153203342619\n"
     ]
    }
   ],
   "source": [
    "def test_accuracy(test_actual_classes, test_class_pred):\n",
    "    test_acc=0\n",
    "    for i in range(len(test_actual_classes)):\n",
    "        if (list(test_class_pred)[i]==list(test_actual_classes)[i]):\n",
    "            test_acc+=1\n",
    "    return(test_acc/len(test_actual_classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The Test Accuracy of the MultinomialNB model trained with TF-IDF is = \", test_accuracy(test_actual_classes, test_class_pred))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TF-IDF - Multinomial Naive Bayes using Library - bigrams and unigrams together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(analyzer=\"word\", stop_words='english',sublinear_tf=True, ngram_range=(1,2))\n",
    "tfidf_train = vec.fit_transform(tweets_train_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=[0.5, 0.5], fit_prior=True)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = MultinomialNB(class_prior=[phi_0,phi_4])\n",
    "nb.fit(tfidf_train, list(train_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_test = vec.transform(tweets_test_pd)\n",
    "test_class_pred = nb.predict(tfidf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The Test Accuracy of the MultinomialNB model trained with TF-IDF is = \", test_accuracy(test_actual_classes, test_class_pred))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TF-IDF - Multinomial Naive Bayes using Library - trigrams and unigrams together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(analyzer=\"word\", stop_words='english',sublinear_tf=True, ngram_range=(1,3))\n",
    "tfidf_train = vec.fit_transform(tweets_train_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=[0.5, 0.5], fit_prior=True)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = MultinomialNB(class_prior=[phi_0,phi_4])\n",
    "nb.fit(tfidf_train, list(train_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_test = vec.transform(tweets_test_pd)\n",
    "test_class_pred = nb.predict(tfidf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The Test Accuracy of the MultinomialNB model trained with TF-IDF is = \", test_accuracy(test_actual_classes, test_class_pred))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TFIDF - Using Gaussian Naive Bayes - And using minibatch training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_minibatch(tweets, classes, batch_size):\n",
    "    l = len(tweets)\n",
    "    for i in range(0, l, batch_size):\n",
    "        X_batch=tweets[i:min(i + batch_size, l)]\n",
    "        Y_batch=classes[i:min(i + batch_size, l)]\n",
    "        yield X_batch,Y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gauss = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classes = train_actual_classes.apply(lambda x: 0 if x=='\"0\"' else 1)\n",
    "test_classes = test_actual_classes.apply(lambda x: 0 if x=='\"0\"' else 1)\n",
    "all_classes = train_classes.unique()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Only using Hashing Vectorizer that will create a sparse matrix of token occurrences and store the counts. The ocunts will be normalized with euclidean distance or l2 norm here. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create minibatches\n",
    "vec = HashingVectorizer(analyzer=\"word\", stop_words='english',norm='l2')\n",
    "X = vec.transform(tweets_train_pd)\n",
    "\n",
    "for (X_batch, Y_batch) in create_minibatch(tweets_train_pd, train_classes, 1000):\n",
    "    tfidf_train = vec.transform(X_batch)\n",
    "    model_gauss.partial_fit(tfidf_train.toarray(), Y_batch, classes=all_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = vec.transform(tweets_test_pd)\n",
    "test_accuracy = model.score(X_test.toarray(), test_classes)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Using Count Vectorizer with TFIDF transform\n",
    "that will create a sparse matrix of token  counts. \n",
    "Then we apply TFIDF transform on the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create minibatches\n",
    "count_vec = CountVectorizer(analyzer=\"word\", stop_words='english')\n",
    "tfidf_vec = TfidfTransformer(norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=True)\n",
    "X_count = tfidf_vec.transform(count_vec.fit_transform(tweets_train_pd))\n",
    "\n",
    "for (X_batch, Y_batch) in create_minibatch(tweets_train_pd, train_classes, 1000):\n",
    "    tfidf_train = tfidf_vec.transform(count_vec.fit_transform(X_batch)\n",
    "    model_gauss.partial_fit(tfidf_train.toarray(), Y_batch, classes=all_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = count_vec.transform(tweets_test_pd)\n",
    "test_accuracy = model.score(X_test.toarray(), test_classes)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "b) Using SelectPercentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(analyzer=\"word\", stop_words='english',sublinear_tf=True, ngram_range=(1,1))\n",
    "tfidf_train = vec.fit_transform(tweets_train_pd)\n",
    "tfidf_train_new = SelectPercentile(chi2, percentile=50).fit_transform(tfidf_train, train_classes)\n",
    "tfidf_train_new"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Parallelize code for minibatch training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import parallel_backend\n",
    "\n",
    "with parallel_backend('threading', n_jobs=2):\n",
    "    # Your scikit-learn code here"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tfidf_vec= TfidfTransformer(sublinear_tf=True)\n",
    "tfidf_vec.transform(X)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "pipe = Pipeline([('hash', HashingVectorizer(analyzer=\"word\", stop_words='english',norm='l2')),\n",
    "            ('tfidf', TfidfTransformer())]).fit(tweets_train_pd)\n",
    "#model.fit(tfidf_train, list(train_classes))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pipe['hash'].transform(tweets_test_pd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
